{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 05: Visualizing Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This tutorial describes the process of visualizing and replaying the results of Flow experiments run using RL. The process of visualizing results breaks down into two main components:\n",
    "\n",
    "- reward plotting\n",
    "\n",
    "- policy replay\n",
    "\n",
    "Furthermore, visualization is different depending on whether your experiments were run using rllab or RLlib. Accordingly, this tutorial is divided in two parts (one for rllab and one for RLlib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rllab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An essential step in evaluating the effectiveness and training progress of RL agents is visualization of reward. rllab includes a tool to plot the _average cumulative reward per rollout_ against _iteration number_ to show training progress. This \"reward plot\" can be generated for just one experiment or many. The tool to be called is rllab's `frontend.py`, which is inside the directory `rllab/viskit/` (assuming a user is already inside the directory `rllab-multiagent`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`frontend.py` requires only one command-line input: the path to the result directory that a user wants to visualize. The directory should contain a `progress.csv` and `params.json` fileâ€”pickle files containing per-iteration results are not necessary. An example call to `frontend.py` is below. Click on the link to http://localhost:5000 to view reward over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../../../rllab/viskit/frontend.py /path/to/result/directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replaying a Trained Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow includes a tool for visualizing a trained policy in its environment using SUMO's GUI. This enables more granular analysis of policies beyond their accrued reward, which in turn allows users to tweak actions, observations, and rewards in order to produce desired behavior. The visualizer also generates plots of observations and a plot of reward over the course of the rollout. The tool to be called is `visualizer_rllab.py` within `flow/visualize` (assuming a user is already inside the parent directory `flow`). \n",
    "\n",
    "`visualizer_rllab.py` requires one command-line input and has three additional optional arguments. The required input is the path to the pickle file to be visualized (this is usually within an rllab result directory). The optional inputs are: \n",
    "- `--num_rollouts`, the number of rollouts to be visualized. The default value is 100. This argument takes integer input.\n",
    "- `--plotname`, the name of the plot generated by the visualizer. The default value is `traffic_plot`. This argument takes string input.\n",
    "- `--emission_to_csv`, whether to convert SUMO's emission file into a CSV file. Emission files will be discussed later in this tutorial. By default, emission CSV files are not stored. `--emission_to_csv` is a flag and takes no input.\n",
    "\n",
    "An example call to `visualizer_rllab.py` is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../../flow/visualize/visualizer_rllab.py /path/to/result.pkl --num_rollouts 1 --plotname plot_test --emission_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Reward\n",
    "\n",
    "Similarly to how rllab handles reward plotting, RLlib supports reward visualization over the period of training using `tensorboard`. `tensorboard` takes one command-line input, `--logdir`, which is an rllib result directory (usually located within an experiment directory inside your `ray_results` directory). An example function call is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tensorboard --logdir /ray_results/experiment_dir/result/directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replaying a Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-02-19_12-12-03_25019/logs.\n",
      "Waiting for redis server at 127.0.0.1:26483 to respond...\n",
      "Waiting for redis server at 127.0.0.1:18293 to respond...\n",
      "WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 6465294336 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n",
      "Starting the Plasma object store with 6.5546633210000005 GB memory using /tmp.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8888/notebooks/ray_ui.ipynb?token=f15d738b939330324621c96ad2207691e072b9ca23770b03\n",
      "======================================================================\n",
      "\n",
      "Created LogSyncer for /home/thorsten/ray_results/PPO_IntersectionEnv-v0_2019-02-19_12-12-05xbl1zo4m -> None\n",
      "2019-02-19 12:12:05,103\tWARNING ppo.py:137 -- By default, observations will be normalized with MeanStdFilter\n",
      "Loading configuration... done.\n",
      "Success.\n",
      " Starting SUMO on port 42053\n",
      "Loading configuration... done.\n",
      "13.380538125585508\n",
      "3.471695374219206\n",
      "2019-02-19 12:12:07,395\tINFO policy_evaluator.py:262 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "/home/thorsten/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "2019-02-19 12:12:08,643\tINFO multi_gpu_optimizer.py:74 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "11.252743144161869\n",
      "0.7638129105947011\n",
      "Crash anzahl\n",
      "Round 0, Return: 206.59162653230678\n",
      "Average, std return: 206.59162653230678, 0.0\n",
      "Average, std speed: 2.1977832609819874, 0.0\n",
      "Average, std outflow: 38.33865814696485, 0.0\n",
      "Closing connection to TraCI and stopping simulation.\n",
      "Note, this may print an error message when it closes.\n",
      "Closing connection to TraCI and stopping simulation.\n",
      "Note, this may print an error message when it closes.\n",
      "Skip automatic termination. Connection is probably already closed.\n"
     ]
    }
   ],
   "source": [
    "! python ../flow/visualize/visualizer_rllib.py /home/thorsten/ray_results/IntersectionExample/PPO_IntersectionEnv-v0_0_2019-02-18_14-47-00481w1ltd 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Collection and Analysis\n",
    "Any Flow experiment can output its results to a CSV file containing the contents of SUMO's built-in `emission.xml` files, specifying speed, position, time, fuel consumption, and many other metrics for all vehicles in a network over time. \n",
    "\n",
    "This section describes how to generate those `emission.csv` files when replaying and analyzing a trained policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rllab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the visualizer with the flag --emission_to_csv replays the policy and creates an emission file\n",
    "! python ../../flow/visualize/visualizer_rllab.py path/to/result.pkl --emission_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The generated `emission.csv` is placed in the directory `test_time_rollout/` inside the directory from which you've just run the visualizer. That emission file can be opened in Excel, loaded in Python and plotted, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --emission_to_csv does the same as above\n",
    "! python ../../flow/visualize/visualizer_rllib.py results/sample_checkpoint 1 --emission-to-csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the rllab case, the `emission.csv` file can be found in `test_time_rollout/` and used from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO\n",
    "SUMO-only experiments can generate emission CSV files as well, based on an argument to the `experiment.run` method. `run` takes in arguments `(num_runs, num_steps, rl_actions=None, convert_to_csv=False)`. To generate an `emission.csv` file, pass in `convert_to_csv=True` in the Python file running your SUMO experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow_2)",
   "language": "python",
   "name": "flow_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
