{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING I3W\n",
    "\n",
    "\n",
    "# A) Create Envorinment, Vehicles etc\n",
    "\n",
    "### General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scenarios:\n",
      "['Scenario', 'BayBridgeScenario', 'BayBridgeTollScenario', 'BottleneckScenario', 'Figure8Scenario', 'SimpleGridScenario', 'HighwayScenario', 'LoopScenario', 'MergeScenario', 'TwoLoopsOneMergingScenario', 'MultiLoopScenario', 'IntersectionScenarioTW']\n",
      "\n",
      "Available environments:\n",
      "['Env', 'AccelEnv', 'LaneChangeAccelEnv', 'LaneChangeAccelPOEnv', 'LaneChangeAccelEnv_speed', 'GreenWaveTestEnv', 'GreenWaveTestEnv', 'WaveAttenuationMergePOEnv', 'TwoLoopsMergePOEnv', 'BottleneckEnv', 'BottleNeckAccelEnv', 'WaveAttenuationEnv', 'WaveAttenuationPOEnv', 'TrafficLightGridEnv', 'PO_TrafficLightGridEnv', 'DesiredVelocityEnv', 'TestEnv', 'BayBridgeEnv', 'IntersectionEnv']\n"
     ]
    }
   ],
   "source": [
    "# Define horizon as a variable to ensure consistent use across notebook (length of one rollout)\n",
    "HORIZON=500\n",
    "\n",
    "# name of the experiment\n",
    "experiment_name = \"IntersectionExample\"\n",
    "\n",
    "# scenario class\n",
    "import flow.scenarios as scenarios\n",
    "print(\"Available scenarios:\")\n",
    "print(scenarios.__all__)\n",
    "scenario_name = \"IntersectionTWScenario\"\n",
    "\n",
    "# environment class\n",
    "import flow.envs as flowenvs\n",
    "print(\"\\nAvailable environments:\")\n",
    "print(flowenvs.__all__)\n",
    "env_name = \"IntersectionEnv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "from flow.scenarios.intersection import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "additionalNetParams = {\n",
    "            \"edge_length\": 40,\n",
    "            \"lanes\": 1,\n",
    "            \"speed_limit\": 30\n",
    "        }\n",
    "\n",
    "net_params = NetParams( no_internal_links=False,                  #default: True   !! damit Kreuzungen nicht Ã¼berspr. werden\n",
    "                        inflows=None,                             #default: None\n",
    "                        osm_path=None,                            #default: None\n",
    "                        netfile=None,                             #default: None\n",
    "                        additional_params=additionalNetParams     #default: None   !!\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InitialConfig Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "\n",
    "initial_config = InitialConfig( shuffle=True,                            #default: False         !!\n",
    "                                spacing=\"custom\",                        #default: \"uniform\"     !!\n",
    "                                min_gap=10,                              #default: 0\n",
    "                                perturbation=30.0,                       #default: 0.0            !!        \n",
    "                                x0=0,                                    #default: 0\n",
    "                                bunching=0,                              #default: 0\n",
    "                                lanes_distribution=float(\"inf\"),         #default: float(\"inf\")\n",
    "                                edges_distribution=\"all\",                #default: \"all\"\n",
    "                                additional_params=None )                 #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams( port = None,                  #default: None\n",
    "                          sim_step=0.1,                 #default: 0.1\n",
    "                          emission_path=None,           #default: None\n",
    "                          lateral_resolution=None,      #default: None\n",
    "                          no_step_log=True,             #default: True\n",
    "                          render=False,                 #default: False\n",
    "                          save_render=False,            #default: False\n",
    "                          sight_radius=25,              #default: 25\n",
    "                          show_radius=False,            #default: False\n",
    "                          pxpm=2,                       #default: 2\n",
    "                          overtake_right=False,         #default: False    \n",
    "                          seed=None,                    #default: None\n",
    "                          restart_instance=False,       #default: False\n",
    "                          print_warnings=True,          #default: True\n",
    "                          teleport_time=-1,             #default: -1\n",
    "                          num_clients=1,                #default: 1\n",
    "                          sumo_binary=None )            #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "additionalEnvParams = {\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 3,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 3,\n",
    "        \"target_velocity\": 30\n",
    "    }\n",
    "\n",
    "env_params = EnvParams( additional_params=additionalEnvParams, #default: None    !!\n",
    "                        horizon=HORIZON,                       #default: 500     !!\n",
    "                        warmup_steps=0,                        #default: 0       \n",
    "                        sims_per_step=1,                       #default: 1\n",
    "                        evaluate=False )                       #default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# import vehicles dynamics models\n",
    "#from flow.controllers import SumoCarFollowingController\n",
    "from flow.controllers import ContinuousRouter\n",
    "#from flow.controllers.lane_change_controllers import SumoLaneChangeController\n",
    "from flow.controllers.lane_change_controllers import StaticLaneChanger\n",
    "from flow.controllers import RLController\n",
    "from flow.core.params import SumoLaneChangeParams\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "from random import *\n",
    "\n",
    "vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RL-Agent controlled vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car following parameters, default: None\n",
    "cf_parameter = SumoCarFollowingParams(\n",
    "                speed_mode=\"aggressive\")\n",
    "# lane change parameters, default: None\n",
    "lc_parameter =  None\n",
    "\n",
    "vehicles.add( # name of the vehicle\n",
    "                veh_id = \"rl\",\n",
    "              # acceleration controller, default: (SumoCarFollowingController, {})\n",
    "                acceleration_controller=(RLController, {}),\n",
    "              # lane_change_controller, default: (SumoLaneChangeController, {})\n",
    "                lane_change_controller=(StaticLaneChanger,{}),\n",
    "              # routing controller, default: None\n",
    "                routing_controller=(ContinuousRouter, {}),\n",
    "              # initial speed, default: 0\n",
    "                initial_speed=0,\n",
    "              # number of vehicles, default: 1 \n",
    "                num_vehicles=2,\n",
    "                \n",
    "                car_following_params=cf_parameter\n",
    "              # speed mode, default: \"right_of_way\"\n",
    "                #speed_mode=\"aggressive\",\n",
    "              # lane change mode, default: \"no_lat_collide\"\n",
    "                #lane_change_mode=\"aggressive\", \n",
    "              # car following parameter, default: None\n",
    "                #sumo_car_following_params=cf_parameter,\n",
    "              # lane change parameter, default: None\n",
    "                #sumo_lc_params=lc_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict( # name of the experiment\n",
    "                      exp_tag=experiment_name,\n",
    "                    # name of the flow environment the experiment is running on\n",
    "                      env_name=env_name,\n",
    "                    # name of the scenario class the experiment uses\n",
    "                      scenario=scenario_name,\n",
    "                    # simulator that is used by the experiment\n",
    "                      simulator='traci',\n",
    "                    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "                      sim=sumo_params,\n",
    "                    # environment related parameters (see flow.core.params.EnvParams)\n",
    "                      env=env_params,\n",
    "                    # network-related parameters (see flow.core.params.NetParams and\n",
    "                    # the scenario's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "                      net=net_params,\n",
    "                    # vehicles to be placed in the network at the start of a rollout \n",
    "                    # (see flow.core.vehicles.Vehicles)\n",
    "                      veh=vehicles,\n",
    "                   # (optional) parameters affecting the positioning of vehicles upon \n",
    "                   # initialization/reset (see flow.core.params.InitialConfig)\n",
    "                      initial=initial_config\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-03-20_16-21-49_14070/logs.\n",
      "Waiting for redis server at 127.0.0.1:32971 to respond...\n",
      "Waiting for redis server at 127.0.0.1:58310 to respond...\n",
      "Starting the Plasma object store with 6.554658406 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=ecae69d6f14073706d8dbd67070d19ae6afb98f59b9eda28\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.16.123.117',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-03-20_16-21-49_14070/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-03-20_16-21-49_14070/sockets/raylet'],\n",
       " 'redis_address': '172.16.123.117:32971',\n",
       " 'webui_url': 'http://localhost:8889/notebooks/ray_ui.ipynb?token=ecae69d6f14073706d8dbd67070d19ae6afb98f59b9eda28'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "\n",
    "ray.init(redirect_output=True, num_cpus=N_CPUS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [100, 50, 25]})  # size of hidden layers in network default 64 32\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "#config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "#config[\"sample_batch_size\"] = config[\"train_batch_size\"]/config[\"num_workers\"] # 200 default, trotzdem zu hoch?\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "\n",
      "Created LogSyncer for /home/thorsten/ray_results/IntersectionExample/PPO_IntersectionEnv-v0_0_2019-03-20_16-21-50csiczf5f -> \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-22-40\n",
      "  done: false\n",
      "  episode_len_mean: 486.5\n",
      "  episode_reward_max: 198.5259663830813\n",
      "  episode_reward_mean: 88.7530959373219\n",
      "  episode_reward_min: -50.308129412295926\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.8368449211120605\n",
      "      kl: 0.00423041544854641\n",
      "      policy_loss: -0.004394978750497103\n",
      "      total_loss: 71.37330627441406\n",
      "      vf_explained_var: 0.018452197313308716\n",
      "      vf_loss: 71.37684631347656\n",
      "    grad_time_ms: 2468.13\n",
      "    load_time_ms: 45.286\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "    sample_time_ms: 18461.757\n",
      "    update_time_ms: 593.696\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 21.623180389404297\n",
      "  time_this_iter_s: 21.623180389404297\n",
      "  time_total_s: 21.623180389404297\n",
      "  timestamp: 1553095360\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 21 s, 1 iter, 10000 ts, 88.8 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-23-01\n",
      "  done: false\n",
      "  episode_len_mean: 474.0731707317073\n",
      "  episode_reward_max: 241.68079247524688\n",
      "  episode_reward_mean: 90.31560776165405\n",
      "  episode_reward_min: -60.224339108210415\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 41\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.838320732116699\n",
      "      kl: 0.004038394428789616\n",
      "      policy_loss: -0.0032324569765478373\n",
      "      total_loss: 119.95105743408203\n",
      "      vf_explained_var: 0.08084525167942047\n",
      "      vf_loss: 119.95388793945312\n",
      "    grad_time_ms: 2530.394\n",
      "    load_time_ms: 23.351\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    sample_time_ms: 18410.763\n",
      "    update_time_ms: 299.904\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 42.59511947631836\n",
      "  time_this_iter_s: 20.971939086914062\n",
      "  time_total_s: 42.59511947631836\n",
      "  timestamp: 1553095381\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 42 s, 2 iter, 20000 ts, 90.3 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-23-23\n",
      "  done: false\n",
      "  episode_len_mean: 469.92063492063494\n",
      "  episode_reward_max: 287.65959909440136\n",
      "  episode_reward_mean: 106.29194815712734\n",
      "  episode_reward_min: -60.224339108210415\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 63\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.835862874984741\n",
      "      kl: 0.0044748494401574135\n",
      "      policy_loss: -0.004755289759486914\n",
      "      total_loss: 192.70651245117188\n",
      "      vf_explained_var: 0.0961911752820015\n",
      "      vf_loss: 192.71104431152344\n",
      "    grad_time_ms: 2346.97\n",
      "    load_time_ms: 15.971\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "    sample_time_ms: 18854.899\n",
      "    update_time_ms: 201.798\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 64.33622789382935\n",
      "  time_this_iter_s: 21.741108417510986\n",
      "  time_total_s: 64.33622789382935\n",
      "  timestamp: 1553095403\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 64 s, 3 iter, 30000 ts, 106 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-23-44\n",
      "  done: false\n",
      "  episode_len_mean: 466.4\n",
      "  episode_reward_max: 306.37903794484276\n",
      "  episode_reward_mean: 102.30982001986733\n",
      "  episode_reward_min: -60.224339108210415\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 85\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.8495256900787354\n",
      "      kl: 0.00261569838039577\n",
      "      policy_loss: -0.002634105272591114\n",
      "      total_loss: 152.4438934326172\n",
      "      vf_explained_var: 0.159928560256958\n",
      "      vf_loss: 152.44647216796875\n",
      "    grad_time_ms: 2248.851\n",
      "    load_time_ms: 12.433\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    sample_time_ms: 18812.58\n",
      "    update_time_ms: 152.861\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 84.99806356430054\n",
      "  time_this_iter_s: 20.66183567047119\n",
      "  time_total_s: 84.99806356430054\n",
      "  timestamp: 1553095424\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 84 s, 4 iter, 40000 ts, 102 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-24-03\n",
      "  done: false\n",
      "  episode_len_mean: 455.06\n",
      "  episode_reward_max: 337.5805393842585\n",
      "  episode_reward_mean: 106.40913324273664\n",
      "  episode_reward_min: -60.224339108210415\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 109\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.849898338317871\n",
      "      kl: 0.0065195560455322266\n",
      "      policy_loss: -0.004594114143401384\n",
      "      total_loss: 240.87757873535156\n",
      "      vf_explained_var: 0.16402295231819153\n",
      "      vf_loss: 240.88211059570312\n",
      "    grad_time_ms: 2191.979\n",
      "    load_time_ms: 10.25\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "    sample_time_ms: 18514.77\n",
      "    update_time_ms: 123.478\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 104.30641412734985\n",
      "  time_this_iter_s: 19.308350563049316\n",
      "  time_total_s: 104.30641412734985\n",
      "  timestamp: 1553095443\n",
      "  timesteps_since_restore: 50000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 5\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 104 s, 5 iter, 50000 ts, 106 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-24-22\n",
      "  done: false\n",
      "  episode_len_mean: 447.46\n",
      "  episode_reward_max: 357.58573751857136\n",
      "  episode_reward_mean: 117.39873243992633\n",
      "  episode_reward_min: -60.224339108210415\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 130\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.8359954357147217\n",
      "      kl: 0.006922605913132429\n",
      "      policy_loss: -0.002720989752560854\n",
      "      total_loss: 241.62998962402344\n",
      "      vf_explained_var: 0.1715109795331955\n",
      "      vf_loss: 241.6326446533203\n",
      "    grad_time_ms: 2150.176\n",
      "    load_time_ms: 8.775\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    sample_time_ms: 18252.387\n",
      "    update_time_ms: 103.875\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 123.2085554599762\n",
      "  time_this_iter_s: 18.902141332626343\n",
      "  time_total_s: 123.2085554599762\n",
      "  timestamp: 1553095462\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 6\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 123 s, 6 iter, 60000 ts, 117 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-24-41\n",
      "  done: false\n",
      "  episode_len_mean: 440.8\n",
      "  episode_reward_max: 357.58573751857136\n",
      "  episode_reward_mean: 120.25630109250862\n",
      "  episode_reward_min: -60.638715080140535\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 154\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.8416595458984375\n",
      "      kl: 0.0023808185942471027\n",
      "      policy_loss: -0.0016981770750135183\n",
      "      total_loss: 237.8536834716797\n",
      "      vf_explained_var: 0.28178611397743225\n",
      "      vf_loss: 237.8554229736328\n",
      "    grad_time_ms: 2121.588\n",
      "    load_time_ms: 7.699\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "    sample_time_ms: 18091.758\n",
      "    update_time_ms: 89.682\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 142.30387115478516\n",
      "  time_this_iter_s: 19.09531569480896\n",
      "  time_total_s: 142.30387115478516\n",
      "  timestamp: 1553095481\n",
      "  timesteps_since_restore: 70000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 142 s, 7 iter, 70000 ts, 120 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-25-00\n",
      "  done: false\n",
      "  episode_len_mean: 431.7\n",
      "  episode_reward_max: 369.2326520957405\n",
      "  episode_reward_mean: 128.5916922546623\n",
      "  episode_reward_min: -60.638715080140535\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 177\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.839751720428467\n",
      "      kl: 0.0036587531212717295\n",
      "      policy_loss: -0.0030459207482635975\n",
      "      total_loss: 289.07904052734375\n",
      "      vf_explained_var: 0.3010106682777405\n",
      "      vf_loss: 289.0820617675781\n",
      "    grad_time_ms: 2103.586\n",
      "    load_time_ms: 6.95\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    sample_time_ms: 17988.993\n",
      "    update_time_ms: 79.256\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 161.57247734069824\n",
      "  time_this_iter_s: 19.268606185913086\n",
      "  time_total_s: 161.57247734069824\n",
      "  timestamp: 1553095500\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 161 s, 8 iter, 80000 ts, 129 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-25-19\n",
      "  done: false\n",
      "  episode_len_mean: 410.0\n",
      "  episode_reward_max: 369.2326520957405\n",
      "  episode_reward_mean: 126.55235436431387\n",
      "  episode_reward_min: -60.638715080140535\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 206\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.8478856086730957\n",
      "      kl: 0.003997655585408211\n",
      "      policy_loss: -0.003416597843170166\n",
      "      total_loss: 339.3840026855469\n",
      "      vf_explained_var: 0.29091009497642517\n",
      "      vf_loss: 339.38739013671875\n",
      "    grad_time_ms: 2086.751\n",
      "    load_time_ms: 6.319\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "    sample_time_ms: 17852.665\n",
      "    update_time_ms: 71.05\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 180.30412030220032\n",
      "  time_this_iter_s: 18.731642961502075\n",
      "  time_total_s: 180.30412030220032\n",
      "  timestamp: 1553095519\n",
      "  timesteps_since_restore: 90000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 9\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 180 s, 9 iter, 90000 ts, 127 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-25-39\n",
      "  done: false\n",
      "  episode_len_mean: 383.76\n",
      "  episode_reward_max: 369.2326520957405\n",
      "  episode_reward_mean: 115.80618423444608\n",
      "  episode_reward_min: -61.4173845053034\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 234\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.817678689956665\n",
      "      kl: 0.0071173859760165215\n",
      "      policy_loss: -0.004366802982985973\n",
      "      total_loss: 465.6321716308594\n",
      "      vf_explained_var: 0.2912134826183319\n",
      "      vf_loss: 465.6365051269531\n",
      "    grad_time_ms: 2148.535\n",
      "    load_time_ms: 5.828\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    sample_time_ms: 17780.703\n",
      "    update_time_ms: 64.537\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 200.16070222854614\n",
      "  time_this_iter_s: 19.856581926345825\n",
      "  time_total_s: 200.16070222854614\n",
      "  timestamp: 1553095539\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 200 s, 10 iter, 100000 ts, 116 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-25-59\n",
      "  done: false\n",
      "  episode_len_mean: 364.18\n",
      "  episode_reward_max: 381.75599933693377\n",
      "  episode_reward_mean: 137.40250851036828\n",
      "  episode_reward_min: -61.4173845053034\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 263\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.7888035774230957\n",
      "      kl: 0.011308248154819012\n",
      "      policy_loss: -0.003975537605583668\n",
      "      total_loss: 563.75390625\n",
      "      vf_explained_var: 0.3233511447906494\n",
      "      vf_loss: 563.7579345703125\n",
      "    grad_time_ms: 2098.724\n",
      "    load_time_ms: 1.417\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "    sample_time_ms: 17724.592\n",
      "    update_time_ms: 5.606\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 220.0479815006256\n",
      "  time_this_iter_s: 19.887279272079468\n",
      "  time_total_s: 220.0479815006256\n",
      "  timestamp: 1553095559\n",
      "  timesteps_since_restore: 110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 11\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 220 s, 11 iter, 110000 ts, 137 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-26-21\n",
      "  done: false\n",
      "  episode_len_mean: 337.84\n",
      "  episode_reward_max: 381.75599933693377\n",
      "  episode_reward_mean: 136.90023439123664\n",
      "  episode_reward_min: -63.559883028216596\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 292\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.856715440750122\n",
      "      kl: 0.005193036515265703\n",
      "      policy_loss: -0.003393302671611309\n",
      "      total_loss: 408.3100280761719\n",
      "      vf_explained_var: 0.4414618909358978\n",
      "      vf_loss: 408.31341552734375\n",
      "    grad_time_ms: 2195.417\n",
      "    load_time_ms: 1.394\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    sample_time_ms: 17783.219\n",
      "    update_time_ms: 5.463\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 242.57704997062683\n",
      "  time_this_iter_s: 22.52906847000122\n",
      "  time_total_s: 242.57704997062683\n",
      "  timestamp: 1553095581\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 12\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 242 s, 12 iter, 120000 ts, 137 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-26-43\n",
      "  done: false\n",
      "  episode_len_mean: 342.55\n",
      "  episode_reward_max: 381.75599933693377\n",
      "  episode_reward_mean: 158.71631256974644\n",
      "  episode_reward_min: -63.559883028216596\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 324\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.804187059402466\n",
      "      kl: 0.01465668249875307\n",
      "      policy_loss: -0.006689372938126326\n",
      "      total_loss: 549.803955078125\n",
      "      vf_explained_var: 0.34292691946029663\n",
      "      vf_loss: 549.8106079101562\n",
      "    grad_time_ms: 2275.527\n",
      "    load_time_ms: 1.392\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "    sample_time_ms: 17707.441\n",
      "    update_time_ms: 5.54\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 264.362265586853\n",
      "  time_this_iter_s: 21.785215616226196\n",
      "  time_total_s: 264.362265586853\n",
      "  timestamp: 1553095603\n",
      "  timesteps_since_restore: 130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 13\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 264 s, 13 iter, 130000 ts, 159 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-27-05\n",
      "  done: false\n",
      "  episode_len_mean: 331.65\n",
      "  episode_reward_max: 374.896396172842\n",
      "  episode_reward_mean: 125.93841431756539\n",
      "  episode_reward_min: -63.559883028216596\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 353\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.9007465839385986\n",
      "      kl: 0.017512202262878418\n",
      "      policy_loss: -0.006586275063455105\n",
      "      total_loss: 366.3147888183594\n",
      "      vf_explained_var: 0.44783666729927063\n",
      "      vf_loss: 366.32135009765625\n",
      "    grad_time_ms: 2272.939\n",
      "    load_time_ms: 1.386\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    sample_time_ms: 17827.906\n",
      "    update_time_ms: 5.404\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 286.20214223861694\n",
      "  time_this_iter_s: 21.839876651763916\n",
      "  time_total_s: 286.20214223861694\n",
      "  timestamp: 1553095625\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 14\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 286 s, 14 iter, 140000 ts, 126 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-27-26\n",
      "  done: false\n",
      "  episode_len_mean: 328.55\n",
      "  episode_reward_max: 384.07405423277095\n",
      "  episode_reward_mean: 142.32044209717318\n",
      "  episode_reward_min: -60.812061439861225\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 384\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.853447198867798\n",
      "      kl: 0.005097122397273779\n",
      "      policy_loss: -0.002961099147796631\n",
      "      total_loss: 496.9719543457031\n",
      "      vf_explained_var: 0.37005582451820374\n",
      "      vf_loss: 496.9749450683594\n",
      "    grad_time_ms: 2273.371\n",
      "    load_time_ms: 1.404\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "    sample_time_ms: 17949.668\n",
      "    update_time_ms: 5.272\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 306.7309854030609\n",
      "  time_this_iter_s: 20.52884316444397\n",
      "  time_total_s: 306.7309854030609\n",
      "  timestamp: 1553095646\n",
      "  timesteps_since_restore: 150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 15\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 306 s, 15 iter, 150000 ts, 142 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-27-47\n",
      "  done: false\n",
      "  episode_len_mean: 329.99\n",
      "  episode_reward_max: 384.07405423277095\n",
      "  episode_reward_mean: 143.3615188842576\n",
      "  episode_reward_min: -60.812061439861225\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 414\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.790924549102783\n",
      "      kl: 0.02112589217722416\n",
      "      policy_loss: -0.004426190629601479\n",
      "      total_loss: 601.911376953125\n",
      "      vf_explained_var: 0.28686559200286865\n",
      "      vf_loss: 601.9158935546875\n",
      "    grad_time_ms: 2292.367\n",
      "    load_time_ms: 1.395\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    sample_time_ms: 18156.33\n",
      "    update_time_ms: 5.125\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 327.88774585723877\n",
      "  time_this_iter_s: 21.156760454177856\n",
      "  time_total_s: 327.88774585723877\n",
      "  timestamp: 1553095667\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 16\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 327 s, 16 iter, 160000 ts, 143 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-28-11\n",
      "  done: false\n",
      "  episode_len_mean: 334.13\n",
      "  episode_reward_max: 384.07405423277095\n",
      "  episode_reward_mean: 166.02206420377243\n",
      "  episode_reward_min: -51.40576779889829\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 442\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.7940073013305664\n",
      "      kl: 0.009207082912325859\n",
      "      policy_loss: -0.004575891420245171\n",
      "      total_loss: 527.5454711914062\n",
      "      vf_explained_var: 0.31716489791870117\n",
      "      vf_loss: 527.5501098632812\n",
      "    grad_time_ms: 2343.447\n",
      "    load_time_ms: 1.405\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "    sample_time_ms: 18569.298\n",
      "    update_time_ms: 5.227\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 351.6299571990967\n",
      "  time_this_iter_s: 23.74221134185791\n",
      "  time_total_s: 351.6299571990967\n",
      "  timestamp: 1553095691\n",
      "  timesteps_since_restore: 170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 17\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 351 s, 17 iter, 170000 ts, 166 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-28-34\n",
      "  done: false\n",
      "  episode_len_mean: 352.53\n",
      "  episode_reward_max: 384.07405423277095\n",
      "  episode_reward_mean: 177.26759860333456\n",
      "  episode_reward_min: -51.22765258735306\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 468\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.9102067947387695\n",
      "      kl: 0.004033761098980904\n",
      "      policy_loss: -0.0024287987034767866\n",
      "      total_loss: 308.21234130859375\n",
      "      vf_explained_var: 0.39668571949005127\n",
      "      vf_loss: 308.21478271484375\n",
      "    grad_time_ms: 2348.136\n",
      "    load_time_ms: 1.412\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    sample_time_ms: 18932.666\n",
      "    update_time_ms: 5.208\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 374.5794370174408\n",
      "  time_this_iter_s: 22.949479818344116\n",
      "  time_total_s: 374.5794370174408\n",
      "  timestamp: 1553095714\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 18\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 374 s, 18 iter, 180000 ts, 177 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-28-53\n",
      "  done: false\n",
      "  episode_len_mean: 359.41\n",
      "  episode_reward_max: 383.423357222175\n",
      "  episode_reward_mean: 154.78030104282405\n",
      "  episode_reward_min: -57.24286616178049\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 497\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.89862322807312\n",
      "      kl: 0.02140015922486782\n",
      "      policy_loss: -0.0068793329410254955\n",
      "      total_loss: 377.1121826171875\n",
      "      vf_explained_var: 0.4501374661922455\n",
      "      vf_loss: 377.1190490722656\n",
      "    grad_time_ms: 2348.436\n",
      "    load_time_ms: 1.408\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "    sample_time_ms: 18972.871\n",
      "    update_time_ms: 5.189\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 393.7167069911957\n",
      "  time_this_iter_s: 19.137269973754883\n",
      "  time_total_s: 393.7167069911957\n",
      "  timestamp: 1553095733\n",
      "  timesteps_since_restore: 190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 19\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 393 s, 19 iter, 190000 ts, 155 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-29-11\n",
      "  done: false\n",
      "  episode_len_mean: 375.26\n",
      "  episode_reward_max: 383.423357222175\n",
      "  episode_reward_mean: 149.26024898500157\n",
      "  episode_reward_min: -57.24286616178049\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 521\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.943817377090454\n",
      "      kl: 0.011519750580191612\n",
      "      policy_loss: -0.004527389071881771\n",
      "      total_loss: 295.9447326660156\n",
      "      vf_explained_var: 0.44996604323387146\n",
      "      vf_loss: 295.9492492675781\n",
      "    grad_time_ms: 2273.467\n",
      "    load_time_ms: 1.39\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    sample_time_ms: 18914.441\n",
      "    update_time_ms: 4.991\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 412.2360773086548\n",
      "  time_this_iter_s: 18.519370317459106\n",
      "  time_total_s: 412.2360773086548\n",
      "  timestamp: 1553095751\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 20\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 412 s, 20 iter, 200000 ts, 149 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-29-30\n",
      "  done: false\n",
      "  episode_len_mean: 393.91\n",
      "  episode_reward_max: 351.5691810371224\n",
      "  episode_reward_mean: 136.95690866461192\n",
      "  episode_reward_min: -57.24286616178049\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 544\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 3.124293327331543\n",
      "      kl: 0.019291898235678673\n",
      "      policy_loss: -0.005787191912531853\n",
      "      total_loss: 228.9550018310547\n",
      "      vf_explained_var: 0.4025712013244629\n",
      "      vf_loss: 228.9608154296875\n",
      "    grad_time_ms: 2269.909\n",
      "    load_time_ms: 1.385\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "    sample_time_ms: 18781.239\n",
      "    update_time_ms: 5.193\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 430.75759625434875\n",
      "  time_this_iter_s: 18.52151894569397\n",
      "  time_total_s: 430.75759625434875\n",
      "  timestamp: 1553095770\n",
      "  timesteps_since_restore: 210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 21\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 430 s, 21 iter, 210000 ts, 137 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-29-48\n",
      "  done: false\n",
      "  episode_len_mean: 395.99\n",
      "  episode_reward_max: 366.2024421797516\n",
      "  episode_reward_mean: 150.5588272727364\n",
      "  episode_reward_min: -53.96376756206058\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 570\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.951357364654541\n",
      "      kl: 0.004895779769867659\n",
      "      policy_loss: -0.003544592298567295\n",
      "      total_loss: 348.7801208496094\n",
      "      vf_explained_var: 0.34024062752723694\n",
      "      vf_loss: 348.78369140625\n",
      "    grad_time_ms: 2113.714\n",
      "    load_time_ms: 1.386\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    sample_time_ms: 18484.995\n",
      "    update_time_ms: 5.279\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 448.7565953731537\n",
      "  time_this_iter_s: 17.99899911880493\n",
      "  time_total_s: 448.7565953731537\n",
      "  timestamp: 1553095788\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 22\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 448 s, 22 iter, 220000 ts, 151 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-30-07\n",
      "  done: false\n",
      "  episode_len_mean: 401.69\n",
      "  episode_reward_max: 366.2024421797516\n",
      "  episode_reward_mean: 168.08199906228023\n",
      "  episode_reward_min: -46.82467773301603\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 596\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.913294553756714\n",
      "      kl: 0.018427442759275436\n",
      "      policy_loss: -0.005114133004099131\n",
      "      total_loss: 413.8544921875\n",
      "      vf_explained_var: 0.5790175199508667\n",
      "      vf_loss: 413.8595886230469\n",
      "    grad_time_ms: 2032.499\n",
      "    load_time_ms: 1.419\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "    sample_time_ms: 18248.541\n",
      "    update_time_ms: 5.158\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 467.3648302555084\n",
      "  time_this_iter_s: 18.608234882354736\n",
      "  time_total_s: 467.3648302555084\n",
      "  timestamp: 1553095807\n",
      "  timesteps_since_restore: 230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 23\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 467 s, 23 iter, 230000 ts, 168 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 401.27\n",
      "  episode_reward_max: 366.2024421797516\n",
      "  episode_reward_mean: 161.89922159643038\n",
      "  episode_reward_min: -46.82467773301603\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 621\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 3.120464324951172\n",
      "      kl: 0.01091853529214859\n",
      "      policy_loss: -0.003611915744841099\n",
      "      total_loss: 261.07171630859375\n",
      "      vf_explained_var: 0.5405271649360657\n",
      "      vf_loss: 261.0752868652344\n",
      "    grad_time_ms: 2034.333\n",
      "    load_time_ms: 1.397\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    sample_time_ms: 17908.926\n",
      "    update_time_ms: 5.15\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 485.8237769603729\n",
      "  time_this_iter_s: 18.458946704864502\n",
      "  time_total_s: 485.8237769603729\n",
      "  timestamp: 1553095825\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 24\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 485 s, 24 iter, 240000 ts, 162 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-30-48\n",
      "  done: false\n",
      "  episode_len_mean: 394.19\n",
      "  episode_reward_max: 366.2024421797516\n",
      "  episode_reward_mean: 177.9922496910989\n",
      "  episode_reward_min: -44.80449284150819\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 645\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.9152536392211914\n",
      "      kl: 0.006397399585694075\n",
      "      policy_loss: -0.0031154791358858347\n",
      "      total_loss: 363.1375732421875\n",
      "      vf_explained_var: 0.5871943831443787\n",
      "      vf_loss: 363.14068603515625\n",
      "    grad_time_ms: 2041.394\n",
      "    load_time_ms: 1.441\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "    sample_time_ms: 18132.193\n",
      "    update_time_ms: 5.313\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 508.6592354774475\n",
      "  time_this_iter_s: 22.835458517074585\n",
      "  time_total_s: 508.6592354774475\n",
      "  timestamp: 1553095848\n",
      "  timesteps_since_restore: 250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 25\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 508 s, 25 iter, 250000 ts, 178 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-31-07\n",
      "  done: false\n",
      "  episode_len_mean: 390.21\n",
      "  episode_reward_max: 364.1583319755383\n",
      "  episode_reward_mean: 178.4981419764756\n",
      "  episode_reward_min: -42.542208828833466\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 673\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.9372754096984863\n",
      "      kl: 0.02367440238595009\n",
      "      policy_loss: -0.01024030614644289\n",
      "      total_loss: 386.818359375\n",
      "      vf_explained_var: 0.5442095398902893\n",
      "      vf_loss: 386.8286437988281\n",
      "    grad_time_ms: 2020.974\n",
      "    load_time_ms: 1.433\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    sample_time_ms: 17893.529\n",
      "    update_time_ms: 5.736\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 527.2269623279572\n",
      "  time_this_iter_s: 18.567726850509644\n",
      "  time_total_s: 527.2269623279572\n",
      "  timestamp: 1553095867\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 26\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 527 s, 26 iter, 260000 ts, 178 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-31-26\n",
      "  done: false\n",
      "  episode_len_mean: 394.95\n",
      "  episode_reward_max: 364.1583319755383\n",
      "  episode_reward_mean: 179.3520221498515\n",
      "  episode_reward_min: -42.542208828833466\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 697\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.9138360023498535\n",
      "      kl: 0.004771159030497074\n",
      "      policy_loss: -0.003619374707341194\n",
      "      total_loss: 294.1417236328125\n",
      "      vf_explained_var: 0.6413182020187378\n",
      "      vf_loss: 294.1453552246094\n",
      "    grad_time_ms: 1971.074\n",
      "    load_time_ms: 1.418\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "    sample_time_ms: 17464.08\n",
      "    update_time_ms: 5.734\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 546.1702709197998\n",
      "  time_this_iter_s: 18.94330859184265\n",
      "  time_total_s: 546.1702709197998\n",
      "  timestamp: 1553095886\n",
      "  timesteps_since_restore: 270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 27\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 546 s, 27 iter, 270000 ts, 179 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-31-45\n",
      "  done: false\n",
      "  episode_len_mean: 393.96\n",
      "  episode_reward_max: 361.95803369331725\n",
      "  episode_reward_mean: 201.98676915622298\n",
      "  episode_reward_min: -44.469054413896956\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 722\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.8242781162261963\n",
      "      kl: 0.010985519737005234\n",
      "      policy_loss: -0.004386512096971273\n",
      "      total_loss: 355.5837097167969\n",
      "      vf_explained_var: 0.4376644194126129\n",
      "      vf_loss: 355.58807373046875\n",
      "    grad_time_ms: 1963.012\n",
      "    load_time_ms: 1.385\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    sample_time_ms: 17123.151\n",
      "    update_time_ms: 5.604\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 565.6265530586243\n",
      "  time_this_iter_s: 19.456282138824463\n",
      "  time_total_s: 565.6265530586243\n",
      "  timestamp: 1553095905\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 28\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 565 s, 28 iter, 280000 ts, 202 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-32-04\n",
      "  done: false\n",
      "  episode_len_mean: 394.24\n",
      "  episode_reward_max: 367.1585200557318\n",
      "  episode_reward_mean: 210.76298999035822\n",
      "  episode_reward_min: -47.88009560231952\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 747\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.7554335594177246\n",
      "      kl: 0.020512452349066734\n",
      "      policy_loss: -0.005285661201924086\n",
      "      total_loss: 407.4916076660156\n",
      "      vf_explained_var: 0.49160751700401306\n",
      "      vf_loss: 407.49688720703125\n",
      "    grad_time_ms: 1962.333\n",
      "    load_time_ms: 1.383\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "    sample_time_ms: 17111.829\n",
      "    update_time_ms: 5.543\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 584.6441819667816\n",
      "  time_this_iter_s: 19.01762890815735\n",
      "  time_total_s: 584.6441819667816\n",
      "  timestamp: 1553095924\n",
      "  timesteps_since_restore: 290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 29\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 584 s, 29 iter, 290000 ts, 211 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-32-23\n",
      "  done: false\n",
      "  episode_len_mean: 400.57\n",
      "  episode_reward_max: 370.6434506118061\n",
      "  episode_reward_mean: 232.7399218391804\n",
      "  episode_reward_min: -55.130417153016175\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 773\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.7171616554260254\n",
      "      kl: 0.013265643268823624\n",
      "      policy_loss: -0.0033313296735286713\n",
      "      total_loss: 441.0679626464844\n",
      "      vf_explained_var: 0.3827856183052063\n",
      "      vf_loss: 441.0712585449219\n",
      "    grad_time_ms: 1965.182\n",
      "    load_time_ms: 1.408\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    sample_time_ms: 17094.593\n",
      "    update_time_ms: 5.657\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 603.022088766098\n",
      "  time_this_iter_s: 18.377906799316406\n",
      "  time_total_s: 603.022088766098\n",
      "  timestamp: 1553095943\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 30\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 603 s, 30 iter, 300000 ts, 233 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-32-42\n",
      "  done: false\n",
      "  episode_len_mean: 399.56\n",
      "  episode_reward_max: 375.6774620972217\n",
      "  episode_reward_mean: 242.14745683633998\n",
      "  episode_reward_min: -55.130417153016175\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 798\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.9072303771972656\n",
      "      kl: 0.016274306923151016\n",
      "      policy_loss: -0.004865695256739855\n",
      "      total_loss: 406.87548828125\n",
      "      vf_explained_var: 0.5063589215278625\n",
      "      vf_loss: 406.88037109375\n",
      "    grad_time_ms: 1979.382\n",
      "    load_time_ms: 1.415\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "    sample_time_ms: 17144.426\n",
      "    update_time_ms: 5.64\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 622.1845350265503\n",
      "  time_this_iter_s: 19.16244626045227\n",
      "  time_total_s: 622.1845350265503\n",
      "  timestamp: 1553095962\n",
      "  timesteps_since_restore: 310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 31\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 622 s, 31 iter, 310000 ts, 242 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-33-01\n",
      "  done: false\n",
      "  episode_len_mean: 385.87\n",
      "  episode_reward_max: 375.6774620972217\n",
      "  episode_reward_mean: 240.11209588137083\n",
      "  episode_reward_min: -55.130417153016175\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 825\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.9010367393493652\n",
      "      kl: 0.011658094823360443\n",
      "      policy_loss: -0.004212612751871347\n",
      "      total_loss: 414.88250732421875\n",
      "      vf_explained_var: 0.5075303316116333\n",
      "      vf_loss: 414.8867492675781\n",
      "    grad_time_ms: 2003.863\n",
      "    load_time_ms: 1.412\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    sample_time_ms: 17259.471\n",
      "    update_time_ms: 5.691\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 641.5809950828552\n",
      "  time_this_iter_s: 19.39646005630493\n",
      "  time_total_s: 641.5809950828552\n",
      "  timestamp: 1553095981\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 32\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 641 s, 32 iter, 320000 ts, 240 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-33-24\n",
      "  done: false\n",
      "  episode_len_mean: 379.42\n",
      "  episode_reward_max: 375.6774620972217\n",
      "  episode_reward_mean: 221.2934797130943\n",
      "  episode_reward_min: -57.34086304216991\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 852\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.9547932147979736\n",
      "      kl: 0.026597844436764717\n",
      "      policy_loss: -0.0066270544193685055\n",
      "      total_loss: 391.3948974609375\n",
      "      vf_explained_var: 0.560563862323761\n",
      "      vf_loss: 391.4015197753906\n",
      "    grad_time_ms: 2046.327\n",
      "    load_time_ms: 1.416\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "    sample_time_ms: 17595.553\n",
      "    update_time_ms: 5.74\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 663.9765615463257\n",
      "  time_this_iter_s: 22.39556646347046\n",
      "  time_total_s: 663.9765615463257\n",
      "  timestamp: 1553096004\n",
      "  timesteps_since_restore: 330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 33\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_IntersectionEnv-v0_0:\tRUNNING [pid=14120], 663 s, 33 iter, 330000 ts, 221 rew\n",
      "\n",
      "Result for PPO_IntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_16-33-48\n",
      "  done: false\n",
      "  episode_len_mean: 379.14\n",
      "  episode_reward_max: 384.2019033620516\n",
      "  episode_reward_mean: 210.61592098957698\n",
      "  episode_reward_min: -57.34086304216991\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 878\n",
      "  experiment_id: 7017d3ece0a74e7c8a3a6917ef92853c\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    default:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 3.189467191696167\n",
      "      kl: 0.0020133797079324722\n",
      "      policy_loss: -0.0012629956472665071\n",
      "      total_loss: 301.2796325683594\n",
      "      vf_explained_var: 0.5100914239883423\n",
      "      vf_loss: 301.28094482421875\n",
      "    grad_time_ms: 2053.528\n",
      "    load_time_ms: 1.396\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    sample_time_ms: 18140.843\n",
      "    update_time_ms: 6.116\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 14120\n",
      "  policy_reward_mean: {}\n",
      "  time_since_restore: 687.9640316963196\n",
      "  time_this_iter_s: 23.987470149993896\n",
      "  time_total_s: 687.9640316963196\n",
      "  timestamp: 1553096028\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 34\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,  # RL algorithm to run\n",
    "        \"env\": gym_name,  # environment name generated earlier\n",
    "        \"config\": {  # configuration params (must match \"run\" value)\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1000,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow_2)",
   "language": "python",
   "name": "flow_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
