{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING I3W\n",
    "\n",
    "\n",
    "# A) Create Envorinment, Vehicles etc\n",
    "\n",
    "### General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scenarios:\n",
      "['Scenario', 'BayBridgeScenario', 'BayBridgeTollScenario', 'BottleneckScenario', 'Figure8Scenario', 'SimpleGridScenario', 'HighwayScenario', 'LoopScenario', 'MergeScenario', 'TwoLoopsOneMergingScenario', 'MultiLoopScenario', 'IntersectionScenarioTW', 'TenaciousDScenario']\n",
      "\n",
      "Available environments:\n",
      "['MultiEnv', 'MultiAgentAccelEnv', 'MultiWaveAttenuationPOEnv', 'MultiAgentIntersectionEnv', 'MultiAgentTeamSpiritIntersectionEnv', 'MultiAgentIntersectionEnv_baseline_1', 'MultiAgentIntersectionEnv_baseline_2', 'MultiAgentIntersectionEnv_baseline_3', 'MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit', 'MultiTenaciousDEnv']\n"
     ]
    }
   ],
   "source": [
    "# Define horizon as a variable to ensure consistent use across notebook (length of one rollout)\n",
    "HORIZON= 250                                 #103 max Horizon, wenn es vor verlassen abbrechen soll!, default war 500\n",
    "\n",
    "# name of the experiment\n",
    "experiment_name = \"TenaciousD\"\n",
    "\n",
    "# scenario class\n",
    "import flow.scenarios as scenarios\n",
    "print(\"Available scenarios:\")\n",
    "print(scenarios.__all__)\n",
    "scenario_name = \"TenaciousDScenario\"\n",
    "\n",
    "# environment class\n",
    "import flow.multiagent_envs as flowenvs\n",
    "print(\"\\nAvailable environments:\")\n",
    "print(flowenvs.__all__)\n",
    "env_name = \"MultiTenaciousDEnv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "from flow.scenarios.intersection import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "additionalNetParams={\n",
    "            'length': 230,\n",
    "            'lanes': 1,\n",
    "            'speed_limit': 15,\n",
    "            'resolution': 40,\n",
    "        }\n",
    "\n",
    "net_params = NetParams( no_internal_links=False,                  #default: True   !! damit Kreuzungen nicht Ã¼berspr. werden\n",
    "                        inflows=None,                             #default: None\n",
    "                        osm_path=None,                            #default: None\n",
    "                        netfile=None,                             #default: None\n",
    "                        additional_params=additionalNetParams     #default: None   !!\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InitialConfig Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['right_upper', 'right_lower', 'left_upper', 'left_lower']\n"
     ]
    }
   ],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "\n",
    "initialEdges = []\n",
    "initialEdges.append(\"right_upper\")\n",
    "initialEdges.append(\"right_lower\")\n",
    "initialEdges.append(\"left_upper\")\n",
    "initialEdges.append(\"left_lower\")\n",
    "print(initialEdges)\n",
    "\n",
    "initial_config = InitialConfig( shuffle=True,                           #default: False          !!\n",
    "                                spacing=\"custom\",                        #default: \"uniform\"      !!\n",
    "                                min_gap=10,                              #default: 0\n",
    "                                perturbation=0.0,                        #default: 0.0            !!        \n",
    "                                x0=0,                                    #default: 0\n",
    "                                bunching=0,                              #default: 0\n",
    "                                lanes_distribution=float(\"inf\"),         #default: float(\"inf\")\n",
    "                                edges_distribution=initialEdges,         #default: \"all\"          !!\n",
    "                                additional_params=None )                 #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams( port = None,                  #default: None\n",
    "                          sim_step=0.1,                 #default: 0.1\n",
    "                          emission_path=None,           #default: None\n",
    "                          lateral_resolution=None,      #default: None\n",
    "                          no_step_log=True,             #default: True\n",
    "                          render=False,                 #default: False !!\n",
    "                          save_render=False,            #default: False\n",
    "                          sight_radius=25,              #default: 25\n",
    "                          show_radius=False,            #default: False\n",
    "                          pxpm=2,                       #default: 2\n",
    "                          overtake_right=False,         #default: False    \n",
    "                          seed=None,                    #default: None\n",
    "                          restart_instance=False,       #default: False\n",
    "                          print_warnings=True,          #default: True\n",
    "                          teleport_time=-1,             #default: -1\n",
    "                          num_clients=1,                #default: 1\n",
    "                          sumo_binary=None )            #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "additionalEnvParams={\n",
    "            'max_accel': 1,\n",
    "            'max_decel': 1,\n",
    "            'ring_length': [230, 230],\n",
    "            'target_velocity': 4\n",
    "        }\n",
    "\n",
    "env_params = EnvParams( additional_params=additionalEnvParams, #default: None    !!\n",
    "                        horizon=HORIZON,                       #default: 500     !!\n",
    "                        warmup_steps=0,                        #default: 0       \n",
    "                        sims_per_step=1,                       #default: 1\n",
    "                        evaluate=False )                       #default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# import vehicles dynamics models\n",
    "#from flow.controllers import SumoCarFollowingController\n",
    "from flow.controllers import ContinuousRouter\n",
    "from flow.controllers import TenaciousDRouter\n",
    "#from flow.controllers.lane_change_controllers import SumoLaneChangeController\n",
    "from flow.controllers.lane_change_controllers import StaticLaneChanger\n",
    "from flow.controllers import RLController\n",
    "from flow.core.params import SumoLaneChangeParams\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "from random import *\n",
    "\n",
    "vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RL-Agent controlled vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car following parameters, default: None\n",
    "cf_parameter = SumoCarFollowingParams(\n",
    "                speed_mode=\"aggressive\")\n",
    "# lane change parameters, default: None\n",
    "lc_parameter =  None\n",
    "\n",
    "vehicles.add( # name of the vehicle\n",
    "                veh_id = \"rl\",\n",
    "              # acceleration controller, default: (SumoCarFollowingController, {})\n",
    "                acceleration_controller=(RLController, {}),\n",
    "              # lane_change_controller, default: (SumoLaneChangeController, {})\n",
    "                lane_change_controller=(StaticLaneChanger,{}),\n",
    "              # routing controller, default: None\n",
    "                routing_controller=(TenaciousDRouter, {}),\n",
    "              # initial speed, default: 0\n",
    "                initial_speed=0,\n",
    "              # number of vehicles, default: 1 \n",
    "                num_vehicles=2,\n",
    "                \n",
    "                car_following_params=cf_parameter\n",
    "              # speed mode, default: \"right_of_way\"\n",
    "                #speed_mode=\"aggressive\",\n",
    "              # lane change mode, default: \"no_lat_collide\"\n",
    "                #lane_change_mode=\"aggressive\", \n",
    "              # car following parameter, default: None\n",
    "                #sumo_car_following_params=cf_parameter,\n",
    "              # lane change parameter, default: None\n",
    "                #sumo_lc_params=lc_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict( # name of the experiment\n",
    "                      exp_tag=experiment_name,\n",
    "                    # name of the flow environment the experiment is running on\n",
    "                      env_name=env_name,\n",
    "                    # name of the scenario class the experiment uses\n",
    "                      scenario=scenario_name,\n",
    "                    # simulator that is used by the experiment\n",
    "                      simulator='traci',\n",
    "                    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "                      sim=sumo_params,\n",
    "                    # environment related parameters (see flow.core.params.EnvParams)\n",
    "                      env=env_params,\n",
    "                    # network-related parameters (see flow.core.params.NetParams and\n",
    "                    # the scenario's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "                      net=net_params,\n",
    "                    # vehicles to be placed in the network at the start of a rollout \n",
    "                    # (see flow.core.vehicles.Vehicles)\n",
    "                      veh=vehicles,\n",
    "                   # (optional) parameters affecting the positioning of vehicles upon \n",
    "                   # initialization/reset (see flow.core.params.InitialConfig)\n",
    "                      initial=initial_config\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-04-30_21-06-55_21411/logs.\n",
      "Waiting for redis server at 127.0.0.1:56915 to respond...\n",
      "Waiting for redis server at 127.0.0.1:19476 to respond...\n",
      "Starting the Plasma object store with 6.554658406 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8888/notebooks/ray_ui.ipynb?token=19cd5e26f99c4fded114ee1d7c03c3b6fc72b1be25a567c0\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.16.123.117',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-04-30_21-06-55_21411/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-04-30_21-06-55_21411/sockets/raylet'],\n",
       " 'redis_address': '172.16.123.117:56915',\n",
       " 'webui_url': 'http://localhost:8888/notebooks/ray_ui.ipynb?token=19cd5e26f99c4fded114ee1d7c03c3b6fc72b1be25a567c0'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "\n",
    "ray.init(redirect_output=True, num_cpus=N_CPUS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate default 0.999\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [100, 50, 25]})  # size of hidden layers in network defaule 64 32\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "#config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "#config[\"sample_batch_size\"] = config[\"train_batch_size\"]/config[\"num_workers\"] # 200 default, trotzdem zu hoch?\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 55157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Teamspirit:\n",
      "-0.5307131062447403\n",
      "0.19154012965531098\n",
      "[('left_lower', 17.9513335971836), ('right_lower', 2.693978064496614)]\n"
     ]
    }
   ],
   "source": [
    "# multi agent policy mapping\n",
    "test_env = create_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "\n",
    "def gen_policy():\n",
    "    return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "# Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "policy_graphs = {'rl_0': gen_policy()}\n",
    "    \n",
    "def policy_mapping_fn(agent_id):\n",
    "    return 'rl_0'\n",
    "\n",
    "config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn),\n",
    "            'policies_to_train': ['rl_0']\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.7/16.4 GB\n",
      "\n",
      "Created LogSyncer for /home/thorsten/ray_results/TenaciousD/PPO_MultiTenaciousDEnv-v0_0_2019-04-30_21-06-58ub1zbzov -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-08-07\n",
      "  done: false\n",
      "  episode_len_mean: 250.0\n",
      "  episode_reward_max: 72.02327294833994\n",
      "  episode_reward_mean: 35.444386482561875\n",
      "  episode_reward_min: 12.833629996117578\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 19\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3545.259\n",
      "    load_time_ms: 48.157\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4190486669540405\n",
      "      kl: 0.0028392584063112736\n",
      "      policy_loss: -0.00617074454203248\n",
      "      total_loss: 4.3195013999938965\n",
      "      vf_explained_var: 0.16561394929885864\n",
      "      vf_loss: 4.325104236602783\n",
      "    sample_time_ms: 17704.689\n",
      "    update_time_ms: 994.998\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 17.722193241280944\n",
      "  time_since_restore: 22.365365982055664\n",
      "  time_this_iter_s: 22.365365982055664\n",
      "  time_total_s: 22.365365982055664\n",
      "  timestamp: 1556651287\n",
      "  timesteps_since_restore: 5000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 22 s, 1 iter, 5000 ts, 35.4 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-08-21\n",
      "  done: false\n",
      "  episode_len_mean: 250.0\n",
      "  episode_reward_max: 93.72399756850714\n",
      "  episode_reward_mean: 48.53142671665538\n",
      "  episode_reward_min: 12.833629996117578\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 39\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.641\n",
      "    load_time_ms: 25.125\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4023183584213257\n",
      "      kl: 0.01492101140320301\n",
      "      policy_loss: -0.009099663235247135\n",
      "      total_loss: 10.857969284057617\n",
      "      vf_explained_var: 0.11258873343467712\n",
      "      vf_loss: 10.865578651428223\n",
      "    sample_time_ms: 14784.398\n",
      "    update_time_ms: 505.05\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.26571335832769\n",
      "  time_since_restore: 36.963762521743774\n",
      "  time_this_iter_s: 14.59839653968811\n",
      "  time_total_s: 36.963762521743774\n",
      "  timestamp: 1556651301\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 36 s, 2 iter, 10000 ts, 48.5 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-08-36\n",
      "  done: false\n",
      "  episode_len_mean: 247.79661016949152\n",
      "  episode_reward_max: 180.09256207306308\n",
      "  episode_reward_mean: 59.86725362702397\n",
      "  episode_reward_min: -111.63794422054133\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 59\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3022.881\n",
      "    load_time_ms: 17.198\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3975353240966797\n",
      "      kl: 0.0020984229631721973\n",
      "      policy_loss: -0.002450265921652317\n",
      "      total_loss: 127.26249694824219\n",
      "      vf_explained_var: 0.06906306743621826\n",
      "      vf_loss: 127.26472473144531\n",
      "    sample_time_ms: 13796.067\n",
      "    update_time_ms: 340.846\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.93362681351199\n",
      "  time_since_restore: 51.633315324783325\n",
      "  time_this_iter_s: 14.66955280303955\n",
      "  time_total_s: 51.633315324783325\n",
      "  timestamp: 1556651316\n",
      "  timesteps_since_restore: 15000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 51 s, 3 iter, 15000 ts, 59.9 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-08-51\n",
      "  done: false\n",
      "  episode_len_mean: 246.6375\n",
      "  episode_reward_max: 210.06881511820754\n",
      "  episode_reward_mean: 70.39410392156834\n",
      "  episode_reward_min: -122.20418513617574\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 80\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2961.876\n",
      "    load_time_ms: 13.222\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3846162557601929\n",
      "      kl: 0.009056838229298592\n",
      "      policy_loss: -0.005415456369519234\n",
      "      total_loss: 140.71070861816406\n",
      "      vf_explained_var: 0.07858914881944656\n",
      "      vf_loss: 140.7156524658203\n",
      "    sample_time_ms: 13332.24\n",
      "    update_time_ms: 257.131\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.19705196078418\n",
      "  time_since_restore: 66.37565922737122\n",
      "  time_this_iter_s: 14.74234390258789\n",
      "  time_total_s: 66.37565922737122\n",
      "  timestamp: 1556651331\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 66 s, 4 iter, 20000 ts, 70.4 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-09-06\n",
      "  done: false\n",
      "  episode_len_mean: 244.27\n",
      "  episode_reward_max: 212.18640714473287\n",
      "  episode_reward_mean: 81.67652432487542\n",
      "  episode_reward_min: -158.01989317867452\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 101\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2925.995\n",
      "    load_time_ms: 10.917\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3808459043502808\n",
      "      kl: 0.006903575733304024\n",
      "      policy_loss: -0.002837912878021598\n",
      "      total_loss: 170.48092651367188\n",
      "      vf_explained_var: 0.028186539188027382\n",
      "      vf_loss: 170.48362731933594\n",
      "    sample_time_ms: 13138.901\n",
      "    update_time_ms: 207.804\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.838262162437715\n",
      "  time_since_restore: 81.55195760726929\n",
      "  time_this_iter_s: 15.176298379898071\n",
      "  time_total_s: 81.55195760726929\n",
      "  timestamp: 1556651346\n",
      "  timesteps_since_restore: 25000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 5\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 81 s, 5 iter, 25000 ts, 81.7 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-09-23\n",
      "  done: false\n",
      "  episode_len_mean: 236.37\n",
      "  episode_reward_max: 277.3762538356018\n",
      "  episode_reward_mean: 101.89552162606361\n",
      "  episode_reward_min: -158.01989317867452\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 125\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2889.696\n",
      "    load_time_ms: 9.447\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3667470216751099\n",
      "      kl: 0.008067733608186245\n",
      "      policy_loss: -0.0027757249772548676\n",
      "      total_loss: 265.32000732421875\n",
      "      vf_explained_var: 0.033978354185819626\n",
      "      vf_loss: 265.3227233886719\n",
      "    sample_time_ms: 13344.062\n",
      "    update_time_ms: 174.761\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.9477608130318\n",
      "  time_since_restore: 98.6580057144165\n",
      "  time_this_iter_s: 17.106048107147217\n",
      "  time_total_s: 98.6580057144165\n",
      "  timestamp: 1556651363\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 98 s, 6 iter, 30000 ts, 102 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-09-38\n",
      "  done: false\n",
      "  episode_len_mean: 225.04\n",
      "  episode_reward_max: 287.6153861091617\n",
      "  episode_reward_mean: 119.68530886713664\n",
      "  episode_reward_min: -158.01989317867452\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 150\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2861.233\n",
      "    load_time_ms: 8.326\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3533785343170166\n",
      "      kl: 0.0034895704593509436\n",
      "      policy_loss: -0.0020293849520385265\n",
      "      total_loss: 348.9954528808594\n",
      "      vf_explained_var: 0.020705832168459892\n",
      "      vf_loss: 348.9974670410156\n",
      "    sample_time_ms: 13154.39\n",
      "    update_time_ms: 152.001\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 59.84265443356831\n",
      "  time_since_restore: 113.39740300178528\n",
      "  time_this_iter_s: 14.739397287368774\n",
      "  time_total_s: 113.39740300178528\n",
      "  timestamp: 1556651378\n",
      "  timesteps_since_restore: 35000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 113 s, 7 iter, 35000 ts, 120 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-09-53\n",
      "  done: false\n",
      "  episode_len_mean: 213.75\n",
      "  episode_reward_max: 292.2419924967542\n",
      "  episode_reward_mean: 146.38857283872713\n",
      "  episode_reward_min: -158.01989317867452\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 175\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2889.783\n",
      "    load_time_ms: 7.461\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.349158525466919\n",
      "      kl: 0.007839620113372803\n",
      "      policy_loss: -0.002489216858521104\n",
      "      total_loss: 358.2762145996094\n",
      "      vf_explained_var: 0.002442726632580161\n",
      "      vf_loss: 358.2786560058594\n",
      "    sample_time_ms: 12991.21\n",
      "    update_time_ms: 134.929\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.19428641936356\n",
      "  time_since_restore: 128.374529838562\n",
      "  time_this_iter_s: 14.977126836776733\n",
      "  time_total_s: 128.374529838562\n",
      "  timestamp: 1556651393\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 128 s, 8 iter, 40000 ts, 146 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-10-11\n",
      "  done: false\n",
      "  episode_len_mean: 205.16\n",
      "  episode_reward_max: 292.2419924967542\n",
      "  episode_reward_mean: 153.77245456009052\n",
      "  episode_reward_min: -156.42769214437183\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 199\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2882.189\n",
      "    load_time_ms: 6.84\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3841743469238281\n",
      "      kl: 0.007569932844489813\n",
      "      policy_loss: -0.0033229668624699116\n",
      "      total_loss: 240.72903442382812\n",
      "      vf_explained_var: 0.0034411863889545202\n",
      "      vf_loss: 240.7322998046875\n",
      "    sample_time_ms: 13197.386\n",
      "    update_time_ms: 121.166\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.88622728004525\n",
      "  time_since_restore: 146.07242107391357\n",
      "  time_this_iter_s: 17.697891235351562\n",
      "  time_total_s: 146.07242107391357\n",
      "  timestamp: 1556651411\n",
      "  timesteps_since_restore: 45000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 9\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 146 s, 9 iter, 45000 ts, 154 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-10-27\n",
      "  done: false\n",
      "  episode_len_mean: 193.99\n",
      "  episode_reward_max: 292.2419924967542\n",
      "  episode_reward_mean: 156.42563181961805\n",
      "  episode_reward_min: -162.5051560424602\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 228\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2910.216\n",
      "    load_time_ms: 6.304\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3824466466903687\n",
      "      kl: 0.005292973481118679\n",
      "      policy_loss: -0.0034359432756900787\n",
      "      total_loss: 379.5788879394531\n",
      "      vf_explained_var: 0.009401418268680573\n",
      "      vf_loss: 379.58233642578125\n",
      "    sample_time_ms: 13166.189\n",
      "    update_time_ms: 110.081\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 78.21281590980904\n",
      "  time_since_restore: 162.14752888679504\n",
      "  time_this_iter_s: 16.07510781288147\n",
      "  time_total_s: 162.14752888679504\n",
      "  timestamp: 1556651427\n",
      "  timesteps_since_restore: 50000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 162 s, 10 iter, 50000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 184.78\n",
      "  episode_reward_max: 292.6400071946629\n",
      "  episode_reward_mean: 156.03292257593904\n",
      "  episode_reward_min: -166.9140972576301\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 257\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2809.9\n",
      "    load_time_ms: 1.621\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3871030807495117\n",
      "      kl: 0.0021585384383797646\n",
      "      policy_loss: -0.0006471865926869214\n",
      "      total_loss: 469.3265075683594\n",
      "      vf_explained_var: 0.005717379041016102\n",
      "      vf_loss: 469.32720947265625\n",
      "    sample_time_ms: 12567.547\n",
      "    update_time_ms: 11.527\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 78.0164612879695\n",
      "  time_since_restore: 176.43354487419128\n",
      "  time_this_iter_s: 14.28601598739624\n",
      "  time_total_s: 176.43354487419128\n",
      "  timestamp: 1556651441\n",
      "  timesteps_since_restore: 55000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 11\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 176 s, 11 iter, 55000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-10-55\n",
      "  done: false\n",
      "  episode_len_mean: 179.31\n",
      "  episode_reward_max: 306.4172341068853\n",
      "  episode_reward_mean: 160.96080367274035\n",
      "  episode_reward_min: -166.9140972576301\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 286\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2802.743\n",
      "    load_time_ms: 1.545\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.380293846130371\n",
      "      kl: 0.004193977452814579\n",
      "      policy_loss: -0.0009535656427033246\n",
      "      total_loss: 339.9620056152344\n",
      "      vf_explained_var: 0.0008762303041294217\n",
      "      vf_loss: 339.96295166015625\n",
      "    sample_time_ms: 12525.68\n",
      "    update_time_ms: 10.833\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 80.48040183637018\n",
      "  time_since_restore: 190.53183364868164\n",
      "  time_this_iter_s: 14.098288774490356\n",
      "  time_total_s: 190.53183364868164\n",
      "  timestamp: 1556651455\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 190 s, 12 iter, 60000 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-11-10\n",
      "  done: false\n",
      "  episode_len_mean: 169.55\n",
      "  episode_reward_max: 306.4172341068853\n",
      "  episode_reward_mean: 162.89007745953754\n",
      "  episode_reward_min: -166.9140972576301\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 316\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2784.355\n",
      "    load_time_ms: 1.556\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.392917275428772\n",
      "      kl: 0.002164237666875124\n",
      "      policy_loss: -0.0019564987160265446\n",
      "      total_loss: 433.5718078613281\n",
      "      vf_explained_var: 0.007032986730337143\n",
      "      vf_loss: 433.5738830566406\n",
      "    sample_time_ms: 12553.549\n",
      "    update_time_ms: 10.396\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 81.44503872976877\n",
      "  time_since_restore: 205.29374647140503\n",
      "  time_this_iter_s: 14.761912822723389\n",
      "  time_total_s: 205.29374647140503\n",
      "  timestamp: 1556651470\n",
      "  timesteps_since_restore: 65000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 13\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 205 s, 13 iter, 65000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-11-24\n",
      "  done: false\n",
      "  episode_len_mean: 165.76\n",
      "  episode_reward_max: 306.4172341068853\n",
      "  episode_reward_mean: 170.89408739981403\n",
      "  episode_reward_min: -166.9140972576301\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 347\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2761.265\n",
      "    load_time_ms: 1.56\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4049309492111206\n",
      "      kl: 0.0022030689287930727\n",
      "      policy_loss: -0.001036795903928578\n",
      "      total_loss: 401.8572692871094\n",
      "      vf_explained_var: 0.008752075023949146\n",
      "      vf_loss: 401.85833740234375\n",
      "    sample_time_ms: 12503.509\n",
      "    update_time_ms: 10.741\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 85.44704369990701\n",
      "  time_since_restore: 219.30657386779785\n",
      "  time_this_iter_s: 14.012827396392822\n",
      "  time_total_s: 219.30657386779785\n",
      "  timestamp: 1556651484\n",
      "  timesteps_since_restore: 70000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 14\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 219 s, 14 iter, 70000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-11-39\n",
      "  done: false\n",
      "  episode_len_mean: 160.36\n",
      "  episode_reward_max: 301.36542759814154\n",
      "  episode_reward_mean: 168.29992966773864\n",
      "  episode_reward_min: -161.97949937183057\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 380\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2742.116\n",
      "    load_time_ms: 1.526\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3804289102554321\n",
      "      kl: 0.004213135689496994\n",
      "      policy_loss: -0.0012851681094616652\n",
      "      total_loss: 458.6676330566406\n",
      "      vf_explained_var: 0.010492169298231602\n",
      "      vf_loss: 458.6689147949219\n",
      "    sample_time_ms: 12497.318\n",
      "    update_time_ms: 10.643\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 84.14996483386932\n",
      "  time_since_restore: 234.22801280021667\n",
      "  time_this_iter_s: 14.921438932418823\n",
      "  time_total_s: 234.22801280021667\n",
      "  timestamp: 1556651499\n",
      "  timesteps_since_restore: 75000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 15\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 234 s, 15 iter, 75000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-11-53\n",
      "  done: false\n",
      "  episode_len_mean: 158.25\n",
      "  episode_reward_max: 301.36542759814154\n",
      "  episode_reward_mean: 166.76451594063622\n",
      "  episode_reward_min: -161.4579208736275\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 412\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2732.142\n",
      "    load_time_ms: 1.447\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3773130178451538\n",
      "      kl: 0.004290473181754351\n",
      "      policy_loss: -0.0014035177882760763\n",
      "      total_loss: 518.1430053710938\n",
      "      vf_explained_var: 0.020338715985417366\n",
      "      vf_loss: 518.14453125\n",
      "    sample_time_ms: 12209.706\n",
      "    update_time_ms: 11.86\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 83.3822579703181\n",
      "  time_since_restore: 248.3691532611847\n",
      "  time_this_iter_s: 14.141140460968018\n",
      "  time_total_s: 248.3691532611847\n",
      "  timestamp: 1556651513\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 16\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 248 s, 16 iter, 80000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-12-08\n",
      "  done: false\n",
      "  episode_len_mean: 150.95\n",
      "  episode_reward_max: 311.1265128906813\n",
      "  episode_reward_mean: 142.21237752540907\n",
      "  episode_reward_min: -161.4579208736275\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 447\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2740.457\n",
      "    load_time_ms: 1.433\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3713035583496094\n",
      "      kl: 0.004654244054108858\n",
      "      policy_loss: -0.0016829917440190911\n",
      "      total_loss: 707.2008056640625\n",
      "      vf_explained_var: 0.03309238702058792\n",
      "      vf_loss: 707.2024536132812\n",
      "    sample_time_ms: 12213.345\n",
      "    update_time_ms: 11.551\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.10618876270453\n",
      "  time_since_restore: 263.2220034599304\n",
      "  time_this_iter_s: 14.852850198745728\n",
      "  time_total_s: 263.2220034599304\n",
      "  timestamp: 1556651528\n",
      "  timesteps_since_restore: 85000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 17\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 263 s, 17 iter, 85000 ts, 142 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-12-22\n",
      "  done: false\n",
      "  episode_len_mean: 145.27\n",
      "  episode_reward_max: 311.1265128906813\n",
      "  episode_reward_mean: 140.2829821501003\n",
      "  episode_reward_min: -163.58790706156336\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 482\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2692.655\n",
      "    load_time_ms: 1.437\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3559229373931885\n",
      "      kl: 0.00841008685529232\n",
      "      policy_loss: -0.0018155574798583984\n",
      "      total_loss: 529.7063598632812\n",
      "      vf_explained_var: 0.029321126639842987\n",
      "      vf_loss: 529.7080688476562\n",
      "    sample_time_ms: 12133.395\n",
      "    update_time_ms: 10.861\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.14149107505017\n",
      "  time_since_restore: 276.9080059528351\n",
      "  time_this_iter_s: 13.686002492904663\n",
      "  time_total_s: 276.9080059528351\n",
      "  timestamp: 1556651542\n",
      "  timesteps_since_restore: 90000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 18\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 276 s, 18 iter, 90000 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-12-37\n",
      "  done: false\n",
      "  episode_len_mean: 136.82\n",
      "  episode_reward_max: 311.1265128906813\n",
      "  episode_reward_mean: 134.85259116949257\n",
      "  episode_reward_min: -163.58790706156336\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 521\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2667.485\n",
      "    load_time_ms: 1.46\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3544217348098755\n",
      "      kl: 0.004800364840775728\n",
      "      policy_loss: -0.0008054155041463673\n",
      "      total_loss: 717.8753051757812\n",
      "      vf_explained_var: 0.014807813800871372\n",
      "      vf_loss: 717.8761596679688\n",
      "    sample_time_ms: 11829.45\n",
      "    update_time_ms: 11.168\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.4262955847463\n",
      "  time_since_restore: 291.31558299064636\n",
      "  time_this_iter_s: 14.40757703781128\n",
      "  time_total_s: 291.31558299064636\n",
      "  timestamp: 1556651557\n",
      "  timesteps_since_restore: 95000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 19\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 291 s, 19 iter, 95000 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-12-51\n",
      "  done: false\n",
      "  episode_len_mean: 130.91\n",
      "  episode_reward_max: 304.06788952407203\n",
      "  episode_reward_mean: 129.1924808143733\n",
      "  episode_reward_min: -163.1893018380508\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 561\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2609.722\n",
      "    load_time_ms: 1.456\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3495452404022217\n",
      "      kl: 0.005021794233471155\n",
      "      policy_loss: -0.0012404307490214705\n",
      "      total_loss: 677.4342651367188\n",
      "      vf_explained_var: 0.019441617652773857\n",
      "      vf_loss: 677.435546875\n",
      "    sample_time_ms: 11714.671\n",
      "    update_time_ms: 12.398\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.59624040718666\n",
      "  time_since_restore: 305.6783678531647\n",
      "  time_this_iter_s: 14.36278486251831\n",
      "  time_total_s: 305.6783678531647\n",
      "  timestamp: 1556651571\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 20\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 305 s, 20 iter, 100000 ts, 129 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-13-05\n",
      "  done: false\n",
      "  episode_len_mean: 128.73\n",
      "  episode_reward_max: 308.09043261879026\n",
      "  episode_reward_mean: 135.60743392995067\n",
      "  episode_reward_min: -163.92853433257818\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 598\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2632.675\n",
      "    load_time_ms: 1.459\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3152745962142944\n",
      "      kl: 0.006282227113842964\n",
      "      policy_loss: -0.0036498962435871363\n",
      "      total_loss: 640.2289428710938\n",
      "      vf_explained_var: 0.029828568920493126\n",
      "      vf_loss: 640.2326049804688\n",
      "    sample_time_ms: 11701.658\n",
      "    update_time_ms: 13.13\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.80371696497535\n",
      "  time_since_restore: 320.07054138183594\n",
      "  time_this_iter_s: 14.392173528671265\n",
      "  time_total_s: 320.07054138183594\n",
      "  timestamp: 1556651585\n",
      "  timesteps_since_restore: 105000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 21\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 320 s, 21 iter, 105000 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-13-21\n",
      "  done: false\n",
      "  episode_len_mean: 129.92\n",
      "  episode_reward_max: 308.09043261879026\n",
      "  episode_reward_mean: 163.33054448929\n",
      "  episode_reward_min: -163.92853433257818\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 638\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2642.638\n",
      "    load_time_ms: 1.464\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3155609369277954\n",
      "      kl: 0.001993855694308877\n",
      "      policy_loss: -0.000675855902954936\n",
      "      total_loss: 652.4287719726562\n",
      "      vf_explained_var: 0.02317960374057293\n",
      "      vf_loss: 652.4293823242188\n",
      "    sample_time_ms: 11797.626\n",
      "    update_time_ms: 13.791\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 81.665272244645\n",
      "  time_since_restore: 335.2366921901703\n",
      "  time_this_iter_s: 15.16615080833435\n",
      "  time_total_s: 335.2366921901703\n",
      "  timestamp: 1556651601\n",
      "  timesteps_since_restore: 110000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 22\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 335 s, 22 iter, 110000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-13-36\n",
      "  done: false\n",
      "  episode_len_mean: 128.04\n",
      "  episode_reward_max: 304.454894030646\n",
      "  episode_reward_mean: 165.9838834967711\n",
      "  episode_reward_min: -159.92490578795017\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 679\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2663.879\n",
      "    load_time_ms: 1.468\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3100031614303589\n",
      "      kl: 0.0024906506296247244\n",
      "      policy_loss: -0.0008988204644992948\n",
      "      total_loss: 666.1195678710938\n",
      "      vf_explained_var: 0.02672393247485161\n",
      "      vf_loss: 666.12060546875\n",
      "    sample_time_ms: 11810.817\n",
      "    update_time_ms: 13.629\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 82.99194174838559\n",
      "  time_since_restore: 350.3409833908081\n",
      "  time_this_iter_s: 15.104291200637817\n",
      "  time_total_s: 350.3409833908081\n",
      "  timestamp: 1556651616\n",
      "  timesteps_since_restore: 115000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 23\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 350 s, 23 iter, 115000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-13-51\n",
      "  done: false\n",
      "  episode_len_mean: 126.66\n",
      "  episode_reward_max: 304.454894030646\n",
      "  episode_reward_mean: 169.342560132717\n",
      "  episode_reward_min: -160.37879003935444\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 718\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2678.945\n",
      "    load_time_ms: 1.526\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3121503591537476\n",
      "      kl: 0.0035427070688456297\n",
      "      policy_loss: -0.0017069587484002113\n",
      "      total_loss: 655.9857788085938\n",
      "      vf_explained_var: 0.03376973420381546\n",
      "      vf_loss: 655.9874877929688\n",
      "    sample_time_ms: 11923.073\n",
      "    update_time_ms: 14.619\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 84.67128006635852\n",
      "  time_since_restore: 365.638920545578\n",
      "  time_this_iter_s: 15.297937154769897\n",
      "  time_total_s: 365.638920545578\n",
      "  timestamp: 1556651631\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 24\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 365 s, 24 iter, 120000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-14-06\n",
      "  done: false\n",
      "  episode_len_mean: 126.52\n",
      "  episode_reward_max: 305.8818734633472\n",
      "  episode_reward_mean: 161.19087590325697\n",
      "  episode_reward_min: -164.2894163987253\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 758\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2700.334\n",
      "    load_time_ms: 1.59\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3255072832107544\n",
      "      kl: 0.003586740465834737\n",
      "      policy_loss: -0.002881173510104418\n",
      "      total_loss: 768.4771118164062\n",
      "      vf_explained_var: 0.03852617368102074\n",
      "      vf_loss: 768.4798583984375\n",
      "    sample_time_ms: 11901.231\n",
      "    update_time_ms: 15.321\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 80.59543795162848\n",
      "  time_since_restore: 380.565021276474\n",
      "  time_this_iter_s: 14.926100730895996\n",
      "  time_total_s: 380.565021276474\n",
      "  timestamp: 1556651646\n",
      "  timesteps_since_restore: 125000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 25\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 380 s, 25 iter, 125000 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-14-24\n",
      "  done: false\n",
      "  episode_len_mean: 123.83\n",
      "  episode_reward_max: 305.8818734633472\n",
      "  episode_reward_mean: 158.66680932733476\n",
      "  episode_reward_min: -164.2894163987253\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 800\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2723.292\n",
      "    load_time_ms: 1.612\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2985453605651855\n",
      "      kl: 0.0049796923995018005\n",
      "      policy_loss: -0.0023311821278184652\n",
      "      total_loss: 687.1594848632812\n",
      "      vf_explained_var: 0.05126054957509041\n",
      "      vf_loss: 687.1618041992188\n",
      "    sample_time_ms: 12238.017\n",
      "    update_time_ms: 14.016\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 79.33340466366737\n",
      "  time_since_restore: 398.2937021255493\n",
      "  time_this_iter_s: 17.728680849075317\n",
      "  time_total_s: 398.2937021255493\n",
      "  timestamp: 1556651664\n",
      "  timesteps_since_restore: 130000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 26\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 398 s, 26 iter, 130000 ts, 159 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-14-39\n",
      "  done: false\n",
      "  episode_len_mean: 120.71\n",
      "  episode_reward_max: 305.8818734633472\n",
      "  episode_reward_mean: 150.0518316807579\n",
      "  episode_reward_min: -164.2894163987253\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 841\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2706.018\n",
      "    load_time_ms: 1.599\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.27510404586792\n",
      "      kl: 0.003005317645147443\n",
      "      policy_loss: -0.0016494998708367348\n",
      "      total_loss: 725.8486938476562\n",
      "      vf_explained_var: 0.05111014097929001\n",
      "      vf_loss: 725.850341796875\n",
      "    sample_time_ms: 12280.066\n",
      "    update_time_ms: 15.324\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.02591584037894\n",
      "  time_since_restore: 413.4096281528473\n",
      "  time_this_iter_s: 15.115926027297974\n",
      "  time_total_s: 413.4096281528473\n",
      "  timestamp: 1556651679\n",
      "  timesteps_since_restore: 135000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 27\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 413 s, 27 iter, 135000 ts, 150 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-14-54\n",
      "  done: false\n",
      "  episode_len_mean: 121.2\n",
      "  episode_reward_max: 303.0539037281041\n",
      "  episode_reward_mean: 162.5589059858401\n",
      "  episode_reward_min: -164.14663044521228\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 882\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2702.178\n",
      "    load_time_ms: 1.593\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9802322831784522e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.253729224205017\n",
      "      kl: 0.0034337304532527924\n",
      "      policy_loss: -0.0015421319985762239\n",
      "      total_loss: 713.1813354492188\n",
      "      vf_explained_var: 0.0636524111032486\n",
      "      vf_loss: 713.1829223632812\n",
      "    sample_time_ms: 12434.018\n",
      "    update_time_ms: 15.061\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 81.27945299292007\n",
      "  time_since_restore: 428.5948836803436\n",
      "  time_this_iter_s: 15.185255527496338\n",
      "  time_total_s: 428.5948836803436\n",
      "  timestamp: 1556651694\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 28\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 428 s, 28 iter, 140000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-15-09\n",
      "  done: false\n",
      "  episode_len_mean: 118.98\n",
      "  episode_reward_max: 311.47946095321464\n",
      "  episode_reward_mean: 150.09568120620983\n",
      "  episode_reward_min: -164.8855975359034\n",
      "  episodes_this_iter: 43\n",
      "  episodes_total: 925\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2700.787\n",
      "    load_time_ms: 1.591\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2372509241104126\n",
      "      kl: 0.006982460152357817\n",
      "      policy_loss: -0.0004607807786669582\n",
      "      total_loss: 839.7222290039062\n",
      "      vf_explained_var: 0.04932885617017746\n",
      "      vf_loss: 839.7227783203125\n",
      "    sample_time_ms: 12472.429\n",
      "    update_time_ms: 14.737\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.04784060310493\n",
      "  time_since_restore: 443.3727180957794\n",
      "  time_this_iter_s: 14.777834415435791\n",
      "  time_total_s: 443.3727180957794\n",
      "  timestamp: 1556651709\n",
      "  timesteps_since_restore: 145000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 29\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 443 s, 29 iter, 145000 ts, 150 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-15-24\n",
      "  done: false\n",
      "  episode_len_mean: 112.34\n",
      "  episode_reward_max: 311.47946095321464\n",
      "  episode_reward_mean: 116.4063307115126\n",
      "  episode_reward_min: -164.8855975359034\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 973\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2752.712\n",
      "    load_time_ms: 1.578\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2481536865234375\n",
      "      kl: 0.007066559977829456\n",
      "      policy_loss: -0.004497542977333069\n",
      "      total_loss: 1119.257080078125\n",
      "      vf_explained_var: 0.03744763880968094\n",
      "      vf_loss: 1119.2618408203125\n",
      "    sample_time_ms: 12492.91\n",
      "    update_time_ms: 13.217\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.20316535575629\n",
      "  time_since_restore: 458.44518208503723\n",
      "  time_this_iter_s: 15.072463989257812\n",
      "  time_total_s: 458.44518208503723\n",
      "  timestamp: 1556651724\n",
      "  timesteps_since_restore: 150000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 30\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 458 s, 30 iter, 150000 ts, 116 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-15-39\n",
      "  done: false\n",
      "  episode_len_mean: 115.37\n",
      "  episode_reward_max: 311.47946095321464\n",
      "  episode_reward_mean: 128.4863849927403\n",
      "  episode_reward_min: -164.8855975359034\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 1012\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2749.071\n",
      "    load_time_ms: 1.579\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2231355905532837\n",
      "      kl: 0.009337731637060642\n",
      "      policy_loss: -0.004038049839437008\n",
      "      total_loss: 579.27392578125\n",
      "      vf_explained_var: 0.11290872097015381\n",
      "      vf_loss: 579.2778930664062\n",
      "    sample_time_ms: 12518.148\n",
      "    update_time_ms: 12.81\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.24319249637014\n",
      "  time_since_restore: 473.04839992523193\n",
      "  time_this_iter_s: 14.603217840194702\n",
      "  time_total_s: 473.04839992523193\n",
      "  timestamp: 1556651739\n",
      "  timesteps_since_restore: 155000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 31\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 473 s, 31 iter, 155000 ts, 128 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-15-54\n",
      "  done: false\n",
      "  episode_len_mean: 117.57\n",
      "  episode_reward_max: 302.45605530515456\n",
      "  episode_reward_mean: 155.85081638133929\n",
      "  episode_reward_min: -162.28125381149\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 1057\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2738.628\n",
      "    load_time_ms: 1.65\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.224990963935852\n",
      "      kl: 0.006287361029535532\n",
      "      policy_loss: -0.0018142133485525846\n",
      "      total_loss: 813.5679321289062\n",
      "      vf_explained_var: 0.07504143565893173\n",
      "      vf_loss: 813.5697631835938\n",
      "    sample_time_ms: 12462.727\n",
      "    update_time_ms: 11.979\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 77.92540819066963\n",
      "  time_since_restore: 487.5466582775116\n",
      "  time_this_iter_s: 14.498258352279663\n",
      "  time_total_s: 487.5466582775116\n",
      "  timestamp: 1556651754\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 32\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 487 s, 32 iter, 160000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-16-09\n",
      "  done: false\n",
      "  episode_len_mean: 111.81\n",
      "  episode_reward_max: 301.04770992666784\n",
      "  episode_reward_mean: 143.8302610970693\n",
      "  episode_reward_min: -163.83908065798687\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 1103\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2722.825\n",
      "    load_time_ms: 1.637\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.313225884932663e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1962311267852783\n",
      "      kl: 0.005532815121114254\n",
      "      policy_loss: -0.0025245624128729105\n",
      "      total_loss: 893.5604248046875\n",
      "      vf_explained_var: 0.059314124286174774\n",
      "      vf_loss: 893.56298828125\n",
      "    sample_time_ms: 12517.561\n",
      "    update_time_ms: 12.146\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.91513054853468\n",
      "  time_since_restore: 503.04488468170166\n",
      "  time_this_iter_s: 15.498226404190063\n",
      "  time_total_s: 503.04488468170166\n",
      "  timestamp: 1556651769\n",
      "  timesteps_since_restore: 165000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 33\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 503 s, 33 iter, 165000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-16-24\n",
      "  done: false\n",
      "  episode_len_mean: 108.65\n",
      "  episode_reward_max: 304.91660610628526\n",
      "  episode_reward_mean: 125.0187217263266\n",
      "  episode_reward_min: -163.83908065798687\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 1149\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2713.264\n",
      "    load_time_ms: 1.579\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1843318939208984\n",
      "      kl: 0.004233094397932291\n",
      "      policy_loss: -0.002446444472298026\n",
      "      total_loss: 980.072998046875\n",
      "      vf_explained_var: 0.06053544208407402\n",
      "      vf_loss: 980.0753173828125\n",
      "    sample_time_ms: 12436.16\n",
      "    update_time_ms: 12.141\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.50936086316331\n",
      "  time_since_restore: 517.432094335556\n",
      "  time_this_iter_s: 14.38720965385437\n",
      "  time_total_s: 517.432094335556\n",
      "  timestamp: 1556651784\n",
      "  timesteps_since_restore: 170000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 34\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 517 s, 34 iter, 170000 ts, 125 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-16-38\n",
      "  done: false\n",
      "  episode_len_mean: 109.29\n",
      "  episode_reward_max: 309.05449782799656\n",
      "  episode_reward_mean: 120.92716269203083\n",
      "  episode_reward_min: -169.09757844243563\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 1194\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2691.58\n",
      "    load_time_ms: 1.512\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.186495304107666\n",
      "      kl: 0.003643395844846964\n",
      "      policy_loss: -0.0010348475771024823\n",
      "      total_loss: 1027.307373046875\n",
      "      vf_explained_var: 0.05418221279978752\n",
      "      vf_loss: 1027.3084716796875\n",
      "    sample_time_ms: 12444.037\n",
      "    update_time_ms: 11.085\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.46358134601542\n",
      "  time_since_restore: 532.2049224376678\n",
      "  time_this_iter_s: 14.772828102111816\n",
      "  time_total_s: 532.2049224376678\n",
      "  timestamp: 1556651798\n",
      "  timesteps_since_restore: 175000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 35\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 532 s, 35 iter, 175000 ts, 121 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-16-54\n",
      "  done: false\n",
      "  episode_len_mean: 106.87\n",
      "  episode_reward_max: 309.05449782799656\n",
      "  episode_reward_mean: 100.91186300908365\n",
      "  episode_reward_min: -169.09757844243563\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 1242\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2688.385\n",
      "    load_time_ms: 1.532\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1761587858200073\n",
      "      kl: 0.004308720584958792\n",
      "      policy_loss: -0.0014069664757698774\n",
      "      total_loss: 1046.687744140625\n",
      "      vf_explained_var: 0.07562949508428574\n",
      "      vf_loss: 1046.689208984375\n",
      "    sample_time_ms: 12196.893\n",
      "    update_time_ms: 11.371\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.45593150454184\n",
      "  time_since_restore: 547.4320085048676\n",
      "  time_this_iter_s: 15.227086067199707\n",
      "  time_total_s: 547.4320085048676\n",
      "  timestamp: 1556651814\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 36\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 547 s, 36 iter, 180000 ts, 101 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-17-08\n",
      "  done: false\n",
      "  episode_len_mean: 108.68\n",
      "  episode_reward_max: 305.04039816835575\n",
      "  episode_reward_mean: 120.89947818051157\n",
      "  episode_reward_min: -167.07630182427485\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 1287\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2688.382\n",
      "    load_time_ms: 1.536\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1668044328689575\n",
      "      kl: 0.0055258204229176044\n",
      "      policy_loss: -0.003529258305206895\n",
      "      total_loss: 855.052978515625\n",
      "      vf_explained_var: 0.07749753445386887\n",
      "      vf_loss: 855.0565185546875\n",
      "    sample_time_ms: 12169.595\n",
      "    update_time_ms: 9.385\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.44973909025579\n",
      "  time_since_restore: 562.2547769546509\n",
      "  time_this_iter_s: 14.822768449783325\n",
      "  time_total_s: 562.2547769546509\n",
      "  timestamp: 1556651828\n",
      "  timesteps_since_restore: 185000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 37\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 562 s, 37 iter, 185000 ts, 121 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-17-23\n",
      "  done: false\n",
      "  episode_len_mean: 110.28\n",
      "  episode_reward_max: 306.70082599595327\n",
      "  episode_reward_mean: 151.1735543732107\n",
      "  episode_reward_min: -167.07630182427485\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 1333\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2694.695\n",
      "    load_time_ms: 1.535\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9103830890414573e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1644980907440186\n",
      "      kl: 0.0018724086694419384\n",
      "      policy_loss: -0.0007291074143722653\n",
      "      total_loss: 798.0702514648438\n",
      "      vf_explained_var: 0.12041900306940079\n",
      "      vf_loss: 798.0709838867188\n",
      "    sample_time_ms: 12063.41\n",
      "    update_time_ms: 9.575\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.58677718660536\n",
      "  time_since_restore: 576.4439539909363\n",
      "  time_this_iter_s: 14.1891770362854\n",
      "  time_total_s: 576.4439539909363\n",
      "  timestamp: 1556651843\n",
      "  timesteps_since_restore: 190000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 38\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 576 s, 38 iter, 190000 ts, 151 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-17-38\n",
      "  done: false\n",
      "  episode_len_mean: 108.12\n",
      "  episode_reward_max: 307.90952731251537\n",
      "  episode_reward_mean: 151.74185224817177\n",
      "  episode_reward_min: -167.37003705137477\n",
      "  episodes_this_iter: 47\n",
      "  episodes_total: 1380\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2728.293\n",
      "    load_time_ms: 1.461\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1611472368240356\n",
      "      kl: 0.007623948156833649\n",
      "      policy_loss: -0.0027228754479438066\n",
      "      total_loss: 755.0162963867188\n",
      "      vf_explained_var: 0.13514740765094757\n",
      "      vf_loss: 755.0189208984375\n",
      "    sample_time_ms: 12068.659\n",
      "    update_time_ms: 9.482\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.87092612408587\n",
      "  time_since_restore: 591.6065001487732\n",
      "  time_this_iter_s: 15.162546157836914\n",
      "  time_total_s: 591.6065001487732\n",
      "  timestamp: 1556651858\n",
      "  timesteps_since_restore: 195000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 39\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 591 s, 39 iter, 195000 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-17-53\n",
      "  done: false\n",
      "  episode_len_mean: 109.6\n",
      "  episode_reward_max: 307.90952731251537\n",
      "  episode_reward_mean: 174.2501593482896\n",
      "  episode_reward_min: -167.37003705137477\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 1425\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2678.658\n",
      "    load_time_ms: 1.473\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.156301736831665\n",
      "      kl: 0.00650192704051733\n",
      "      policy_loss: -0.0024892729707062244\n",
      "      total_loss: 624.275634765625\n",
      "      vf_explained_var: 0.20063690841197968\n",
      "      vf_loss: 624.2781372070312\n",
      "    sample_time_ms: 12083.769\n",
      "    update_time_ms: 9.984\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 87.12507967414476\n",
      "  time_since_restore: 606.3368198871613\n",
      "  time_this_iter_s: 14.730319738388062\n",
      "  time_total_s: 606.3368198871613\n",
      "  timestamp: 1556651873\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 40\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 606 s, 40 iter, 200000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-18-11\n",
      "  done: false\n",
      "  episode_len_mean: 103.26\n",
      "  episode_reward_max: 304.89006657249365\n",
      "  episode_reward_mean: 131.35012153590154\n",
      "  episode_reward_min: -163.88499350441947\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 1476\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2709.449\n",
      "    load_time_ms: 1.473\n",
      "    num_steps_sampled: 205000\n",
      "    num_steps_trained: 205000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1508598327636719\n",
      "      kl: 0.010019052773714066\n",
      "      policy_loss: -0.0033540253061801195\n",
      "      total_loss: 1226.88330078125\n",
      "      vf_explained_var: 0.07277591526508331\n",
      "      vf_loss: 1226.8868408203125\n",
      "    sample_time_ms: 12379.463\n",
      "    update_time_ms: 10.71\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.67506076795077\n",
      "  time_since_restore: 624.2141363620758\n",
      "  time_this_iter_s: 17.87731647491455\n",
      "  time_total_s: 624.2141363620758\n",
      "  timestamp: 1556651891\n",
      "  timesteps_since_restore: 205000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 205000\n",
      "  training_iteration: 41\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 624 s, 41 iter, 205000 ts, 131 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-18-26\n",
      "  done: false\n",
      "  episode_len_mean: 99.44\n",
      "  episode_reward_max: 300.77836624764893\n",
      "  episode_reward_mean: 105.3422814494574\n",
      "  episode_reward_min: -165.0138887978151\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1526\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2723.58\n",
      "    load_time_ms: 1.474\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.166243553161621\n",
      "      kl: 0.003026565769687295\n",
      "      policy_loss: -0.0022669711615890265\n",
      "      total_loss: 990.4140625\n",
      "      vf_explained_var: 0.08937586098909378\n",
      "      vf_loss: 990.416259765625\n",
      "    sample_time_ms: 12459.173\n",
      "    update_time_ms: 10.793\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.6711407247287\n",
      "  time_since_restore: 639.6553528308868\n",
      "  time_this_iter_s: 15.441216468811035\n",
      "  time_total_s: 639.6553528308868\n",
      "  timestamp: 1556651906\n",
      "  timesteps_since_restore: 210000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 42\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 639 s, 42 iter, 210000 ts, 105 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-18-42\n",
      "  done: false\n",
      "  episode_len_mean: 103.43\n",
      "  episode_reward_max: 300.77836624764893\n",
      "  episode_reward_mean: 123.18134013683716\n",
      "  episode_reward_min: -165.0138887978151\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 1571\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2804.456\n",
      "    load_time_ms: 1.484\n",
      "    num_steps_sampled: 215000\n",
      "    num_steps_trained: 215000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8189894306509108e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.169293999671936\n",
      "      kl: 0.00487281521782279\n",
      "      policy_loss: -0.001347256824374199\n",
      "      total_loss: 847.3088989257812\n",
      "      vf_explained_var: 0.13326852023601532\n",
      "      vf_loss: 847.310302734375\n",
      "    sample_time_ms: 12369.675\n",
      "    update_time_ms: 10.776\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.590670068418575\n",
      "  time_since_restore: 655.0673398971558\n",
      "  time_this_iter_s: 15.411987066268921\n",
      "  time_total_s: 655.0673398971558\n",
      "  timestamp: 1556651922\n",
      "  timesteps_since_restore: 215000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 215000\n",
      "  training_iteration: 43\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 655 s, 43 iter, 215000 ts, 123 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-18-56\n",
      "  done: false\n",
      "  episode_len_mean: 103.37\n",
      "  episode_reward_max: 308.57630751325826\n",
      "  episode_reward_mean: 122.97240135460945\n",
      "  episode_reward_min: -166.88134229999048\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 1623\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2805.996\n",
      "    load_time_ms: 1.548\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1695175170898438\n",
      "      kl: 0.00738240871578455\n",
      "      policy_loss: -0.0026436601765453815\n",
      "      total_loss: 1236.875244140625\n",
      "      vf_explained_var: 0.07091265171766281\n",
      "      vf_loss: 1236.8780517578125\n",
      "    sample_time_ms: 12370.858\n",
      "    update_time_ms: 9.493\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.48620067730472\n",
      "  time_since_restore: 669.471893787384\n",
      "  time_this_iter_s: 14.404553890228271\n",
      "  time_total_s: 669.471893787384\n",
      "  timestamp: 1556651936\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 44\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 669 s, 44 iter, 220000 ts, 123 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-19-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.46\n",
      "  episode_reward_max: 308.57630751325826\n",
      "  episode_reward_mean: 120.62266671378927\n",
      "  episode_reward_min: -166.88134229999048\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 1671\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2810.709\n",
      "    load_time_ms: 1.551\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1701464653015137\n",
      "      kl: 0.0067734490148723125\n",
      "      policy_loss: -0.002243492053821683\n",
      "      total_loss: 891.9090576171875\n",
      "      vf_explained_var: 0.12908078730106354\n",
      "      vf_loss: 891.9112548828125\n",
      "    sample_time_ms: 12355.653\n",
      "    update_time_ms: 9.977\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.311333356894636\n",
      "  time_since_restore: 684.1474537849426\n",
      "  time_this_iter_s: 14.675559997558594\n",
      "  time_total_s: 684.1474537849426\n",
      "  timestamp: 1556651951\n",
      "  timesteps_since_restore: 225000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 45\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 684 s, 45 iter, 225000 ts, 121 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-19-26\n",
      "  done: false\n",
      "  episode_len_mean: 101.55\n",
      "  episode_reward_max: 307.13122503913763\n",
      "  episode_reward_mean: 135.8493106444618\n",
      "  episode_reward_min: -166.0047184222978\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 1722\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2784.126\n",
      "    load_time_ms: 1.576\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1606690883636475\n",
      "      kl: 0.006834616884589195\n",
      "      policy_loss: -0.0014463600236922503\n",
      "      total_loss: 992.0401611328125\n",
      "      vf_explained_var: 0.10625681281089783\n",
      "      vf_loss: 992.0415649414062\n",
      "    sample_time_ms: 12357.985\n",
      "    update_time_ms: 10.337\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.92465532223092\n",
      "  time_since_restore: 699.1361196041107\n",
      "  time_this_iter_s: 14.98866581916809\n",
      "  time_total_s: 699.1361196041107\n",
      "  timestamp: 1556651966\n",
      "  timesteps_since_restore: 230000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 46\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 699 s, 46 iter, 230000 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-19-41\n",
      "  done: false\n",
      "  episode_len_mean: 99.65\n",
      "  episode_reward_max: 307.13122503913763\n",
      "  episode_reward_mean: 120.26192068143753\n",
      "  episode_reward_min: -166.25178118147278\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 1771\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2788.125\n",
      "    load_time_ms: 1.573\n",
      "    num_steps_sampled: 235000\n",
      "    num_steps_trained: 235000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1368683941568192e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1496936082839966\n",
      "      kl: 0.00629250006750226\n",
      "      policy_loss: -0.002560162218287587\n",
      "      total_loss: 1098.5916748046875\n",
      "      vf_explained_var: 0.09313029795885086\n",
      "      vf_loss: 1098.59423828125\n",
      "    sample_time_ms: 12333.78\n",
      "    update_time_ms: 10.543\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.13096034071877\n",
      "  time_since_restore: 713.7626054286957\n",
      "  time_this_iter_s: 14.626485824584961\n",
      "  time_total_s: 713.7626054286957\n",
      "  timestamp: 1556651981\n",
      "  timesteps_since_restore: 235000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 235000\n",
      "  training_iteration: 47\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 713 s, 47 iter, 235000 ts, 120 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-19-55\n",
      "  done: false\n",
      "  episode_len_mean: 99.52\n",
      "  episode_reward_max: 306.0177255628034\n",
      "  episode_reward_mean: 117.55834648655934\n",
      "  episode_reward_min: -166.25178118147278\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1821\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2782.136\n",
      "    load_time_ms: 1.56\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1504676342010498\n",
      "      kl: 0.00628162594512105\n",
      "      policy_loss: -0.0016935255844146013\n",
      "      total_loss: 1057.5936279296875\n",
      "      vf_explained_var: 0.09789121896028519\n",
      "      vf_loss: 1057.5953369140625\n",
      "    sample_time_ms: 12385.669\n",
      "    update_time_ms: 11.259\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.77917324327966\n",
      "  time_since_restore: 728.4163417816162\n",
      "  time_this_iter_s: 14.653736352920532\n",
      "  time_total_s: 728.4163417816162\n",
      "  timestamp: 1556651995\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 48\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 728 s, 48 iter, 240000 ts, 118 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-20-10\n",
      "  done: false\n",
      "  episode_len_mean: 101.52\n",
      "  episode_reward_max: 306.0177255628034\n",
      "  episode_reward_mean: 127.07096760788117\n",
      "  episode_reward_min: -164.5507431945115\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 1870\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2774.098\n",
      "    load_time_ms: 1.634\n",
      "    num_steps_sampled: 245000\n",
      "    num_steps_trained: 245000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.842170985392048e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1364613771438599\n",
      "      kl: 0.00725970882922411\n",
      "      policy_loss: -0.002002191497012973\n",
      "      total_loss: 908.2328491210938\n",
      "      vf_explained_var: 0.13422782719135284\n",
      "      vf_loss: 908.23486328125\n",
      "    sample_time_ms: 12394.33\n",
      "    update_time_ms: 11.023\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.535483803940586\n",
      "  time_since_restore: 743.5865828990936\n",
      "  time_this_iter_s: 15.170241117477417\n",
      "  time_total_s: 743.5865828990936\n",
      "  timestamp: 1556652010\n",
      "  timesteps_since_restore: 245000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 245000\n",
      "  training_iteration: 49\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 743 s, 49 iter, 245000 ts, 127 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-20-25\n",
      "  done: false\n",
      "  episode_len_mean: 104.54\n",
      "  episode_reward_max: 301.40198387885687\n",
      "  episode_reward_mean: 137.4650104828604\n",
      "  episode_reward_min: -164.5507431945115\n",
      "  episodes_this_iter: 47\n",
      "  episodes_total: 1917\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2774.431\n",
      "    load_time_ms: 1.634\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.421085492696024e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1173509359359741\n",
      "      kl: 0.0053971610032022\n",
      "      policy_loss: -0.00044265598990023136\n",
      "      total_loss: 833.5034790039062\n",
      "      vf_explained_var: 0.16009211540222168\n",
      "      vf_loss: 833.5039672851562\n",
      "    sample_time_ms: 12395.306\n",
      "    update_time_ms: 10.593\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.7325052414302\n",
      "  time_since_restore: 758.3261766433716\n",
      "  time_this_iter_s: 14.739593744277954\n",
      "  time_total_s: 758.3261766433716\n",
      "  timestamp: 1556652025\n",
      "  timesteps_since_restore: 250000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 50\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 10.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 758 s, 50 iter, 250000 ts, 137 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-20-40\n",
      "  done: false\n",
      "  episode_len_mean: 103.91\n",
      "  episode_reward_max: 304.75588075392733\n",
      "  episode_reward_mean: 134.810764223969\n",
      "  episode_reward_min: -161.09065836004706\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 1966\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2725.991\n",
      "    load_time_ms: 1.628\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1036880016326904\n",
      "      kl: 0.0071923695504665375\n",
      "      policy_loss: -0.0025251805782318115\n",
      "      total_loss: 932.75\n",
      "      vf_explained_var: 0.13730938732624054\n",
      "      vf_loss: 932.7525024414062\n",
      "    sample_time_ms: 12090.463\n",
      "    update_time_ms: 9.695\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.40538211198451\n",
      "  time_since_restore: 772.6604001522064\n",
      "  time_this_iter_s: 14.334223508834839\n",
      "  time_total_s: 772.6604001522064\n",
      "  timestamp: 1556652040\n",
      "  timesteps_since_restore: 255000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 51\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 772 s, 51 iter, 255000 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-20-55\n",
      "  done: false\n",
      "  episode_len_mean: 104.97\n",
      "  episode_reward_max: 304.75588075392733\n",
      "  episode_reward_mean: 149.27924866112397\n",
      "  episode_reward_min: -168.4157366392522\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 2012\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2783.649\n",
      "    load_time_ms: 1.547\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.55271373174006e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0981320142745972\n",
      "      kl: 0.004829224199056625\n",
      "      policy_loss: -0.0015215390594676137\n",
      "      total_loss: 838.4501953125\n",
      "      vf_explained_var: 0.16244284808635712\n",
      "      vf_loss: 838.4517822265625\n",
      "    sample_time_ms: 11996.7\n",
      "    update_time_ms: 10.964\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.639624330562\n",
      "  time_since_restore: 787.7527143955231\n",
      "  time_this_iter_s: 15.09231424331665\n",
      "  time_total_s: 787.7527143955231\n",
      "  timestamp: 1556652055\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 52\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 787 s, 52 iter, 260000 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-21-09\n",
      "  done: false\n",
      "  episode_len_mean: 102.21\n",
      "  episode_reward_max: 303.18921086396784\n",
      "  episode_reward_mean: 118.30899887103875\n",
      "  episode_reward_min: -168.4157366392522\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 2065\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2697.992\n",
      "    load_time_ms: 1.548\n",
      "    num_steps_sampled: 265000\n",
      "    num_steps_trained: 265000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.77635686587003e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0820088386535645\n",
      "      kl: 0.00621216231957078\n",
      "      policy_loss: -0.004411743953824043\n",
      "      total_loss: 1416.9229736328125\n",
      "      vf_explained_var: 0.06451784819364548\n",
      "      vf_loss: 1416.9276123046875\n",
      "    sample_time_ms: 11977.636\n",
      "    update_time_ms: 11.524\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 59.15449943551936\n",
      "  time_since_restore: 802.1197452545166\n",
      "  time_this_iter_s: 14.36703085899353\n",
      "  time_total_s: 802.1197452545166\n",
      "  timestamp: 1556652069\n",
      "  timesteps_since_restore: 265000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 265000\n",
      "  training_iteration: 53\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 802 s, 53 iter, 265000 ts, 118 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-21-23\n",
      "  done: false\n",
      "  episode_len_mean: 97.97\n",
      "  episode_reward_max: 303.18921086396784\n",
      "  episode_reward_mean: 82.67370054109688\n",
      "  episode_reward_min: -164.24227236173263\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2115\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2697.515\n",
      "    load_time_ms: 1.485\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.88178432935015e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0880571603775024\n",
      "      kl: 0.005720173008739948\n",
      "      policy_loss: -0.0022681679110974073\n",
      "      total_loss: 1268.1697998046875\n",
      "      vf_explained_var: 0.08484773337841034\n",
      "      vf_loss: 1268.171875\n",
      "    sample_time_ms: 11955.093\n",
      "    update_time_ms: 12.275\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.33685027054844\n",
      "  time_since_restore: 816.2993540763855\n",
      "  time_this_iter_s: 14.179608821868896\n",
      "  time_total_s: 816.2993540763855\n",
      "  timestamp: 1556652083\n",
      "  timesteps_since_restore: 270000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 54\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 816 s, 54 iter, 270000 ts, 82.7 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-21-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.49\n",
      "  episode_reward_max: 307.1604807605338\n",
      "  episode_reward_mean: 103.95226364843678\n",
      "  episode_reward_min: -165.29861707553164\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 2164\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2729.283\n",
      "    load_time_ms: 1.488\n",
      "    num_steps_sampled: 275000\n",
      "    num_steps_trained: 275000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0797197818756104\n",
      "      kl: 0.003807185450568795\n",
      "      policy_loss: -0.0022368961945176125\n",
      "      total_loss: 1107.316162109375\n",
      "      vf_explained_var: 0.10080625116825104\n",
      "      vf_loss: 1107.3182373046875\n",
      "    sample_time_ms: 12008.184\n",
      "    update_time_ms: 11.779\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.976131824218385\n",
      "  time_since_restore: 831.8183476924896\n",
      "  time_this_iter_s: 15.518993616104126\n",
      "  time_total_s: 831.8183476924896\n",
      "  timestamp: 1556652099\n",
      "  timesteps_since_restore: 275000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 275000\n",
      "  training_iteration: 55\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 831 s, 55 iter, 275000 ts, 104 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-21-53\n",
      "  done: false\n",
      "  episode_len_mean: 101.46\n",
      "  episode_reward_max: 308.4027883344504\n",
      "  episode_reward_mean: 115.78672560410439\n",
      "  episode_reward_min: -165.29861707553164\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2214\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2733.545\n",
      "    load_time_ms: 1.466\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2204460823375376e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0896440744400024\n",
      "      kl: 0.004563904367387295\n",
      "      policy_loss: -0.0013830092502757907\n",
      "      total_loss: 1164.351318359375\n",
      "      vf_explained_var: 0.08918863534927368\n",
      "      vf_loss: 1164.3526611328125\n",
      "    sample_time_ms: 11917.611\n",
      "    update_time_ms: 11.284\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.89336280205218\n",
      "  time_since_restore: 845.9385931491852\n",
      "  time_this_iter_s: 14.120245456695557\n",
      "  time_total_s: 845.9385931491852\n",
      "  timestamp: 1556652113\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 56\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 845 s, 56 iter, 280000 ts, 116 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-22-08\n",
      "  done: false\n",
      "  episode_len_mean: 104.35\n",
      "  episode_reward_max: 308.4027883344504\n",
      "  episode_reward_mean: 134.36736085054648\n",
      "  episode_reward_min: -163.1418515671905\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 2260\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2736.649\n",
      "    load_time_ms: 1.467\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1102230411687688e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0951589345932007\n",
      "      kl: 0.007828345522284508\n",
      "      policy_loss: -0.003836517222225666\n",
      "      total_loss: 877.990966796875\n",
      "      vf_explained_var: 0.1535860151052475\n",
      "      vf_loss: 877.9948120117188\n",
      "    sample_time_ms: 11947.43\n",
      "    update_time_ms: 11.971\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.18368042527324\n",
      "  time_since_restore: 860.8974165916443\n",
      "  time_this_iter_s: 14.958823442459106\n",
      "  time_total_s: 860.8974165916443\n",
      "  timestamp: 1556652128\n",
      "  timesteps_since_restore: 285000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 57\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 860 s, 57 iter, 285000 ts, 134 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-22-23\n",
      "  done: false\n",
      "  episode_len_mean: 103.02\n",
      "  episode_reward_max: 308.3469950590029\n",
      "  episode_reward_mean: 137.10317526261133\n",
      "  episode_reward_min: -166.0160305985495\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 2311\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2753.523\n",
      "    load_time_ms: 1.554\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.551115205843844e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1023685932159424\n",
      "      kl: 0.004672559909522533\n",
      "      policy_loss: -0.0017678227741271257\n",
      "      total_loss: 1073.2852783203125\n",
      "      vf_explained_var: 0.11239149421453476\n",
      "      vf_loss: 1073.287109375\n",
      "    sample_time_ms: 11914.386\n",
      "    update_time_ms: 11.412\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.55158763130568\n",
      "  time_since_restore: 875.3867671489716\n",
      "  time_this_iter_s: 14.48935055732727\n",
      "  time_total_s: 875.3867671489716\n",
      "  timestamp: 1556652143\n",
      "  timesteps_since_restore: 290000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 58\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 875 s, 58 iter, 290000 ts, 137 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-22-38\n",
      "  done: false\n",
      "  episode_len_mean: 102.05\n",
      "  episode_reward_max: 308.3469950590029\n",
      "  episode_reward_mean: 134.6237586198649\n",
      "  episode_reward_min: -166.0160305985495\n",
      "  episodes_this_iter: 47\n",
      "  episodes_total: 2358\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2765.04\n",
      "    load_time_ms: 1.518\n",
      "    num_steps_sampled: 295000\n",
      "    num_steps_trained: 295000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.775557602921922e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0952858924865723\n",
      "      kl: 0.011217815801501274\n",
      "      policy_loss: -0.0024208480026572943\n",
      "      total_loss: 817.4066162109375\n",
      "      vf_explained_var: 0.188696026802063\n",
      "      vf_loss: 817.4090576171875\n",
      "    sample_time_ms: 11903.033\n",
      "    update_time_ms: 13.101\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.31187930993246\n",
      "  time_since_restore: 890.5727467536926\n",
      "  time_this_iter_s: 15.18597960472107\n",
      "  time_total_s: 890.5727467536926\n",
      "  timestamp: 1556652158\n",
      "  timesteps_since_restore: 295000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 295000\n",
      "  training_iteration: 59\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 890 s, 59 iter, 295000 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-22-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.52\n",
      "  episode_reward_max: 306.57238833330354\n",
      "  episode_reward_mean: 125.41956396046153\n",
      "  episode_reward_min: -164.32342776976222\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 2409\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2764.297\n",
      "    load_time_ms: 1.566\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.775557602921922e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0968317985534668\n",
      "      kl: 0.0025609321892261505\n",
      "      policy_loss: -4.875139347859658e-05\n",
      "      total_loss: 1131.922607421875\n",
      "      vf_explained_var: 0.12151918560266495\n",
      "      vf_loss: 1131.9227294921875\n",
      "    sample_time_ms: 11848.794\n",
      "    update_time_ms: 13.017\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.709781980230765\n",
      "  time_since_restore: 904.7624428272247\n",
      "  time_this_iter_s: 14.189696073532104\n",
      "  time_total_s: 904.7624428272247\n",
      "  timestamp: 1556652172\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 60\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 904 s, 60 iter, 300000 ts, 125 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-23-07\n",
      "  done: false\n",
      "  episode_len_mean: 97.89\n",
      "  episode_reward_max: 306.57238833330354\n",
      "  episode_reward_mean: 111.71371448700587\n",
      "  episode_reward_min: -164.32342776976222\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 2460\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2845.563\n",
      "    load_time_ms: 1.696\n",
      "    num_steps_sampled: 305000\n",
      "    num_steps_trained: 305000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.387778801460961e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0924493074417114\n",
      "      kl: 0.002980972407385707\n",
      "      policy_loss: -0.0009940889431163669\n",
      "      total_loss: 1136.564453125\n",
      "      vf_explained_var: 0.13215944170951843\n",
      "      vf_loss: 1136.5653076171875\n",
      "    sample_time_ms: 11819.335\n",
      "    update_time_ms: 13.448\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.85685724350293\n",
      "  time_since_restore: 919.6259670257568\n",
      "  time_this_iter_s: 14.863524198532104\n",
      "  time_total_s: 919.6259670257568\n",
      "  timestamp: 1556652187\n",
      "  timesteps_since_restore: 305000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 305000\n",
      "  training_iteration: 61\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 919 s, 61 iter, 305000 ts, 112 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-23-22\n",
      "  done: false\n",
      "  episode_len_mean: 96.53\n",
      "  episode_reward_max: 298.80240434569635\n",
      "  episode_reward_mean: 96.59195357478382\n",
      "  episode_reward_min: -165.35317505155666\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 2513\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2770.838\n",
      "    load_time_ms: 1.699\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.938894007304805e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.086759090423584\n",
      "      kl: 0.0068803331814706326\n",
      "      policy_loss: -0.0026072922628372908\n",
      "      total_loss: 1407.245849609375\n",
      "      vf_explained_var: 0.08633076399564743\n",
      "      vf_loss: 1407.2486572265625\n",
      "    sample_time_ms: 11855.056\n",
      "    update_time_ms: 12.521\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.2959767873919\n",
      "  time_since_restore: 934.3137395381927\n",
      "  time_this_iter_s: 14.687772512435913\n",
      "  time_total_s: 934.3137395381927\n",
      "  timestamp: 1556652202\n",
      "  timesteps_since_restore: 310000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 62\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 934 s, 62 iter, 310000 ts, 96.6 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-23-39\n",
      "  done: false\n",
      "  episode_len_mean: 93.5\n",
      "  episode_reward_max: 309.9315296439559\n",
      "  episode_reward_mean: 74.22258246705894\n",
      "  episode_reward_min: -166.63830495373165\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 2567\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2835.352\n",
      "    load_time_ms: 1.698\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.4694470036524025e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.104573369026184\n",
      "      kl: 0.004147042520344257\n",
      "      policy_loss: -0.0021152144763618708\n",
      "      total_loss: 1406.455810546875\n",
      "      vf_explained_var: 0.08985575288534164\n",
      "      vf_loss: 1406.4578857421875\n",
      "    sample_time_ms: 12111.571\n",
      "    update_time_ms: 11.981\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.11129123352948\n",
      "  time_since_restore: 951.8897175788879\n",
      "  time_this_iter_s: 17.57597804069519\n",
      "  time_total_s: 951.8897175788879\n",
      "  timestamp: 1556652219\n",
      "  timesteps_since_restore: 315000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 63\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 951 s, 63 iter, 315000 ts, 74.2 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-23-54\n",
      "  done: false\n",
      "  episode_len_mean: 95.33\n",
      "  episode_reward_max: 309.9315296439559\n",
      "  episode_reward_mean: 100.33589402209202\n",
      "  episode_reward_min: -166.63830495373165\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 2618\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2843.982\n",
      "    load_time_ms: 1.818\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0907366275787354\n",
      "      kl: 0.005051010753959417\n",
      "      policy_loss: -0.0013259805273264647\n",
      "      total_loss: 1026.6905517578125\n",
      "      vf_explained_var: 0.13434603810310364\n",
      "      vf_loss: 1026.6920166015625\n",
      "    sample_time_ms: 12179.673\n",
      "    update_time_ms: 11.539\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.16794701104602\n",
      "  time_since_restore: 966.833625793457\n",
      "  time_this_iter_s: 14.943908214569092\n",
      "  time_total_s: 966.833625793457\n",
      "  timestamp: 1556652234\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 64\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 966 s, 64 iter, 320000 ts, 100 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-24-09\n",
      "  done: false\n",
      "  episode_len_mean: 97.9\n",
      "  episode_reward_max: 302.0357784009515\n",
      "  episode_reward_mean: 116.89091885024729\n",
      "  episode_reward_min: -166.3449416904136\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 2669\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2812.199\n",
      "    load_time_ms: 1.812\n",
      "    num_steps_sampled: 325000\n",
      "    num_steps_trained: 325000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0943360328674316\n",
      "      kl: 0.004946333821862936\n",
      "      policy_loss: -0.0030601229518651962\n",
      "      total_loss: 1161.53662109375\n",
      "      vf_explained_var: 0.1210327297449112\n",
      "      vf_loss: 1161.5399169921875\n",
      "    sample_time_ms: 12114.52\n",
      "    update_time_ms: 12.759\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.445459425123644\n",
      "  time_since_restore: 981.3941538333893\n",
      "  time_this_iter_s: 14.560528039932251\n",
      "  time_total_s: 981.3941538333893\n",
      "  timestamp: 1556652249\n",
      "  timesteps_since_restore: 325000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 325000\n",
      "  training_iteration: 65\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 981 s, 65 iter, 325000 ts, 117 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-24-24\n",
      "  done: false\n",
      "  episode_len_mean: 97.2\n",
      "  episode_reward_max: 302.0357784009515\n",
      "  episode_reward_mean: 101.16649965087063\n",
      "  episode_reward_min: -161.8224315901213\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 2720\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2826.401\n",
      "    load_time_ms: 1.836\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.336808754565503e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.082587718963623\n",
      "      kl: 0.005243158433586359\n",
      "      policy_loss: -0.0014533903449773788\n",
      "      total_loss: 1219.61474609375\n",
      "      vf_explained_var: 0.1292554885149002\n",
      "      vf_loss: 1219.6163330078125\n",
      "    sample_time_ms: 12168.151\n",
      "    update_time_ms: 12.517\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.5832498254353\n",
      "  time_since_restore: 996.1881608963013\n",
      "  time_this_iter_s: 14.794007062911987\n",
      "  time_total_s: 996.1881608963013\n",
      "  timestamp: 1556652264\n",
      "  timesteps_since_restore: 330000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 66\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 996 s, 66 iter, 330000 ts, 101 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-24-39\n",
      "  done: false\n",
      "  episode_len_mean: 97.85\n",
      "  episode_reward_max: 299.6123325669608\n",
      "  episode_reward_mean: 112.95737098121646\n",
      "  episode_reward_min: -156.8258828750067\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 2770\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2824.155\n",
      "    load_time_ms: 1.848\n",
      "    num_steps_sampled: 335000\n",
      "    num_steps_trained: 335000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.1684043772827515e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0714919567108154\n",
      "      kl: 0.0024036907125264406\n",
      "      policy_loss: -0.001124037429690361\n",
      "      total_loss: 904.9825439453125\n",
      "      vf_explained_var: 0.17732588946819305\n",
      "      vf_loss: 904.9837646484375\n",
      "    sample_time_ms: 12156.461\n",
      "    update_time_ms: 12.123\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.47868549060823\n",
      "  time_since_restore: 1011.0039761066437\n",
      "  time_this_iter_s: 14.815815210342407\n",
      "  time_total_s: 1011.0039761066437\n",
      "  timestamp: 1556652279\n",
      "  timesteps_since_restore: 335000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 335000\n",
      "  training_iteration: 67\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1011 s, 67 iter, 335000 ts, 113 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-24-54\n",
      "  done: false\n",
      "  episode_len_mean: 96.68\n",
      "  episode_reward_max: 299.6123325669608\n",
      "  episode_reward_mean: 115.18262739530863\n",
      "  episode_reward_min: -165.14718919714366\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 2823\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2808.824\n",
      "    load_time_ms: 1.775\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842021886413758e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0741655826568604\n",
      "      kl: 0.007310081738978624\n",
      "      policy_loss: -0.002385462634265423\n",
      "      total_loss: 1118.5882568359375\n",
      "      vf_explained_var: 0.13321733474731445\n",
      "      vf_loss: 1118.5908203125\n",
      "    sample_time_ms: 12204.353\n",
      "    update_time_ms: 12.012\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.591313697654314\n",
      "  time_since_restore: 1025.8183765411377\n",
      "  time_this_iter_s: 14.814400434494019\n",
      "  time_total_s: 1025.8183765411377\n",
      "  timestamp: 1556652294\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 68\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1025 s, 68 iter, 340000 ts, 115 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-25-09\n",
      "  done: false\n",
      "  episode_len_mean: 93.45\n",
      "  episode_reward_max: 313.8803750363147\n",
      "  episode_reward_mean: 97.95269460728656\n",
      "  episode_reward_min: -166.4933739439923\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 2878\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2777.385\n",
      "    load_time_ms: 1.753\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.421010943206879e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0542528629302979\n",
      "      kl: 0.008511857129633427\n",
      "      policy_loss: -0.0034293311182409525\n",
      "      total_loss: 1223.995849609375\n",
      "      vf_explained_var: 0.12684139609336853\n",
      "      vf_loss: 1223.9993896484375\n",
      "    sample_time_ms: 12222.081\n",
      "    update_time_ms: 10.538\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.97634730364328\n",
      "  time_since_restore: 1040.8502514362335\n",
      "  time_this_iter_s: 15.031874895095825\n",
      "  time_total_s: 1040.8502514362335\n",
      "  timestamp: 1556652309\n",
      "  timesteps_since_restore: 345000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 69\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1040 s, 69 iter, 345000 ts, 98 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-25-23\n",
      "  done: false\n",
      "  episode_len_mean: 94.83\n",
      "  episode_reward_max: 313.8803750363147\n",
      "  episode_reward_mean: 102.2547067685759\n",
      "  episode_reward_min: -166.99961018569064\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 2929\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2779.63\n",
      "    load_time_ms: 1.701\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.7105054716034394e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0629703998565674\n",
      "      kl: 0.0025201747193932533\n",
      "      policy_loss: -0.0012547887163236737\n",
      "      total_loss: 1146.0074462890625\n",
      "      vf_explained_var: 0.11661671102046967\n",
      "      vf_loss: 1146.0086669921875\n",
      "    sample_time_ms: 12259.634\n",
      "    update_time_ms: 10.739\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.12735338428795\n",
      "  time_since_restore: 1055.4389626979828\n",
      "  time_this_iter_s: 14.588711261749268\n",
      "  time_total_s: 1055.4389626979828\n",
      "  timestamp: 1556652323\n",
      "  timesteps_since_restore: 350000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 70\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1055 s, 70 iter, 350000 ts, 102 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-25-38\n",
      "  done: false\n",
      "  episode_len_mean: 98.29\n",
      "  episode_reward_max: 304.1460483673539\n",
      "  episode_reward_mean: 107.96869848036312\n",
      "  episode_reward_min: -166.99961018569064\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 2980\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2713.362\n",
      "    load_time_ms: 1.571\n",
      "    num_steps_sampled: 355000\n",
      "    num_steps_trained: 355000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3552527358017197e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0734288692474365\n",
      "      kl: 0.0019338878337293863\n",
      "      policy_loss: -0.0003392550570424646\n",
      "      total_loss: 1198.1375732421875\n",
      "      vf_explained_var: 0.14433880150318146\n",
      "      vf_loss: 1198.137939453125\n",
      "    sample_time_ms: 12287.71\n",
      "    update_time_ms: 10.671\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.984349240181565\n",
      "  time_since_restore: 1069.9148933887482\n",
      "  time_this_iter_s: 14.47593069076538\n",
      "  time_total_s: 1069.9148933887482\n",
      "  timestamp: 1556652338\n",
      "  timesteps_since_restore: 355000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 355000\n",
      "  training_iteration: 71\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1069 s, 71 iter, 355000 ts, 108 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-25-55\n",
      "  done: false\n",
      "  episode_len_mean: 98.89\n",
      "  episode_reward_max: 303.3232482266013\n",
      "  episode_reward_mean: 115.31380429429258\n",
      "  episode_reward_min: -164.31492301867723\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3030\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2878.34\n",
      "    load_time_ms: 1.746\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.776263679008599e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0799401998519897\n",
      "      kl: 0.0030241182539612055\n",
      "      policy_loss: -0.001200423575937748\n",
      "      total_loss: 905.7265625\n",
      "      vf_explained_var: 0.15664935111999512\n",
      "      vf_loss: 905.7278442382812\n",
      "    sample_time_ms: 12309.792\n",
      "    update_time_ms: 10.695\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.656902147146305\n",
      "  time_since_restore: 1086.4822878837585\n",
      "  time_this_iter_s: 16.567394495010376\n",
      "  time_total_s: 1086.4822878837585\n",
      "  timestamp: 1556652355\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 72\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1086 s, 72 iter, 360000 ts, 115 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-26-11\n",
      "  done: false\n",
      "  episode_len_mean: 99.38\n",
      "  episode_reward_max: 304.52962206046425\n",
      "  episode_reward_mean: 127.03437093238736\n",
      "  episode_reward_min: -165.11165228531183\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 3081\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2836.776\n",
      "    load_time_ms: 1.741\n",
      "    num_steps_sampled: 365000\n",
      "    num_steps_trained: 365000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.3881318395042993e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0742813348770142\n",
      "      kl: 0.005060110241174698\n",
      "      policy_loss: -0.000869046023581177\n",
      "      total_loss: 944.0115966796875\n",
      "      vf_explained_var: 0.17828606069087982\n",
      "      vf_loss: 944.01220703125\n",
      "    sample_time_ms: 12228.381\n",
      "    update_time_ms: 10.995\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.51718546619368\n",
      "  time_since_restore: 1102.830646276474\n",
      "  time_this_iter_s: 16.348358392715454\n",
      "  time_total_s: 1102.830646276474\n",
      "  timestamp: 1556652371\n",
      "  timesteps_since_restore: 365000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 365000\n",
      "  training_iteration: 73\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1102 s, 73 iter, 365000 ts, 127 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-26-26\n",
      "  done: false\n",
      "  episode_len_mean: 96.2\n",
      "  episode_reward_max: 304.52962206046425\n",
      "  episode_reward_mean: 107.71124770465921\n",
      "  episode_reward_min: -166.1593334752532\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 3135\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2842.468\n",
      "    load_time_ms: 1.629\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6940659197521496e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0684832334518433\n",
      "      kl: 0.006398432422429323\n",
      "      policy_loss: -0.0015889637870714068\n",
      "      total_loss: 1278.0155029296875\n",
      "      vf_explained_var: 0.09677306562662125\n",
      "      vf_loss: 1278.0172119140625\n",
      "    sample_time_ms: 12263.104\n",
      "    update_time_ms: 10.812\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.8556238523296\n",
      "  time_since_restore: 1118.1778600215912\n",
      "  time_this_iter_s: 15.347213745117188\n",
      "  time_total_s: 1118.1778600215912\n",
      "  timestamp: 1556652386\n",
      "  timesteps_since_restore: 370000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 74\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1118 s, 74 iter, 370000 ts, 108 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-26-41\n",
      "  done: false\n",
      "  episode_len_mean: 94.09\n",
      "  episode_reward_max: 303.62731327958187\n",
      "  episode_reward_mean: 90.67285237952346\n",
      "  episode_reward_min: -166.1593334752532\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 3187\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2838.558\n",
      "    load_time_ms: 1.626\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.470329598760748e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0749057531356812\n",
      "      kl: 0.00452598137781024\n",
      "      policy_loss: -0.002034089993685484\n",
      "      total_loss: 1085.9766845703125\n",
      "      vf_explained_var: 0.12341751158237457\n",
      "      vf_loss: 1085.978759765625\n",
      "    sample_time_ms: 12279.769\n",
      "    update_time_ms: 9.876\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.33642618976173\n",
      "  time_since_restore: 1132.8592429161072\n",
      "  time_this_iter_s: 14.681382894515991\n",
      "  time_total_s: 1132.8592429161072\n",
      "  timestamp: 1556652401\n",
      "  timesteps_since_restore: 375000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 75\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1132 s, 75 iter, 375000 ts, 90.7 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-26-56\n",
      "  done: false\n",
      "  episode_len_mean: 101.19\n",
      "  episode_reward_max: 303.8611450623784\n",
      "  episode_reward_mean: 134.57568129133807\n",
      "  episode_reward_min: -162.7213851065411\n",
      "  episodes_this_iter: 47\n",
      "  episodes_total: 3234\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2862.792\n",
      "    load_time_ms: 1.58\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.235164799380374e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0409070253372192\n",
      "      kl: 0.00898787658661604\n",
      "      policy_loss: -0.004790260456502438\n",
      "      total_loss: 691.4677734375\n",
      "      vf_explained_var: 0.18349571526050568\n",
      "      vf_loss: 691.4725952148438\n",
      "    sample_time_ms: 12254.55\n",
      "    update_time_ms: 10.777\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.28784064566904\n",
      "  time_since_restore: 1147.6527705192566\n",
      "  time_this_iter_s: 14.793527603149414\n",
      "  time_total_s: 1147.6527705192566\n",
      "  timestamp: 1556652416\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 76\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1147 s, 76 iter, 380000 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-27-12\n",
      "  done: false\n",
      "  episode_len_mean: 102.92\n",
      "  episode_reward_max: 304.02701093942375\n",
      "  episode_reward_mean: 137.82488270354452\n",
      "  episode_reward_min: -162.7213851065411\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3284\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2891.879\n",
      "    load_time_ms: 1.569\n",
      "    num_steps_sampled: 385000\n",
      "    num_steps_trained: 385000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.117582399690187e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.059496283531189\n",
      "      kl: 0.0060229613445699215\n",
      "      policy_loss: -0.002497714478522539\n",
      "      total_loss: 1054.4244384765625\n",
      "      vf_explained_var: 0.1804078221321106\n",
      "      vf_loss: 1054.4267578125\n",
      "    sample_time_ms: 12296.856\n",
      "    update_time_ms: 10.841\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.91244135177224\n",
      "  time_since_restore: 1163.1820709705353\n",
      "  time_this_iter_s: 15.529300451278687\n",
      "  time_total_s: 1163.1820709705353\n",
      "  timestamp: 1556652432\n",
      "  timesteps_since_restore: 385000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 385000\n",
      "  training_iteration: 77\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1163 s, 77 iter, 385000 ts, 138 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-27-27\n",
      "  done: false\n",
      "  episode_len_mean: 97.47\n",
      "  episode_reward_max: 304.02701093942375\n",
      "  episode_reward_mean: 105.22666531560844\n",
      "  episode_reward_min: -158.69215435770806\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 3336\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2889.701\n",
      "    load_time_ms: 1.568\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0511034727096558\n",
      "      kl: 0.004600574728101492\n",
      "      policy_loss: -0.00129034579731524\n",
      "      total_loss: 1103.6715087890625\n",
      "      vf_explained_var: 0.13526397943496704\n",
      "      vf_loss: 1103.6727294921875\n",
      "    sample_time_ms: 12372.491\n",
      "    update_time_ms: 11.377\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.61333265780422\n",
      "  time_since_restore: 1178.7338852882385\n",
      "  time_this_iter_s: 15.551814317703247\n",
      "  time_total_s: 1178.7338852882385\n",
      "  timestamp: 1556652447\n",
      "  timesteps_since_restore: 390000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 78\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1178 s, 78 iter, 390000 ts, 105 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-27-42\n",
      "  done: false\n",
      "  episode_len_mean: 93.93\n",
      "  episode_reward_max: 303.19935794693356\n",
      "  episode_reward_mean: 97.47621008158653\n",
      "  episode_reward_min: -167.08020508004836\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 3389\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2884.404\n",
      "    load_time_ms: 1.551\n",
      "    num_steps_sampled: 395000\n",
      "    num_steps_trained: 395000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0322192907333374\n",
      "      kl: 0.00669130077585578\n",
      "      policy_loss: -0.0018171400297433138\n",
      "      total_loss: 1058.98681640625\n",
      "      vf_explained_var: 0.1438632756471634\n",
      "      vf_loss: 1058.988525390625\n",
      "    sample_time_ms: 12374.121\n",
      "    update_time_ms: 11.388\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.738105040793265\n",
      "  time_since_restore: 1193.7306780815125\n",
      "  time_this_iter_s: 14.996792793273926\n",
      "  time_total_s: 1193.7306780815125\n",
      "  timestamp: 1556652462\n",
      "  timesteps_since_restore: 395000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 395000\n",
      "  training_iteration: 79\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1193 s, 79 iter, 395000 ts, 97.5 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-27-57\n",
      "  done: false\n",
      "  episode_len_mean: 98.11\n",
      "  episode_reward_max: 305.2566356873479\n",
      "  episode_reward_mean: 123.36413197243292\n",
      "  episode_reward_min: -167.08020508004836\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 3438\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2904.225\n",
      "    load_time_ms: 1.554\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.646977999612734e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.028026819229126\n",
      "      kl: 0.008563682436943054\n",
      "      policy_loss: -0.003466205671429634\n",
      "      total_loss: 817.2395629882812\n",
      "      vf_explained_var: 0.18751613795757294\n",
      "      vf_loss: 817.2430419921875\n",
      "    sample_time_ms: 12389.467\n",
      "    update_time_ms: 11.236\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.68206598621646\n",
      "  time_since_restore: 1208.670358657837\n",
      "  time_this_iter_s: 14.939680576324463\n",
      "  time_total_s: 1208.670358657837\n",
      "  timestamp: 1556652477\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 80\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1208 s, 80 iter, 400000 ts, 123 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-28-19\n",
      "  done: false\n",
      "  episode_len_mean: 98.48\n",
      "  episode_reward_max: 305.2566356873479\n",
      "  episode_reward_mean: 124.76768103689595\n",
      "  episode_reward_min: -163.2843129054728\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 3491\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.06\n",
      "    load_time_ms: 1.565\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.323488999806367e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0245715379714966\n",
      "      kl: 0.007298080716282129\n",
      "      policy_loss: -0.00341263459995389\n",
      "      total_loss: 1139.843994140625\n",
      "      vf_explained_var: 0.14153778553009033\n",
      "      vf_loss: 1139.84716796875\n",
      "    sample_time_ms: 12869.206\n",
      "    update_time_ms: 11.033\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.383840518447975\n",
      "  time_since_restore: 1230.3439228534698\n",
      "  time_this_iter_s: 21.673564195632935\n",
      "  time_total_s: 1230.3439228534698\n",
      "  timestamp: 1556652499\n",
      "  timesteps_since_restore: 405000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 81\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1230 s, 81 iter, 405000 ts, 125 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-28-39\n",
      "  done: false\n",
      "  episode_len_mean: 97.84\n",
      "  episode_reward_max: 304.0408483115213\n",
      "  episode_reward_mean: 125.78214590623917\n",
      "  episode_reward_min: -163.2843129054728\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3541\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3105.797\n",
      "    load_time_ms: 1.883\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.617444999031835e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9917829632759094\n",
      "      kl: 0.00545762712135911\n",
      "      policy_loss: -0.0012793452478945255\n",
      "      total_loss: 906.0020141601562\n",
      "      vf_explained_var: 0.16005928814411163\n",
      "      vf_loss: 906.0031127929688\n",
      "    sample_time_ms: 13296.471\n",
      "    update_time_ms: 12.033\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.89107295311958\n",
      "  time_since_restore: 1250.8189640045166\n",
      "  time_this_iter_s: 20.475041151046753\n",
      "  time_total_s: 1250.8189640045166\n",
      "  timestamp: 1556652519\n",
      "  timesteps_since_restore: 410000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 82\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1250 s, 82 iter, 410000 ts, 126 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-29-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.44\n",
      "  episode_reward_max: 305.56625206949485\n",
      "  episode_reward_mean: 131.239162553112\n",
      "  episode_reward_min: -163.60637205697668\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 3590\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3433.783\n",
      "    load_time_ms: 2.131\n",
      "    num_steps_sampled: 415000\n",
      "    num_steps_trained: 415000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.3087224995159173e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9800689816474915\n",
      "      kl: 0.009372947737574577\n",
      "      policy_loss: -0.002825820352882147\n",
      "      total_loss: 998.8040161132812\n",
      "      vf_explained_var: 0.2143029421567917\n",
      "      vf_loss: 998.806884765625\n",
      "    sample_time_ms: 14025.338\n",
      "    update_time_ms: 11.997\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.61958127655602\n",
      "  time_since_restore: 1277.7694101333618\n",
      "  time_this_iter_s: 26.950446128845215\n",
      "  time_total_s: 1277.7694101333618\n",
      "  timestamp: 1556652546\n",
      "  timesteps_since_restore: 415000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 415000\n",
      "  training_iteration: 83\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1277 s, 83 iter, 415000 ts, 131 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-29-58\n",
      "  done: false\n",
      "  episode_len_mean: 102.6\n",
      "  episode_reward_max: 305.56625206949485\n",
      "  episode_reward_mean: 151.3590980500517\n",
      "  episode_reward_min: -163.60637205697668\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 3639\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4556.731\n",
      "    load_time_ms: 2.404\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6543612497579586e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9658523797988892\n",
      "      kl: 0.0036912269424647093\n",
      "      policy_loss: -0.0006641390500590205\n",
      "      total_loss: 631.5906372070312\n",
      "      vf_explained_var: 0.2774967551231384\n",
      "      vf_loss: 631.59130859375\n",
      "    sample_time_ms: 16495.554\n",
      "    update_time_ms: 13.857\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.67954902502584\n",
      "  time_since_restore: 1329.1315479278564\n",
      "  time_this_iter_s: 51.36213779449463\n",
      "  time_total_s: 1329.1315479278564\n",
      "  timestamp: 1556652598\n",
      "  timesteps_since_restore: 420000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 84\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1329 s, 84 iter, 420000 ts, 151 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-31-07\n",
      "  done: false\n",
      "  episode_len_mean: 94.63\n",
      "  episode_reward_max: 306.17206489874206\n",
      "  episode_reward_mean: 115.43071525208651\n",
      "  episode_reward_min: -167.81482802582315\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 3695\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6032.6\n",
      "    load_time_ms: 2.629\n",
      "    num_steps_sampled: 425000\n",
      "    num_steps_trained: 425000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.271806248789793e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9716457724571228\n",
      "      kl: 0.004472449421882629\n",
      "      policy_loss: -0.0013095461763441563\n",
      "      total_loss: 1256.431640625\n",
      "      vf_explained_var: 0.14977169036865234\n",
      "      vf_loss: 1256.4327392578125\n",
      "    sample_time_ms: 20461.611\n",
      "    update_time_ms: 15.789\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.71535762604324\n",
      "  time_since_restore: 1398.3395552635193\n",
      "  time_this_iter_s: 69.20800733566284\n",
      "  time_total_s: 1398.3395552635193\n",
      "  timestamp: 1556652667\n",
      "  timesteps_since_restore: 425000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 425000\n",
      "  training_iteration: 85\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1398 s, 85 iter, 425000 ts, 115 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-32-20\n",
      "  done: false\n",
      "  episode_len_mean: 93.77\n",
      "  episode_reward_max: 303.49580493075774\n",
      "  episode_reward_mean: 107.65596052959154\n",
      "  episode_reward_min: -167.81482802582315\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3745\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7106.033\n",
      "    load_time_ms: 2.926\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.1359031243948966e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9384802579879761\n",
      "      kl: 0.003820632118731737\n",
      "      policy_loss: -0.00044804581557400525\n",
      "      total_loss: 885.5961303710938\n",
      "      vf_explained_var: 0.21192440390586853\n",
      "      vf_loss: 885.5965576171875\n",
      "    sample_time_ms: 25103.584\n",
      "    update_time_ms: 20.36\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.82798026479578\n",
      "  time_since_restore: 1470.3582062721252\n",
      "  time_this_iter_s: 72.01865100860596\n",
      "  time_total_s: 1470.3582062721252\n",
      "  timestamp: 1556652740\n",
      "  timesteps_since_restore: 430000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 86\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1470 s, 86 iter, 430000 ts, 108 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-33-16\n",
      "  done: false\n",
      "  episode_len_mean: 98.99\n",
      "  episode_reward_max: 303.49580493075774\n",
      "  episode_reward_mean: 136.77048304636583\n",
      "  episode_reward_min: -166.5416462144346\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 3796\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8007.435\n",
      "    load_time_ms: 3.557\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.0679515621974483e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9245396852493286\n",
      "      kl: 0.004328000824898481\n",
      "      policy_loss: -0.0018994262209162116\n",
      "      total_loss: 816.5201416015625\n",
      "      vf_explained_var: 0.23317737877368927\n",
      "      vf_loss: 816.5219116210938\n",
      "    sample_time_ms: 28241.988\n",
      "    update_time_ms: 21.41\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.38524152318291\n",
      "  time_since_restore: 1526.340248823166\n",
      "  time_this_iter_s: 55.98204255104065\n",
      "  time_total_s: 1526.340248823166\n",
      "  timestamp: 1556652796\n",
      "  timesteps_since_restore: 435000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 87\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1526 s, 87 iter, 435000 ts, 137 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-34-19\n",
      "  done: false\n",
      "  episode_len_mean: 98.11\n",
      "  episode_reward_max: 302.8439900075728\n",
      "  episode_reward_mean: 133.60864890641568\n",
      "  episode_reward_min: -166.5416462144346\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 3847\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 9310.222\n",
      "    load_time_ms: 3.727\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0339757810987241e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.912704586982727\n",
      "      kl: 0.005392658989876509\n",
      "      policy_loss: -0.0015834241639822721\n",
      "      total_loss: 921.3209838867188\n",
      "      vf_explained_var: 0.21491192281246185\n",
      "      vf_loss: 921.3226318359375\n",
      "    sample_time_ms: 31686.25\n",
      "    update_time_ms: 23.701\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.80432445320784\n",
      "  time_since_restore: 1589.4414734840393\n",
      "  time_this_iter_s: 63.10122466087341\n",
      "  time_total_s: 1589.4414734840393\n",
      "  timestamp: 1556652859\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 88\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1589 s, 88 iter, 440000 ts, 134 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-35-27\n",
      "  done: false\n",
      "  episode_len_mean: 96.0\n",
      "  episode_reward_max: 302.8439900075728\n",
      "  episode_reward_mean: 123.69774091425838\n",
      "  episode_reward_min: -165.38246961572364\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 3900\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 10712.659\n",
      "    load_time_ms: 4.017\n",
      "    num_steps_sampled: 445000\n",
      "    num_steps_trained: 445000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.169878905493621e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9306215047836304\n",
      "      kl: 0.0028493781574070454\n",
      "      policy_loss: -0.0013077951734885573\n",
      "      total_loss: 977.2787475585938\n",
      "      vf_explained_var: 0.20073550939559937\n",
      "      vf_loss: 977.280029296875\n",
      "    sample_time_ms: 35564.06\n",
      "    update_time_ms: 26.646\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.84887045712919\n",
      "  time_since_restore: 1657.3230764865875\n",
      "  time_this_iter_s: 67.88160300254822\n",
      "  time_total_s: 1657.3230764865875\n",
      "  timestamp: 1556652927\n",
      "  timesteps_since_restore: 445000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 445000\n",
      "  training_iteration: 89\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/TenaciousD\n",
      "RUNNING trials:\n",
      " - PPO_MultiTenaciousDEnv-v0_0:\tRUNNING [pid=21459], 1657 s, 89 iter, 445000 ts, 124 rew\n",
      "\n",
      "Result for PPO_MultiTenaciousDEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_21-36-38\n",
      "  done: false\n",
      "  episode_len_mean: 98.08\n",
      "  episode_reward_max: 307.05109722831213\n",
      "  episode_reward_mean: 143.51425488383606\n",
      "  episode_reward_min: -161.47708715994654\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 3950\n",
      "  experiment_id: 29ef68f31c7149f1bf937c50e572ef8b\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 11806.603\n",
      "    load_time_ms: 4.241\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5849394527468104e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9472668170928955\n",
      "      kl: 0.002647615037858486\n",
      "      policy_loss: -0.0013408288359642029\n",
      "      total_loss: 747.0938110351562\n",
      "      vf_explained_var: 0.2724476456642151\n",
      "      vf_loss: 747.0950317382812\n",
      "    sample_time_ms: 40038.139\n",
      "    update_time_ms: 29.578\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 21459\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.75712744191804\n",
      "  time_since_restore: 1728.0096666812897\n",
      "  time_this_iter_s: 70.68659019470215\n",
      "  time_total_s: 1728.0096666812897\n",
      "  timestamp: 1556652998\n",
      "  timesteps_since_restore: 450000\n",
      "  timesteps_this_iter: 5000\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 90\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,  # RL algorithm to run\n",
    "        \"env\": gym_name,  # environment name generated earlier\n",
    "        \"config\": {  # configuration params (must match \"run\" value)\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1000,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow_2)",
   "language": "python",
   "name": "flow_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
