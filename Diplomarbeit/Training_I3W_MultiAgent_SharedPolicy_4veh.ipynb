{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING I3W\n",
    "\n",
    "\n",
    "# A) Create Envorinment, Vehicles etc\n",
    "\n",
    "### General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scenarios:\n",
      "['Scenario', 'BayBridgeScenario', 'BayBridgeTollScenario', 'BottleneckScenario', 'Figure8Scenario', 'SimpleGridScenario', 'HighwayScenario', 'LoopScenario', 'MergeScenario', 'TwoLoopsOneMergingScenario', 'MultiLoopScenario', 'IntersectionScenarioTW', 'TenaciousDScenario', 'IntersectionTWScenario_2']\n",
      "\n",
      "Available environments:\n",
      "['MultiEnv', 'MultiAgentAccelEnv', 'MultiWaveAttenuationPOEnv', 'MultiAgentIntersectionEnv', 'MultiAgentTeamSpiritIntersectionEnv', 'MultiAgentIntersectionEnv_baseline_1', 'MultiAgentIntersectionEnv_baseline_2', 'MultiAgentIntersectionEnv_baseline_3', 'MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit', 'MultiTenaciousDEnv', 'MultiAgentIntersectionEnv_sharedPolicy_2veh', 'MultiAgentIntersectionEnv_sharedPolicy_4veh']\n"
     ]
    }
   ],
   "source": [
    "# Define horizon as a variable to ensure consistent use across notebook (length of one rollout)\n",
    "HORIZON=120                                #103 max Horizon, wenn es vor verlassen abbrechen soll!, default war 500\n",
    "\n",
    "# name of the experiment\n",
    "experiment_name = \"IntersectionExample\"\n",
    "\n",
    "# scenario class\n",
    "import flow.scenarios as scenarios\n",
    "print(\"Available scenarios:\")\n",
    "print(scenarios.__all__)\n",
    "scenario_name = \"IntersectionTWScenario_2\"\n",
    "\n",
    "# environment class\n",
    "import flow.multiagent_envs as flowenvs\n",
    "print(\"\\nAvailable environments:\")\n",
    "print(flowenvs.__all__)\n",
    "env_name = \"MultiAgentIntersectionEnv_sharedPolicy_4veh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "from flow.scenarios.intersection import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "additionalNetParams = {\n",
    "            \"edge_length\": 80,\n",
    "            \"lanes\": 1,\n",
    "            \"speed_limit\": 30\n",
    "        }\n",
    "\n",
    "net_params = NetParams( no_internal_links=False,                  #default: True   !! damit Kreuzungen nicht Ã¼berspr. werden\n",
    "                        inflows=None,                             #default: None\n",
    "                        osm_path=None,                            #default: None\n",
    "                        netfile=None,                             #default: None\n",
    "                        additional_params=additionalNetParams     #default: None   !!\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InitialConfig Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "\n",
    "initial_config = InitialConfig( shuffle=True,                            #default: False         !!\n",
    "                                spacing=\"custom\",                        #default: \"uniform\"     !!\n",
    "                                min_gap=0,                               #default: 0\n",
    "                                perturbation=29.99,                      #default: 0.0            !!        \n",
    "                                x0=0,                                    #default: 0\n",
    "                                bunching=0,                              #default: 0\n",
    "                                lanes_distribution=float(\"inf\"),         #default: float(\"inf\")\n",
    "                                edges_distribution=\"all\",                #default: \"all\"\n",
    "                                additional_params=None )                 #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams( port = None,                  #default: None\n",
    "                          sim_step=0.1,                 #default: 0.1\n",
    "                          emission_path=None,           #default: None\n",
    "                          lateral_resolution=None,      #default: None\n",
    "                          no_step_log=True,             #default: True\n",
    "                          render=False,                 #default: False\n",
    "                          save_render=False,            #default: False\n",
    "                          sight_radius=25,              #default: 25\n",
    "                          show_radius=False,            #default: False\n",
    "                          pxpm=2,                       #default: 2\n",
    "                          overtake_right=False,         #default: False    \n",
    "                          seed=None,                    #default: None\n",
    "                          restart_instance=False,       #default: False\n",
    "                          print_warnings=True,          #default: True\n",
    "                          teleport_time=-1,             #default: -1\n",
    "                          num_clients=1,                #default: 1\n",
    "                          sumo_binary=None )            #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "additionalEnvParams = {\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 3,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 3,\n",
    "        \"target_velocity\": 30\n",
    "    }\n",
    "\n",
    "env_params = EnvParams( additional_params=additionalEnvParams, #default: None    !!\n",
    "                        horizon=HORIZON,                       #default: 500     !!\n",
    "                        warmup_steps=0,                        #default: 0       \n",
    "                        sims_per_step=1,                       #default: 1\n",
    "                        evaluate=False )                       #default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# import vehicles dynamics models\n",
    "#from flow.controllers import SumoCarFollowingController\n",
    "from flow.controllers import ContinuousRouter\n",
    "#from flow.controllers.lane_change_controllers import SumoLaneChangeController\n",
    "from flow.controllers.lane_change_controllers import StaticLaneChanger\n",
    "from flow.controllers import RLController\n",
    "from flow.core.params import SumoLaneChangeParams\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "from random import *\n",
    "\n",
    "vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RL-Agent controlled vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car following parameters, default: None\n",
    "cf_parameter = SumoCarFollowingParams(\n",
    "                speed_mode=\"aggressive\")\n",
    "# lane change parameters, default: None\n",
    "lc_parameter =  None\n",
    "\n",
    "vehicles.add( # name of the vehicle\n",
    "                veh_id = \"rl\",\n",
    "              # acceleration controller, default: (SumoCarFollowingController, {})\n",
    "                acceleration_controller=(RLController, {}),\n",
    "              # lane_change_controller, default: (SumoLaneChangeController, {})\n",
    "                lane_change_controller=(StaticLaneChanger,{}),\n",
    "              # routing controller, default: None\n",
    "                routing_controller=(ContinuousRouter, {}),\n",
    "              # initial speed, default: 0\n",
    "                initial_speed=0,\n",
    "              # number of vehicles, default: 1 \n",
    "                num_vehicles=4,\n",
    "                \n",
    "                car_following_params=cf_parameter\n",
    "              # speed mode, default: \"right_of_way\"\n",
    "                #speed_mode=\"aggressive\",\n",
    "              # lane change mode, default: \"no_lat_collide\"\n",
    "                #lane_change_mode=\"aggressive\", \n",
    "              # car following parameter, default: None\n",
    "                #sumo_car_following_params=cf_parameter,\n",
    "              # lane change parameter, default: None\n",
    "                #sumo_lc_params=lc_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict( # name of the experiment\n",
    "                      exp_tag=experiment_name,\n",
    "                    # name of the flow environment the experiment is running on\n",
    "                      env_name=env_name,\n",
    "                    # name of the scenario class the experiment uses\n",
    "                      scenario=scenario_name,\n",
    "                    # simulator that is used by the experiment\n",
    "                      simulator='traci',\n",
    "                    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "                      sim=sumo_params,\n",
    "                    # environment related parameters (see flow.core.params.EnvParams)\n",
    "                      env=env_params,\n",
    "                    # network-related parameters (see flow.core.params.NetParams and\n",
    "                    # the scenario's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "                      net=net_params,\n",
    "                    # vehicles to be placed in the network at the start of a rollout \n",
    "                    # (see flow.core.vehicles.Vehicles)\n",
    "                      veh=vehicles,\n",
    "                   # (optional) parameters affecting the positioning of vehicles upon \n",
    "                   # initialization/reset (see flow.core.params.InitialConfig)\n",
    "                      initial=initial_config\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-01_23-30-28_18468/logs.\n",
      "Waiting for redis server at 127.0.0.1:42238 to respond...\n",
      "Waiting for redis server at 127.0.0.1:29592 to respond...\n",
      "Starting the Plasma object store with 6.554658406 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8890/notebooks/ray_ui.ipynb?token=5120a71e8e2258f6b02cd3375819c8caddf04367c9b3883c\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.2.102',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-05-01_23-30-28_18468/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-05-01_23-30-28_18468/sockets/raylet'],\n",
       " 'redis_address': '192.168.2.102:42238',\n",
       " 'webui_url': 'http://localhost:8890/notebooks/ray_ui.ipynb?token=5120a71e8e2258f6b02cd3375819c8caddf04367c9b3883c'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 1\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 40\n",
    "\n",
    "ray.init(redirect_output=True, num_cpus=N_CPUS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate default 0.999\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [100, 50, 25]})  # size of hidden layers in network defaule 64 32\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "#config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "#config[\"sample_batch_size\"] = config[\"train_batch_size\"]/config[\"num_workers\"] # 200 default, trotzdem zu hoch?\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_paramshttps://www.tourdatenarchiv.de/setlist/71/07/Es-wird-eng/Frankfurt-Main-Festhalle/\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 54019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Teamspirit:\n",
      "-0.31793004235288946\n",
      "0.15303236157604805\n",
      "[('bottom_intersection', 34.11223706755223), ('bottom_intersection', 67.00817311605738), ('top_intersection', 8.275041899139783), ('top_intersection', 50.88762744358271)]\n"
     ]
    }
   ],
   "source": [
    "# multi agent policy mapping\n",
    "test_env = create_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "\n",
    "def gen_policy():\n",
    "    return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "# Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "policy_graphs = {'rl_0': gen_policy()}\n",
    "    \n",
    "def policy_mapping_fn(agent_id):\n",
    "    return 'rl_0'\n",
    "\n",
    "config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn),\n",
    "            'policies_to_train': ['rl_0']\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.2/16.4 GB\n",
      "\n",
      "Created LogSyncer for /home/thorsten/ray_results/IntersectionExample/PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0_2019-05-01_23-30-329daunr1k -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-32-49\n",
      "  done: false\n",
      "  episode_len_mean: 119.05\n",
      "  episode_reward_max: 43.1687284426817\n",
      "  episode_reward_mean: 13.646119597348985\n",
      "  episode_reward_min: -378.2460432724797\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 10453.109\n",
      "    load_time_ms: 81.181\n",
      "    num_steps_sampled: 4800\n",
      "    num_steps_trained: 4800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.20000001788139343\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.422437310218811\n",
      "      kl: 0.0014218979049474\n",
      "      policy_loss: -0.001724293571896851\n",
      "      total_loss: 33.42020034790039\n",
      "      vf_explained_var: -0.007731533143669367\n",
      "      vf_loss: 33.42163848876953\n",
      "    sample_time_ms: 49704.615\n",
      "    update_time_ms: 2215.312\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 3.4115298993372463\n",
      "  time_since_restore: 62.58481574058533\n",
      "  time_this_iter_s: 62.58481574058533\n",
      "  time_total_s: 62.58481574058533\n",
      "  timestamp: 1556746369\n",
      "  timesteps_since_restore: 4800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 4800\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 62 s, 1 iter, 4800 ts, 13.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-33-42\n",
      "  done: false\n",
      "  episode_len_mean: 119.1\n",
      "  episode_reward_max: 52.8622995672684\n",
      "  episode_reward_mean: 10.662702326816524\n",
      "  episode_reward_min: -378.2460432724797\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8445.678\n",
      "    load_time_ms: 41.44\n",
      "    num_steps_sampled: 9600\n",
      "    num_steps_trained: 9600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.10000000894069672\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4188969135284424\n",
      "      kl: 0.004024383146315813\n",
      "      policy_loss: -0.0033940307330340147\n",
      "      total_loss: 59.858848571777344\n",
      "      vf_explained_var: 0.04240647703409195\n",
      "      vf_loss: 59.86183547973633\n",
      "    sample_time_ms: 48148.309\n",
      "    update_time_ms: 1116.453\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 2.6656755817041313\n",
      "  time_since_restore: 115.65342903137207\n",
      "  time_this_iter_s: 53.06861329078674\n",
      "  time_total_s: 115.65342903137207\n",
      "  timestamp: 1556746422\n",
      "  timesteps_since_restore: 9600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 9600\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 115 s, 2 iter, 9600 ts, 10.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-34-25\n",
      "  done: false\n",
      "  episode_len_mean: 119.33\n",
      "  episode_reward_max: 55.703081332236394\n",
      "  episode_reward_mean: 12.41557852521624\n",
      "  episode_reward_min: -379.5269558174946\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7857.081\n",
      "    load_time_ms: 28.327\n",
      "    num_steps_sampled: 14400\n",
      "    num_steps_trained: 14400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.05000000447034836\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.410569190979004\n",
      "      kl: 0.003576126880943775\n",
      "      policy_loss: -0.0030132487881928682\n",
      "      total_loss: 46.33308410644531\n",
      "      vf_explained_var: 0.12951725721359253\n",
      "      vf_loss: 46.335914611816406\n",
      "    sample_time_ms: 43915.156\n",
      "    update_time_ms: 746.063\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 3.1038946313040605\n",
      "  time_since_restore: 157.81103420257568\n",
      "  time_this_iter_s: 42.15760517120361\n",
      "  time_total_s: 157.81103420257568\n",
      "  timestamp: 1556746465\n",
      "  timesteps_since_restore: 14400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 14400\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 157 s, 3 iter, 14400 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-35-02\n",
      "  done: false\n",
      "  episode_len_mean: 118.78\n",
      "  episode_reward_max: 70.67315666199968\n",
      "  episode_reward_mean: 8.245531292816644\n",
      "  episode_reward_min: -379.5269558174946\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 161\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7602.525\n",
      "    load_time_ms: 21.778\n",
      "    num_steps_sampled: 19200\n",
      "    num_steps_trained: 19200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.02500000223517418\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.409286618232727\n",
      "      kl: 0.0033964018803089857\n",
      "      policy_loss: -0.002038997830823064\n",
      "      total_loss: 86.82886505126953\n",
      "      vf_explained_var: 0.1832849681377411\n",
      "      vf_loss: 86.8308334350586\n",
      "    sample_time_ms: 40619.183\n",
      "    update_time_ms: 561.849\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 2.0613828232041618\n",
      "  time_since_restore: 195.41667819023132\n",
      "  time_this_iter_s: 37.60564398765564\n",
      "  time_total_s: 195.41667819023132\n",
      "  timestamp: 1556746502\n",
      "  timesteps_since_restore: 19200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 19200\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 195 s, 4 iter, 19200 ts, 8.25 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-35-40\n",
      "  done: false\n",
      "  episode_len_mean: 117.72\n",
      "  episode_reward_max: 87.5493983584939\n",
      "  episode_reward_mean: 8.13427766524545\n",
      "  episode_reward_min: -379.5269558174946\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 202\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7444.597\n",
      "    load_time_ms: 17.729\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.01250000111758709\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.407469391822815\n",
      "      kl: 0.00260497466661036\n",
      "      policy_loss: -0.002523417817428708\n",
      "      total_loss: 158.86785888671875\n",
      "      vf_explained_var: 0.10365590453147888\n",
      "      vf_loss: 158.870361328125\n",
      "    sample_time_ms: 38757.526\n",
      "    update_time_ms: 451.895\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 2.0335694163113636\n",
      "  time_since_restore: 233.58000493049622\n",
      "  time_this_iter_s: 38.16332674026489\n",
      "  time_total_s: 233.58000493049622\n",
      "  timestamp: 1556746540\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 5\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 233 s, 5 iter, 24000 ts, 8.13 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-36-22\n",
      "  done: false\n",
      "  episode_len_mean: 118.25\n",
      "  episode_reward_max: 103.82215575277931\n",
      "  episode_reward_mean: 17.197381650528175\n",
      "  episode_reward_min: -379.21993892573585\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 242\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7574.872\n",
      "    load_time_ms: 15.073\n",
      "    num_steps_sampled: 28800\n",
      "    num_steps_trained: 28800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.006250000558793545\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4119046926498413\n",
      "      kl: 0.0074527994729578495\n",
      "      policy_loss: -0.004131938796490431\n",
      "      total_loss: 90.48049926757812\n",
      "      vf_explained_var: 0.26968541741371155\n",
      "      vf_loss: 90.48458099365234\n",
      "    sample_time_ms: 37770.846\n",
      "    update_time_ms: 378.008\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 4.2993454126320465\n",
      "  time_since_restore: 274.68744230270386\n",
      "  time_this_iter_s: 41.10743737220764\n",
      "  time_total_s: 274.68744230270386\n",
      "  timestamp: 1556746582\n",
      "  timesteps_since_restore: 28800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 28800\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 274 s, 6 iter, 28800 ts, 17.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-37-01\n",
      "  done: false\n",
      "  episode_len_mean: 117.72\n",
      "  episode_reward_max: 114.69784276291912\n",
      "  episode_reward_mean: 3.824956912489098\n",
      "  episode_reward_min: -377.62923263639385\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 283\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7489.404\n",
      "    load_time_ms: 13.236\n",
      "    num_steps_sampled: 33600\n",
      "    num_steps_trained: 33600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031250002793967724\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.393907070159912\n",
      "      kl: 0.009548735804855824\n",
      "      policy_loss: -0.004910090938210487\n",
      "      total_loss: 280.399658203125\n",
      "      vf_explained_var: 0.18656830489635468\n",
      "      vf_loss: 280.404541015625\n",
      "    sample_time_ms: 36969.349\n",
      "    update_time_ms: 325.388\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 0.9562392281222748\n",
      "  time_since_restore: 313.85664319992065\n",
      "  time_this_iter_s: 39.1692008972168\n",
      "  time_total_s: 313.85664319992065\n",
      "  timestamp: 1556746621\n",
      "  timesteps_since_restore: 33600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 33600\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 313 s, 7 iter, 33600 ts, 3.82 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-37-40\n",
      "  done: false\n",
      "  episode_len_mean: 114.25\n",
      "  episode_reward_max: 135.3976231953738\n",
      "  episode_reward_mean: -44.929737803663585\n",
      "  episode_reward_min: -357.5589901594285\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 327\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7459.381\n",
      "    load_time_ms: 11.838\n",
      "    num_steps_sampled: 38400\n",
      "    num_steps_trained: 38400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0015625001396983862\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3817973136901855\n",
      "      kl: 0.0039515383541584015\n",
      "      policy_loss: -0.003322504460811615\n",
      "      total_loss: 513.0971069335938\n",
      "      vf_explained_var: 0.16555464267730713\n",
      "      vf_loss: 513.1004028320312\n",
      "    sample_time_ms: 36275.344\n",
      "    update_time_ms: 285.722\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -11.232434450915894\n",
      "  time_since_restore: 352.5571985244751\n",
      "  time_this_iter_s: 38.70055532455444\n",
      "  time_total_s: 352.5571985244751\n",
      "  timestamp: 1556746660\n",
      "  timesteps_since_restore: 38400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 38400\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 352 s, 8 iter, 38400 ts, -44.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-38-18\n",
      "  done: false\n",
      "  episode_len_mean: 112.43\n",
      "  episode_reward_max: 146.4334777256394\n",
      "  episode_reward_mean: -39.998528114291986\n",
      "  episode_reward_min: -344.77608904458685\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 369\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7370.507\n",
      "    load_time_ms: 10.693\n",
      "    num_steps_sampled: 43200\n",
      "    num_steps_trained: 43200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0007812500698491931\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.372701644897461\n",
      "      kl: 0.008483710698783398\n",
      "      policy_loss: -0.0040304637514054775\n",
      "      total_loss: 339.9320983886719\n",
      "      vf_explained_var: 0.1719706505537033\n",
      "      vf_loss: 339.9361572265625\n",
      "    sample_time_ms: 35797.263\n",
      "    update_time_ms: 254.936\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -9.999632028572996\n",
      "  time_since_restore: 391.216965675354\n",
      "  time_this_iter_s: 38.659767150878906\n",
      "  time_total_s: 391.216965675354\n",
      "  timestamp: 1556746698\n",
      "  timesteps_since_restore: 43200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 43200\n",
      "  training_iteration: 9\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 391 s, 9 iter, 43200 ts, -40 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-39-00\n",
      "  done: false\n",
      "  episode_len_mean: 112.73\n",
      "  episode_reward_max: 169.72426360877714\n",
      "  episode_reward_mean: -33.85362315482383\n",
      "  episode_reward_min: -344.77608904458685\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 411\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7474.576\n",
      "    load_time_ms: 9.989\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00039062503492459655\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3567941188812256\n",
      "      kl: 0.006169305182993412\n",
      "      policy_loss: -0.003079364774748683\n",
      "      total_loss: 374.315185546875\n",
      "      vf_explained_var: 0.21012698113918304\n",
      "      vf_loss: 374.3182373046875\n",
      "    sample_time_ms: 35542.42\n",
      "    update_time_ms: 229.964\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -8.463405788705957\n",
      "  time_since_restore: 432.911988735199\n",
      "  time_this_iter_s: 41.69502305984497\n",
      "  time_total_s: 432.911988735199\n",
      "  timestamp: 1556746740\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 432 s, 10 iter, 48000 ts, -33.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-39-39\n",
      "  done: false\n",
      "  episode_len_mean: 111.45\n",
      "  episode_reward_max: 197.76874508690798\n",
      "  episode_reward_mean: -31.24078015199023\n",
      "  episode_reward_min: -359.04687327074595\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 455\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7079.109\n",
      "    load_time_ms: 2.032\n",
      "    num_steps_sampled: 52800\n",
      "    num_steps_trained: 52800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00019531251746229827\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3478200435638428\n",
      "      kl: 0.004401156213134527\n",
      "      policy_loss: -0.0022245885338634253\n",
      "      total_loss: 477.9120788574219\n",
      "      vf_explained_var: 0.14057382941246033\n",
      "      vf_loss: 477.91436767578125\n",
      "    sample_time_ms: 33807.554\n",
      "    update_time_ms: 9.559\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -7.810195037997556\n",
      "  time_since_restore: 471.79495310783386\n",
      "  time_this_iter_s: 38.88296437263489\n",
      "  time_total_s: 471.79495310783386\n",
      "  timestamp: 1556746779\n",
      "  timesteps_since_restore: 52800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 52800\n",
      "  training_iteration: 11\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 471 s, 11 iter, 52800 ts, -31.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-40-18\n",
      "  done: false\n",
      "  episode_len_mean: 108.08\n",
      "  episode_reward_max: 197.76874508690798\n",
      "  episode_reward_mean: -46.0724587577479\n",
      "  episode_reward_min: -359.04687327074595\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 500\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7112.39\n",
      "    load_time_ms: 2.124\n",
      "    num_steps_sampled: 57600\n",
      "    num_steps_trained: 57600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.765625873114914e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3448160886764526\n",
      "      kl: 0.005020794924348593\n",
      "      policy_loss: -0.0019414647249504924\n",
      "      total_loss: 494.0638427734375\n",
      "      vf_explained_var: 0.13163529336452484\n",
      "      vf_loss: 494.0657958984375\n",
      "    sample_time_ms: 32334.776\n",
      "    update_time_ms: 8.661\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -11.51811468943698\n",
      "  time_since_restore: 510.46156001091003\n",
      "  time_this_iter_s: 38.66660690307617\n",
      "  time_total_s: 510.46156001091003\n",
      "  timestamp: 1556746818\n",
      "  timesteps_since_restore: 57600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 57600\n",
      "  training_iteration: 12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 510 s, 12 iter, 57600 ts, -46.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-40-56\n",
      "  done: false\n",
      "  episode_len_mean: 106.46\n",
      "  episode_reward_max: 204.50072768791026\n",
      "  episode_reward_mean: -44.011032859678586\n",
      "  episode_reward_min: -352.3128146493077\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 546\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7146.694\n",
      "    load_time_ms: 2.058\n",
      "    num_steps_sampled: 62400\n",
      "    num_steps_trained: 62400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.882812936557457e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3387068510055542\n",
      "      kl: 0.005705740302801132\n",
      "      policy_loss: -0.0028822545427829027\n",
      "      total_loss: 482.26910400390625\n",
      "      vf_explained_var: 0.18415550887584686\n",
      "      vf_loss: 482.27197265625\n",
      "    sample_time_ms: 31932.352\n",
      "    update_time_ms: 8.61\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -11.00275821491965\n",
      "  time_since_restore: 548.9404799938202\n",
      "  time_this_iter_s: 38.478919982910156\n",
      "  time_total_s: 548.9404799938202\n",
      "  timestamp: 1556746856\n",
      "  timesteps_since_restore: 62400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 62400\n",
      "  training_iteration: 13\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 548 s, 13 iter, 62400 ts, -44 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-41-37\n",
      "  done: false\n",
      "  episode_len_mean: 102.15\n",
      "  episode_reward_max: 222.46638301567037\n",
      "  episode_reward_mean: -89.88951449146485\n",
      "  episode_reward_min: -348.7475698257532\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 594\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7302.705\n",
      "    load_time_ms: 2.029\n",
      "    num_steps_sampled: 67200\n",
      "    num_steps_trained: 67200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4414064682787284e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3328152894973755\n",
      "      kl: 0.005847199819982052\n",
      "      policy_loss: -0.002292910125106573\n",
      "      total_loss: 619.5294799804688\n",
      "      vf_explained_var: 0.1680772602558136\n",
      "      vf_loss: 619.53173828125\n",
      "    sample_time_ms: 32073.733\n",
      "    update_time_ms: 8.632\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -22.47237862286621\n",
      "  time_since_restore: 589.5189785957336\n",
      "  time_this_iter_s: 40.57849860191345\n",
      "  time_total_s: 589.5189785957336\n",
      "  timestamp: 1556746897\n",
      "  timesteps_since_restore: 67200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 67200\n",
      "  training_iteration: 14\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 589 s, 14 iter, 67200 ts, -89.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-42-17\n",
      "  done: false\n",
      "  episode_len_mean: 101.33\n",
      "  episode_reward_max: 230.98605915840994\n",
      "  episode_reward_mean: -74.3567125037964\n",
      "  episode_reward_min: -347.666747682378\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 640\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7315.316\n",
      "    load_time_ms: 2.038\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2207032341393642e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3314766883850098\n",
      "      kl: 0.0057153585366904736\n",
      "      policy_loss: -0.003310452913865447\n",
      "      total_loss: 465.8896789550781\n",
      "      vf_explained_var: 0.1781725287437439\n",
      "      vf_loss: 465.8930358886719\n",
      "    sample_time_ms: 32269.335\n",
      "    update_time_ms: 8.613\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -18.5891781259491\n",
      "  time_since_restore: 629.7546994686127\n",
      "  time_this_iter_s: 40.23572087287903\n",
      "  time_total_s: 629.7546994686127\n",
      "  timestamp: 1556746937\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 15\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 629 s, 15 iter, 72000 ts, -74.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-42-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.87\n",
      "  episode_reward_max: 245.39437177681734\n",
      "  episode_reward_mean: -60.46086997120362\n",
      "  episode_reward_min: -351.44938336731974\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 690\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7175.634\n",
      "    load_time_ms: 2.061\n",
      "    num_steps_sampled: 76800\n",
      "    num_steps_trained: 76800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.103516170696821e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.321302056312561\n",
      "      kl: 0.007557978853583336\n",
      "      policy_loss: -0.0022298889234662056\n",
      "      total_loss: 620.0477905273438\n",
      "      vf_explained_var: 0.19255660474300385\n",
      "      vf_loss: 620.050048828125\n",
      "    sample_time_ms: 32132.369\n",
      "    update_time_ms: 8.587\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -15.115217492800912\n",
      "  time_since_restore: 668.0806486606598\n",
      "  time_this_iter_s: 38.32594919204712\n",
      "  time_total_s: 668.0806486606598\n",
      "  timestamp: 1556746976\n",
      "  timesteps_since_restore: 76800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 76800\n",
      "  training_iteration: 16\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 668 s, 16 iter, 76800 ts, -60.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 98.93\n",
      "  episode_reward_max: 250.4739690340127\n",
      "  episode_reward_mean: -65.24751405077275\n",
      "  episode_reward_min: -357.0307937735096\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 738\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7102.551\n",
      "    load_time_ms: 2.016\n",
      "    num_steps_sampled: 81600\n",
      "    num_steps_trained: 81600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.0517580853484105e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3167181015014648\n",
      "      kl: 0.00708889402449131\n",
      "      policy_loss: -0.0029626362957060337\n",
      "      total_loss: 519.4139404296875\n",
      "      vf_explained_var: 0.27090930938720703\n",
      "      vf_loss: 519.4169921875\n",
      "    sample_time_ms: 32140.43\n",
      "    update_time_ms: 8.39\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -16.31187851269319\n",
      "  time_since_restore: 706.5999569892883\n",
      "  time_this_iter_s: 38.51930832862854\n",
      "  time_total_s: 706.5999569892883\n",
      "  timestamp: 1556747014\n",
      "  timesteps_since_restore: 81600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 81600\n",
      "  training_iteration: 17\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 706 s, 17 iter, 81600 ts, -65.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-44-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.03\n",
      "  episode_reward_max: 263.49108886925865\n",
      "  episode_reward_mean: -18.062653125694204\n",
      "  episode_reward_min: -357.0307937735096\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 786\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7203.177\n",
      "    load_time_ms: 1.969\n",
      "    num_steps_sampled: 86400\n",
      "    num_steps_trained: 86400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5258790426742053e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3040751218795776\n",
      "      kl: 0.007138578221201897\n",
      "      policy_loss: -0.002178283641114831\n",
      "      total_loss: 524.7171630859375\n",
      "      vf_explained_var: 0.2782633900642395\n",
      "      vf_loss: 524.7193603515625\n",
      "    sample_time_ms: 32158.666\n",
      "    update_time_ms: 8.519\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -4.515663281423556\n",
      "  time_since_restore: 746.4893798828125\n",
      "  time_this_iter_s: 39.88942289352417\n",
      "  time_total_s: 746.4893798828125\n",
      "  timestamp: 1556747054\n",
      "  timesteps_since_restore: 86400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 86400\n",
      "  training_iteration: 18\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 746 s, 18 iter, 86400 ts, -18.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-44-56\n",
      "  done: false\n",
      "  episode_len_mean: 98.57\n",
      "  episode_reward_max: 280.5198441038898\n",
      "  episode_reward_mean: -1.9743694138528232\n",
      "  episode_reward_min: -342.62425622129933\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 835\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7282.487\n",
      "    load_time_ms: 2.154\n",
      "    num_steps_sampled: 91200\n",
      "    num_steps_trained: 91200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.629395213371026e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3017470836639404\n",
      "      kl: 0.007765392307192087\n",
      "      policy_loss: -0.0028874287381768227\n",
      "      total_loss: 541.4719848632812\n",
      "      vf_explained_var: 0.32563650608062744\n",
      "      vf_loss: 541.4749145507812\n",
      "    sample_time_ms: 32380.099\n",
      "    update_time_ms: 8.466\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -0.49359235346320784\n",
      "  time_since_restore: 788.1573679447174\n",
      "  time_this_iter_s: 41.66798806190491\n",
      "  time_total_s: 788.1573679447174\n",
      "  timestamp: 1556747096\n",
      "  timesteps_since_restore: 91200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 91200\n",
      "  training_iteration: 19\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 788 s, 19 iter, 91200 ts, -1.97 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 97.21\n",
      "  episode_reward_max: 293.68704327664443\n",
      "  episode_reward_mean: 4.197836639729434\n",
      "  episode_reward_min: -342.62425622129933\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 885\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7165.272\n",
      "    load_time_ms: 1.945\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814697606685513e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.296943187713623\n",
      "      kl: 0.006653988268226385\n",
      "      policy_loss: -0.0020140076521784067\n",
      "      total_loss: 534.6123046875\n",
      "      vf_explained_var: 0.3796868324279785\n",
      "      vf_loss: 534.6142578125\n",
      "    sample_time_ms: 32173.652\n",
      "    update_time_ms: 8.84\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 1.0494591599323573\n",
      "  time_since_restore: 826.6092450618744\n",
      "  time_this_iter_s: 38.45187711715698\n",
      "  time_total_s: 826.6092450618744\n",
      "  timestamp: 1556747134\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 20\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 826 s, 20 iter, 96000 ts, 4.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-46-13\n",
      "  done: false\n",
      "  episode_len_mean: 95.46\n",
      "  episode_reward_max: 309.1053301752648\n",
      "  episode_reward_mean: 6.721108064019027\n",
      "  episode_reward_min: -346.7944410200006\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 936\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7190.818\n",
      "    load_time_ms: 1.938\n",
      "    num_steps_sampled: 100800\n",
      "    num_steps_trained: 100800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9073488033427566e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2902127504348755\n",
      "      kl: 0.004582264460623264\n",
      "      policy_loss: -0.0019150120206177235\n",
      "      total_loss: 614.3219604492188\n",
      "      vf_explained_var: 0.37826359272003174\n",
      "      vf_loss: 614.3239135742188\n",
      "    sample_time_ms: 32129.872\n",
      "    update_time_ms: 9.014\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 1.680277016004762\n",
      "  time_since_restore: 865.3119976520538\n",
      "  time_this_iter_s: 38.70275259017944\n",
      "  time_total_s: 865.3119976520538\n",
      "  timestamp: 1556747173\n",
      "  timesteps_since_restore: 100800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 100800\n",
      "  training_iteration: 21\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 865 s, 21 iter, 100800 ts, 6.72 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-46-55\n",
      "  done: false\n",
      "  episode_len_mean: 93.07\n",
      "  episode_reward_max: 322.42185980968225\n",
      "  episode_reward_mean: -13.710186334353244\n",
      "  episode_reward_min: -350.26809520539376\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 988\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7329.263\n",
      "    load_time_ms: 1.855\n",
      "    num_steps_sampled: 105600\n",
      "    num_steps_trained: 105600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.536744016713783e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2748862504959106\n",
      "      kl: 0.00546769006177783\n",
      "      policy_loss: -0.0014272554544731975\n",
      "      total_loss: 680.9215698242188\n",
      "      vf_explained_var: 0.31161433458328247\n",
      "      vf_loss: 680.9229736328125\n",
      "    sample_time_ms: 32287.526\n",
      "    update_time_ms: 9.356\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -3.42754658358831\n",
      "  time_since_restore: 906.9456133842468\n",
      "  time_this_iter_s: 41.63361573219299\n",
      "  time_total_s: 906.9456133842468\n",
      "  timestamp: 1556747215\n",
      "  timesteps_since_restore: 105600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 105600\n",
      "  training_iteration: 22\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 906 s, 22 iter, 105600 ts, -13.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-47-34\n",
      "  done: false\n",
      "  episode_len_mean: 94.97\n",
      "  episode_reward_max: 329.8814642669707\n",
      "  episode_reward_mean: -2.3610455282812084\n",
      "  episode_reward_min: -350.26809520539376\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 1037\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7314.891\n",
      "    load_time_ms: 1.912\n",
      "    num_steps_sampled: 110400\n",
      "    num_steps_trained: 110400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.7683720083568915e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2668871879577637\n",
      "      kl: 0.007436884101480246\n",
      "      policy_loss: -0.001779194688424468\n",
      "      total_loss: 636.7134399414062\n",
      "      vf_explained_var: 0.3973066806793213\n",
      "      vf_loss: 636.7152099609375\n",
      "    sample_time_ms: 32405.9\n",
      "    update_time_ms: 9.825\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -0.590261382070305\n",
      "  time_since_restore: 946.4643514156342\n",
      "  time_this_iter_s: 39.51873803138733\n",
      "  time_total_s: 946.4643514156342\n",
      "  timestamp: 1556747254\n",
      "  timesteps_since_restore: 110400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 110400\n",
      "  training_iteration: 23\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 946 s, 23 iter, 110400 ts, -2.36 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-48-14\n",
      "  done: false\n",
      "  episode_len_mean: 99.15\n",
      "  episode_reward_max: 332.1108006551575\n",
      "  episode_reward_mean: 48.36241021066645\n",
      "  episode_reward_min: -349.8479319948635\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 1085\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7156.449\n",
      "    load_time_ms: 1.887\n",
      "    num_steps_sampled: 115200\n",
      "    num_steps_trained: 115200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841860041784457e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2644164562225342\n",
      "      kl: 0.006299473345279694\n",
      "      policy_loss: -0.00221831351518631\n",
      "      total_loss: 607.3375854492188\n",
      "      vf_explained_var: 0.3920595943927765\n",
      "      vf_loss: 607.3397827148438\n",
      "    sample_time_ms: 32482.833\n",
      "    update_time_ms: 9.424\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 12.090602552666615\n",
      "  time_since_restore: 986.2177002429962\n",
      "  time_this_iter_s: 39.75334882736206\n",
      "  time_total_s: 986.2177002429962\n",
      "  timestamp: 1556747294\n",
      "  timesteps_since_restore: 115200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 115200\n",
      "  training_iteration: 24\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 986 s, 24 iter, 115200 ts, 48.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-48-54\n",
      "  done: false\n",
      "  episode_len_mean: 98.75\n",
      "  episode_reward_max: 340.2382769546924\n",
      "  episode_reward_mean: 42.84780477983969\n",
      "  episode_reward_min: -349.8479319948635\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 1134\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7133.57\n",
      "    load_time_ms: 1.933\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1920930020892229e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2544162273406982\n",
      "      kl: 0.00436257803812623\n",
      "      policy_loss: -0.001304478500969708\n",
      "      total_loss: 704.9358520507812\n",
      "      vf_explained_var: 0.38676077127456665\n",
      "      vf_loss: 704.9371337890625\n",
      "    sample_time_ms: 32438.005\n",
      "    update_time_ms: 8.855\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 10.711951194959926\n",
      "  time_since_restore: 1025.7728312015533\n",
      "  time_this_iter_s: 39.55513095855713\n",
      "  time_total_s: 1025.7728312015533\n",
      "  timestamp: 1556747334\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 25\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1025 s, 25 iter, 120000 ts, 42.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-49-36\n",
      "  done: false\n",
      "  episode_len_mean: 97.04\n",
      "  episode_reward_max: 352.9092736500208\n",
      "  episode_reward_mean: 34.314744718935124\n",
      "  episode_reward_min: -356.5687038798979\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 1183\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7282.171\n",
      "    load_time_ms: 1.995\n",
      "    num_steps_sampled: 124800\n",
      "    num_steps_trained: 124800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.960465010446114e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2407710552215576\n",
      "      kl: 0.007180582731962204\n",
      "      policy_loss: -0.00372534547932446\n",
      "      total_loss: 661.2197875976562\n",
      "      vf_explained_var: 0.3836885094642639\n",
      "      vf_loss: 661.2234497070312\n",
      "    sample_time_ms: 32653.202\n",
      "    update_time_ms: 8.888\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 8.57868617973378\n",
      "  time_since_restore: 1067.7402532100677\n",
      "  time_this_iter_s: 41.967422008514404\n",
      "  time_total_s: 1067.7402532100677\n",
      "  timestamp: 1556747376\n",
      "  timesteps_since_restore: 124800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 124800\n",
      "  training_iteration: 26\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1067 s, 26 iter, 124800 ts, 34.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 91.78\n",
      "  episode_reward_max: 367.9434675563407\n",
      "  episode_reward_mean: 10.665686475622724\n",
      "  episode_reward_min: -356.5687038798979\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 1238\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7339.719\n",
      "    load_time_ms: 1.99\n",
      "    num_steps_sampled: 129600\n",
      "    num_steps_trained: 129600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.980232505223057e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.227564811706543\n",
      "      kl: 0.005102261900901794\n",
      "      policy_loss: -0.0013821680331602693\n",
      "      total_loss: 760.5642700195312\n",
      "      vf_explained_var: 0.45358094573020935\n",
      "      vf_loss: 760.5656127929688\n",
      "    sample_time_ms: 33003.494\n",
      "    update_time_ms: 9.765\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 2.6664216189056806\n",
      "  time_since_restore: 1110.3434677124023\n",
      "  time_this_iter_s: 42.603214502334595\n",
      "  time_total_s: 1110.3434677124023\n",
      "  timestamp: 1556747418\n",
      "  timesteps_since_restore: 129600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 129600\n",
      "  training_iteration: 27\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1110 s, 27 iter, 129600 ts, 10.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-51-00\n",
      "  done: false\n",
      "  episode_len_mean: 83.68\n",
      "  episode_reward_max: 367.9434675563407\n",
      "  episode_reward_mean: -51.72741135555274\n",
      "  episode_reward_min: -355.36631459345165\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 1297\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7438.288\n",
      "    load_time_ms: 2.047\n",
      "    num_steps_sampled: 134400\n",
      "    num_steps_trained: 134400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4901162526115286e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2299062013626099\n",
      "      kl: 0.0061493366956710815\n",
      "      policy_loss: -0.0030590672977268696\n",
      "      total_loss: 880.2993774414062\n",
      "      vf_explained_var: 0.4597896337509155\n",
      "      vf_loss: 880.302490234375\n",
      "    sample_time_ms: 33093.649\n",
      "    update_time_ms: 9.571\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -12.931852838888181\n",
      "  time_since_restore: 1152.1186075210571\n",
      "  time_this_iter_s: 41.775139808654785\n",
      "  time_total_s: 1152.1186075210571\n",
      "  timestamp: 1556747460\n",
      "  timesteps_since_restore: 134400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 134400\n",
      "  training_iteration: 28\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1152 s, 28 iter, 134400 ts, -51.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-51-43\n",
      "  done: false\n",
      "  episode_len_mean: 83.11\n",
      "  episode_reward_max: 353.87837545346736\n",
      "  episode_reward_mean: -61.80217835507602\n",
      "  episode_reward_min: -347.7046595720471\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 1352\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7440.276\n",
      "    load_time_ms: 1.9\n",
      "    num_steps_sampled: 139200\n",
      "    num_steps_trained: 139200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.450581263057643e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2211447954177856\n",
      "      kl: 0.0038633313961327076\n",
      "      policy_loss: -0.001246353960596025\n",
      "      total_loss: 815.1661987304688\n",
      "      vf_explained_var: 0.4469062387943268\n",
      "      vf_loss: 815.16748046875\n",
      "    sample_time_ms: 33174.954\n",
      "    update_time_ms: 9.797\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -15.450544588769002\n",
      "  time_since_restore: 1194.62211561203\n",
      "  time_this_iter_s: 42.5035080909729\n",
      "  time_total_s: 1194.62211561203\n",
      "  timestamp: 1556747503\n",
      "  timesteps_since_restore: 139200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 139200\n",
      "  training_iteration: 29\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1194 s, 29 iter, 139200 ts, -61.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-52-28\n",
      "  done: false\n",
      "  episode_len_mean: 89.89\n",
      "  episode_reward_max: 368.2479741711354\n",
      "  episode_reward_mean: -1.1796736152074312\n",
      "  episode_reward_min: -348.26287310012515\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 1405\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7714.336\n",
      "    load_time_ms: 2.038\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7252906315288215e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.208020806312561\n",
      "      kl: 0.005119247827678919\n",
      "      policy_loss: -0.00153586664237082\n",
      "      total_loss: 680.8875122070312\n",
      "      vf_explained_var: 0.5107716917991638\n",
      "      vf_loss: 680.8890380859375\n",
      "    sample_time_ms: 33594.561\n",
      "    update_time_ms: 9.403\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -0.29491840380185863\n",
      "  time_since_restore: 1240.0191838741302\n",
      "  time_this_iter_s: 45.39706826210022\n",
      "  time_total_s: 1240.0191838741302\n",
      "  timestamp: 1556747548\n",
      "  timesteps_since_restore: 144000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 30\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1240 s, 30 iter, 144000 ts, -1.18 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-53-11\n",
      "  done: false\n",
      "  episode_len_mean: 92.97\n",
      "  episode_reward_max: 368.2479741711354\n",
      "  episode_reward_mean: 52.19164253440454\n",
      "  episode_reward_min: -348.26287310012515\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 1457\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7726.242\n",
      "    load_time_ms: 2.076\n",
      "    num_steps_sampled: 148800\n",
      "    num_steps_trained: 148800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8626453157644107e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2009841203689575\n",
      "      kl: 0.006013661157339811\n",
      "      policy_loss: -0.002465578028932214\n",
      "      total_loss: 646.8211669921875\n",
      "      vf_explained_var: 0.5434311032295227\n",
      "      vf_loss: 646.8236694335938\n",
      "    sample_time_ms: 33982.429\n",
      "    update_time_ms: 8.878\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 13.047910633601134\n",
      "  time_since_restore: 1282.7219245433807\n",
      "  time_this_iter_s: 42.70274066925049\n",
      "  time_total_s: 1282.7219245433807\n",
      "  timestamp: 1556747591\n",
      "  timesteps_since_restore: 148800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 148800\n",
      "  training_iteration: 31\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1282 s, 31 iter, 148800 ts, 52.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-53-51\n",
      "  done: false\n",
      "  episode_len_mean: 93.07\n",
      "  episode_reward_max: 368.3890887631394\n",
      "  episode_reward_mean: 56.57593410837119\n",
      "  episode_reward_min: -347.7865635234471\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 1509\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7592.12\n",
      "    load_time_ms: 2.113\n",
      "    num_steps_sampled: 153600\n",
      "    num_steps_trained: 153600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.313226578822054e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1991913318634033\n",
      "      kl: 0.00693876575678587\n",
      "      policy_loss: -0.0026512329932302237\n",
      "      total_loss: 728.4497680664062\n",
      "      vf_explained_var: 0.5397673845291138\n",
      "      vf_loss: 728.452392578125\n",
      "    sample_time_ms: 33907.873\n",
      "    update_time_ms: 8.306\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 14.143983527092805\n",
      "  time_since_restore: 1322.2574536800385\n",
      "  time_this_iter_s: 39.535529136657715\n",
      "  time_total_s: 1322.2574536800385\n",
      "  timestamp: 1556747631\n",
      "  timesteps_since_restore: 153600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 153600\n",
      "  training_iteration: 32\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1322 s, 32 iter, 153600 ts, 56.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-54-34\n",
      "  done: false\n",
      "  episode_len_mean: 92.68\n",
      "  episode_reward_max: 387.3371954163415\n",
      "  episode_reward_mean: 40.39765701374343\n",
      "  episode_reward_min: -349.5373619310327\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 1560\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7802.551\n",
      "    load_time_ms: 2.052\n",
      "    num_steps_sampled: 158400\n",
      "    num_steps_trained: 158400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.656613289411027e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1901484727859497\n",
      "      kl: 0.007620789110660553\n",
      "      policy_loss: -0.0023944643326103687\n",
      "      total_loss: 689.37841796875\n",
      "      vf_explained_var: 0.5983080863952637\n",
      "      vf_loss: 689.3809204101562\n",
      "    sample_time_ms: 34090.904\n",
      "    update_time_ms: 8.251\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 10.099414253435866\n",
      "  time_since_restore: 1365.7065482139587\n",
      "  time_this_iter_s: 43.44909453392029\n",
      "  time_total_s: 1365.7065482139587\n",
      "  timestamp: 1556747674\n",
      "  timesteps_since_restore: 158400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 158400\n",
      "  training_iteration: 33\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1365 s, 33 iter, 158400 ts, 40.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-55-16\n",
      "  done: false\n",
      "  episode_len_mean: 93.32\n",
      "  episode_reward_max: 387.3371954163415\n",
      "  episode_reward_mean: 54.741185471042265\n",
      "  episode_reward_min: -349.5373619310327\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 1611\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7867.364\n",
      "    load_time_ms: 2.162\n",
      "    num_steps_sampled: 163200\n",
      "    num_steps_trained: 163200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283066447055134e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.174811840057373\n",
      "      kl: 0.005716010928153992\n",
      "      policy_loss: -0.0016257186653092504\n",
      "      total_loss: 763.6312255859375\n",
      "      vf_explained_var: 0.4921599328517914\n",
      "      vf_loss: 763.6328125\n",
      "    sample_time_ms: 34200.443\n",
      "    update_time_ms: 8.721\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 13.685296367760568\n",
      "  time_since_restore: 1407.2151429653168\n",
      "  time_this_iter_s: 41.50859475135803\n",
      "  time_total_s: 1407.2151429653168\n",
      "  timestamp: 1556747716\n",
      "  timesteps_since_restore: 163200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 163200\n",
      "  training_iteration: 34\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1407 s, 34 iter, 163200 ts, 54.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-56-06\n",
      "  done: false\n",
      "  episode_len_mean: 91.0\n",
      "  episode_reward_max: 385.010592920445\n",
      "  episode_reward_mean: 28.026459963469673\n",
      "  episode_reward_min: -340.09029057765366\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 1665\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7950.566\n",
      "    load_time_ms: 2.19\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1641533223527567e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.181855320930481\n",
      "      kl: 0.005931418854743242\n",
      "      policy_loss: -0.00251374626532197\n",
      "      total_loss: 793.2595825195312\n",
      "      vf_explained_var: 0.5517458915710449\n",
      "      vf_loss: 793.2621459960938\n",
      "    sample_time_ms: 35170.588\n",
      "    update_time_ms: 9.413\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 7.006614990867413\n",
      "  time_since_restore: 1457.3188869953156\n",
      "  time_this_iter_s: 50.10374402999878\n",
      "  time_total_s: 1457.3188869953156\n",
      "  timestamp: 1556747766\n",
      "  timesteps_since_restore: 168000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 35\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1457 s, 35 iter, 168000 ts, 28 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-56-45\n",
      "  done: false\n",
      "  episode_len_mean: 87.76\n",
      "  episode_reward_max: 385.18718338074007\n",
      "  episode_reward_mean: 5.025667689533679\n",
      "  episode_reward_min: -340.09029057765366\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 1719\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7819.751\n",
      "    load_time_ms: 2.357\n",
      "    num_steps_sampled: 172800\n",
      "    num_steps_trained: 172800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.8207666117637835e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1858776807785034\n",
      "      kl: 0.003675328101962805\n",
      "      policy_loss: -0.0010670027695596218\n",
      "      total_loss: 746.916015625\n",
      "      vf_explained_var: 0.5467853546142578\n",
      "      vf_loss: 746.9170532226562\n",
      "    sample_time_ms: 35008.181\n",
      "    update_time_ms: 9.318\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 1.2564169223834187\n",
      "  time_since_restore: 1496.3574776649475\n",
      "  time_this_iter_s: 39.03859066963196\n",
      "  time_total_s: 1496.3574776649475\n",
      "  timestamp: 1556747805\n",
      "  timesteps_since_restore: 172800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 172800\n",
      "  training_iteration: 36\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1496 s, 36 iter, 172800 ts, 5.03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-57-25\n",
      "  done: false\n",
      "  episode_len_mean: 89.42\n",
      "  episode_reward_max: 398.4490938142514\n",
      "  episode_reward_mean: 31.0205983530087\n",
      "  episode_reward_min: -351.2543846546991\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 1772\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7858.367\n",
      "    load_time_ms: 2.436\n",
      "    num_steps_sampled: 177600\n",
      "    num_steps_trained: 177600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9103833058818918e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1921875476837158\n",
      "      kl: 0.0032306869979947805\n",
      "      policy_loss: -0.0009471488883718848\n",
      "      total_loss: 715.7472534179688\n",
      "      vf_explained_var: 0.5794863104820251\n",
      "      vf_loss: 715.7482299804688\n",
      "    sample_time_ms: 34683.185\n",
      "    update_time_ms: 9.992\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 7.755149588252177\n",
      "  time_since_restore: 1536.099922657013\n",
      "  time_this_iter_s: 39.74244499206543\n",
      "  time_total_s: 1536.099922657013\n",
      "  timestamp: 1556747845\n",
      "  timesteps_since_restore: 177600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 177600\n",
      "  training_iteration: 37\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1536 s, 37 iter, 177600 ts, 31 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-58-03\n",
      "  done: false\n",
      "  episode_len_mean: 91.59\n",
      "  episode_reward_max: 398.4490938142514\n",
      "  episode_reward_mean: 54.29224213201997\n",
      "  episode_reward_min: -354.24675094644726\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 1823\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7607.228\n",
      "    load_time_ms: 2.365\n",
      "    num_steps_sampled: 182400\n",
      "    num_steps_trained: 182400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4551916529409459e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1859546899795532\n",
      "      kl: 0.004920527804642916\n",
      "      policy_loss: -0.0020559898111969233\n",
      "      total_loss: 734.552734375\n",
      "      vf_explained_var: 0.56118243932724\n",
      "      vf_loss: 734.5548706054688\n",
      "    sample_time_ms: 34632.219\n",
      "    update_time_ms: 10.817\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 13.573060533004991\n",
      "  time_since_restore: 1574.8566329479218\n",
      "  time_this_iter_s: 38.75671029090881\n",
      "  time_total_s: 1574.8566329479218\n",
      "  timestamp: 1556747883\n",
      "  timesteps_since_restore: 182400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 182400\n",
      "  training_iteration: 38\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1574 s, 38 iter, 182400 ts, 54.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-58-50\n",
      "  done: false\n",
      "  episode_len_mean: 92.17\n",
      "  episode_reward_max: 406.55544685237345\n",
      "  episode_reward_mean: 30.455765699225594\n",
      "  episode_reward_min: -354.24675094644726\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 1878\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7633.996\n",
      "    load_time_ms: 2.392\n",
      "    num_steps_sampled: 187200\n",
      "    num_steps_trained: 187200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.275958264704729e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1775388717651367\n",
      "      kl: 0.005508604925125837\n",
      "      policy_loss: -0.001527586136944592\n",
      "      total_loss: 736.3308715820312\n",
      "      vf_explained_var: 0.5679143071174622\n",
      "      vf_loss: 736.3323974609375\n",
      "    sample_time_ms: 35018.302\n",
      "    update_time_ms: 10.367\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 7.613941424806387\n",
      "  time_since_restore: 1621.4921984672546\n",
      "  time_this_iter_s: 46.635565519332886\n",
      "  time_total_s: 1621.4921984672546\n",
      "  timestamp: 1556747930\n",
      "  timesteps_since_restore: 187200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 187200\n",
      "  training_iteration: 39\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1621 s, 39 iter, 187200 ts, 30.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_23-59-32\n",
      "  done: false\n",
      "  episode_len_mean: 86.89\n",
      "  episode_reward_max: 406.55544685237345\n",
      "  episode_reward_mean: -0.4492501687058194\n",
      "  episode_reward_min: -350.45790835399623\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 1934\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7362.098\n",
      "    load_time_ms: 2.366\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379791323523647e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1585404872894287\n",
      "      kl: 0.004747319035232067\n",
      "      policy_loss: -0.0013822488253936172\n",
      "      total_loss: 837.473876953125\n",
      "      vf_explained_var: 0.5359256863594055\n",
      "      vf_loss: 837.4751586914062\n",
      "    sample_time_ms: 34974.222\n",
      "    update_time_ms: 10.585\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -0.11231254217645784\n",
      "  time_since_restore: 1663.7296957969666\n",
      "  time_this_iter_s: 42.237497329711914\n",
      "  time_total_s: 1663.7296957969666\n",
      "  timestamp: 1556747972\n",
      "  timesteps_since_restore: 192000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 40\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1663 s, 40 iter, 192000 ts, -0.449 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-00-15\n",
      "  done: false\n",
      "  episode_len_mean: 86.39\n",
      "  episode_reward_max: 410.2455283642375\n",
      "  episode_reward_mean: 16.05959566369941\n",
      "  episode_reward_min: -356.02841919690167\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 1988\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7408.515\n",
      "    load_time_ms: 2.343\n",
      "    num_steps_sampled: 196800\n",
      "    num_steps_trained: 196800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8189895661761823e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1695234775543213\n",
      "      kl: 0.006577638443559408\n",
      "      policy_loss: -0.0021234198939055204\n",
      "      total_loss: 626.0234375\n",
      "      vf_explained_var: 0.6619065403938293\n",
      "      vf_loss: 626.025634765625\n",
      "    sample_time_ms: 34951.306\n",
      "    update_time_ms: 10.588\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 4.014898915924853\n",
      "  time_since_restore: 1706.669507741928\n",
      "  time_this_iter_s: 42.93981194496155\n",
      "  time_total_s: 1706.669507741928\n",
      "  timestamp: 1556748015\n",
      "  timesteps_since_restore: 196800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 196800\n",
      "  training_iteration: 41\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1706 s, 41 iter, 196800 ts, 16.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-00-57\n",
      "  done: false\n",
      "  episode_len_mean: 89.36\n",
      "  episode_reward_max: 420.9851092799985\n",
      "  episode_reward_mean: 38.34050954997464\n",
      "  episode_reward_min: -356.57724484064215\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 2042\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7420.842\n",
      "    load_time_ms: 2.279\n",
      "    num_steps_sampled: 201600\n",
      "    num_steps_trained: 201600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.094947830880912e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1610831022262573\n",
      "      kl: 0.0045007667504251\n",
      "      policy_loss: -0.002445356920361519\n",
      "      total_loss: 804.314208984375\n",
      "      vf_explained_var: 0.5879706740379333\n",
      "      vf_loss: 804.316650390625\n",
      "    sample_time_ms: 35157.126\n",
      "    update_time_ms: 11.325\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 9.585127387493662\n",
      "  time_since_restore: 1748.3923716545105\n",
      "  time_this_iter_s: 41.7228639125824\n",
      "  time_total_s: 1748.3923716545105\n",
      "  timestamp: 1556748057\n",
      "  timesteps_since_restore: 201600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 201600\n",
      "  training_iteration: 42\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.8/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1748 s, 42 iter, 201600 ts, 38.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-01-40\n",
      "  done: false\n",
      "  episode_len_mean: 85.95\n",
      "  episode_reward_max: 420.9851092799985\n",
      "  episode_reward_mean: 12.424378538247872\n",
      "  episode_reward_min: -356.57724484064215\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 2098\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7358.924\n",
      "    load_time_ms: 2.341\n",
      "    num_steps_sampled: 206400\n",
      "    num_steps_trained: 206400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473915440456e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.150362491607666\n",
      "      kl: 0.005905465222895145\n",
      "      policy_loss: -0.0018749242881312966\n",
      "      total_loss: 654.7426147460938\n",
      "      vf_explained_var: 0.6811457276344299\n",
      "      vf_loss: 654.7444458007812\n",
      "    sample_time_ms: 35137.052\n",
      "    update_time_ms: 11.653\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 3.1060946345619715\n",
      "  time_since_restore: 1791.0337164402008\n",
      "  time_this_iter_s: 42.64134478569031\n",
      "  time_total_s: 1791.0337164402008\n",
      "  timestamp: 1556748100\n",
      "  timesteps_since_restore: 206400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 206400\n",
      "  training_iteration: 43\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.8/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1791 s, 43 iter, 206400 ts, 12.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-02-20\n",
      "  done: false\n",
      "  episode_len_mean: 87.96\n",
      "  episode_reward_max: 419.44671978116645\n",
      "  episode_reward_mean: 29.75456607551702\n",
      "  episode_reward_min: -355.97739495643486\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 2150\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7371.811\n",
      "    load_time_ms: 2.313\n",
      "    num_steps_sampled: 211200\n",
      "    num_steps_trained: 211200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.273736957720228e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1347379684448242\n",
      "      kl: 0.005433604121208191\n",
      "      policy_loss: -0.0015817669918760657\n",
      "      total_loss: 735.6842651367188\n",
      "      vf_explained_var: 0.6039665341377258\n",
      "      vf_loss: 735.6858520507812\n",
      "    sample_time_ms: 34980.195\n",
      "    update_time_ms: 11.496\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 7.4386415188792565\n",
      "  time_since_restore: 1831.0965535640717\n",
      "  time_this_iter_s: 40.06283712387085\n",
      "  time_total_s: 1831.0965535640717\n",
      "  timestamp: 1556748140\n",
      "  timesteps_since_restore: 211200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 211200\n",
      "  training_iteration: 44\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1831 s, 44 iter, 211200 ts, 29.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-03-04\n",
      "  done: false\n",
      "  episode_len_mean: 88.71\n",
      "  episode_reward_max: 416.0932327984721\n",
      "  episode_reward_mean: 17.00082764687088\n",
      "  episode_reward_min: -361.3072699766033\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 2205\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7344.495\n",
      "    load_time_ms: 2.288\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.136868478860114e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1127839088439941\n",
      "      kl: 0.0059901815839111805\n",
      "      policy_loss: -0.0022547049447894096\n",
      "      total_loss: 808.5925903320312\n",
      "      vf_explained_var: 0.585954487323761\n",
      "      vf_loss: 808.5947875976562\n",
      "    sample_time_ms: 34394.401\n",
      "    update_time_ms: 11.322\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 4.250206911717715\n",
      "  time_since_restore: 1875.0609421730042\n",
      "  time_this_iter_s: 43.964388608932495\n",
      "  time_total_s: 1875.0609421730042\n",
      "  timestamp: 1556748184\n",
      "  timesteps_since_restore: 216000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 45\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.8/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1875 s, 45 iter, 216000 ts, 17 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 84.88\n",
      "  episode_reward_max: 416.8313721126344\n",
      "  episode_reward_mean: -4.261860250338872\n",
      "  episode_reward_min: -361.3072699766033\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 2263\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7320.431\n",
      "    load_time_ms: 2.095\n",
      "    num_steps_sampled: 220800\n",
      "    num_steps_trained: 220800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.68434239430057e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1273462772369385\n",
      "      kl: 0.0071986159309744835\n",
      "      policy_loss: -0.00255215703509748\n",
      "      total_loss: 747.2417602539062\n",
      "      vf_explained_var: 0.6647357940673828\n",
      "      vf_loss: 747.2442626953125\n",
      "    sample_time_ms: 34419.154\n",
      "    update_time_ms: 11.213\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -1.0654650625847193\n",
      "  time_since_restore: 1914.0988466739655\n",
      "  time_this_iter_s: 39.037904500961304\n",
      "  time_total_s: 1914.0988466739655\n",
      "  timestamp: 1556748223\n",
      "  timesteps_since_restore: 220800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 220800\n",
      "  training_iteration: 46\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1914 s, 46 iter, 220800 ts, -4.26 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-04-24\n",
      "  done: false\n",
      "  episode_len_mean: 85.83\n",
      "  episode_reward_max: 429.85164141037626\n",
      "  episode_reward_mean: 23.440203901513645\n",
      "  episode_reward_min: -357.43435968909296\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 2318\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7428.562\n",
      "    load_time_ms: 2.039\n",
      "    num_steps_sampled: 225600\n",
      "    num_steps_trained: 225600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.842171197150285e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1202881336212158\n",
      "      kl: 0.007325107231736183\n",
      "      policy_loss: -0.0037271357141435146\n",
      "      total_loss: 633.2802124023438\n",
      "      vf_explained_var: 0.7036671042442322\n",
      "      vf_loss: 633.2839965820312\n",
      "    sample_time_ms: 34370.58\n",
      "    update_time_ms: 10.021\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 5.860050975378407\n",
      "  time_since_restore: 1954.4337780475616\n",
      "  time_this_iter_s: 40.33493137359619\n",
      "  time_total_s: 1954.4337780475616\n",
      "  timestamp: 1556748264\n",
      "  timesteps_since_restore: 225600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 225600\n",
      "  training_iteration: 47\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1954 s, 47 iter, 225600 ts, 23.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-05-02\n",
      "  done: false\n",
      "  episode_len_mean: 85.36\n",
      "  episode_reward_max: 431.6217326659735\n",
      "  episode_reward_mean: 16.88363750721011\n",
      "  episode_reward_min: -357.43435968909296\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 2375\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7389.771\n",
      "    load_time_ms: 2.09\n",
      "    num_steps_sampled: 230400\n",
      "    num_steps_trained: 230400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4210855985751425e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1087536811828613\n",
      "      kl: 0.008410627953708172\n",
      "      policy_loss: -0.0020777415484189987\n",
      "      total_loss: 856.6275024414062\n",
      "      vf_explained_var: 0.6209690570831299\n",
      "      vf_loss: 856.6295776367188\n",
      "    sample_time_ms: 34396.337\n",
      "    update_time_ms: 9.498\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 4.220909376802528\n",
      "  time_since_restore: 1993.0547606945038\n",
      "  time_this_iter_s: 38.62098264694214\n",
      "  time_total_s: 1993.0547606945038\n",
      "  timestamp: 1556748302\n",
      "  timesteps_since_restore: 230400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 230400\n",
      "  training_iteration: 48\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 1993 s, 48 iter, 230400 ts, 16.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-05-40\n",
      "  done: false\n",
      "  episode_len_mean: 84.63\n",
      "  episode_reward_max: 431.6217326659735\n",
      "  episode_reward_mean: 2.821103103511829\n",
      "  episode_reward_min: -355.24949113108863\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 2431\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7280.966\n",
      "    load_time_ms: 2.009\n",
      "    num_steps_sampled: 235200\n",
      "    num_steps_trained: 235200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.105427992875712e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1010643243789673\n",
      "      kl: 0.007009753957390785\n",
      "      policy_loss: -0.0025178161449730396\n",
      "      total_loss: 691.9231567382812\n",
      "      vf_explained_var: 0.6896104216575623\n",
      "      vf_loss: 691.9257202148438\n",
      "    sample_time_ms: 33640.069\n",
      "    update_time_ms: 9.686\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 0.7052757758779574\n",
      "  time_since_restore: 2031.0339751243591\n",
      "  time_this_iter_s: 37.97921442985535\n",
      "  time_total_s: 2031.0339751243591\n",
      "  timestamp: 1556748340\n",
      "  timesteps_since_restore: 235200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 235200\n",
      "  training_iteration: 49\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2031 s, 49 iter, 235200 ts, 2.82 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-06-18\n",
      "  done: false\n",
      "  episode_len_mean: 83.78\n",
      "  episode_reward_max: 431.34729055414994\n",
      "  episode_reward_mean: 2.961918918523311\n",
      "  episode_reward_min: -355.24949113108863\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 2488\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7224.086\n",
      "    load_time_ms: 1.884\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.552713996437856e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0899546146392822\n",
      "      kl: 0.007426460739225149\n",
      "      policy_loss: -0.00249336170963943\n",
      "      total_loss: 799.0540771484375\n",
      "      vf_explained_var: 0.6464741230010986\n",
      "      vf_loss: 799.0565795898438\n",
      "    sample_time_ms: 33288.087\n",
      "    update_time_ms: 9.714\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 0.7404797296308249\n",
      "  time_since_restore: 2069.169866323471\n",
      "  time_this_iter_s: 38.13589119911194\n",
      "  time_total_s: 2069.169866323471\n",
      "  timestamp: 1556748378\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 50\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.8/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2069 s, 50 iter, 240000 ts, 2.96 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-06-56\n",
      "  done: false\n",
      "  episode_len_mean: 86.22\n",
      "  episode_reward_max: 447.32059893229\n",
      "  episode_reward_mean: 27.227143032721315\n",
      "  episode_reward_min: -357.0921775592532\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 2545\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7187.461\n",
      "    load_time_ms: 1.856\n",
      "    num_steps_sampled: 244800\n",
      "    num_steps_trained: 244800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.776356998218928e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.088266372680664\n",
      "      kl: 0.006644051056355238\n",
      "      policy_loss: -0.003203969681635499\n",
      "      total_loss: 728.83935546875\n",
      "      vf_explained_var: 0.7009902596473694\n",
      "      vf_loss: 728.8425903320312\n",
      "    sample_time_ms: 32748.084\n",
      "    update_time_ms: 9.403\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 6.806785758180329\n",
      "  time_since_restore: 2106.336368560791\n",
      "  time_this_iter_s: 37.166502237319946\n",
      "  time_total_s: 2106.336368560791\n",
      "  timestamp: 1556748416\n",
      "  timesteps_since_restore: 244800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 244800\n",
      "  training_iteration: 51\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2106 s, 51 iter, 244800 ts, 27.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-07-40\n",
      "  done: false\n",
      "  episode_len_mean: 83.03\n",
      "  episode_reward_max: 450.98207752228564\n",
      "  episode_reward_mean: -4.704771819228139\n",
      "  episode_reward_min: -355.3363881575554\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 2602\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7148.685\n",
      "    load_time_ms: 1.849\n",
      "    num_steps_sampled: 249600\n",
      "    num_steps_trained: 249600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.88178499109464e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.084509015083313\n",
      "      kl: 0.005876616574823856\n",
      "      policy_loss: -0.0019464683718979359\n",
      "      total_loss: 866.5357055664062\n",
      "      vf_explained_var: 0.6224270462989807\n",
      "      vf_loss: 866.5377197265625\n",
      "    sample_time_ms: 33080.521\n",
      "    update_time_ms: 8.97\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -1.1761929548070282\n",
      "  time_since_restore: 2150.993305206299\n",
      "  time_this_iter_s: 44.65693664550781\n",
      "  time_total_s: 2150.993305206299\n",
      "  timestamp: 1556748460\n",
      "  timesteps_since_restore: 249600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 249600\n",
      "  training_iteration: 52\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2150 s, 52 iter, 249600 ts, -4.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-08-20\n",
      "  done: false\n",
      "  episode_len_mean: 85.22\n",
      "  episode_reward_max: 450.98207752228564\n",
      "  episode_reward_mean: 18.698285023948724\n",
      "  episode_reward_min: -354.76948075572284\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 2660\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7023.811\n",
      "    load_time_ms: 1.854\n",
      "    num_steps_sampled: 254400\n",
      "    num_steps_trained: 254400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.44089249554732e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0732460021972656\n",
      "      kl: 0.005552081391215324\n",
      "      policy_loss: -0.001789115252904594\n",
      "      total_loss: 714.1422729492188\n",
      "      vf_explained_var: 0.6906608939170837\n",
      "      vf_loss: 714.14404296875\n",
      "    sample_time_ms: 32904.194\n",
      "    update_time_ms: 8.236\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 4.6745712559871855\n",
      "  time_since_restore: 2190.617617368698\n",
      "  time_this_iter_s: 39.62431216239929\n",
      "  time_total_s: 2190.617617368698\n",
      "  timestamp: 1556748500\n",
      "  timesteps_since_restore: 254400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 254400\n",
      "  training_iteration: 53\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2190 s, 53 iter, 254400 ts, 18.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-09-00\n",
      "  done: false\n",
      "  episode_len_mean: 80.97\n",
      "  episode_reward_max: 449.6036344987853\n",
      "  episode_reward_mean: 8.141556067117966\n",
      "  episode_reward_min: -348.38143365569704\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 2719\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6942.475\n",
      "    load_time_ms: 1.763\n",
      "    num_steps_sampled: 259200\n",
      "    num_steps_trained: 259200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.22044624777366e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.072232723236084\n",
      "      kl: 0.007463429588824511\n",
      "      policy_loss: -0.0023754106368869543\n",
      "      total_loss: 690.9515380859375\n",
      "      vf_explained_var: 0.7161417007446289\n",
      "      vf_loss: 690.953857421875\n",
      "    sample_time_ms: 32974.078\n",
      "    update_time_ms: 8.155\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 2.0353890167794937\n",
      "  time_since_restore: 2230.5606019496918\n",
      "  time_this_iter_s: 39.94298458099365\n",
      "  time_total_s: 2230.5606019496918\n",
      "  timestamp: 1556748540\n",
      "  timesteps_since_restore: 259200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 259200\n",
      "  training_iteration: 54\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2230 s, 54 iter, 259200 ts, 8.14 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-09-38\n",
      "  done: false\n",
      "  episode_len_mean: 80.0\n",
      "  episode_reward_max: 447.1117749086088\n",
      "  episode_reward_mean: -14.106064901893431\n",
      "  episode_reward_min: -350.51122523139685\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 2775\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6901.007\n",
      "    load_time_ms: 1.69\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.11022312388683e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.03426992893219\n",
      "      kl: 0.0054623158648610115\n",
      "      policy_loss: -0.0013789930380880833\n",
      "      total_loss: 814.305908203125\n",
      "      vf_explained_var: 0.6475675106048584\n",
      "      vf_loss: 814.3073120117188\n",
      "    sample_time_ms: 32461.72\n",
      "    update_time_ms: 7.597\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -3.5265162254733617\n",
      "  time_since_restore: 2268.9808161258698\n",
      "  time_this_iter_s: 38.42021417617798\n",
      "  time_total_s: 2268.9808161258698\n",
      "  timestamp: 1556748578\n",
      "  timesteps_since_restore: 264000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 55\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2268 s, 55 iter, 264000 ts, -14.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-10-19\n",
      "  done: false\n",
      "  episode_len_mean: 87.29\n",
      "  episode_reward_max: 446.7638018177016\n",
      "  episode_reward_mean: 30.91774392098088\n",
      "  episode_reward_min: -350.51122523139685\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 2829\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6890.251\n",
      "    load_time_ms: 1.635\n",
      "    num_steps_sampled: 268800\n",
      "    num_steps_trained: 268800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.55111561943415e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.028296709060669\n",
      "      kl: 0.007661644835025072\n",
      "      policy_loss: -0.002903508022427559\n",
      "      total_loss: 802.7217407226562\n",
      "      vf_explained_var: 0.6357654929161072\n",
      "      vf_loss: 802.724609375\n",
      "    sample_time_ms: 32625.63\n",
      "    update_time_ms: 7.661\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 7.729435980245221\n",
      "  time_since_restore: 2309.548553466797\n",
      "  time_this_iter_s: 40.567737340927124\n",
      "  time_total_s: 2309.548553466797\n",
      "  timestamp: 1556748619\n",
      "  timesteps_since_restore: 268800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 268800\n",
      "  training_iteration: 56\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2309 s, 56 iter, 268800 ts, 30.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-10-57\n",
      "  done: false\n",
      "  episode_len_mean: 87.89\n",
      "  episode_reward_max: 448.47293426804225\n",
      "  episode_reward_mean: 39.183163359645484\n",
      "  episode_reward_min: -349.0666784821027\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 2884\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6706.897\n",
      "    load_time_ms: 1.609\n",
      "    num_steps_sampled: 273600\n",
      "    num_steps_trained: 273600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.775557809717075e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.030106782913208\n",
      "      kl: 0.008708569221198559\n",
      "      policy_loss: -0.003233283292502165\n",
      "      total_loss: 766.544921875\n",
      "      vf_explained_var: 0.6684990525245667\n",
      "      vf_loss: 766.548095703125\n",
      "    sample_time_ms: 32558.531\n",
      "    update_time_ms: 7.326\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 9.79579083991137\n",
      "  time_since_restore: 2347.371072292328\n",
      "  time_this_iter_s: 37.822518825531006\n",
      "  time_total_s: 2347.371072292328\n",
      "  timestamp: 1556748657\n",
      "  timesteps_since_restore: 273600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 273600\n",
      "  training_iteration: 57\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.0/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2347 s, 57 iter, 273600 ts, 39.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-11-39\n",
      "  done: false\n",
      "  episode_len_mean: 86.82\n",
      "  episode_reward_max: 461.0010451494052\n",
      "  episode_reward_mean: 46.294584700420764\n",
      "  episode_reward_min: -350.2742971530389\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 2939\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6716.16\n",
      "    load_time_ms: 1.571\n",
      "    num_steps_sampled: 278400\n",
      "    num_steps_trained: 278400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3877789048585376e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.016228199005127\n",
      "      kl: 0.006756179500371218\n",
      "      policy_loss: -0.0031695389188826084\n",
      "      total_loss: 743.9110107421875\n",
      "      vf_explained_var: 0.6922829151153564\n",
      "      vf_loss: 743.9141845703125\n",
      "    sample_time_ms: 32836.302\n",
      "    update_time_ms: 7.58\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 11.573646175105187\n",
      "  time_since_restore: 2388.8701338768005\n",
      "  time_this_iter_s: 41.499061584472656\n",
      "  time_total_s: 2388.8701338768005\n",
      "  timestamp: 1556748699\n",
      "  timesteps_since_restore: 278400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 278400\n",
      "  training_iteration: 58\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2388 s, 58 iter, 278400 ts, 46.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-12-18\n",
      "  done: false\n",
      "  episode_len_mean: 85.52\n",
      "  episode_reward_max: 455.89227884675023\n",
      "  episode_reward_mean: 37.84575228262764\n",
      "  episode_reward_min: -356.30838347036257\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 2994\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6697.135\n",
      "    load_time_ms: 1.597\n",
      "    num_steps_sampled: 283200\n",
      "    num_steps_trained: 283200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.938894524292688e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9995487332344055\n",
      "      kl: 0.006408382207155228\n",
      "      policy_loss: -0.0010070219868794084\n",
      "      total_loss: 723.9481201171875\n",
      "      vf_explained_var: 0.685930073261261\n",
      "      vf_loss: 723.9490966796875\n",
      "    sample_time_ms: 33022.144\n",
      "    update_time_ms: 9.176\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 9.461438070656904\n",
      "  time_since_restore: 2428.5388588905334\n",
      "  time_this_iter_s: 39.66872501373291\n",
      "  time_total_s: 2428.5388588905334\n",
      "  timestamp: 1556748738\n",
      "  timesteps_since_restore: 283200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 283200\n",
      "  training_iteration: 59\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2428 s, 59 iter, 283200 ts, 37.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-13-00\n",
      "  done: false\n",
      "  episode_len_mean: 89.12\n",
      "  episode_reward_max: 463.45845358474025\n",
      "  episode_reward_mean: 84.27271738948897\n",
      "  episode_reward_min: -356.30838347036257\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 3048\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6687.153\n",
      "    load_time_ms: 1.606\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.469447262146344e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.997454047203064\n",
      "      kl: 0.006525432225316763\n",
      "      policy_loss: -0.0024030928034335375\n",
      "      total_loss: 658.5634155273438\n",
      "      vf_explained_var: 0.735207200050354\n",
      "      vf_loss: 658.5657348632812\n",
      "    sample_time_ms: 33383.614\n",
      "    update_time_ms: 9.788\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 21.068179347372244\n",
      "  time_since_restore: 2470.201554775238\n",
      "  time_this_iter_s: 41.66269588470459\n",
      "  time_total_s: 2470.201554775238\n",
      "  timestamp: 1556748780\n",
      "  timesteps_since_restore: 288000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 60\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2470 s, 60 iter, 288000 ts, 84.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-13-38\n",
      "  done: false\n",
      "  episode_len_mean: 83.4\n",
      "  episode_reward_max: 463.45845358474025\n",
      "  episode_reward_mean: 21.339581206132408\n",
      "  episode_reward_min: -355.40463839512245\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 3108\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6608.909\n",
      "    load_time_ms: 1.604\n",
      "    num_steps_sampled: 292800\n",
      "    num_steps_trained: 292800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.734723631073172e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0072662830352783\n",
      "      kl: 0.0048789591528475285\n",
      "      policy_loss: -0.0015495101688429713\n",
      "      total_loss: 824.6506958007812\n",
      "      vf_explained_var: 0.6835875511169434\n",
      "      vf_loss: 824.6521606445312\n",
      "    sample_time_ms: 33554.314\n",
      "    update_time_ms: 9.79\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 5.334895301533103\n",
      "  time_since_restore: 2508.2912900447845\n",
      "  time_this_iter_s: 38.08973526954651\n",
      "  time_total_s: 2508.2912900447845\n",
      "  timestamp: 1556748818\n",
      "  timesteps_since_restore: 292800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 292800\n",
      "  training_iteration: 61\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2508 s, 61 iter, 292800 ts, 21.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-14-16\n",
      "  done: false\n",
      "  episode_len_mean: 80.15\n",
      "  episode_reward_max: 453.62282588354236\n",
      "  episode_reward_mean: -27.718231596618153\n",
      "  episode_reward_min: -352.4121204493928\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 3168\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6655.223\n",
      "    load_time_ms: 1.684\n",
      "    num_steps_sampled: 297600\n",
      "    num_steps_trained: 297600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.67361815536586e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0076110363006592\n",
      "      kl: 0.006154932081699371\n",
      "      policy_loss: -0.002737496979534626\n",
      "      total_loss: 792.65966796875\n",
      "      vf_explained_var: 0.7201286554336548\n",
      "      vf_loss: 792.6624145507812\n",
      "    sample_time_ms: 32856.632\n",
      "    update_time_ms: 10.304\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -6.929557899154534\n",
      "  time_since_restore: 2546.4496705532074\n",
      "  time_this_iter_s: 38.15838050842285\n",
      "  time_total_s: 2546.4496705532074\n",
      "  timestamp: 1556748856\n",
      "  timesteps_since_restore: 297600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 297600\n",
      "  training_iteration: 62\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2546 s, 62 iter, 297600 ts, -27.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-14-54\n",
      "  done: false\n",
      "  episode_len_mean: 80.87\n",
      "  episode_reward_max: 462.74399203197373\n",
      "  episode_reward_mean: -20.021842892487204\n",
      "  episode_reward_min: -347.1888770879498\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 3225\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6594.586\n",
      "    load_time_ms: 1.619\n",
      "    num_steps_sampled: 302400\n",
      "    num_steps_trained: 302400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.33680907768293e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.989057183265686\n",
      "      kl: 0.007879722863435745\n",
      "      policy_loss: -0.0026127633173018694\n",
      "      total_loss: 732.7633666992188\n",
      "      vf_explained_var: 0.7139318585395813\n",
      "      vf_loss: 732.7660522460938\n",
      "    sample_time_ms: 32729.647\n",
      "    update_time_ms: 10.47\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -5.005460723121802\n",
      "  time_since_restore: 2584.1880905628204\n",
      "  time_this_iter_s: 37.73842000961304\n",
      "  time_total_s: 2584.1880905628204\n",
      "  timestamp: 1556748894\n",
      "  timesteps_since_restore: 302400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 302400\n",
      "  training_iteration: 63\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2584 s, 63 iter, 302400 ts, -20 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-15-40\n",
      "  done: false\n",
      "  episode_len_mean: 84.29\n",
      "  episode_reward_max: 462.74399203197373\n",
      "  episode_reward_mean: 47.87227864721627\n",
      "  episode_reward_min: -347.1888770879498\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 3280\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6776.081\n",
      "    load_time_ms: 1.652\n",
      "    num_steps_sampled: 307200\n",
      "    num_steps_trained: 307200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.168404538841465e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9822554588317871\n",
      "      kl: 0.007362775970250368\n",
      "      policy_loss: -0.0025825584307312965\n",
      "      total_loss: 627.1466674804688\n",
      "      vf_explained_var: 0.7528408765792847\n",
      "      vf_loss: 627.1492919921875\n",
      "    sample_time_ms: 33120.562\n",
      "    update_time_ms: 10.377\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 11.968069661804071\n",
      "  time_since_restore: 2629.872612953186\n",
      "  time_this_iter_s: 45.6845223903656\n",
      "  time_total_s: 2629.872612953186\n",
      "  timestamp: 1556748940\n",
      "  timesteps_since_restore: 307200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 307200\n",
      "  training_iteration: 64\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2629 s, 64 iter, 307200 ts, 47.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-16-20\n",
      "  done: false\n",
      "  episode_len_mean: 83.27\n",
      "  episode_reward_max: 474.8669966885839\n",
      "  episode_reward_mean: 54.862480309735076\n",
      "  episode_reward_min: -357.21022234965017\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 3337\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6781.528\n",
      "    load_time_ms: 1.71\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842022694207325e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9728164672851562\n",
      "      kl: 0.0064305504783988\n",
      "      policy_loss: -0.0024656658060848713\n",
      "      total_loss: 682.6201782226562\n",
      "      vf_explained_var: 0.7693604826927185\n",
      "      vf_loss: 682.6226196289062\n",
      "    sample_time_ms: 33235.863\n",
      "    update_time_ms: 11.354\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 13.715620077433778\n",
      "  time_since_restore: 2669.5109186172485\n",
      "  time_this_iter_s: 39.6383056640625\n",
      "  time_total_s: 2669.5109186172485\n",
      "  timestamp: 1556748980\n",
      "  timesteps_since_restore: 312000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 65\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2669 s, 65 iter, 312000 ts, 54.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-16-57\n",
      "  done: false\n",
      "  episode_len_mean: 88.93\n",
      "  episode_reward_max: 474.8669966885839\n",
      "  episode_reward_mean: 90.92213286696219\n",
      "  episode_reward_min: -357.21022234965017\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 3389\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6834.08\n",
      "    load_time_ms: 1.727\n",
      "    num_steps_sampled: 316800\n",
      "    num_steps_trained: 316800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.421011347103662e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9487302899360657\n",
      "      kl: 0.005550355184823275\n",
      "      policy_loss: -0.0019445667276158929\n",
      "      total_loss: 711.9459228515625\n",
      "      vf_explained_var: 0.7084688544273376\n",
      "      vf_loss: 711.9478149414062\n",
      "    sample_time_ms: 32885.255\n",
      "    update_time_ms: 11.524\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.730533216740547\n",
      "  time_since_restore: 2707.1012053489685\n",
      "  time_this_iter_s: 37.59028673171997\n",
      "  time_total_s: 2707.1012053489685\n",
      "  timestamp: 1556749017\n",
      "  timesteps_since_restore: 316800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 316800\n",
      "  training_iteration: 66\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2707 s, 66 iter, 316800 ts, 90.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-17-35\n",
      "  done: false\n",
      "  episode_len_mean: 88.9\n",
      "  episode_reward_max: 482.31435097772624\n",
      "  episode_reward_mean: 96.34378891552018\n",
      "  episode_reward_min: -347.83006286264026\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 3444\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6844.381\n",
      "    load_time_ms: 1.75\n",
      "    num_steps_sampled: 321600\n",
      "    num_steps_trained: 321600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.710505673551831e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9349677562713623\n",
      "      kl: 0.008178508840501308\n",
      "      policy_loss: -0.0024383023846894503\n",
      "      total_loss: 581.4639282226562\n",
      "      vf_explained_var: 0.7909978032112122\n",
      "      vf_loss: 581.4664306640625\n",
      "    sample_time_ms: 32835.765\n",
      "    update_time_ms: 11.277\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.08594722888004\n",
      "  time_since_restore: 2744.526776075363\n",
      "  time_this_iter_s: 37.42557072639465\n",
      "  time_total_s: 2744.526776075363\n",
      "  timestamp: 1556749055\n",
      "  timesteps_since_restore: 321600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 321600\n",
      "  training_iteration: 67\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2744 s, 67 iter, 321600 ts, 96.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-18-14\n",
      "  done: false\n",
      "  episode_len_mean: 80.16\n",
      "  episode_reward_max: 479.9055501634796\n",
      "  episode_reward_mean: 38.3322691756534\n",
      "  episode_reward_min: -347.83006286264026\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 3505\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6985.412\n",
      "    load_time_ms: 1.734\n",
      "    num_steps_sampled: 326400\n",
      "    num_steps_trained: 326400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3552528367759156e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9540868997573853\n",
      "      kl: 0.005402802489697933\n",
      "      policy_loss: -0.001889502163976431\n",
      "      total_loss: 665.369873046875\n",
      "      vf_explained_var: 0.787628173828125\n",
      "      vf_loss: 665.3716430664062\n",
      "    sample_time_ms: 32497.354\n",
      "    update_time_ms: 10.582\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 9.583067293913345\n",
      "  time_since_restore: 2784.0443239212036\n",
      "  time_this_iter_s: 39.517547845840454\n",
      "  time_total_s: 2784.0443239212036\n",
      "  timestamp: 1556749094\n",
      "  timesteps_since_restore: 326400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 326400\n",
      "  training_iteration: 68\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2784 s, 68 iter, 326400 ts, 38.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-18-53\n",
      "  done: false\n",
      "  episode_len_mean: 81.43\n",
      "  episode_reward_max: 490.3718926963422\n",
      "  episode_reward_mean: 33.32567377445406\n",
      "  episode_reward_min: -352.36623128480267\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 3562\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6982.068\n",
      "    load_time_ms: 1.712\n",
      "    num_steps_sampled: 331200\n",
      "    num_steps_trained: 331200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.776264183879578e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9477842450141907\n",
      "      kl: 0.004868021234869957\n",
      "      policy_loss: -0.0017810696735978127\n",
      "      total_loss: 744.4619140625\n",
      "      vf_explained_var: 0.7494577169418335\n",
      "      vf_loss: 744.4637451171875\n",
      "    sample_time_ms: 32417.07\n",
      "    update_time_ms: 9.259\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 8.331418443613508\n",
      "  time_since_restore: 2822.859782934189\n",
      "  time_this_iter_s: 38.81545901298523\n",
      "  time_total_s: 2822.859782934189\n",
      "  timestamp: 1556749133\n",
      "  timesteps_since_restore: 331200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 331200\n",
      "  training_iteration: 69\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2822 s, 69 iter, 331200 ts, 33.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-19-32\n",
      "  done: false\n",
      "  episode_len_mean: 82.7\n",
      "  episode_reward_max: 501.60393566005297\n",
      "  episode_reward_mean: 33.17429176066457\n",
      "  episode_reward_min: -356.6226391828769\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 3619\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7192.292\n",
      "    load_time_ms: 1.84\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.388132091939789e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9307132959365845\n",
      "      kl: 0.007860410958528519\n",
      "      policy_loss: -0.0031048059463500977\n",
      "      total_loss: 606.6445922851562\n",
      "      vf_explained_var: 0.8105583786964417\n",
      "      vf_loss: 606.647705078125\n",
      "    sample_time_ms: 31931.942\n",
      "    update_time_ms: 8.703\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 8.293572940166143\n",
      "  time_since_restore: 2861.770503282547\n",
      "  time_this_iter_s: 38.910720348358154\n",
      "  time_total_s: 2861.770503282547\n",
      "  timestamp: 1556749172\n",
      "  timesteps_since_restore: 336000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 70\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2861 s, 70 iter, 336000 ts, 33.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-20-11\n",
      "  done: false\n",
      "  episode_len_mean: 80.01\n",
      "  episode_reward_max: 501.60393566005297\n",
      "  episode_reward_mean: 33.91509495494679\n",
      "  episode_reward_min: -349.1029604163916\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 3679\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7192.247\n",
      "    load_time_ms: 1.87\n",
      "    num_steps_sampled: 340800\n",
      "    num_steps_trained: 340800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6940660459698945e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.935599684715271\n",
      "      kl: 0.006341335363686085\n",
      "      policy_loss: -0.0023726406507194042\n",
      "      total_loss: 700.0701293945312\n",
      "      vf_explained_var: 0.7851060032844543\n",
      "      vf_loss: 700.0723876953125\n",
      "    sample_time_ms: 32039.215\n",
      "    update_time_ms: 9.11\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 8.478773738736699\n",
      "  time_since_restore: 2900.9380242824554\n",
      "  time_this_iter_s: 39.16752099990845\n",
      "  time_total_s: 2900.9380242824554\n",
      "  timestamp: 1556749211\n",
      "  timesteps_since_restore: 340800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 340800\n",
      "  training_iteration: 71\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2900 s, 71 iter, 340800 ts, 33.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-20-48\n",
      "  done: false\n",
      "  episode_len_mean: 81.58\n",
      "  episode_reward_max: 482.871564072628\n",
      "  episode_reward_mean: 45.84888195715581\n",
      "  episode_reward_min: -354.28035646805836\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 3736\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7166.635\n",
      "    load_time_ms: 1.813\n",
      "    num_steps_sampled: 345600\n",
      "    num_steps_trained: 345600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.470330229849472e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.915915310382843\n",
      "      kl: 0.007116504479199648\n",
      "      policy_loss: -0.0026855459436774254\n",
      "      total_loss: 732.4703369140625\n",
      "      vf_explained_var: 0.7754508852958679\n",
      "      vf_loss: 732.472900390625\n",
      "    sample_time_ms: 31915.89\n",
      "    update_time_ms: 8.3\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 11.462220489288947\n",
      "  time_since_restore: 2937.5938062667847\n",
      "  time_this_iter_s: 36.655781984329224\n",
      "  time_total_s: 2937.5938062667847\n",
      "  timestamp: 1556749248\n",
      "  timesteps_since_restore: 345600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 345600\n",
      "  training_iteration: 72\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2937 s, 72 iter, 345600 ts, 45.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-21-29\n",
      "  done: false\n",
      "  episode_len_mean: 80.67\n",
      "  episode_reward_max: 485.24721120371\n",
      "  episode_reward_mean: 12.13525342280821\n",
      "  episode_reward_min: -356.2693049829697\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 3795\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7195.15\n",
      "    load_time_ms: 1.894\n",
      "    num_steps_sampled: 350400\n",
      "    num_steps_trained: 350400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.235165114924736e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9199456572532654\n",
      "      kl: 0.006624572444707155\n",
      "      policy_loss: -0.0019715989474207163\n",
      "      total_loss: 778.1656494140625\n",
      "      vf_explained_var: 0.7832416892051697\n",
      "      vf_loss: 778.1676635742188\n",
      "    sample_time_ms: 32241.452\n",
      "    update_time_ms: 8.543\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 3.03381335570206\n",
      "  time_since_restore: 2978.8858959674835\n",
      "  time_this_iter_s: 41.29208970069885\n",
      "  time_total_s: 2978.8858959674835\n",
      "  timestamp: 1556749289\n",
      "  timesteps_since_restore: 350400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 350400\n",
      "  training_iteration: 73\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 2978 s, 73 iter, 350400 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-22-08\n",
      "  done: false\n",
      "  episode_len_mean: 80.87\n",
      "  episode_reward_max: 494.1042980707279\n",
      "  episode_reward_mean: 23.197669725119052\n",
      "  episode_reward_min: -356.2693049829697\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 3854\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7007.015\n",
      "    load_time_ms: 1.865\n",
      "    num_steps_sampled: 355200\n",
      "    num_steps_trained: 355200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.117582557462368e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9156453609466553\n",
      "      kl: 0.00638918299227953\n",
      "      policy_loss: -0.0021276124753057957\n",
      "      total_loss: 665.9732055664062\n",
      "      vf_explained_var: 0.7905358672142029\n",
      "      vf_loss: 665.975341796875\n",
      "    sample_time_ms: 31694.901\n",
      "    update_time_ms: 8.469\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 5.799417431279767\n",
      "  time_since_restore: 3017.211418390274\n",
      "  time_this_iter_s: 38.32552242279053\n",
      "  time_total_s: 3017.211418390274\n",
      "  timestamp: 1556749328\n",
      "  timesteps_since_restore: 355200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 355200\n",
      "  training_iteration: 74\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3017 s, 74 iter, 355200 ts, 23.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-22-46\n",
      "  done: false\n",
      "  episode_len_mean: 85.32\n",
      "  episode_reward_max: 494.1042980707279\n",
      "  episode_reward_mean: 92.03802082291475\n",
      "  episode_reward_min: -350.65776410599716\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 3908\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7010.325\n",
      "    load_time_ms: 1.816\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.058791278731184e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9197521805763245\n",
      "      kl: 0.008914700709283352\n",
      "      policy_loss: -0.0035149487666785717\n",
      "      total_loss: 509.7260437011719\n",
      "      vf_explained_var: 0.8442767858505249\n",
      "      vf_loss: 509.7295837402344\n",
      "    sample_time_ms: 31506.63\n",
      "    update_time_ms: 7.765\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.009505205728683\n",
      "  time_since_restore: 3054.995118379593\n",
      "  time_this_iter_s: 37.78369998931885\n",
      "  time_total_s: 3054.995118379593\n",
      "  timestamp: 1556749366\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 75\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3054 s, 75 iter, 360000 ts, 92 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-23-26\n",
      "  done: false\n",
      "  episode_len_mean: 91.65\n",
      "  episode_reward_max: 491.53198013226324\n",
      "  episode_reward_mean: 155.96724890760308\n",
      "  episode_reward_min: -350.65776410599716\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 3959\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7003.595\n",
      "    load_time_ms: 1.804\n",
      "    num_steps_sampled: 364800\n",
      "    num_steps_trained: 364800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.29395639365592e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.937522292137146\n",
      "      kl: 0.00651741586625576\n",
      "      policy_loss: -0.0011389326537027955\n",
      "      total_loss: 617.0531005859375\n",
      "      vf_explained_var: 0.7777485251426697\n",
      "      vf_loss: 617.0543212890625\n",
      "    sample_time_ms: 31778.269\n",
      "    update_time_ms: 7.577\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.99181222690077\n",
      "  time_since_restore: 3095.236565589905\n",
      "  time_this_iter_s: 40.24144721031189\n",
      "  time_total_s: 3095.236565589905\n",
      "  timestamp: 1556749406\n",
      "  timesteps_since_restore: 364800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 364800\n",
      "  training_iteration: 76\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3095 s, 76 iter, 364800 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-24-07\n",
      "  done: false\n",
      "  episode_len_mean: 78.35\n",
      "  episode_reward_max: 491.53198013226324\n",
      "  episode_reward_mean: 11.275320135290501\n",
      "  episode_reward_min: -346.6141167902703\n",
      "  episodes_this_iter: 66\n",
      "  episodes_total: 4025\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6969.323\n",
      "    load_time_ms: 1.758\n",
      "    num_steps_sampled: 369600\n",
      "    num_steps_trained: 369600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.64697819682796e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8913909792900085\n",
      "      kl: 0.008622425608336926\n",
      "      policy_loss: -0.0025770016945898533\n",
      "      total_loss: 795.2704467773438\n",
      "      vf_explained_var: 0.8025006651878357\n",
      "      vf_loss: 795.2730102539062\n",
      "    sample_time_ms: 32190.239\n",
      "    update_time_ms: 7.921\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 2.8188300338226315\n",
      "  time_since_restore: 3136.4401071071625\n",
      "  time_this_iter_s: 41.20354151725769\n",
      "  time_total_s: 3136.4401071071625\n",
      "  timestamp: 1556749447\n",
      "  timesteps_since_restore: 369600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 369600\n",
      "  training_iteration: 77\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3136 s, 77 iter, 369600 ts, 11.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-24-44\n",
      "  done: false\n",
      "  episode_len_mean: 76.38\n",
      "  episode_reward_max: 493.7193151536748\n",
      "  episode_reward_mean: -10.437787877307487\n",
      "  episode_reward_min: -352.7289095819987\n",
      "  episodes_this_iter: 63\n",
      "  episodes_total: 4088\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6883.521\n",
      "    load_time_ms: 1.749\n",
      "    num_steps_sampled: 374400\n",
      "    num_steps_trained: 374400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.32348909841398e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8511034250259399\n",
      "      kl: 0.006201539188623428\n",
      "      policy_loss: -0.002134369220584631\n",
      "      total_loss: 694.4754028320312\n",
      "      vf_explained_var: 0.7935932874679565\n",
      "      vf_loss: 694.4774780273438\n",
      "    sample_time_ms: 32035.987\n",
      "    update_time_ms: 7.917\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -2.6094469693268696\n",
      "  time_since_restore: 3173.554617881775\n",
      "  time_this_iter_s: 37.11451077461243\n",
      "  time_total_s: 3173.554617881775\n",
      "  timestamp: 1556749484\n",
      "  timesteps_since_restore: 374400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 374400\n",
      "  training_iteration: 78\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.8/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3173 s, 78 iter, 374400 ts, -10.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-25-22\n",
      "  done: false\n",
      "  episode_len_mean: 81.48\n",
      "  episode_reward_max: 493.7193151536748\n",
      "  episode_reward_mean: 39.8818146697655\n",
      "  episode_reward_min: -360.62506886243057\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 4143\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6934.46\n",
      "    load_time_ms: 1.796\n",
      "    num_steps_sampled: 379200\n",
      "    num_steps_trained: 379200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.6174454920699e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8655419945716858\n",
      "      kl: 0.007052066270262003\n",
      "      policy_loss: -0.0022089944686740637\n",
      "      total_loss: 562.0120239257812\n",
      "      vf_explained_var: 0.8305402398109436\n",
      "      vf_loss: 562.0142211914062\n",
      "    sample_time_ms: 31817.573\n",
      "    update_time_ms: 7.447\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 9.97045366744138\n",
      "  time_since_restore: 3210.687672138214\n",
      "  time_this_iter_s: 37.13305425643921\n",
      "  time_total_s: 3210.687672138214\n",
      "  timestamp: 1556749522\n",
      "  timesteps_since_restore: 379200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 379200\n",
      "  training_iteration: 79\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3210 s, 79 iter, 379200 ts, 39.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-26-00\n",
      "  done: false\n",
      "  episode_len_mean: 78.58\n",
      "  episode_reward_max: 490.4652108259909\n",
      "  episode_reward_mean: 19.62884813449938\n",
      "  episode_reward_min: -360.62506886243057\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 4203\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6670.491\n",
      "    load_time_ms: 1.671\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.30872274603495e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8632514476776123\n",
      "      kl: 0.005460409913212061\n",
      "      policy_loss: -0.0012314255582168698\n",
      "      total_loss: 561.2254028320312\n",
      "      vf_explained_var: 0.8430723547935486\n",
      "      vf_loss: 561.2266845703125\n",
      "    sample_time_ms: 32009.512\n",
      "    update_time_ms: 7.342\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 4.907212033624844\n",
      "  time_since_restore: 3248.868556022644\n",
      "  time_this_iter_s: 38.18088388442993\n",
      "  time_total_s: 3248.868556022644\n",
      "  timestamp: 1556749560\n",
      "  timesteps_since_restore: 384000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 80\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.8/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3248 s, 80 iter, 384000 ts, 19.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-26-41\n",
      "  done: false\n",
      "  episode_len_mean: 83.86\n",
      "  episode_reward_max: 493.5557666877566\n",
      "  episode_reward_mean: 83.38279321268939\n",
      "  episode_reward_min: -351.19711802192836\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 4259\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6773.16\n",
      "    load_time_ms: 1.941\n",
      "    num_steps_sampled: 388800\n",
      "    num_steps_trained: 388800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.654361373017475e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8643379807472229\n",
      "      kl: 0.006074851378798485\n",
      "      policy_loss: -0.002270590513944626\n",
      "      total_loss: 576.1375732421875\n",
      "      vf_explained_var: 0.8324385285377502\n",
      "      vf_loss: 576.1397705078125\n",
      "    sample_time_ms: 32088.545\n",
      "    update_time_ms: 7.425\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 20.845698303172348\n",
      "  time_since_restore: 3289.8596029281616\n",
      "  time_this_iter_s: 40.99104690551758\n",
      "  time_total_s: 3289.8596029281616\n",
      "  timestamp: 1556749601\n",
      "  timesteps_since_restore: 388800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 388800\n",
      "  training_iteration: 81\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.8/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3289 s, 81 iter, 388800 ts, 83.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-27-20\n",
      "  done: false\n",
      "  episode_len_mean: 80.66\n",
      "  episode_reward_max: 494.2285441972313\n",
      "  episode_reward_mean: 51.707214922505166\n",
      "  episode_reward_min: -347.18337936258627\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 4320\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6769.658\n",
      "    load_time_ms: 1.927\n",
      "    num_steps_sampled: 393600\n",
      "    num_steps_trained: 393600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.271806865087375e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8476192951202393\n",
      "      kl: 0.009080532938241959\n",
      "      policy_loss: -0.003216539742425084\n",
      "      total_loss: 529.6942749023438\n",
      "      vf_explained_var: 0.8673736453056335\n",
      "      vf_loss: 529.697509765625\n",
      "    sample_time_ms: 32288.764\n",
      "    update_time_ms: 8.21\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 12.926803730626288\n",
      "  time_since_restore: 3328.4895157814026\n",
      "  time_this_iter_s: 38.62991285324097\n",
      "  time_total_s: 3328.4895157814026\n",
      "  timestamp: 1556749640\n",
      "  timesteps_since_restore: 393600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 393600\n",
      "  training_iteration: 82\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.8/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3328 s, 82 iter, 393600 ts, 51.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-28-00\n",
      "  done: false\n",
      "  episode_len_mean: 81.79\n",
      "  episode_reward_max: 501.8744581943869\n",
      "  episode_reward_mean: 54.59866567738167\n",
      "  episode_reward_min: -352.3673458935719\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 4375\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6776.834\n",
      "    load_time_ms: 1.873\n",
      "    num_steps_sampled: 398400\n",
      "    num_steps_trained: 398400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.1359034325436877e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8172711133956909\n",
      "      kl: 0.007361210882663727\n",
      "      policy_loss: -0.002946588909253478\n",
      "      total_loss: 625.9811401367188\n",
      "      vf_explained_var: 0.8193516135215759\n",
      "      vf_loss: 625.9840698242188\n",
      "    sample_time_ms: 32241.859\n",
      "    update_time_ms: 8.297\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 13.649666419345422\n",
      "  time_since_restore: 3369.374242067337\n",
      "  time_this_iter_s: 40.88472628593445\n",
      "  time_total_s: 3369.374242067337\n",
      "  timestamp: 1556749680\n",
      "  timesteps_since_restore: 398400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 398400\n",
      "  training_iteration: 83\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.8/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3369 s, 83 iter, 398400 ts, 54.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-28-41\n",
      "  done: false\n",
      "  episode_len_mean: 86.09\n",
      "  episode_reward_max: 501.8744581943869\n",
      "  episode_reward_mean: 97.34048453044656\n",
      "  episode_reward_min: -355.93897283766967\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 4431\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6756.409\n",
      "    load_time_ms: 1.963\n",
      "    num_steps_sampled: 403200\n",
      "    num_steps_trained: 403200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.0679517162718438e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8461409211158752\n",
      "      kl: 0.009006982669234276\n",
      "      policy_loss: -0.002343193395063281\n",
      "      total_loss: 521.9994506835938\n",
      "      vf_explained_var: 0.8514211773872375\n",
      "      vf_loss: 522.0017700195312\n",
      "    sample_time_ms: 32478.254\n",
      "    update_time_ms: 8.444\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.33512113261163\n",
      "  time_since_restore: 3409.86918926239\n",
      "  time_this_iter_s: 40.4949471950531\n",
      "  time_total_s: 3409.86918926239\n",
      "  timestamp: 1556749721\n",
      "  timesteps_since_restore: 403200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 403200\n",
      "  training_iteration: 84\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.8/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3409 s, 84 iter, 403200 ts, 97.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-29-23\n",
      "  done: false\n",
      "  episode_len_mean: 85.04\n",
      "  episode_reward_max: 509.3703997134733\n",
      "  episode_reward_mean: 89.31467581772564\n",
      "  episode_reward_min: -355.93897283766967\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 4487\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6846.497\n",
      "    load_time_ms: 2.019\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0339758581359219e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8117775917053223\n",
      "      kl: 0.008182018995285034\n",
      "      policy_loss: -0.0026301443576812744\n",
      "      total_loss: 550.6478881835938\n",
      "      vf_explained_var: 0.8452164530754089\n",
      "      vf_loss: 550.6505126953125\n",
      "    sample_time_ms: 32819.828\n",
      "    update_time_ms: 8.518\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.3286689544314\n",
      "  time_since_restore: 3451.9739348888397\n",
      "  time_this_iter_s: 42.104745626449585\n",
      "  time_total_s: 3451.9739348888397\n",
      "  timestamp: 1556749763\n",
      "  timesteps_since_restore: 408000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 85\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.8/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3451 s, 85 iter, 408000 ts, 89.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-30-01\n",
      "  done: false\n",
      "  episode_len_mean: 85.42\n",
      "  episode_reward_max: 516.3094338915703\n",
      "  episode_reward_mean: 100.86002100842762\n",
      "  episode_reward_min: -356.88810830167944\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 4541\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6798.711\n",
      "    load_time_ms: 1.982\n",
      "    num_steps_sampled: 412800\n",
      "    num_steps_trained: 412800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.1698792906796096e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8126910328865051\n",
      "      kl: 0.007610978092998266\n",
      "      policy_loss: -0.002597027225419879\n",
      "      total_loss: 391.97308349609375\n",
      "      vf_explained_var: 0.8815608620643616\n",
      "      vf_loss: 391.9756774902344\n",
      "    sample_time_ms: 32607.08\n",
      "    update_time_ms: 8.685\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.215005252106913\n",
      "  time_since_restore: 3489.604317188263\n",
      "  time_this_iter_s: 37.63038229942322\n",
      "  time_total_s: 3489.604317188263\n",
      "  timestamp: 1556749801\n",
      "  timesteps_since_restore: 412800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 412800\n",
      "  training_iteration: 86\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3489 s, 86 iter, 412800 ts, 101 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-30-39\n",
      "  done: false\n",
      "  episode_len_mean: 86.03\n",
      "  episode_reward_max: 522.1411380065338\n",
      "  episode_reward_mean: 99.7486396022465\n",
      "  episode_reward_min: -355.5434791588947\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 4598\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6853.394\n",
      "    load_time_ms: 2.003\n",
      "    num_steps_sampled: 417600\n",
      "    num_steps_trained: 417600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5849396453398048e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8177393674850464\n",
      "      kl: 0.00714136753231287\n",
      "      policy_loss: -0.001931979088112712\n",
      "      total_loss: 618.8154296875\n",
      "      vf_explained_var: 0.8465991616249084\n",
      "      vf_loss: 618.8173217773438\n",
      "    sample_time_ms: 32275.151\n",
      "    update_time_ms: 8.425\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.93715990056163\n",
      "  time_since_restore: 3528.036835670471\n",
      "  time_this_iter_s: 38.43251848220825\n",
      "  time_total_s: 3528.036835670471\n",
      "  timestamp: 1556749839\n",
      "  timesteps_since_restore: 417600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 417600\n",
      "  training_iteration: 87\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3528 s, 87 iter, 417600 ts, 99.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-31-17\n",
      "  done: false\n",
      "  episode_len_mean: 86.07\n",
      "  episode_reward_max: 522.1411380065338\n",
      "  episode_reward_mean: 90.94507589958388\n",
      "  episode_reward_min: -356.396305978374\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 4654\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6846.866\n",
      "    load_time_ms: 2.073\n",
      "    num_steps_sampled: 422400\n",
      "    num_steps_trained: 422400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2924698226699024e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7865538597106934\n",
      "      kl: 0.006979021243751049\n",
      "      policy_loss: -0.0018038376001641154\n",
      "      total_loss: 685.5689697265625\n",
      "      vf_explained_var: 0.8294551372528076\n",
      "      vf_loss: 685.5708618164062\n",
      "    sample_time_ms: 32355.734\n",
      "    update_time_ms: 8.695\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.736268974895975\n",
      "  time_since_restore: 3565.899659395218\n",
      "  time_this_iter_s: 37.862823724746704\n",
      "  time_total_s: 3565.899659395218\n",
      "  timestamp: 1556749877\n",
      "  timesteps_since_restore: 422400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 422400\n",
      "  training_iteration: 88\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3565 s, 88 iter, 422400 ts, 90.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-32-02\n",
      "  done: false\n",
      "  episode_len_mean: 81.15\n",
      "  episode_reward_max: 515.9856479916798\n",
      "  episode_reward_mean: 45.67030076904561\n",
      "  episode_reward_min: -352.06580048410245\n",
      "  episodes_this_iter: 62\n",
      "  episodes_total: 4716\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7000.237\n",
      "    load_time_ms: 2.102\n",
      "    num_steps_sampled: 427200\n",
      "    num_steps_trained: 427200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.462349113349512e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7918748259544373\n",
      "      kl: 0.0074981204234063625\n",
      "      policy_loss: -0.0018955859122797847\n",
      "      total_loss: 592.870849609375\n",
      "      vf_explained_var: 0.8571716547012329\n",
      "      vf_loss: 592.8727416992188\n",
      "    sample_time_ms: 32927.071\n",
      "    update_time_ms: 9.01\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 11.417575192261396\n",
      "  time_since_restore: 3610.289406299591\n",
      "  time_this_iter_s: 44.38974690437317\n",
      "  time_total_s: 3610.289406299591\n",
      "  timestamp: 1556749922\n",
      "  timesteps_since_restore: 427200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 427200\n",
      "  training_iteration: 89\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.0/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3610 s, 89 iter, 427200 ts, 45.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-32-41\n",
      "  done: false\n",
      "  episode_len_mean: 79.05\n",
      "  episode_reward_max: 515.9856479916798\n",
      "  episode_reward_mean: 46.078713840223124\n",
      "  episode_reward_min: -352.06580048410245\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 4775\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7051.526\n",
      "    load_time_ms: 2.136\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.231174556674756e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7642082571983337\n",
      "      kl: 0.010638615116477013\n",
      "      policy_loss: -0.002909261267632246\n",
      "      total_loss: 404.5239562988281\n",
      "      vf_explained_var: 0.8940222263336182\n",
      "      vf_loss: 404.5268859863281\n",
      "    sample_time_ms: 32986.179\n",
      "    update_time_ms: 9.337\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 11.519678460055776\n",
      "  time_since_restore: 3649.5926938056946\n",
      "  time_this_iter_s: 39.303287506103516\n",
      "  time_total_s: 3649.5926938056946\n",
      "  timestamp: 1556749961\n",
      "  timesteps_since_restore: 432000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 90\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.0/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3649 s, 90 iter, 432000 ts, 46.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-33-19\n",
      "  done: false\n",
      "  episode_len_mean: 83.27\n",
      "  episode_reward_max: 514.7869291328583\n",
      "  episode_reward_mean: 70.13398141785659\n",
      "  episode_reward_min: -349.4692215154928\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 4829\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7029.879\n",
      "    load_time_ms: 1.897\n",
      "    num_steps_sampled: 436800\n",
      "    num_steps_trained: 436800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.231174556674756e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7671472430229187\n",
      "      kl: 0.006072347518056631\n",
      "      policy_loss: -0.0021513267420232296\n",
      "      total_loss: 611.2864379882812\n",
      "      vf_explained_var: 0.8443423509597778\n",
      "      vf_loss: 611.2885131835938\n",
      "    sample_time_ms: 32700.252\n",
      "    update_time_ms: 9.355\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 17.53349535446415\n",
      "  time_since_restore: 3687.5050041675568\n",
      "  time_this_iter_s: 37.91231036186218\n",
      "  time_total_s: 3687.5050041675568\n",
      "  timestamp: 1556749999\n",
      "  timesteps_since_restore: 436800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 436800\n",
      "  training_iteration: 91\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.0/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3687 s, 91 iter, 436800 ts, 70.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-33-58\n",
      "  done: false\n",
      "  episode_len_mean: 86.08\n",
      "  episode_reward_max: 528.89950761439\n",
      "  episode_reward_mean: 90.05056435251483\n",
      "  episode_reward_min: -357.37731582494854\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 4887\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7029.179\n",
      "    load_time_ms: 1.963\n",
      "    num_steps_sampled: 441600\n",
      "    num_steps_trained: 441600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.615587278337378e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7790814638137817\n",
      "      kl: 0.0072662862949073315\n",
      "      policy_loss: -0.0032684695906937122\n",
      "      total_loss: 599.17919921875\n",
      "      vf_explained_var: 0.8481505513191223\n",
      "      vf_loss: 599.1824951171875\n",
      "    sample_time_ms: 32716.508\n",
      "    update_time_ms: 8.892\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.512641088128714\n",
      "  time_since_restore: 3726.2832446098328\n",
      "  time_this_iter_s: 38.778240442276\n",
      "  time_total_s: 3726.2832446098328\n",
      "  timestamp: 1556750038\n",
      "  timesteps_since_restore: 441600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 441600\n",
      "  training_iteration: 92\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.0/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3726 s, 92 iter, 441600 ts, 90.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-34-38\n",
      "  done: false\n",
      "  episode_len_mean: 82.56\n",
      "  episode_reward_max: 534.7576330436066\n",
      "  episode_reward_mean: 75.99183267507314\n",
      "  episode_reward_min: -357.37731582494854\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 4944\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7145.01\n",
      "    load_time_ms: 1.969\n",
      "    num_steps_sampled: 446400\n",
      "    num_steps_trained: 446400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.07793639168689e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7922409176826477\n",
      "      kl: 0.008210127241909504\n",
      "      policy_loss: -0.0030206087976694107\n",
      "      total_loss: 486.1016540527344\n",
      "      vf_explained_var: 0.8795536160469055\n",
      "      vf_loss: 486.1046447753906\n",
      "    sample_time_ms: 32502.523\n",
      "    update_time_ms: 9.044\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 18.997958168768285\n",
      "  time_since_restore: 3766.187331676483\n",
      "  time_this_iter_s: 39.90408706665039\n",
      "  time_total_s: 3766.187331676483\n",
      "  timestamp: 1556750078\n",
      "  timesteps_since_restore: 446400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 446400\n",
      "  training_iteration: 93\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3766 s, 93 iter, 446400 ts, 76 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-35-17\n",
      "  done: false\n",
      "  episode_len_mean: 87.34\n",
      "  episode_reward_max: 534.7576330436066\n",
      "  episode_reward_mean: 135.05472918444906\n",
      "  episode_reward_min: -347.293407903112\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 4997\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7204.258\n",
      "    load_time_ms: 1.868\n",
      "    num_steps_sampled: 451200\n",
      "    num_steps_trained: 451200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.038968195843445e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.783237874507904\n",
      "      kl: 0.008593311533331871\n",
      "      policy_loss: -0.0036234664730727673\n",
      "      total_loss: 609.34716796875\n",
      "      vf_explained_var: 0.8406070470809937\n",
      "      vf_loss: 609.350830078125\n",
      "    sample_time_ms: 32341.292\n",
      "    update_time_ms: 9.312\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.76368229611228\n",
      "  time_since_restore: 3805.652600288391\n",
      "  time_this_iter_s: 39.46526861190796\n",
      "  time_total_s: 3805.652600288391\n",
      "  timestamp: 1556750117\n",
      "  timesteps_since_restore: 451200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 451200\n",
      "  training_iteration: 94\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3805 s, 94 iter, 451200 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-35-59\n",
      "  done: false\n",
      "  episode_len_mean: 84.95\n",
      "  episode_reward_max: 530.3597274261438\n",
      "  episode_reward_mean: 101.92912214727899\n",
      "  episode_reward_min: -345.2071620259742\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 5055\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7183.524\n",
      "    load_time_ms: 1.924\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.0194840979217225e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7515163421630859\n",
      "      kl: 0.00869669858366251\n",
      "      policy_loss: -0.002475847490131855\n",
      "      total_loss: 615.0845947265625\n",
      "      vf_explained_var: 0.8475655317306519\n",
      "      vf_loss: 615.0870971679688\n",
      "    sample_time_ms: 32275.479\n",
      "    update_time_ms: 8.892\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.482280536819747\n",
      "  time_since_restore: 3846.8812685012817\n",
      "  time_this_iter_s: 41.228668212890625\n",
      "  time_total_s: 3846.8812685012817\n",
      "  timestamp: 1556750159\n",
      "  timesteps_since_restore: 456000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 95\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 14.9/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3846 s, 95 iter, 456000 ts, 102 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-36-39\n",
      "  done: false\n",
      "  episode_len_mean: 81.6\n",
      "  episode_reward_max: 527.9756714484729\n",
      "  episode_reward_mean: 64.14505396689351\n",
      "  episode_reward_min: -353.59508517098396\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 5116\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7214.562\n",
      "    load_time_ms: 1.948\n",
      "    num_steps_sampled: 460800\n",
      "    num_steps_trained: 460800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0097420489608613e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7545409202575684\n",
      "      kl: 0.009465320967137814\n",
      "      policy_loss: -0.002828693250194192\n",
      "      total_loss: 495.71551513671875\n",
      "      vf_explained_var: 0.8823407888412476\n",
      "      vf_loss: 495.71832275390625\n",
      "    sample_time_ms: 32526.544\n",
      "    update_time_ms: 8.655\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.036263491723375\n",
      "  time_since_restore: 3887.33806681633\n",
      "  time_this_iter_s: 40.45679831504822\n",
      "  time_total_s: 3887.33806681633\n",
      "  timestamp: 1556750199\n",
      "  timesteps_since_restore: 460800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 460800\n",
      "  training_iteration: 96\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.0/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3887 s, 96 iter, 460800 ts, 64.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-37-18\n",
      "  done: false\n",
      "  episode_len_mean: 86.22\n",
      "  episode_reward_max: 527.9756714484729\n",
      "  episode_reward_mean: 116.69740003331509\n",
      "  episode_reward_min: -353.59508517098396\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 5169\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7313.767\n",
      "    load_time_ms: 1.925\n",
      "    num_steps_sampled: 465600\n",
      "    num_steps_trained: 465600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.048710244804306e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7514452338218689\n",
      "      kl: 0.006464523263275623\n",
      "      policy_loss: -0.001135745202191174\n",
      "      total_loss: 554.3624877929688\n",
      "      vf_explained_var: 0.8486908078193665\n",
      "      vf_loss: 554.3636474609375\n",
      "    sample_time_ms: 32486.614\n",
      "    update_time_ms: 8.675\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.174350008328773\n",
      "  time_since_restore: 3926.3695554733276\n",
      "  time_this_iter_s: 39.03148865699768\n",
      "  time_total_s: 3926.3695554733276\n",
      "  timestamp: 1556750238\n",
      "  timesteps_since_restore: 465600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 465600\n",
      "  training_iteration: 97\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.0/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3926 s, 97 iter, 465600 ts, 117 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-37-59\n",
      "  done: false\n",
      "  episode_len_mean: 83.59\n",
      "  episode_reward_max: 532.3870022573686\n",
      "  episode_reward_mean: 94.4547329322979\n",
      "  episode_reward_min: -357.58295396096787\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 5229\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7289.785\n",
      "    load_time_ms: 1.913\n",
      "    num_steps_sampled: 470400\n",
      "    num_steps_trained: 470400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.524355122402153e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7328048944473267\n",
      "      kl: 0.009686265140771866\n",
      "      policy_loss: -0.0015490191290155053\n",
      "      total_loss: 504.6986389160156\n",
      "      vf_explained_var: 0.874772310256958\n",
      "      vf_loss: 504.7001953125\n",
      "    sample_time_ms: 32766.643\n",
      "    update_time_ms: 8.804\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.61368323307448\n",
      "  time_since_restore: 3966.7888209819794\n",
      "  time_this_iter_s: 40.41926550865173\n",
      "  time_total_s: 3966.7888209819794\n",
      "  timestamp: 1556750279\n",
      "  timesteps_since_restore: 470400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 470400\n",
      "  training_iteration: 98\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.0/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 3966 s, 98 iter, 470400 ts, 94.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-38-36\n",
      "  done: false\n",
      "  episode_len_mean: 85.86\n",
      "  episode_reward_max: 532.3870022573686\n",
      "  episode_reward_mean: 97.59988655228005\n",
      "  episode_reward_min: -357.58295396096787\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 5281\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7131.288\n",
      "    load_time_ms: 2.015\n",
      "    num_steps_sampled: 475200\n",
      "    num_steps_trained: 475200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2621775612010766e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7469915747642517\n",
      "      kl: 0.00824702624231577\n",
      "      policy_loss: -0.0020387358963489532\n",
      "      total_loss: 788.537109375\n",
      "      vf_explained_var: 0.768460750579834\n",
      "      vf_loss: 788.5391235351562\n",
      "    sample_time_ms: 32214.265\n",
      "    update_time_ms: 8.621\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.39997163807001\n",
      "  time_since_restore: 4004.0608248710632\n",
      "  time_this_iter_s: 37.27200388908386\n",
      "  time_total_s: 4004.0608248710632\n",
      "  timestamp: 1556750316\n",
      "  timesteps_since_restore: 475200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 475200\n",
      "  training_iteration: 99\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.0/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4004 s, 99 iter, 475200 ts, 97.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-39-13\n",
      "  done: false\n",
      "  episode_len_mean: 87.49\n",
      "  episode_reward_max: 528.5952211796686\n",
      "  episode_reward_mean: 118.00786352044068\n",
      "  episode_reward_min: -340.7499848325765\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 5340\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7115.726\n",
      "    load_time_ms: 1.975\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.310887806005383e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7089183330535889\n",
      "      kl: 0.0069887638092041016\n",
      "      policy_loss: -0.0013305067550390959\n",
      "      total_loss: 780.0350952148438\n",
      "      vf_explained_var: 0.7792460918426514\n",
      "      vf_loss: 780.0364379882812\n",
      "    sample_time_ms: 32025.929\n",
      "    update_time_ms: 8.201\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.501965880110156\n",
      "  time_since_restore: 4041.3108677864075\n",
      "  time_this_iter_s: 37.25004291534424\n",
      "  time_total_s: 4041.3108677864075\n",
      "  timestamp: 1556750353\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 100\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.1/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4041 s, 100 iter, 480000 ts, 118 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-39-55\n",
      "  done: false\n",
      "  episode_len_mean: 82.01\n",
      "  episode_reward_max: 528.5952211796686\n",
      "  episode_reward_mean: 69.7195560077166\n",
      "  episode_reward_min: -340.7499848325765\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 5396\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7161.875\n",
      "    load_time_ms: 1.95\n",
      "    num_steps_sampled: 484800\n",
      "    num_steps_trained: 484800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.1554439030026914e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7132413983345032\n",
      "      kl: 0.005620833020657301\n",
      "      policy_loss: -0.0016443983186036348\n",
      "      total_loss: 614.8569946289062\n",
      "      vf_explained_var: 0.8476470708847046\n",
      "      vf_loss: 614.8585815429688\n",
      "    sample_time_ms: 32374.062\n",
      "    update_time_ms: 8.052\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 17.429889001929144\n",
      "  time_since_restore: 4083.1686420440674\n",
      "  time_this_iter_s: 41.85777425765991\n",
      "  time_total_s: 4083.1686420440674\n",
      "  timestamp: 1556750395\n",
      "  timesteps_since_restore: 484800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 484800\n",
      "  training_iteration: 101\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.1/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4083 s, 101 iter, 484800 ts, 69.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-40-36\n",
      "  done: false\n",
      "  episode_len_mean: 80.69\n",
      "  episode_reward_max: 529.9628421557069\n",
      "  episode_reward_mean: 56.389176949631675\n",
      "  episode_reward_min: -354.1899593428109\n",
      "  episodes_this_iter: 63\n",
      "  episodes_total: 5459\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7209.59\n",
      "    load_time_ms: 1.878\n",
      "    num_steps_sampled: 489600\n",
      "    num_steps_trained: 489600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5777219515013457e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6972619891166687\n",
      "      kl: 0.009457043372094631\n",
      "      policy_loss: -0.004335570149123669\n",
      "      total_loss: 564.6094970703125\n",
      "      vf_explained_var: 0.8700898289680481\n",
      "      vf_loss: 564.6138305664062\n",
      "    sample_time_ms: 32574.398\n",
      "    update_time_ms: 8.155\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 14.097294237407914\n",
      "  time_since_restore: 4124.429430961609\n",
      "  time_this_iter_s: 41.260788917541504\n",
      "  time_total_s: 4124.429430961609\n",
      "  timestamp: 1556750436\n",
      "  timesteps_since_restore: 489600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 489600\n",
      "  training_iteration: 102\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.1/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4124 s, 102 iter, 489600 ts, 56.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-41-14\n",
      "  done: false\n",
      "  episode_len_mean: 87.16\n",
      "  episode_reward_max: 529.9628421557069\n",
      "  episode_reward_mean: 127.61023483362365\n",
      "  episode_reward_min: -354.1899593428109\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 5512\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7062.684\n",
      "    load_time_ms: 1.963\n",
      "    num_steps_sampled: 494400\n",
      "    num_steps_trained: 494400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.888609757506729e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.693284809589386\n",
      "      kl: 0.006947115063667297\n",
      "      policy_loss: -0.0022759106941521168\n",
      "      total_loss: 433.2547912597656\n",
      "      vf_explained_var: 0.9012366533279419\n",
      "      vf_loss: 433.2570495605469\n",
      "    sample_time_ms: 32487.511\n",
      "    update_time_ms: 7.994\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.902558708405913\n",
      "  time_since_restore: 4162.001654624939\n",
      "  time_this_iter_s: 37.57222366333008\n",
      "  time_total_s: 4162.001654624939\n",
      "  timestamp: 1556750474\n",
      "  timesteps_since_restore: 494400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 494400\n",
      "  training_iteration: 103\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.1/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4162 s, 103 iter, 494400 ts, 128 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 84.48\n",
      "  episode_reward_max: 525.3082947147195\n",
      "  episode_reward_mean: 114.20075668562603\n",
      "  episode_reward_min: -351.3182846736094\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 5570\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6974.689\n",
      "    load_time_ms: 1.961\n",
      "    num_steps_sampled: 499200\n",
      "    num_steps_trained: 499200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.9443048787533643e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6812801361083984\n",
      "      kl: 0.01118260994553566\n",
      "      policy_loss: -0.0032186745665967464\n",
      "      total_loss: 365.73712158203125\n",
      "      vf_explained_var: 0.9197685718536377\n",
      "      vf_loss: 365.7403564453125\n",
      "    sample_time_ms: 32434.694\n",
      "    update_time_ms: 7.502\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.550189171406508\n",
      "  time_since_restore: 4200.051404237747\n",
      "  time_this_iter_s: 38.04974961280823\n",
      "  time_total_s: 4200.051404237747\n",
      "  timestamp: 1556750512\n",
      "  timesteps_since_restore: 499200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 499200\n",
      "  training_iteration: 104\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.3/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4200 s, 104 iter, 499200 ts, 114 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-42-29\n",
      "  done: false\n",
      "  episode_len_mean: 81.97\n",
      "  episode_reward_max: 536.5680995032034\n",
      "  episode_reward_mean: 77.16013279253342\n",
      "  episode_reward_min: -353.72623305433206\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 5626\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6845.192\n",
      "    load_time_ms: 1.854\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.9443048787533643e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6945483684539795\n",
      "      kl: 0.011463440954685211\n",
      "      policy_loss: -0.0045765358954668045\n",
      "      total_loss: 585.7093505859375\n",
      "      vf_explained_var: 0.8603614568710327\n",
      "      vf_loss: 585.7139282226562\n",
      "    sample_time_ms: 32159.398\n",
      "    update_time_ms: 7.799\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 19.290033198133354\n",
      "  time_since_restore: 4237.233603954315\n",
      "  time_this_iter_s: 37.18219971656799\n",
      "  time_total_s: 4237.233603954315\n",
      "  timestamp: 1556750549\n",
      "  timesteps_since_restore: 504000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 105\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.1/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4237 s, 105 iter, 504000 ts, 77.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-43-12\n",
      "  done: false\n",
      "  episode_len_mean: 87.03\n",
      "  episode_reward_max: 536.5680995032034\n",
      "  episode_reward_mean: 132.4221491099459\n",
      "  episode_reward_min: -354.2804601503716\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 5679\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6922.556\n",
      "    load_time_ms: 1.821\n",
      "    num_steps_sampled: 508800\n",
      "    num_steps_trained: 508800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.9443048787533643e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6901257634162903\n",
      "      kl: 0.006643270142376423\n",
      "      policy_loss: -0.0022609750740230083\n",
      "      total_loss: 489.5936584472656\n",
      "      vf_explained_var: 0.871685802936554\n",
      "      vf_loss: 489.5958251953125\n",
      "    sample_time_ms: 32255.326\n",
      "    update_time_ms: 8.184\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.10553727748647\n",
      "  time_since_restore: 4279.419133424759\n",
      "  time_this_iter_s: 42.185529470443726\n",
      "  time_total_s: 4279.419133424759\n",
      "  timestamp: 1556750592\n",
      "  timesteps_since_restore: 508800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 508800\n",
      "  training_iteration: 106\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.2/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4279 s, 106 iter, 508800 ts, 132 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-43-52\n",
      "  done: false\n",
      "  episode_len_mean: 82.7\n",
      "  episode_reward_max: 530.0673986435573\n",
      "  episode_reward_mean: 76.03009731136085\n",
      "  episode_reward_min: -352.9442768165079\n",
      "  episodes_this_iter: 62\n",
      "  episodes_total: 5741\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6899.718\n",
      "    load_time_ms: 1.935\n",
      "    num_steps_sampled: 513600\n",
      "    num_steps_trained: 513600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9721524393766821e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6682995557785034\n",
      "      kl: 0.006379823666065931\n",
      "      policy_loss: -0.0014777644537389278\n",
      "      total_loss: 612.09521484375\n",
      "      vf_explained_var: 0.8704416751861572\n",
      "      vf_loss: 612.0967407226562\n",
      "    sample_time_ms: 32449.656\n",
      "    update_time_ms: 8.109\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 19.007524327840205\n",
      "  time_since_restore: 4320.170209169388\n",
      "  time_this_iter_s: 40.751075744628906\n",
      "  time_total_s: 4320.170209169388\n",
      "  timestamp: 1556750632\n",
      "  timesteps_since_restore: 513600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 513600\n",
      "  training_iteration: 107\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.2/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4320 s, 107 iter, 513600 ts, 76 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-44-32\n",
      "  done: false\n",
      "  episode_len_mean: 76.86\n",
      "  episode_reward_max: 530.7917268910637\n",
      "  episode_reward_mean: 12.646605886052532\n",
      "  episode_reward_min: -352.9442768165079\n",
      "  episodes_this_iter: 62\n",
      "  episodes_total: 5803\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6918.417\n",
      "    load_time_ms: 1.881\n",
      "    num_steps_sampled: 518400\n",
      "    num_steps_trained: 518400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.860762196883411e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6622582077980042\n",
      "      kl: 0.00888583529740572\n",
      "      policy_loss: -0.0017440979136154056\n",
      "      total_loss: 549.2606811523438\n",
      "      vf_explained_var: 0.8737028241157532\n",
      "      vf_loss: 549.2625122070312\n",
      "    sample_time_ms: 32347.611\n",
      "    update_time_ms: 7.861\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 3.161651471513125\n",
      "  time_since_restore: 4359.754760026932\n",
      "  time_this_iter_s: 39.584550857543945\n",
      "  time_total_s: 4359.754760026932\n",
      "  timestamp: 1556750672\n",
      "  timesteps_since_restore: 518400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 518400\n",
      "  training_iteration: 108\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.2/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4359 s, 108 iter, 518400 ts, 12.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-45-11\n",
      "  done: false\n",
      "  episode_len_mean: 76.93\n",
      "  episode_reward_max: 530.7917268910637\n",
      "  episode_reward_mean: 16.165016682730663\n",
      "  episode_reward_min: -348.5055238833788\n",
      "  episodes_this_iter: 63\n",
      "  episodes_total: 5866\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6918.625\n",
      "    load_time_ms: 1.705\n",
      "    num_steps_sampled: 523200\n",
      "    num_steps_trained: 523200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.9303810984417053e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.645146369934082\n",
      "      kl: 0.006362144835293293\n",
      "      policy_loss: -0.002496786415576935\n",
      "      total_loss: 691.1250610351562\n",
      "      vf_explained_var: 0.8457622528076172\n",
      "      vf_loss: 691.1275024414062\n",
      "    sample_time_ms: 32470.971\n",
      "    update_time_ms: 8.555\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 4.0412541706826595\n",
      "  time_since_restore: 4398.267507076263\n",
      "  time_this_iter_s: 38.512747049331665\n",
      "  time_total_s: 4398.267507076263\n",
      "  timestamp: 1556750711\n",
      "  timesteps_since_restore: 523200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 523200\n",
      "  training_iteration: 109\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.2/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4398 s, 109 iter, 523200 ts, 16.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-45-51\n",
      "  done: false\n",
      "  episode_len_mean: 81.72\n",
      "  episode_reward_max: 542.7361257120178\n",
      "  episode_reward_mean: 70.67931128663459\n",
      "  episode_reward_min: -356.91520563911183\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 5921\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7070.619\n",
      "    load_time_ms: 1.789\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4651905492208527e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6513431072235107\n",
      "      kl: 0.008430783636868\n",
      "      policy_loss: -0.0018436582759022713\n",
      "      total_loss: 360.33233642578125\n",
      "      vf_explained_var: 0.9222263693809509\n",
      "      vf_loss: 360.3341979980469\n",
      "    sample_time_ms: 32633.932\n",
      "    update_time_ms: 8.576\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 17.669827821658636\n",
      "  time_since_restore: 4438.671209096909\n",
      "  time_this_iter_s: 40.40370202064514\n",
      "  time_total_s: 4438.671209096909\n",
      "  timestamp: 1556750751\n",
      "  timesteps_since_restore: 528000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 110\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.2/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4438 s, 110 iter, 528000 ts, 70.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-46-30\n",
      "  done: false\n",
      "  episode_len_mean: 84.6\n",
      "  episode_reward_max: 542.7361257120178\n",
      "  episode_reward_mean: 107.68397771015017\n",
      "  episode_reward_min: -356.91520563911183\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 5981\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6979.433\n",
      "    load_time_ms: 1.775\n",
      "    num_steps_sampled: 532800\n",
      "    num_steps_trained: 532800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2325952746104263e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6310718059539795\n",
      "      kl: 0.007108776364475489\n",
      "      policy_loss: -0.0017698566662147641\n",
      "      total_loss: 599.8565063476562\n",
      "      vf_explained_var: 0.8636065721511841\n",
      "      vf_loss: 599.8582763671875\n",
      "    sample_time_ms: 32405.767\n",
      "    update_time_ms: 8.798\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.920994427537536\n",
      "  time_since_restore: 4477.324610710144\n",
      "  time_this_iter_s: 38.653401613235474\n",
      "  time_total_s: 4477.324610710144\n",
      "  timestamp: 1556750790\n",
      "  timesteps_since_restore: 532800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 532800\n",
      "  training_iteration: 111\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.3/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4477 s, 111 iter, 532800 ts, 108 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-47-08\n",
      "  done: false\n",
      "  episode_len_mean: 79.97\n",
      "  episode_reward_max: 530.4035933414998\n",
      "  episode_reward_mean: 54.48250953466256\n",
      "  episode_reward_min: -354.18464262724234\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 6040\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6957.198\n",
      "    load_time_ms: 1.868\n",
      "    num_steps_sampled: 537600\n",
      "    num_steps_trained: 537600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.162976373052132e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6131161451339722\n",
      "      kl: 0.010963610373437405\n",
      "      policy_loss: -0.002463718643411994\n",
      "      total_loss: 380.0413513183594\n",
      "      vf_explained_var: 0.9191381931304932\n",
      "      vf_loss: 380.0437927246094\n",
      "    sample_time_ms: 32100.966\n",
      "    update_time_ms: 8.353\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 13.620627383665637\n",
      "  time_since_restore: 4515.311199188232\n",
      "  time_this_iter_s: 37.98658847808838\n",
      "  time_total_s: 4515.311199188232\n",
      "  timestamp: 1556750828\n",
      "  timesteps_since_restore: 537600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 537600\n",
      "  training_iteration: 112\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.3/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4515 s, 112 iter, 537600 ts, 54.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-47-46\n",
      "  done: false\n",
      "  episode_len_mean: 89.18\n",
      "  episode_reward_max: 532.09611744704\n",
      "  episode_reward_mean: 147.57500191579652\n",
      "  episode_reward_min: -345.03620356524533\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 6090\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7009.349\n",
      "    load_time_ms: 1.75\n",
      "    num_steps_sampled: 542400\n",
      "    num_steps_trained: 542400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.162976373052132e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6396499872207642\n",
      "      kl: 0.008748456835746765\n",
      "      policy_loss: -0.0017616780241951346\n",
      "      total_loss: 489.39727783203125\n",
      "      vf_explained_var: 0.8496689796447754\n",
      "      vf_loss: 489.3990478515625\n",
      "    sample_time_ms: 32100.46\n",
      "    update_time_ms: 7.86\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.89375047894912\n",
      "  time_since_restore: 4553.391674757004\n",
      "  time_this_iter_s: 38.08047556877136\n",
      "  time_total_s: 4553.391674757004\n",
      "  timestamp: 1556750866\n",
      "  timesteps_since_restore: 542400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 542400\n",
      "  training_iteration: 113\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.2/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4553 s, 113 iter, 542400 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-48-26\n",
      "  done: false\n",
      "  episode_len_mean: 90.62\n",
      "  episode_reward_max: 533.9059063310841\n",
      "  episode_reward_mean: 159.9541213283525\n",
      "  episode_reward_min: -338.54325322440604\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6146\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7209.035\n",
      "    load_time_ms: 1.843\n",
      "    num_steps_sampled: 547200\n",
      "    num_steps_trained: 547200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.081488186526066e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5903859734535217\n",
      "      kl: 0.008310590870678425\n",
      "      policy_loss: -0.001644912175834179\n",
      "      total_loss: 459.6434326171875\n",
      "      vf_explained_var: 0.8956624269485474\n",
      "      vf_loss: 459.6451110839844\n",
      "    sample_time_ms: 32112.204\n",
      "    update_time_ms: 8.165\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.98853033208813\n",
      "  time_since_restore: 4593.565719842911\n",
      "  time_this_iter_s: 40.17404508590698\n",
      "  time_total_s: 4593.565719842911\n",
      "  timestamp: 1556750906\n",
      "  timesteps_since_restore: 547200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 547200\n",
      "  training_iteration: 114\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.2/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4593 s, 114 iter, 547200 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-49-06\n",
      "  done: false\n",
      "  episode_len_mean: 86.43\n",
      "  episode_reward_max: 530.978762763806\n",
      "  episode_reward_mean: 117.61060391407125\n",
      "  episode_reward_min: -349.4956153696587\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6202\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7201.022\n",
      "    load_time_ms: 1.837\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.540744093263033e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.602862536907196\n",
      "      kl: 0.008295537903904915\n",
      "      policy_loss: -0.0017495984211564064\n",
      "      total_loss: 392.50433349609375\n",
      "      vf_explained_var: 0.9147650003433228\n",
      "      vf_loss: 392.50604248046875\n",
      "    sample_time_ms: 32411.425\n",
      "    update_time_ms: 8.705\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.40265097851782\n",
      "  time_since_restore: 4633.662610769272\n",
      "  time_this_iter_s: 40.096890926361084\n",
      "  time_total_s: 4633.662610769272\n",
      "  timestamp: 1556750946\n",
      "  timesteps_since_restore: 552000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 115\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.2/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4633 s, 115 iter, 552000 ts, 118 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-49-46\n",
      "  done: false\n",
      "  episode_len_mean: 82.62\n",
      "  episode_reward_max: 530.800024942157\n",
      "  episode_reward_mean: 55.82097212387519\n",
      "  episode_reward_min: -349.4956153696587\n",
      "  episodes_this_iter: 64\n",
      "  episodes_total: 6266\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7142.907\n",
      "    load_time_ms: 1.881\n",
      "    num_steps_sampled: 556800\n",
      "    num_steps_trained: 556800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.703720466315165e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5868619680404663\n",
      "      kl: 0.007397280540317297\n",
      "      policy_loss: -0.0026392654981464148\n",
      "      total_loss: 667.0790405273438\n",
      "      vf_explained_var: 0.8671376705169678\n",
      "      vf_loss: 667.081787109375\n",
      "    sample_time_ms: 32186.02\n",
      "    update_time_ms: 9.176\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 13.955243030968802\n",
      "  time_since_restore: 4673.026904344559\n",
      "  time_this_iter_s: 39.364293575286865\n",
      "  time_total_s: 4673.026904344559\n",
      "  timestamp: 1556750986\n",
      "  timesteps_since_restore: 556800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 556800\n",
      "  training_iteration: 116\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.4/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4673 s, 116 iter, 556800 ts, 55.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-50-24\n",
      "  done: false\n",
      "  episode_len_mean: 72.72\n",
      "  episode_reward_max: 530.800024942157\n",
      "  episode_reward_mean: -27.052006571019888\n",
      "  episode_reward_min: -354.4341122901145\n",
      "  episodes_this_iter: 66\n",
      "  episodes_total: 6332\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7098.103\n",
      "    load_time_ms: 1.767\n",
      "    num_steps_sampled: 561600\n",
      "    num_steps_trained: 561600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.8518602331575823e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5844957232475281\n",
      "      kl: 0.006480053067207336\n",
      "      policy_loss: -0.0014911823673173785\n",
      "      total_loss: 481.0604248046875\n",
      "      vf_explained_var: 0.9067612886428833\n",
      "      vf_loss: 481.0618896484375\n",
      "    sample_time_ms: 31999.838\n",
      "    update_time_ms: 9.614\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: -6.763001642754975\n",
      "  time_since_restore: 4711.46376490593\n",
      "  time_this_iter_s: 38.43686056137085\n",
      "  time_total_s: 4711.46376490593\n",
      "  timestamp: 1556751024\n",
      "  timesteps_since_restore: 561600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 561600\n",
      "  training_iteration: 117\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.3/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4711 s, 117 iter, 561600 ts, -27.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-51-05\n",
      "  done: false\n",
      "  episode_len_mean: 75.8\n",
      "  episode_reward_max: 533.2656280208535\n",
      "  episode_reward_mean: 18.93769164801548\n",
      "  episode_reward_min: -356.1617827136737\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 6391\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7236.937\n",
      "    load_time_ms: 1.8\n",
      "    num_steps_sampled: 566400\n",
      "    num_steps_trained: 566400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9259301165787911e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.549441397190094\n",
      "      kl: 0.008177647367119789\n",
      "      policy_loss: -0.0018647249089553952\n",
      "      total_loss: 324.5919189453125\n",
      "      vf_explained_var: 0.9275667071342468\n",
      "      vf_loss: 324.5937805175781\n",
      "    sample_time_ms: 31992.92\n",
      "    update_time_ms: 9.828\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 4.7344229120038674\n",
      "  time_since_restore: 4752.37572145462\n",
      "  time_this_iter_s: 40.911956548690796\n",
      "  time_total_s: 4752.37572145462\n",
      "  timestamp: 1556751065\n",
      "  timesteps_since_restore: 566400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 566400\n",
      "  training_iteration: 118\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.3/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4752 s, 118 iter, 566400 ts, 18.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-51-47\n",
      "  done: false\n",
      "  episode_len_mean: 83.57\n",
      "  episode_reward_max: 533.6954220908323\n",
      "  episode_reward_mean: 88.045779496717\n",
      "  episode_reward_min: -356.1617827136737\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 6448\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7347.606\n",
      "    load_time_ms: 1.874\n",
      "    num_steps_sampled: 571200\n",
      "    num_steps_trained: 571200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.629650582893956e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.541684091091156\n",
      "      kl: 0.009429370984435081\n",
      "      policy_loss: -0.0022016214206814766\n",
      "      total_loss: 556.5942993164062\n",
      "      vf_explained_var: 0.8792724609375\n",
      "      vf_loss: 556.596435546875\n",
      "    sample_time_ms: 32171.644\n",
      "    update_time_ms: 9.054\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.01144487417925\n",
      "  time_since_restore: 4793.785271167755\n",
      "  time_this_iter_s: 41.409549713134766\n",
      "  time_total_s: 4793.785271167755\n",
      "  timestamp: 1556751107\n",
      "  timesteps_since_restore: 571200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 571200\n",
      "  training_iteration: 119\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.4/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4793 s, 119 iter, 571200 ts, 88 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-52-26\n",
      "  done: false\n",
      "  episode_len_mean: 85.13\n",
      "  episode_reward_max: 533.6954220908323\n",
      "  episode_reward_mean: 114.02291920502492\n",
      "  episode_reward_min: -355.9008911424778\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 6503\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7203.208\n",
      "    load_time_ms: 1.802\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.814825291446978e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5258354544639587\n",
      "      kl: 0.008885345421731472\n",
      "      policy_loss: -0.0021584732457995415\n",
      "      total_loss: 396.0677490234375\n",
      "      vf_explained_var: 0.9134734869003296\n",
      "      vf_loss: 396.0699462890625\n",
      "    sample_time_ms: 32179.077\n",
      "    update_time_ms: 9.451\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.505729801256237\n",
      "  time_since_restore: 4832.814258575439\n",
      "  time_this_iter_s: 39.028987407684326\n",
      "  time_total_s: 4832.814258575439\n",
      "  timestamp: 1556751146\n",
      "  timesteps_since_restore: 576000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 120\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.3/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4832 s, 120 iter, 576000 ts, 114 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-53-04\n",
      "  done: false\n",
      "  episode_len_mean: 86.39\n",
      "  episode_reward_max: 533.6232579346656\n",
      "  episode_reward_mean: 133.15043242605313\n",
      "  episode_reward_min: -352.10707039696524\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6559\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7214.207\n",
      "    load_time_ms: 1.814\n",
      "    num_steps_sampled: 580800\n",
      "    num_steps_trained: 580800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.407412645723489e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5262795090675354\n",
      "      kl: 0.010315793566405773\n",
      "      policy_loss: -0.002105201594531536\n",
      "      total_loss: 439.5861511230469\n",
      "      vf_explained_var: 0.9152319431304932\n",
      "      vf_loss: 439.5882263183594\n",
      "    sample_time_ms: 32079.573\n",
      "    update_time_ms: 8.882\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.28760810651327\n",
      "  time_since_restore: 4870.579742670059\n",
      "  time_this_iter_s: 37.76548409461975\n",
      "  time_total_s: 4870.579742670059\n",
      "  timestamp: 1556751184\n",
      "  timesteps_since_restore: 580800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 580800\n",
      "  training_iteration: 121\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.3/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4870 s, 121 iter, 580800 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-53-42\n",
      "  done: false\n",
      "  episode_len_mean: 84.13\n",
      "  episode_reward_max: 533.6232579346656\n",
      "  episode_reward_mean: 100.56710319656486\n",
      "  episode_reward_min: -357.61360897652924\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 6616\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7183.548\n",
      "    load_time_ms: 1.715\n",
      "    num_steps_sampled: 585600\n",
      "    num_steps_trained: 585600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.407412645723489e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5371379852294922\n",
      "      kl: 0.009724468924105167\n",
      "      policy_loss: -0.003531252034008503\n",
      "      total_loss: 426.10906982421875\n",
      "      vf_explained_var: 0.9022902250289917\n",
      "      vf_loss: 426.11260986328125\n",
      "    sample_time_ms: 32186.315\n",
      "    update_time_ms: 8.999\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.14177579914121\n",
      "  time_since_restore: 4909.3271498680115\n",
      "  time_this_iter_s: 38.74740719795227\n",
      "  time_total_s: 4909.3271498680115\n",
      "  timestamp: 1556751222\n",
      "  timesteps_since_restore: 585600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 585600\n",
      "  training_iteration: 122\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.5/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4909 s, 122 iter, 585600 ts, 101 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 86.52\n",
      "  episode_reward_max: 534.7646204248582\n",
      "  episode_reward_mean: 117.00603787823749\n",
      "  episode_reward_min: -354.5374659721047\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 6671\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7163.068\n",
      "    load_time_ms: 1.763\n",
      "    num_steps_sampled: 590400\n",
      "    num_steps_trained: 590400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037063228617445e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5293899774551392\n",
      "      kl: 0.006343417335301638\n",
      "      policy_loss: -0.0011348235420882702\n",
      "      total_loss: 350.7185363769531\n",
      "      vf_explained_var: 0.9296192526817322\n",
      "      vf_loss: 350.7197265625\n",
      "    sample_time_ms: 32504.328\n",
      "    update_time_ms: 9.883\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.251509469559377\n",
      "  time_since_restore: 4950.395492553711\n",
      "  time_this_iter_s: 41.06834268569946\n",
      "  time_total_s: 4950.395492553711\n",
      "  timestamp: 1556751264\n",
      "  timesteps_since_restore: 590400\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 590400\n",
      "  training_iteration: 123\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.4/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4950 s, 123 iter, 590400 ts, 117 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-55-00\n",
      "  done: false\n",
      "  episode_len_mean: 80.46\n",
      "  episode_reward_max: 534.7646204248582\n",
      "  episode_reward_mean: 57.95406095835269\n",
      "  episode_reward_min: -353.2300630384102\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 6732\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6963.303\n",
      "    load_time_ms: 1.736\n",
      "    num_steps_sampled: 595200\n",
      "    num_steps_trained: 595200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4961532652378082\n",
      "      kl: 0.010827618651092052\n",
      "      policy_loss: -0.0033767223358154297\n",
      "      total_loss: 463.6646728515625\n",
      "      vf_explained_var: 0.9086049199104309\n",
      "      vf_loss: 463.66802978515625\n",
      "    sample_time_ms: 32363.769\n",
      "    update_time_ms: 9.936\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 14.488515239588182\n",
      "  time_since_restore: 4987.162175416946\n",
      "  time_this_iter_s: 36.766682863235474\n",
      "  time_total_s: 4987.162175416946\n",
      "  timestamp: 1556751300\n",
      "  timesteps_since_restore: 595200\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 595200\n",
      "  training_iteration: 124\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.4/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 4987 s, 124 iter, 595200 ts, 58 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-55-39\n",
      "  done: false\n",
      "  episode_len_mean: 80.64\n",
      "  episode_reward_max: 533.3774778360215\n",
      "  episode_reward_mean: 76.50673933530419\n",
      "  episode_reward_min: -359.3138697954109\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 6790\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7109.213\n",
      "    load_time_ms: 1.729\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5184828639030457\n",
      "      kl: 0.008620099164545536\n",
      "      policy_loss: -0.003046653000637889\n",
      "      total_loss: 506.8421936035156\n",
      "      vf_explained_var: 0.8898317217826843\n",
      "      vf_loss: 506.8452453613281\n",
      "    sample_time_ms: 32100.395\n",
      "    update_time_ms: 9.812\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 19.12668483382605\n",
      "  time_since_restore: 5026.102876424789\n",
      "  time_this_iter_s: 38.94070100784302\n",
      "  time_total_s: 5026.102876424789\n",
      "  timestamp: 1556751339\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 125\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.5/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 5026 s, 125 iter, 600000 ts, 76.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-56-18\n",
      "  done: false\n",
      "  episode_len_mean: 81.24\n",
      "  episode_reward_max: 543.1831056032261\n",
      "  episode_reward_mean: 80.33355406004607\n",
      "  episode_reward_min: -359.3138697954109\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 6847\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7095.162\n",
      "    load_time_ms: 1.69\n",
      "    num_steps_sampled: 604800\n",
      "    num_steps_trained: 604800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.009265807154361e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5230634212493896\n",
      "      kl: 0.007831928320229053\n",
      "      policy_loss: -0.0009626515675336123\n",
      "      total_loss: 371.44232177734375\n",
      "      vf_explained_var: 0.9337120652198792\n",
      "      vf_loss: 371.4432678222656\n",
      "    sample_time_ms: 32022.147\n",
      "    update_time_ms: 9.827\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 20.08338851501151\n",
      "  time_since_restore: 5064.538280010223\n",
      "  time_this_iter_s: 38.43540358543396\n",
      "  time_total_s: 5064.538280010223\n",
      "  timestamp: 1556751378\n",
      "  timesteps_since_restore: 604800\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 604800\n",
      "  training_iteration: 126\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.5/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 5064 s, 126 iter, 604800 ts, 80.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_00-56-59\n",
      "  done: false\n",
      "  episode_len_mean: 83.24\n",
      "  episode_reward_max: 543.1831056032261\n",
      "  episode_reward_mean: 90.1031020961859\n",
      "  episode_reward_min: -341.7320735783519\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 6906\n",
      "  experiment_id: 66f68a9af4c24638b2d0f530e68e804a\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7018.887\n",
      "    load_time_ms: 1.741\n",
      "    num_steps_sampled: 609600\n",
      "    num_steps_trained: 609600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5046329035771806e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5060017108917236\n",
      "      kl: 0.008875939063727856\n",
      "      policy_loss: -0.003077904926612973\n",
      "      total_loss: 485.9670715332031\n",
      "      vf_explained_var: 0.9019517302513123\n",
      "      vf_loss: 485.9700927734375\n",
      "    sample_time_ms: 32323.234\n",
      "    update_time_ms: 9.549\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18511\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.525775524046477\n",
      "  time_since_restore: 5105.2187077999115\n",
      "  time_this_iter_s: 40.68042778968811\n",
      "  time_total_s: 5105.2187077999115\n",
      "  timestamp: 1556751419\n",
      "  timesteps_since_restore: 609600\n",
      "  timesteps_this_iter: 4800\n",
      "  timesteps_total: 609600\n",
      "  training_iteration: 127\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.5/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=18511], 5105 s, 127 iter, 609600 ts, 90.1 rew\n",
      "\n",
      "Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/tune/trial_runner.py\", line 261, in _process_events\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/tune/ray_trial_executor.py\", line 211, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/worker.py\", line 2386, in get\n",
      "    raise value\n",
      "ray.worker.RayTaskError: \u001b[36mray_PPOAgent:train()\u001b[39m (pid=18511, host=Gandalf)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/rllib/agents/agent.py\", line 279, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/tune/trainable.py\", line 146, in train\n",
      "    result = self._train()\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/rllib/agents/ppo/ppo.py\", line 101, in _train\n",
      "    fetches = self.optimizer.step()\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py\", line 125, in step\n",
      "    self.num_envs_per_worker, self.train_batch_size)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/rllib/optimizers/rollout.py\", line 28, in collect_samples\n",
      "    next_sample = ray.get(fut_sample)\n",
      "ray.worker.RayTaskError: \u001b[36mray_PolicyEvaluator:sample()\u001b[39m (pid=18542, host=Gandalf)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/memory_monitor.py\", line 78, in raise_if_low_memory\n",
      "    self.error_threshold))\n",
      "ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node Gandalf is used (15.58 / 16.39 GB). The top 5 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "3043\t10.06GB\t/usr/lib/firefox/firefox\n",
      "18742\t0.83GB\tray_PPOAgent:__ray_terminate__()\n",
      "18511\t0.81GB\tray_PPOAgent:train()\n",
      "18542\t0.72GB\tray_PolicyEvaluator:sample()\n",
      "18764\t0.7GB\tray_PolicyEvaluator\n",
      "\n",
      "In addition, ~0.86 GB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray, and the max Redis size with `redis_max_memory`.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to recover trial state from last checkpoint.\n",
      "Error restoring runner.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/tune/trial_runner.py\", line 261, in _process_events\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/tune/ray_trial_executor.py\", line 211, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/worker.py\", line 2386, in get\n",
      "    raise value\n",
      "ray.worker.RayTaskError: \u001b[36mray_PPOAgent:train()\u001b[39m (pid=18511, host=Gandalf)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/rllib/agents/agent.py\", line 279, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/tune/trainable.py\", line 146, in train\n",
      "    result = self._train()\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/rllib/agents/ppo/ppo.py\", line 101, in _train\n",
      "    fetches = self.optimizer.step()\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py\", line 125, in step\n",
      "    self.num_envs_per_worker, self.train_batch_size)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/rllib/optimizers/rollout.py\", line 28, in collect_samples\n",
      "    next_sample = ray.get(fut_sample)\n",
      "ray.worker.RayTaskError: \u001b[36mray_PolicyEvaluator:sample()\u001b[39m (pid=18542, host=Gandalf)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/memory_monitor.py\", line 78, in raise_if_low_memory\n",
      "    self.error_threshold))\n",
      "ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node Gandalf is used (15.58 / 16.39 GB). The top 5 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "3043\t10.06GB\t/usr/lib/firefox/firefox\n",
      "18742\t0.83GB\tray_PPOAgent:__ray_terminate__()\n",
      "18511\t0.81GB\tray_PPOAgent:train()\n",
      "18542\t0.72GB\tray_PolicyEvaluator:sample()\n",
      "18764\t0.7GB\tray_PolicyEvaluator\n",
      "\n",
      "In addition, ~0.86 GB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray, and the max Redis size with `redis_max_memory`.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/tune/ray_trial_executor.py\", line 313, in restore\n",
      "    ray.get(trial.runner.restore.remote(value))\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/worker.py\", line 2386, in get\n",
      "    raise value\n",
      "ray.worker.RayTaskError: \u001b[36mray_PPOAgent:restore()\u001b[39m (pid=18510, host=Gandalf)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/memory_monitor.py\", line 78, in raise_if_low_memory\n",
      "    self.error_threshold))\n",
      "ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node Gandalf is used (15.57 / 16.39 GB). The top 5 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "3043\t10.06GB\t/usr/lib/firefox/firefox\n",
      "18511\t0.82GB\tray_PPOAgent:__ray_terminate__()\n",
      "18542\t0.72GB\tray_PolicyEvaluator\n",
      "18764\t0.7GB\tray_PolicyEvaluator\n",
      "18468\t0.34GB\t/home/thorsten/anaconda3/envs/flow_2/bin/python -m ipykernel_launcher -f /run/user/1000/jupyter/kern\n",
      "\n",
      "In addition, ~0.86 GB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray, and the max Redis size with `redis_max_memory`.\n",
      "\n",
      "Error recovering trial from checkpoint, abort.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/tune/trial_runner.py\", line 261, in _process_events\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/tune/ray_trial_executor.py\", line 211, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/worker.py\", line 2386, in get\n",
      "    raise value\n",
      "ray.worker.RayTaskError: \u001b[36mray_PPOAgent:train()\u001b[39m (pid=18511, host=Gandalf)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/rllib/agents/agent.py\", line 279, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/tune/trainable.py\", line 146, in train\n",
      "    result = self._train()\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/rllib/agents/ppo/ppo.py\", line 101, in _train\n",
      "    fetches = self.optimizer.step()\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py\", line 125, in step\n",
      "    self.num_envs_per_worker, self.train_batch_size)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/rllib/optimizers/rollout.py\", line 28, in collect_samples\n",
      "    next_sample = ray.get(fut_sample)\n",
      "ray.worker.RayTaskError: \u001b[36mray_PolicyEvaluator:sample()\u001b[39m (pid=18542, host=Gandalf)\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/memory_monitor.py\", line 78, in raise_if_low_memory\n",
      "    self.error_threshold))\n",
      "ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node Gandalf is used (15.58 / 16.39 GB). The top 5 memory consumers are:\n",
      "\n",
      "PID\tMEM\tCOMMAND\n",
      "3043\t10.06GB\t/usr/lib/firefox/firefox\n",
      "18742\t0.83GB\tray_PPOAgent:__ray_terminate__()\n",
      "18511\t0.81GB\tray_PPOAgent:train()\n",
      "18542\t0.72GB\tray_PolicyEvaluator:sample()\n",
      "18764\t0.7GB\tray_PolicyEvaluator\n",
      "\n",
      "In addition, ~0.86 GB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray, and the max Redis size with `redis_max_memory`.\n",
      "\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thorsten/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/tune/trial_runner.py\", line 335, in _try_recover\n",
      "    raise RuntimeError(\"Trial did not start correctly.\")\n",
      "RuntimeError: Trial did not start correctly.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.4/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "ERROR trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tERROR, 1 failures: /home/thorsten/ray_results/IntersectionExample/PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0_2019-05-01_23-30-329daunr1k/error_2019-05-02_00-57-17.txt [pid=18511], 5105 s, 127 iter, 609600 ts, 90.1 rew\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2/2 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 15.4/16.4 GB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "ERROR trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tERROR, 1 failures: /home/thorsten/ray_results/IntersectionExample/PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0_2019-05-01_23-30-329daunr1k/error_2019-05-02_00-57-17.txt [pid=18511], 5105 s, 127 iter, 609600 ts, 90.1 rew\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e821cc1ed92d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m\"max_failures\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \"stop\": {  # stopping conditions\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;34m\"training_iteration\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# number of iterations to stop after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         },\n\u001b[1;32m     13\u001b[0m     },\n",
      "\u001b[0;32m~/anaconda3/envs/flow_2/lib/python3.5/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(experiments, search_alg, scheduler, with_server, server_port, verbose, queue_trials, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0])"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,  # RL algorithm to run\n",
    "        \"env\": gym_name,  # environment name generated earlier\n",
    "        \"config\": {  # configuration params (must match \"run\" value)\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1000,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow_2)",
   "language": "python",
   "name": "flow_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
