{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING I3W\n",
    "\n",
    "\n",
    "# A) Create Envorinment, Vehicles etc\n",
    "\n",
    "### General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scenarios:\n",
      "['Scenario', 'BayBridgeScenario', 'BayBridgeTollScenario', 'BottleneckScenario', 'Figure8Scenario', 'SimpleGridScenario', 'HighwayScenario', 'LoopScenario', 'MergeScenario', 'TwoLoopsOneMergingScenario', 'MultiLoopScenario', 'IntersectionScenarioTW']\n",
      "\n",
      "Available environments:\n",
      "['MultiEnv', 'MultiAgentAccelEnv', 'MultiWaveAttenuationPOEnv', 'MultiAgentIntersectionEnv', 'MultiAgentTeamSpiritIntersectionEnv']\n"
     ]
    }
   ],
   "source": [
    "# Define horizon as a variable to ensure consistent use across notebook (length of one rollout)\n",
    "HORIZON=500                                 #103 max Horizon, wenn es vor verlassen abbrechen soll!, default war 500\n",
    "\n",
    "# name of the experiment\n",
    "experiment_name = \"IntersectionExample\"\n",
    "\n",
    "# scenario class\n",
    "import flow.scenarios as scenarios\n",
    "print(\"Available scenarios:\")\n",
    "print(scenarios.__all__)\n",
    "scenario_name = \"IntersectionTWScenario\"\n",
    "\n",
    "# environment class\n",
    "import flow.multiagent_envs as flowenvs\n",
    "print(\"\\nAvailable environments:\")\n",
    "print(flowenvs.__all__)\n",
    "env_name = \"MultiAgentIntersectionEnv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "from flow.scenarios.intersection import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "additionalNetParams = {\n",
    "            \"edge_length\": 40,\n",
    "            \"lanes\": 1,\n",
    "            \"speed_limit\": 30\n",
    "        }\n",
    "\n",
    "net_params = NetParams( no_internal_links=False,                  #default: True   !! damit Kreuzungen nicht Ã¼berspr. werden\n",
    "                        inflows=None,                             #default: None\n",
    "                        osm_path=None,                            #default: None\n",
    "                        netfile=None,                             #default: None\n",
    "                        additional_params=additionalNetParams     #default: None   !!\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InitialConfig Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "\n",
    "initial_config = InitialConfig( shuffle=True,                            #default: False         !!\n",
    "                                spacing=\"custom\",                        #default: \"uniform\"     !!\n",
    "                                min_gap=10,                              #default: 0\n",
    "                                perturbation=29.99,                      #default: 0.0            !!        \n",
    "                                x0=0,                                    #default: 0\n",
    "                                bunching=0,                              #default: 0\n",
    "                                lanes_distribution=float(\"inf\"),         #default: float(\"inf\")\n",
    "                                edges_distribution=\"all\",                #default: \"all\"\n",
    "                                additional_params=None )                 #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams( port = None,                  #default: None\n",
    "                          sim_step=0.1,                 #default: 0.1\n",
    "                          emission_path=None,           #default: None\n",
    "                          lateral_resolution=None,      #default: None\n",
    "                          no_step_log=True,             #default: True\n",
    "                          render=False,                 #default: False\n",
    "                          save_render=False,            #default: False\n",
    "                          sight_radius=25,              #default: 25\n",
    "                          show_radius=False,            #default: False\n",
    "                          pxpm=2,                       #default: 2\n",
    "                          overtake_right=False,         #default: False    \n",
    "                          seed=None,                    #default: None\n",
    "                          restart_instance=False,       #default: False\n",
    "                          print_warnings=True,          #default: True\n",
    "                          teleport_time=-1,             #default: -1\n",
    "                          num_clients=1,                #default: 1\n",
    "                          sumo_binary=None )            #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "additionalEnvParams = {\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 3,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 3,\n",
    "        \"target_velocity\": 30\n",
    "    }\n",
    "\n",
    "env_params = EnvParams( additional_params=additionalEnvParams, #default: None    !!\n",
    "                        horizon=HORIZON,                       #default: 500     !!\n",
    "                        warmup_steps=0,                        #default: 0       \n",
    "                        sims_per_step=1,                       #default: 1\n",
    "                        evaluate=False )                       #default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# import vehicles dynamics models\n",
    "#from flow.controllers import SumoCarFollowingController\n",
    "from flow.controllers import ContinuousRouter\n",
    "#from flow.controllers.lane_change_controllers import SumoLaneChangeController\n",
    "from flow.controllers.lane_change_controllers import StaticLaneChanger\n",
    "from flow.controllers import RLController\n",
    "from flow.core.params import SumoLaneChangeParams\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "from random import *\n",
    "\n",
    "vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RL-Agent controlled vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car following parameters, default: None\n",
    "cf_parameter = SumoCarFollowingParams(\n",
    "                speed_mode=\"aggressive\")\n",
    "# lane change parameters, default: None\n",
    "lc_parameter =  None\n",
    "\n",
    "vehicles.add( # name of the vehicle\n",
    "                veh_id = \"rl\",\n",
    "              # acceleration controller, default: (SumoCarFollowingController, {})\n",
    "                acceleration_controller=(RLController, {}),\n",
    "              # lane_change_controller, default: (SumoLaneChangeController, {})\n",
    "                lane_change_controller=(StaticLaneChanger,{}),\n",
    "              # routing controller, default: None\n",
    "                routing_controller=(ContinuousRouter, {}),\n",
    "              # initial speed, default: 0\n",
    "                initial_speed=0,\n",
    "              # number of vehicles, default: 1 \n",
    "                num_vehicles=2,\n",
    "                \n",
    "                car_following_params=cf_parameter\n",
    "              # speed mode, default: \"right_of_way\"\n",
    "                #speed_mode=\"aggressive\",\n",
    "              # lane change mode, default: \"no_lat_collide\"\n",
    "                #lane_change_mode=\"aggressive\", \n",
    "              # car following parameter, default: None\n",
    "                #sumo_car_following_params=cf_parameter,\n",
    "              # lane change parameter, default: None\n",
    "                #sumo_lc_params=lc_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict( # name of the experiment\n",
    "                      exp_tag=experiment_name,\n",
    "                    # name of the flow environment the experiment is running on\n",
    "                      env_name=env_name,\n",
    "                    # name of the scenario class the experiment uses\n",
    "                      scenario=scenario_name,\n",
    "                    # simulator that is used by the experiment\n",
    "                      simulator='traci',\n",
    "                    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "                      sim=sumo_params,\n",
    "                    # environment related parameters (see flow.core.params.EnvParams)\n",
    "                      env=env_params,\n",
    "                    # network-related parameters (see flow.core.params.NetParams and\n",
    "                    # the scenario's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "                      net=net_params,\n",
    "                    # vehicles to be placed in the network at the start of a rollout \n",
    "                    # (see flow.core.vehicles.Vehicles)\n",
    "                      veh=vehicles,\n",
    "                   # (optional) parameters affecting the positioning of vehicles upon \n",
    "                   # initialization/reset (see flow.core.params.InitialConfig)\n",
    "                      initial=initial_config\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-03-14_14-59-27_15055/logs.\n",
      "Waiting for redis server at 127.0.0.1:41538 to respond...\n",
      "Waiting for redis server at 127.0.0.1:42848 to respond...\n",
      "Starting the Plasma object store with 6.554658406 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=5529d516fdc919df987ccd1ccf94fefb3de0cf496ff147bb\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.16.123.117',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-03-14_14-59-27_15055/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-03-14_14-59-27_15055/sockets/raylet'],\n",
       " 'redis_address': '172.16.123.117:41538',\n",
       " 'webui_url': 'http://localhost:8889/notebooks/ray_ui.ipynb?token=5529d516fdc919df987ccd1ccf94fefb3de0cf496ff147bb'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "\n",
    "ray.init(redirect_output=True, num_cpus=N_CPUS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [64, 32]})  # size of hidden layers in network\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "#config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "#config[\"sample_batch_size\"] = config[\"train_batch_size\"]/config[\"num_workers\"] # 200 default, trotzdem zu hoch?\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 44893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.609282163268665\n",
      "6.485841348992942\n"
     ]
    }
   ],
   "source": [
    "# multi agent policy mapping\n",
    "test_env = create_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "\n",
    "def gen_policy():\n",
    "    return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "# Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "policy_graphs = {'rl_0': gen_policy(), 'rl_1': gen_policy()}\n",
    "    \n",
    "def policy_mapping_fn(agent_id):\n",
    "    return agent_id\n",
    "\n",
    "config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn)\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "\n",
      "Created LogSyncer for /home/thorsten/ray_results/IntersectionExample/PPO_MultiAgentIntersectionEnv-v0_0_2019-03-14_14-59-309afvmegl -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-00-51\n",
      "  done: false\n",
      "  episode_len_mean: 461.4761904761905\n",
      "  episode_reward_max: 225.63778774179508\n",
      "  episode_reward_mean: 50.44007541538655\n",
      "  episode_reward_min: -152.90116484063756\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 21\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8428.011\n",
      "    load_time_ms: 205.795\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4168651103973389\n",
      "      kl: 0.0003168605035170913\n",
      "      policy_loss: -0.00039855370414443314\n",
      "      total_loss: 102.63119506835938\n",
      "      vf_explained_var: 0.024821553379297256\n",
      "      vf_loss: 102.63153839111328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4223048686981201\n",
      "      kl: 0.002157653449103236\n",
      "      policy_loss: -0.003097109729424119\n",
      "      total_loss: 108.90647888183594\n",
      "      vf_explained_var: 0.03314186632633209\n",
      "      vf_loss: 108.90911865234375\n",
      "    sample_time_ms: 32612.099\n",
      "    update_time_ms: 1812.434\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 18.426408152161564\n",
      "    rl_1: 32.01366726322501\n",
      "  time_since_restore: 43.25976514816284\n",
      "  time_this_iter_s: 43.25976514816284\n",
      "  time_total_s: 43.25976514816284\n",
      "  timestamp: 1552572051\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 43 s, 1 iter, 10000 ts, 50.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-01-24\n",
      "  done: false\n",
      "  episode_len_mean: 460.8809523809524\n",
      "  episode_reward_max: 225.63778774179508\n",
      "  episode_reward_mean: 56.16544171601855\n",
      "  episode_reward_min: -161.11675332149227\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 42\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5808.767\n",
      "    load_time_ms: 104.09\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4180964231491089\n",
      "      kl: 0.0013267459580674767\n",
      "      policy_loss: -0.0020683258771896362\n",
      "      total_loss: 81.6989517211914\n",
      "      vf_explained_var: 0.06061379238963127\n",
      "      vf_loss: 81.70088195800781\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4202402830123901\n",
      "      kl: 0.002500007161870599\n",
      "      policy_loss: -0.002102799480780959\n",
      "      total_loss: 85.2908706665039\n",
      "      vf_explained_var: 0.09293139725923538\n",
      "      vf_loss: 85.29271697998047\n",
      "    sample_time_ms: 31015.13\n",
      "    update_time_ms: 913.008\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.142765678582688\n",
      "    rl_1: 34.02267603743586\n",
      "  time_since_restore: 75.90180468559265\n",
      "  time_this_iter_s: 32.64203953742981\n",
      "  time_total_s: 75.90180468559265\n",
      "  timestamp: 1552572084\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 75 s, 2 iter, 20000 ts, 56.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-01-49\n",
      "  done: false\n",
      "  episode_len_mean: 468.25396825396825\n",
      "  episode_reward_max: 251.50609857975513\n",
      "  episode_reward_mean: 78.83314154852775\n",
      "  episode_reward_min: -161.11675332149227\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 63\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5018.082\n",
      "    load_time_ms: 70.186\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4157108068466187\n",
      "      kl: 0.0016695302911102772\n",
      "      policy_loss: -0.00234796735458076\n",
      "      total_loss: 45.831180572509766\n",
      "      vf_explained_var: 0.29442712664604187\n",
      "      vf_loss: 45.833438873291016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4193087816238403\n",
      "      kl: 0.0013599229278042912\n",
      "      policy_loss: -0.0019547592382878065\n",
      "      total_loss: 54.80832290649414\n",
      "      vf_explained_var: 0.35476094484329224\n",
      "      vf_loss: 54.81020736694336\n",
      "    sample_time_ms: 27781.084\n",
      "    update_time_ms: 612.685\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.16448532837586\n",
      "    rl_1: 43.668656220151874\n",
      "  time_since_restore: 100.68370771408081\n",
      "  time_this_iter_s: 24.78190302848816\n",
      "  time_total_s: 100.68370771408081\n",
      "  timestamp: 1552572109\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 100 s, 3 iter, 30000 ts, 78.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-02-14\n",
      "  done: false\n",
      "  episode_len_mean: 462.8139534883721\n",
      "  episode_reward_max: 254.11487045727839\n",
      "  episode_reward_mean: 80.88651024799066\n",
      "  episode_reward_min: -161.11675332149227\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 86\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4629.143\n",
      "    load_time_ms: 53.387\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.415672779083252\n",
      "      kl: 0.0029382319189608097\n",
      "      policy_loss: -0.002367069711908698\n",
      "      total_loss: 90.79025268554688\n",
      "      vf_explained_var: 0.23983852565288544\n",
      "      vf_loss: 90.79254913330078\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4201388359069824\n",
      "      kl: 0.0017919883830472827\n",
      "      policy_loss: -0.001016818918287754\n",
      "      total_loss: 93.81390380859375\n",
      "      vf_explained_var: 0.12746384739875793\n",
      "      vf_loss: 93.81486511230469\n",
      "    sample_time_ms: 26289.632\n",
      "    update_time_ms: 461.311\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.037702378989685\n",
      "    rl_1: 43.84880786900101\n",
      "  time_since_restore: 125.98922109603882\n",
      "  time_this_iter_s: 25.305513381958008\n",
      "  time_total_s: 125.98922109603882\n",
      "  timestamp: 1552572134\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 125 s, 4 iter, 40000 ts, 80.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-02-42\n",
      "  done: false\n",
      "  episode_len_mean: 452.82\n",
      "  episode_reward_max: 282.4621064690395\n",
      "  episode_reward_mean: 79.4900714626834\n",
      "  episode_reward_min: -161.11675332149227\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 109\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4372.321\n",
      "    load_time_ms: 43.36\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4114465713500977\n",
      "      kl: 0.003478548489511013\n",
      "      policy_loss: -0.0024012215435504913\n",
      "      total_loss: 161.74497985839844\n",
      "      vf_explained_var: 0.10363620519638062\n",
      "      vf_loss: 161.74734497070312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.421002745628357\n",
      "      kl: 0.000883040833286941\n",
      "      policy_loss: -0.0005220648599788547\n",
      "      total_loss: 152.3082275390625\n",
      "      vf_explained_var: 0.14245665073394775\n",
      "      vf_loss: 152.30874633789062\n",
      "    sample_time_ms: 25885.754\n",
      "    update_time_ms: 370.611\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.28559015798808\n",
      "    rl_1: 42.20448130469533\n",
      "  time_since_restore: 153.63425040245056\n",
      "  time_this_iter_s: 27.645029306411743\n",
      "  time_total_s: 153.63425040245056\n",
      "  timestamp: 1552572162\n",
      "  timesteps_since_restore: 50000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 5\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 153 s, 5 iter, 50000 ts, 79.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-03-15\n",
      "  done: false\n",
      "  episode_len_mean: 435.85\n",
      "  episode_reward_max: 282.4621064690395\n",
      "  episode_reward_mean: 80.7561015065934\n",
      "  episode_reward_min: -161.11675332149227\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 134\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4265.773\n",
      "    load_time_ms: 36.9\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4099453687667847\n",
      "      kl: 0.003499489277601242\n",
      "      policy_loss: -0.0026379218325018883\n",
      "      total_loss: 181.15423583984375\n",
      "      vf_explained_var: 0.0742822140455246\n",
      "      vf_loss: 181.1568603515625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4425597190856934\n",
      "      kl: 0.007836302742362022\n",
      "      policy_loss: -0.005500837694853544\n",
      "      total_loss: 171.61334228515625\n",
      "      vf_explained_var: 0.1260068565607071\n",
      "      vf_loss: 171.61880493164062\n",
      "    sample_time_ms: 26430.684\n",
      "    update_time_ms: 309.993\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.95374112544628\n",
      "    rl_1: 40.802360381147096\n",
      "  time_since_restore: 186.55600094795227\n",
      "  time_this_iter_s: 32.92175054550171\n",
      "  time_total_s: 186.55600094795227\n",
      "  timestamp: 1552572195\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 186 s, 6 iter, 60000 ts, 80.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-03-39\n",
      "  done: false\n",
      "  episode_len_mean: 406.15\n",
      "  episode_reward_max: 291.645178199829\n",
      "  episode_reward_mean: 68.64641087925665\n",
      "  episode_reward_min: -156.73619147516735\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 162\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4113.493\n",
      "    load_time_ms: 32.066\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3969756364822388\n",
      "      kl: 0.002729949774220586\n",
      "      policy_loss: -0.0016531202709302306\n",
      "      total_loss: 244.5137176513672\n",
      "      vf_explained_var: 0.1567380428314209\n",
      "      vf_loss: 244.51531982421875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4693982601165771\n",
      "      kl: 0.0050351740792393684\n",
      "      policy_loss: -0.003183349734172225\n",
      "      total_loss: 185.0837860107422\n",
      "      vf_explained_var: 0.17628902196884155\n",
      "      vf_loss: 185.08694458007812\n",
      "    sample_time_ms: 25682.377\n",
      "    update_time_ms: 267.203\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.55221882111263\n",
      "    rl_1: 27.094192058144035\n",
      "  time_since_restore: 210.9809353351593\n",
      "  time_this_iter_s: 24.42493438720703\n",
      "  time_total_s: 210.9809353351593\n",
      "  timestamp: 1552572219\n",
      "  timesteps_since_restore: 70000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 210 s, 7 iter, 70000 ts, 68.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-04-03\n",
      "  done: false\n",
      "  episode_len_mean: 391.21\n",
      "  episode_reward_max: 332.34973107878017\n",
      "  episode_reward_mean: 75.05948786236416\n",
      "  episode_reward_min: -161.6657970811671\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 188\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4004.265\n",
      "    load_time_ms: 28.346\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4039499759674072\n",
      "      kl: 0.0010636562947183847\n",
      "      policy_loss: -0.0010122450767084956\n",
      "      total_loss: 188.00253295898438\n",
      "      vf_explained_var: 0.1918046921491623\n",
      "      vf_loss: 188.00350952148438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4857693910598755\n",
      "      kl: 0.0034641623497009277\n",
      "      policy_loss: -0.003518678480759263\n",
      "      total_loss: 150.59658813476562\n",
      "      vf_explained_var: 0.17794927954673767\n",
      "      vf_loss: 150.60012817382812\n",
      "    sample_time_ms: 24991.471\n",
      "    update_time_ms: 234.659\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.968419475071045\n",
      "    rl_1: 26.09106838729312\n",
      "  time_since_restore: 234.40095615386963\n",
      "  time_this_iter_s: 23.420020818710327\n",
      "  time_total_s: 234.40095615386963\n",
      "  timestamp: 1552572243\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 234 s, 8 iter, 80000 ts, 75.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-04-27\n",
      "  done: false\n",
      "  episode_len_mean: 367.31\n",
      "  episode_reward_max: 333.72259259605477\n",
      "  episode_reward_mean: 73.3254809912233\n",
      "  episode_reward_min: -161.6657970811671\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 217\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3945.259\n",
      "    load_time_ms: 25.455\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3893752098083496\n",
      "      kl: 0.004622334614396095\n",
      "      policy_loss: -0.0036672453861683607\n",
      "      total_loss: 261.8589782714844\n",
      "      vf_explained_var: 0.23861220479011536\n",
      "      vf_loss: 261.86260986328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4876751899719238\n",
      "      kl: 0.0030205741059035063\n",
      "      policy_loss: -0.003422390203922987\n",
      "      total_loss: 235.95367431640625\n",
      "      vf_explained_var: 0.15511973202228546\n",
      "      vf_loss: 235.95703125\n",
      "    sample_time_ms: 24519.45\n",
      "    update_time_ms: 209.463\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.80589768532243\n",
      "    rl_1: 21.519583305900877\n",
      "  time_since_restore: 258.64788365364075\n",
      "  time_this_iter_s: 24.246927499771118\n",
      "  time_total_s: 258.64788365364075\n",
      "  timestamp: 1552572267\n",
      "  timesteps_since_restore: 90000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 9\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 258 s, 9 iter, 90000 ts, 73.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-04-51\n",
      "  done: false\n",
      "  episode_len_mean: 369.99\n",
      "  episode_reward_max: 352.88021066119705\n",
      "  episode_reward_mean: 112.82567416225835\n",
      "  episode_reward_min: -161.6657970811671\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 244\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3870.91\n",
      "    load_time_ms: 23.153\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.388471245765686\n",
      "      kl: 0.0014920317335054278\n",
      "      policy_loss: -0.0014330461854115129\n",
      "      total_loss: 166.1019287109375\n",
      "      vf_explained_var: 0.32546430826187134\n",
      "      vf_loss: 166.10337829589844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.483329176902771\n",
      "      kl: 0.007432669401168823\n",
      "      policy_loss: -0.004250137135386467\n",
      "      total_loss: 153.5008087158203\n",
      "      vf_explained_var: 0.1738443374633789\n",
      "      vf_loss: 153.5050506591797\n",
      "    sample_time_ms: 24086.347\n",
      "    update_time_ms: 189.349\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.34347192796535\n",
      "    rl_1: 42.48220223429299\n",
      "  time_since_restore: 282.06825137138367\n",
      "  time_this_iter_s: 23.42036771774292\n",
      "  time_total_s: 282.06825137138367\n",
      "  timestamp: 1552572291\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 282 s, 10 iter, 100000 ts, 113 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-05-14\n",
      "  done: false\n",
      "  episode_len_mean: 349.72\n",
      "  episode_reward_max: 352.88021066119705\n",
      "  episode_reward_mean: 127.01363601709238\n",
      "  episode_reward_min: -161.6657970811671\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 275\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3356.046\n",
      "    load_time_ms: 2.849\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3865365982055664\n",
      "      kl: 0.0006443443708121777\n",
      "      policy_loss: -0.000983211095444858\n",
      "      total_loss: 247.80551147460938\n",
      "      vf_explained_var: 0.3252006471157074\n",
      "      vf_loss: 247.80648803710938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4818021059036255\n",
      "      kl: 0.001531330868601799\n",
      "      policy_loss: -0.001327425823546946\n",
      "      total_loss: 205.95260620117188\n",
      "      vf_explained_var: 0.23303870856761932\n",
      "      vf_loss: 205.9539337158203\n",
      "    sample_time_ms: 22849.338\n",
      "    update_time_ms: 8.838\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.34463857609774\n",
      "    rl_1: 53.6689974409946\n",
      "  time_since_restore: 305.6187334060669\n",
      "  time_this_iter_s: 23.550482034683228\n",
      "  time_total_s: 305.6187334060669\n",
      "  timestamp: 1552572314\n",
      "  timesteps_since_restore: 110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 11\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 305 s, 11 iter, 110000 ts, 127 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-05-38\n",
      "  done: false\n",
      "  episode_len_mean: 347.27\n",
      "  episode_reward_max: 358.188459650289\n",
      "  episode_reward_mean: 156.31925609854304\n",
      "  episode_reward_min: -161.65194845077946\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 304\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3370.126\n",
      "    load_time_ms: 2.84\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3875292539596558\n",
      "      kl: 0.004544624127447605\n",
      "      policy_loss: -0.001524953986518085\n",
      "      total_loss: 207.24270629882812\n",
      "      vf_explained_var: 0.2740538418292999\n",
      "      vf_loss: 207.24424743652344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4740409851074219\n",
      "      kl: 0.0027114436961710453\n",
      "      policy_loss: -0.000960742065217346\n",
      "      total_loss: 153.5610809326172\n",
      "      vf_explained_var: 0.3016214668750763\n",
      "      vf_loss: 153.56204223632812\n",
      "    sample_time_ms: 21918.134\n",
      "    update_time_ms: 8.801\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 88.10288222110157\n",
      "    rl_1: 68.21637387744146\n",
      "  time_since_restore: 329.08968448638916\n",
      "  time_this_iter_s: 23.470951080322266\n",
      "  time_total_s: 329.08968448638916\n",
      "  timestamp: 1552572338\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 329 s, 12 iter, 120000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-06-02\n",
      "  done: false\n",
      "  episode_len_mean: 313.65\n",
      "  episode_reward_max: 358.188459650289\n",
      "  episode_reward_mean: 144.22536643996145\n",
      "  episode_reward_min: -162.34967452278374\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 340\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3349.345\n",
      "    load_time_ms: 2.82\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3987871408462524\n",
      "      kl: 0.003794614225625992\n",
      "      policy_loss: -0.0023199294228106737\n",
      "      total_loss: 314.2309265136719\n",
      "      vf_explained_var: 0.3224128484725952\n",
      "      vf_loss: 314.2332458496094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4737534523010254\n",
      "      kl: 0.0033272188156843185\n",
      "      policy_loss: -0.0013135926565155387\n",
      "      total_loss: 271.63330078125\n",
      "      vf_explained_var: 0.3460201919078827\n",
      "      vf_loss: 271.6345520019531\n",
      "    sample_time_ms: 21897.093\n",
      "    update_time_ms: 8.608\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 80.60073976129952\n",
      "    rl_1: 63.624626678661905\n",
      "  time_since_restore: 353.44996070861816\n",
      "  time_this_iter_s: 24.360276222229004\n",
      "  time_total_s: 353.44996070861816\n",
      "  timestamp: 1552572362\n",
      "  timesteps_since_restore: 130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 13\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 353 s, 13 iter, 130000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-06-26\n",
      "  done: false\n",
      "  episode_len_mean: 307.52\n",
      "  episode_reward_max: 358.188459650289\n",
      "  episode_reward_mean: 162.14903796296684\n",
      "  episode_reward_min: -162.34967452278374\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 373\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3323.8\n",
      "    load_time_ms: 2.785\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3922113180160522\n",
      "      kl: 0.0019081104546785355\n",
      "      policy_loss: -0.00110359035898\n",
      "      total_loss: 230.3097381591797\n",
      "      vf_explained_var: 0.30799055099487305\n",
      "      vf_loss: 230.31085205078125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.482479453086853\n",
      "      kl: 0.01064413134008646\n",
      "      policy_loss: -0.005123527720570564\n",
      "      total_loss: 201.76510620117188\n",
      "      vf_explained_var: 0.3912605345249176\n",
      "      vf_loss: 201.770263671875\n",
      "    sample_time_ms: 21798.479\n",
      "    update_time_ms: 8.674\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 88.86049065091927\n",
      "    rl_1: 73.28854731204761\n",
      "  time_since_restore: 377.514164686203\n",
      "  time_this_iter_s: 24.06420397758484\n",
      "  time_total_s: 377.514164686203\n",
      "  timestamp: 1552572386\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 14\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 377 s, 14 iter, 140000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-06-49\n",
      "  done: false\n",
      "  episode_len_mean: 293.84\n",
      "  episode_reward_max: 353.29070915225674\n",
      "  episode_reward_mean: 182.0457125177287\n",
      "  episode_reward_min: -159.23639323454432\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 409\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3308.827\n",
      "    load_time_ms: 2.693\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3777779340744019\n",
      "      kl: 0.009372466243803501\n",
      "      policy_loss: -0.003262387355789542\n",
      "      total_loss: 236.38758850097656\n",
      "      vf_explained_var: 0.33991581201553345\n",
      "      vf_loss: 236.39085388183594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.471718192100525\n",
      "      kl: 0.005225044209510088\n",
      "      policy_loss: -0.0032298278529196978\n",
      "      total_loss: 229.97206115722656\n",
      "      vf_explained_var: 0.4020260274410248\n",
      "      vf_loss: 229.9752655029297\n",
      "    sample_time_ms: 21359.284\n",
      "    update_time_ms: 8.675\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 94.01641551802322\n",
      "    rl_1: 88.02929699970551\n",
      "  time_since_restore: 400.616575717926\n",
      "  time_this_iter_s: 23.102411031723022\n",
      "  time_total_s: 400.616575717926\n",
      "  timestamp: 1552572409\n",
      "  timesteps_since_restore: 150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 15\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 400 s, 15 iter, 150000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-07-13\n",
      "  done: false\n",
      "  episode_len_mean: 269.68\n",
      "  episode_reward_max: 358.5439680875464\n",
      "  episode_reward_mean: 195.40692881696313\n",
      "  episode_reward_min: -160.91965933761898\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 449\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3257.155\n",
      "    load_time_ms: 2.459\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3726756572723389\n",
      "      kl: 0.004789444152265787\n",
      "      policy_loss: -0.0007125602569431067\n",
      "      total_loss: 310.3055725097656\n",
      "      vf_explained_var: 0.29379621148109436\n",
      "      vf_loss: 310.3063049316406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4767347574234009\n",
      "      kl: 0.0055036465637385845\n",
      "      policy_loss: -0.002497775247320533\n",
      "      total_loss: 297.977783203125\n",
      "      vf_explained_var: 0.37879252433776855\n",
      "      vf_loss: 297.9803161621094\n",
      "    sample_time_ms: 20514.839\n",
      "    update_time_ms: 8.879\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 100.88569926129148\n",
      "    rl_1: 94.52122955567167\n",
      "  time_since_restore: 424.5718550682068\n",
      "  time_this_iter_s: 23.95527935028076\n",
      "  time_total_s: 424.5718550682068\n",
      "  timestamp: 1552572433\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 16\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 424 s, 16 iter, 160000 ts, 195 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-07-37\n",
      "  done: false\n",
      "  episode_len_mean: 250.7\n",
      "  episode_reward_max: 358.5439680875464\n",
      "  episode_reward_mean: 187.6474458614565\n",
      "  episode_reward_min: -167.05400848467409\n",
      "  episodes_this_iter: 43\n",
      "  episodes_total: 492\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3255.179\n",
      "    load_time_ms: 2.417\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3852787017822266\n",
      "      kl: 0.005088998004794121\n",
      "      policy_loss: -0.001935213222168386\n",
      "      total_loss: 313.4827575683594\n",
      "      vf_explained_var: 0.3161388039588928\n",
      "      vf_loss: 313.4847106933594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4627025127410889\n",
      "      kl: 0.008655562996864319\n",
      "      policy_loss: -0.002292166231200099\n",
      "      total_loss: 378.0893859863281\n",
      "      vf_explained_var: 0.34376901388168335\n",
      "      vf_loss: 378.09173583984375\n",
      "    sample_time_ms: 20423.971\n",
      "    update_time_ms: 8.578\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 88.8488656046789\n",
      "    rl_1: 98.79858025677763\n",
      "  time_since_restore: 448.0647146701813\n",
      "  time_this_iter_s: 23.492859601974487\n",
      "  time_total_s: 448.0647146701813\n",
      "  timestamp: 1552572457\n",
      "  timesteps_since_restore: 170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 17\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 448 s, 17 iter, 170000 ts, 188 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-08-01\n",
      "  done: false\n",
      "  episode_len_mean: 239.04\n",
      "  episode_reward_max: 356.31558003800075\n",
      "  episode_reward_mean: 184.5924950556834\n",
      "  episode_reward_min: -167.05400848467409\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 532\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3250.18\n",
      "    load_time_ms: 2.445\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3964141607284546\n",
      "      kl: 0.0068817296996712685\n",
      "      policy_loss: -0.002619949635118246\n",
      "      total_loss: 252.21682739257812\n",
      "      vf_explained_var: 0.2991997003555298\n",
      "      vf_loss: 252.21945190429688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4657471179962158\n",
      "      kl: 0.008031557314097881\n",
      "      policy_loss: -0.0032946730498224497\n",
      "      total_loss: 330.9541320800781\n",
      "      vf_explained_var: 0.41363075375556946\n",
      "      vf_loss: 330.9574279785156\n",
      "    sample_time_ms: 20444.341\n",
      "    update_time_ms: 8.59\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 80.69795284745018\n",
      "    rl_1: 103.89454220823322\n",
      "  time_since_restore: 471.641108751297\n",
      "  time_this_iter_s: 23.576394081115723\n",
      "  time_total_s: 471.641108751297\n",
      "  timestamp: 1552572481\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 18\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 471 s, 18 iter, 180000 ts, 185 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-08-24\n",
      "  done: false\n",
      "  episode_len_mean: 244.65\n",
      "  episode_reward_max: 356.31558003800075\n",
      "  episode_reward_mean: 219.11305151954596\n",
      "  episode_reward_min: -167.05400848467409\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 574\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3238.288\n",
      "    load_time_ms: 2.541\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4040608406066895\n",
      "      kl: 0.006713903043419123\n",
      "      policy_loss: -0.002573559992015362\n",
      "      total_loss: 203.72637939453125\n",
      "      vf_explained_var: 0.2360527515411377\n",
      "      vf_loss: 203.72897338867188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4326151609420776\n",
      "      kl: 0.009682540781795979\n",
      "      policy_loss: -0.0037591576110571623\n",
      "      total_loss: 389.105712890625\n",
      "      vf_explained_var: 0.4060256779193878\n",
      "      vf_loss: 389.1094665527344\n",
      "    sample_time_ms: 20402.735\n",
      "    update_time_ms: 8.58\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 89.48430872579961\n",
      "    rl_1: 129.62874279374637\n",
      "  time_since_restore: 495.35633087158203\n",
      "  time_this_iter_s: 23.715222120285034\n",
      "  time_total_s: 495.35633087158203\n",
      "  timestamp: 1552572504\n",
      "  timesteps_since_restore: 190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 19\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 495 s, 19 iter, 190000 ts, 219 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-08-48\n",
      "  done: false\n",
      "  episode_len_mean: 230.05\n",
      "  episode_reward_max: 374.8026812543819\n",
      "  episode_reward_mean: 215.33033365358585\n",
      "  episode_reward_min: -159.30313442704693\n",
      "  episodes_this_iter: 47\n",
      "  episodes_total: 621\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3236.785\n",
      "    load_time_ms: 2.541\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4009674787521362\n",
      "      kl: 0.0044362470507621765\n",
      "      policy_loss: -0.0015330089954659343\n",
      "      total_loss: 296.7503356933594\n",
      "      vf_explained_var: 0.2511977553367615\n",
      "      vf_loss: 296.7518615722656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.452364444732666\n",
      "      kl: 0.005988205783069134\n",
      "      policy_loss: -0.002100743120536208\n",
      "      total_loss: 491.4644470214844\n",
      "      vf_explained_var: 0.408317506313324\n",
      "      vf_loss: 491.46649169921875\n",
      "    sample_time_ms: 20456.369\n",
      "    update_time_ms: 8.378\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 81.88916402936438\n",
      "    rl_1: 133.44116962422152\n",
      "  time_since_restore: 519.2933218479156\n",
      "  time_this_iter_s: 23.936990976333618\n",
      "  time_total_s: 519.2933218479156\n",
      "  timestamp: 1552572528\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 20\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 519 s, 20 iter, 200000 ts, 215 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-09-13\n",
      "  done: false\n",
      "  episode_len_mean: 201.38\n",
      "  episode_reward_max: 374.8026812543819\n",
      "  episode_reward_mean: 173.19300776035968\n",
      "  episode_reward_min: -170.49784415587393\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 674\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3230.667\n",
      "    load_time_ms: 2.51\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3864916563034058\n",
      "      kl: 0.005754405166953802\n",
      "      policy_loss: -0.002187345875427127\n",
      "      total_loss: 388.9443054199219\n",
      "      vf_explained_var: 0.27929091453552246\n",
      "      vf_loss: 388.9465026855469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4445061683654785\n",
      "      kl: 0.005807751324027777\n",
      "      policy_loss: -0.0023409579880535603\n",
      "      total_loss: 599.5044555664062\n",
      "      vf_explained_var: 0.3708621561527252\n",
      "      vf_loss: 599.5068359375\n",
      "    sample_time_ms: 20534.938\n",
      "    update_time_ms: 8.298\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.63475877668987\n",
      "    rl_1: 110.55824898366984\n",
      "  time_since_restore: 543.5661664009094\n",
      "  time_this_iter_s: 24.272844552993774\n",
      "  time_total_s: 543.5661664009094\n",
      "  timestamp: 1552572553\n",
      "  timesteps_since_restore: 210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 21\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 543 s, 21 iter, 210000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-09-36\n",
      "  done: false\n",
      "  episode_len_mean: 196.3\n",
      "  episode_reward_max: 361.32588604488484\n",
      "  episode_reward_mean: 172.41935964056498\n",
      "  episode_reward_min: -170.49784415587393\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 723\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3221.571\n",
      "    load_time_ms: 2.523\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.383142113685608\n",
      "      kl: 0.00477213179692626\n",
      "      policy_loss: -0.0010873471619561315\n",
      "      total_loss: 293.8448791503906\n",
      "      vf_explained_var: 0.32071828842163086\n",
      "      vf_loss: 293.84600830078125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4336354732513428\n",
      "      kl: 0.012783159501850605\n",
      "      policy_loss: -0.004086020868271589\n",
      "      total_loss: 542.0247192382812\n",
      "      vf_explained_var: 0.427533894777298\n",
      "      vf_loss: 542.0287475585938\n",
      "    sample_time_ms: 20544.68\n",
      "    update_time_ms: 7.741\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 59.532498527162936\n",
      "    rl_1: 112.88686111340206\n",
      "  time_since_restore: 567.0375623703003\n",
      "  time_this_iter_s: 23.47139596939087\n",
      "  time_total_s: 567.0375623703003\n",
      "  timestamp: 1552572576\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 22\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 567 s, 22 iter, 220000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-10-01\n",
      "  done: false\n",
      "  episode_len_mean: 199.03\n",
      "  episode_reward_max: 365.47983303357404\n",
      "  episode_reward_mean: 197.44512642905084\n",
      "  episode_reward_min: -155.08802154584157\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 774\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3219.42\n",
      "    load_time_ms: 2.611\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4059460163116455\n",
      "      kl: 0.005639208480715752\n",
      "      policy_loss: -0.002426410559564829\n",
      "      total_loss: 224.97015380859375\n",
      "      vf_explained_var: 0.35121047496795654\n",
      "      vf_loss: 224.9726104736328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.371208906173706\n",
      "      kl: 0.015204565599560738\n",
      "      policy_loss: -0.005355552304536104\n",
      "      total_loss: 617.9977416992188\n",
      "      vf_explained_var: 0.3984500467777252\n",
      "      vf_loss: 618.003173828125\n",
      "    sample_time_ms: 20557.636\n",
      "    update_time_ms: 7.698\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.031137544461274\n",
      "    rl_1: 136.41398888458957\n",
      "  time_since_restore: 591.5079171657562\n",
      "  time_this_iter_s: 24.470354795455933\n",
      "  time_total_s: 591.5079171657562\n",
      "  timestamp: 1552572601\n",
      "  timesteps_since_restore: 230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 23\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 591 s, 23 iter, 230000 ts, 197 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-10-25\n",
      "  done: false\n",
      "  episode_len_mean: 186.49\n",
      "  episode_reward_max: 365.47983303357404\n",
      "  episode_reward_mean: 203.36333745187451\n",
      "  episode_reward_min: -161.88239958521294\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 829\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3215.684\n",
      "    load_time_ms: 2.684\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4185433387756348\n",
      "      kl: 0.0006608053809031844\n",
      "      policy_loss: -0.0005319946794770658\n",
      "      total_loss: 161.7174530029297\n",
      "      vf_explained_var: 0.39266473054885864\n",
      "      vf_loss: 161.71798706054688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3291224241256714\n",
      "      kl: 0.014307236298918724\n",
      "      policy_loss: -0.004187677055597305\n",
      "      total_loss: 701.4951782226562\n",
      "      vf_explained_var: 0.29883530735969543\n",
      "      vf_loss: 701.4993286132812\n",
      "    sample_time_ms: 20547.526\n",
      "    update_time_ms: 7.648\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.64375692529348\n",
      "    rl_1: 151.719580526581\n",
      "  time_since_restore: 615.4365706443787\n",
      "  time_this_iter_s: 23.928653478622437\n",
      "  time_total_s: 615.4365706443787\n",
      "  timestamp: 1552572625\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 24\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 615 s, 24 iter, 240000 ts, 203 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-10-49\n",
      "  done: false\n",
      "  episode_len_mean: 179.48\n",
      "  episode_reward_max: 312.8545125531083\n",
      "  episode_reward_mean: 226.33432489181908\n",
      "  episode_reward_min: -161.88239958521294\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 886\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3218.172\n",
      "    load_time_ms: 2.71\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4180691242218018\n",
      "      kl: 0.005351012106984854\n",
      "      policy_loss: -0.002654706360772252\n",
      "      total_loss: 122.2840576171875\n",
      "      vf_explained_var: 0.5155888795852661\n",
      "      vf_loss: 122.28670501708984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.35392165184021\n",
      "      kl: 0.00600973516702652\n",
      "      policy_loss: -0.0030172246042639017\n",
      "      total_loss: 677.0294189453125\n",
      "      vf_explained_var: 0.3418676555156708\n",
      "      vf_loss: 677.032470703125\n",
      "    sample_time_ms: 20652.381\n",
      "    update_time_ms: 7.577\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.40088323331067\n",
      "    rl_1: 169.93344165850843\n",
      "  time_since_restore: 639.612321138382\n",
      "  time_this_iter_s: 24.175750494003296\n",
      "  time_total_s: 639.612321138382\n",
      "  timestamp: 1552572649\n",
      "  timesteps_since_restore: 250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 25\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 639 s, 25 iter, 250000 ts, 226 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-11-13\n",
      "  done: false\n",
      "  episode_len_mean: 175.46\n",
      "  episode_reward_max: 312.8545125531083\n",
      "  episode_reward_mean: 227.61867979237468\n",
      "  episode_reward_min: -140.3099099390709\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 943\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3216.549\n",
      "    load_time_ms: 2.806\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4139018058776855\n",
      "      kl: 0.0036309908609837294\n",
      "      policy_loss: -0.0019463271601125598\n",
      "      total_loss: 102.66162872314453\n",
      "      vf_explained_var: 0.5851422548294067\n",
      "      vf_loss: 102.66358184814453\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3410195112228394\n",
      "      kl: 0.008232500404119492\n",
      "      policy_loss: -0.002939338330179453\n",
      "      total_loss: 688.1898193359375\n",
      "      vf_explained_var: 0.371767520904541\n",
      "      vf_loss: 688.1928100585938\n",
      "    sample_time_ms: 20667.81\n",
      "    update_time_ms: 8.21\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.329399043678315\n",
      "    rl_1: 173.2892807486964\n",
      "  time_since_restore: 663.7175426483154\n",
      "  time_this_iter_s: 24.10522150993347\n",
      "  time_total_s: 663.7175426483154\n",
      "  timestamp: 1552572673\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 26\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 663 s, 26 iter, 260000 ts, 228 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-11-37\n",
      "  done: false\n",
      "  episode_len_mean: 173.17\n",
      "  episode_reward_max: 316.19204043790194\n",
      "  episode_reward_mean: 235.6508790091874\n",
      "  episode_reward_min: -140.3099099390709\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 1000\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3219.595\n",
      "    load_time_ms: 2.778\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9802322831784522e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4014767408370972\n",
      "      kl: 0.004884727299213409\n",
      "      policy_loss: -0.0019121625227853656\n",
      "      total_loss: 120.0474624633789\n",
      "      vf_explained_var: 0.37030690908432007\n",
      "      vf_loss: 120.04939270019531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.355163812637329\n",
      "      kl: 0.008231581188738346\n",
      "      policy_loss: -0.002983283484354615\n",
      "      total_loss: 761.4862670898438\n",
      "      vf_explained_var: 0.1999576985836029\n",
      "      vf_loss: 761.4892578125\n",
      "    sample_time_ms: 20715.361\n",
      "    update_time_ms: 8.301\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.308581133031794\n",
      "    rl_1: 178.34229787615553\n",
      "  time_since_restore: 687.7160792350769\n",
      "  time_this_iter_s: 23.998536586761475\n",
      "  time_total_s: 687.7160792350769\n",
      "  timestamp: 1552572697\n",
      "  timesteps_since_restore: 270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 27\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 687 s, 27 iter, 270000 ts, 236 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-12-01\n",
      "  done: false\n",
      "  episode_len_mean: 167.32\n",
      "  episode_reward_max: 312.4847654514866\n",
      "  episode_reward_mean: 238.3740997566608\n",
      "  episode_reward_min: -155.56353922814822\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 1061\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3221.168\n",
      "    load_time_ms: 2.835\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3844094276428223\n",
      "      kl: 0.002260529901832342\n",
      "      policy_loss: -0.0015225843526422977\n",
      "      total_loss: 125.19410705566406\n",
      "      vf_explained_var: 0.31484973430633545\n",
      "      vf_loss: 125.19562530517578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3360848426818848\n",
      "      kl: 0.012072037905454636\n",
      "      policy_loss: -0.0027111710514873266\n",
      "      total_loss: 793.7819213867188\n",
      "      vf_explained_var: 0.13495637476444244\n",
      "      vf_loss: 793.78466796875\n",
      "    sample_time_ms: 20773.792\n",
      "    update_time_ms: 8.432\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.88107762770239\n",
      "    rl_1: 179.49302212895844\n",
      "  time_since_restore: 711.8943305015564\n",
      "  time_this_iter_s: 24.178251266479492\n",
      "  time_total_s: 711.8943305015564\n",
      "  timestamp: 1552572721\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 28\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 711 s, 28 iter, 280000 ts, 238 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-12-25\n",
      "  done: false\n",
      "  episode_len_mean: 165.86\n",
      "  episode_reward_max: 311.1480943510765\n",
      "  episode_reward_mean: 242.01416438650094\n",
      "  episode_reward_min: -155.56353922814822\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 1121\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3203.588\n",
      "    load_time_ms: 2.815\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3985674381256104\n",
      "      kl: 0.007333675865083933\n",
      "      policy_loss: -0.0035389394033700228\n",
      "      total_loss: 86.22991180419922\n",
      "      vf_explained_var: 0.12033951282501221\n",
      "      vf_loss: 86.23345184326172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3125826120376587\n",
      "      kl: 0.010137943550944328\n",
      "      policy_loss: -0.0036417192313820124\n",
      "      total_loss: 802.0654296875\n",
      "      vf_explained_var: 0.01909850724041462\n",
      "      vf_loss: 802.0690307617188\n",
      "    sample_time_ms: 20762.864\n",
      "    update_time_ms: 8.493\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.17938186480079\n",
      "    rl_1: 184.83478252170013\n",
      "  time_since_restore: 735.3229205608368\n",
      "  time_this_iter_s: 23.428590059280396\n",
      "  time_total_s: 735.3229205608368\n",
      "  timestamp: 1552572745\n",
      "  timesteps_since_restore: 290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 29\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 735 s, 29 iter, 290000 ts, 242 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-12-49\n",
      "  done: false\n",
      "  episode_len_mean: 162.33\n",
      "  episode_reward_max: 284.62871949598616\n",
      "  episode_reward_mean: 243.27058905925128\n",
      "  episode_reward_min: -141.8048505700731\n",
      "  episodes_this_iter: 63\n",
      "  episodes_total: 1184\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3208.885\n",
      "    load_time_ms: 2.837\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3863162994384766\n",
      "      kl: 0.01059985812753439\n",
      "      policy_loss: -0.005376099608838558\n",
      "      total_loss: 105.7929458618164\n",
      "      vf_explained_var: 0.26421016454696655\n",
      "      vf_loss: 105.79832458496094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3194369077682495\n",
      "      kl: 0.007459058426320553\n",
      "      policy_loss: -0.002219891408458352\n",
      "      total_loss: 815.0223388671875\n",
      "      vf_explained_var: 0.17392641305923462\n",
      "      vf_loss: 815.0244750976562\n",
      "    sample_time_ms: 20781.459\n",
      "    update_time_ms: 9.048\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.57486232385747\n",
      "    rl_1: 187.69572673539383\n",
      "  time_since_restore: 759.5046756267548\n",
      "  time_this_iter_s: 24.18175506591797\n",
      "  time_total_s: 759.5046756267548\n",
      "  timestamp: 1552572769\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 30\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 759 s, 30 iter, 300000 ts, 243 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-13-18\n",
      "  done: false\n",
      "  episode_len_mean: 157.99\n",
      "  episode_reward_max: 289.6855017926054\n",
      "  episode_reward_mean: 250.21194882659466\n",
      "  episode_reward_min: 209.5550492171553\n",
      "  episodes_this_iter: 64\n",
      "  episodes_total: 1248\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3273.342\n",
      "    load_time_ms: 2.91\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3672144412994385\n",
      "      kl: 0.007672306150197983\n",
      "      policy_loss: -0.001910079037770629\n",
      "      total_loss: 102.31795501708984\n",
      "      vf_explained_var: 0.01928815431892872\n",
      "      vf_loss: 102.3198471069336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.338184118270874\n",
      "      kl: 0.005008368287235498\n",
      "      policy_loss: -0.0014756920281797647\n",
      "      total_loss: 774.3341064453125\n",
      "      vf_explained_var: 0.23709800839424133\n",
      "      vf_loss: 774.3355102539062\n",
      "    sample_time_ms: 21224.057\n",
      "    update_time_ms: 9.311\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.70382726151508\n",
      "    rl_1: 189.5081215650796\n",
      "  time_since_restore: 788.8532612323761\n",
      "  time_this_iter_s: 29.348585605621338\n",
      "  time_total_s: 788.8532612323761\n",
      "  timestamp: 1552572798\n",
      "  timesteps_since_restore: 310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 31\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 788 s, 31 iter, 310000 ts, 250 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-13-48\n",
      "  done: false\n",
      "  episode_len_mean: 155.91\n",
      "  episode_reward_max: 291.2222838034382\n",
      "  episode_reward_mean: 256.5368882257056\n",
      "  episode_reward_min: 221.29016669715116\n",
      "  episodes_this_iter: 64\n",
      "  episodes_total: 1312\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3476.115\n",
      "    load_time_ms: 2.982\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.367112636566162\n",
      "      kl: 0.013016562908887863\n",
      "      policy_loss: -0.0046059670858085155\n",
      "      total_loss: 115.48700714111328\n",
      "      vf_explained_var: 0.01462665293365717\n",
      "      vf_loss: 115.49162292480469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3403197526931763\n",
      "      kl: 0.0044785053469240665\n",
      "      policy_loss: -0.0010321659501641989\n",
      "      total_loss: 767.3800048828125\n",
      "      vf_explained_var: 0.26328539848327637\n",
      "      vf_loss: 767.3809814453125\n",
      "    sample_time_ms: 21615.894\n",
      "    update_time_ms: 9.368\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.17555633960694\n",
      "    rl_1: 190.36133188609864\n",
      "  time_since_restore: 818.2743859291077\n",
      "  time_this_iter_s: 29.421124696731567\n",
      "  time_total_s: 818.2743859291077\n",
      "  timestamp: 1552572828\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 32\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 818 s, 32 iter, 320000 ts, 257 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-14-13\n",
      "  done: false\n",
      "  episode_len_mean: 154.78\n",
      "  episode_reward_max: 296.54109711903806\n",
      "  episode_reward_mean: 261.22864603283466\n",
      "  episode_reward_min: -151.1660858279269\n",
      "  episodes_this_iter: 65\n",
      "  episodes_total: 1377\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3480.203\n",
      "    load_time_ms: 2.972\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3647902011871338\n",
      "      kl: 0.004715241491794586\n",
      "      policy_loss: -0.001020024181343615\n",
      "      total_loss: 168.15036010742188\n",
      "      vf_explained_var: 0.04071752354502678\n",
      "      vf_loss: 168.15139770507812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9802322831784522e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3735401630401611\n",
      "      kl: 0.0071747517213225365\n",
      "      policy_loss: -0.001847804756835103\n",
      "      total_loss: 785.4200439453125\n",
      "      vf_explained_var: 0.2412029206752777\n",
      "      vf_loss: 785.4219360351562\n",
      "    sample_time_ms: 21676.596\n",
      "    update_time_ms: 9.278\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.6044609769239\n",
      "    rl_1: 188.62418505591074\n",
      "  time_since_restore: 843.398514509201\n",
      "  time_this_iter_s: 25.124128580093384\n",
      "  time_total_s: 843.398514509201\n",
      "  timestamp: 1552572853\n",
      "  timesteps_since_restore: 330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 33\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 843 s, 33 iter, 330000 ts, 261 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-14-42\n",
      "  done: false\n",
      "  episode_len_mean: 153.4\n",
      "  episode_reward_max: 295.90130667701186\n",
      "  episode_reward_mean: 260.53742836245044\n",
      "  episode_reward_min: -151.1660858279269\n",
      "  episodes_this_iter: 66\n",
      "  episodes_total: 1443\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3510.837\n",
      "    load_time_ms: 2.89\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.313225884932663e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3466516733169556\n",
      "      kl: 0.007006624713540077\n",
      "      policy_loss: -0.0012656687758862972\n",
      "      total_loss: 136.85281372070312\n",
      "      vf_explained_var: 0.010474150069057941\n",
      "      vf_loss: 136.85409545898438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3473089933395386\n",
      "      kl: 0.01134853158146143\n",
      "      policy_loss: -0.003501725848764181\n",
      "      total_loss: 824.5767211914062\n",
      "      vf_explained_var: 0.2622828781604767\n",
      "      vf_loss: 824.5802001953125\n",
      "    sample_time_ms: 22135.026\n",
      "    update_time_ms: 9.432\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.6116816965375\n",
      "    rl_1: 188.92574666591295\n",
      "  time_since_restore: 872.2207617759705\n",
      "  time_this_iter_s: 28.82224726676941\n",
      "  time_total_s: 872.2207617759705\n",
      "  timestamp: 1552572882\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 34\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 872 s, 34 iter, 340000 ts, 261 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-15-11\n",
      "  done: false\n",
      "  episode_len_mean: 149.5\n",
      "  episode_reward_max: 306.4606188595306\n",
      "  episode_reward_mean: 266.8796624125178\n",
      "  episode_reward_min: 226.62915356633152\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 1511\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3637.797\n",
      "    load_time_ms: 2.926\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.335817575454712\n",
      "      kl: 0.00470380112528801\n",
      "      policy_loss: -0.0009689115104265511\n",
      "      total_loss: 157.54710388183594\n",
      "      vf_explained_var: 0.07003500312566757\n",
      "      vf_loss: 157.54806518554688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3783642053604126\n",
      "      kl: 0.00544516509398818\n",
      "      policy_loss: -0.0015684895915910602\n",
      "      total_loss: 773.0001831054688\n",
      "      vf_explained_var: 0.39232000708580017\n",
      "      vf_loss: 773.0018310546875\n",
      "    sample_time_ms: 22474.039\n",
      "    update_time_ms: 9.46\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.85822757835274\n",
      "    rl_1: 191.0214348341651\n",
      "  time_since_restore: 901.0587418079376\n",
      "  time_this_iter_s: 28.837980031967163\n",
      "  time_total_s: 901.0587418079376\n",
      "  timestamp: 1552572911\n",
      "  timesteps_since_restore: 350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 35\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 901 s, 35 iter, 350000 ts, 267 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-15-40\n",
      "  done: false\n",
      "  episode_len_mean: 152.29\n",
      "  episode_reward_max: 309.62094114674625\n",
      "  episode_reward_mean: 274.51144609614516\n",
      "  episode_reward_min: 236.5815730491481\n",
      "  episodes_this_iter: 65\n",
      "  episodes_total: 1576\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3698.064\n",
      "    load_time_ms: 2.842\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3216208219528198\n",
      "      kl: 0.011549322865903378\n",
      "      policy_loss: -0.002702507423236966\n",
      "      total_loss: 158.16128540039062\n",
      "      vf_explained_var: 0.22060242295265198\n",
      "      vf_loss: 158.1639862060547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.400614857673645\n",
      "      kl: 0.011671511456370354\n",
      "      policy_loss: -0.0033051841892302036\n",
      "      total_loss: 695.6669311523438\n",
      "      vf_explained_var: 0.4810422658920288\n",
      "      vf_loss: 695.6702880859375\n",
      "    sample_time_ms: 22946.04\n",
      "    update_time_ms: 8.622\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 83.22803291984181\n",
      "    rl_1: 191.2834131763032\n",
      "  time_since_restore: 930.475528717041\n",
      "  time_this_iter_s: 29.416786909103394\n",
      "  time_total_s: 930.475528717041\n",
      "  timestamp: 1552572940\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 36\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 930 s, 36 iter, 360000 ts, 275 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-16-20\n",
      "  done: false\n",
      "  episode_len_mean: 149.7\n",
      "  episode_reward_max: 310.2437531284987\n",
      "  episode_reward_mean: 276.97434801947736\n",
      "  episode_reward_min: 248.1804656932413\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 1644\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3716.145\n",
      "    load_time_ms: 2.947\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3325303792953491\n",
      "      kl: 0.012653876096010208\n",
      "      policy_loss: -0.003827930660918355\n",
      "      total_loss: 124.32499694824219\n",
      "      vf_explained_var: 0.5780811905860901\n",
      "      vf_loss: 124.32884216308594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.40040922164917\n",
      "      kl: 0.01823890581727028\n",
      "      policy_loss: -0.006772755179554224\n",
      "      total_loss: 722.5614013671875\n",
      "      vf_explained_var: 0.5076972246170044\n",
      "      vf_loss: 722.568115234375\n",
      "    sample_time_ms: 24482.803\n",
      "    update_time_ms: 8.636\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 87.27469468081811\n",
      "    rl_1: 189.69965333865923\n",
      "  time_since_restore: 970.0257110595703\n",
      "  time_this_iter_s: 39.5501823425293\n",
      "  time_total_s: 970.0257110595703\n",
      "  timestamp: 1552572980\n",
      "  timesteps_since_restore: 370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 37\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 970 s, 37 iter, 370000 ts, 277 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-16-53\n",
      "  done: false\n",
      "  episode_len_mean: 146.65\n",
      "  episode_reward_max: 308.92183623757796\n",
      "  episode_reward_mean: 264.8513013563676\n",
      "  episode_reward_min: -144.29246132291024\n",
      "  episodes_this_iter: 67\n",
      "  episodes_total: 1711\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3799.571\n",
      "    load_time_ms: 2.903\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3079999685287476\n",
      "      kl: 0.0033142731990665197\n",
      "      policy_loss: -0.001094674807973206\n",
      "      total_loss: 167.5641632080078\n",
      "      vf_explained_var: 0.5395303964614868\n",
      "      vf_loss: 167.5652618408203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3853996992111206\n",
      "      kl: 0.006714936811476946\n",
      "      policy_loss: -0.0014828513376414776\n",
      "      total_loss: 804.7833251953125\n",
      "      vf_explained_var: 0.3966653645038605\n",
      "      vf_loss: 804.7848510742188\n",
      "    sample_time_ms: 25333.096\n",
      "    update_time_ms: 8.838\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 81.93648369558791\n",
      "    rl_1: 182.9148176607797\n",
      "  time_since_restore: 1003.542991399765\n",
      "  time_this_iter_s: 33.5172803401947\n",
      "  time_total_s: 1003.542991399765\n",
      "  timestamp: 1552573013\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 38\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1003 s, 38 iter, 380000 ts, 265 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-17-33\n",
      "  done: false\n",
      "  episode_len_mean: 147.71\n",
      "  episode_reward_max: 308.5045206961728\n",
      "  episode_reward_mean: 264.7095104524728\n",
      "  episode_reward_min: -150.7465394741331\n",
      "  episodes_this_iter: 69\n",
      "  episodes_total: 1780\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3868.531\n",
      "    load_time_ms: 2.972\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.318167805671692\n",
      "      kl: 0.008999506942927837\n",
      "      policy_loss: -0.0018564199563115835\n",
      "      total_loss: 137.65383911132812\n",
      "      vf_explained_var: 0.6636970639228821\n",
      "      vf_loss: 137.65570068359375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3876546621322632\n",
      "      kl: 0.01072023343294859\n",
      "      policy_loss: -0.002625931054353714\n",
      "      total_loss: 703.1616821289062\n",
      "      vf_explained_var: 0.47705861926078796\n",
      "      vf_loss: 703.1642456054688\n",
      "    sample_time_ms: 26827.658\n",
      "    update_time_ms: 9.168\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 82.68014034412812\n",
      "    rl_1: 182.0293701083447\n",
      "  time_since_restore: 1042.609260559082\n",
      "  time_this_iter_s: 39.06626915931702\n",
      "  time_total_s: 1042.609260559082\n",
      "  timestamp: 1552573053\n",
      "  timesteps_since_restore: 390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 39\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1042 s, 39 iter, 390000 ts, 265 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-18-09\n",
      "  done: false\n",
      "  episode_len_mean: 142.92\n",
      "  episode_reward_max: 311.1173947305601\n",
      "  episode_reward_mean: 248.70658965697078\n",
      "  episode_reward_min: -145.41234501312942\n",
      "  episodes_this_iter: 71\n",
      "  episodes_total: 1851\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3917.057\n",
      "    load_time_ms: 2.974\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2958980798721313\n",
      "      kl: 0.004384838044643402\n",
      "      policy_loss: -0.0027951200027018785\n",
      "      total_loss: 234.93515014648438\n",
      "      vf_explained_var: 0.5659813284873962\n",
      "      vf_loss: 234.93789672851562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.402505874633789\n",
      "      kl: 0.010565100237727165\n",
      "      policy_loss: -0.0023806525859981775\n",
      "      total_loss: 848.1351318359375\n",
      "      vf_explained_var: 0.40434959530830383\n",
      "      vf_loss: 848.1376342773438\n",
      "    sample_time_ms: 27978.342\n",
      "    update_time_ms: 9.168\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.66558259420141\n",
      "    rl_1: 172.04100706276938\n",
      "  time_since_restore: 1078.7853298187256\n",
      "  time_this_iter_s: 36.176069259643555\n",
      "  time_total_s: 1078.7853298187256\n",
      "  timestamp: 1552573089\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 40\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1078 s, 40 iter, 400000 ts, 249 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-18-38\n",
      "  done: false\n",
      "  episode_len_mean: 139.48\n",
      "  episode_reward_max: 311.1173947305601\n",
      "  episode_reward_mean: 247.13920242616132\n",
      "  episode_reward_min: -156.26503870902093\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 1923\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3849.874\n",
      "    load_time_ms: 2.886\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9103830890414573e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2981032133102417\n",
      "      kl: 0.006689173169434071\n",
      "      policy_loss: -0.003108435543254018\n",
      "      total_loss: 189.82545471191406\n",
      "      vf_explained_var: 0.6552099585533142\n",
      "      vf_loss: 189.8285675048828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4129951000213623\n",
      "      kl: 0.006919857580214739\n",
      "      policy_loss: -0.004202242940664291\n",
      "      total_loss: 785.8860473632812\n",
      "      vf_explained_var: 0.47390151023864746\n",
      "      vf_loss: 785.8903198242188\n",
      "    sample_time_ms: 28013.197\n",
      "    update_time_ms: 8.914\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.7728445502506\n",
      "    rl_1: 170.3663578759108\n",
      "  time_since_restore: 1107.8079886436462\n",
      "  time_this_iter_s: 29.022658824920654\n",
      "  time_total_s: 1107.8079886436462\n",
      "  timestamp: 1552573118\n",
      "  timesteps_since_restore: 410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 41\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1107 s, 41 iter, 410000 ts, 247 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-19-02\n",
      "  done: false\n",
      "  episode_len_mean: 139.31\n",
      "  episode_reward_max: 308.69187613452095\n",
      "  episode_reward_mean: 270.4733947264466\n",
      "  episode_reward_min: -138.5586992714202\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 1995\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3647.043\n",
      "    load_time_ms: 2.828\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2690067291259766\n",
      "      kl: 0.010724510066211224\n",
      "      policy_loss: -0.0020949889440089464\n",
      "      total_loss: 57.247947692871094\n",
      "      vf_explained_var: 0.8975284099578857\n",
      "      vf_loss: 57.25002670288086\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3979527950286865\n",
      "      kl: 0.006074562203139067\n",
      "      policy_loss: -0.0014225766062736511\n",
      "      total_loss: 705.6497192382812\n",
      "      vf_explained_var: 0.5953922867774963\n",
      "      vf_loss: 705.651123046875\n",
      "    sample_time_ms: 27685.329\n",
      "    update_time_ms: 8.868\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 85.77338158810697\n",
      "    rl_1: 184.70001313833967\n",
      "  time_since_restore: 1131.9201288223267\n",
      "  time_this_iter_s: 24.11214017868042\n",
      "  time_total_s: 1131.9201288223267\n",
      "  timestamp: 1552573142\n",
      "  timesteps_since_restore: 420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 42\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1131 s, 42 iter, 420000 ts, 270 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-19-26\n",
      "  done: false\n",
      "  episode_len_mean: 139.96\n",
      "  episode_reward_max: 307.49118950269326\n",
      "  episode_reward_mean: 280.9051508447599\n",
      "  episode_reward_min: 252.34132078848896\n",
      "  episodes_this_iter: 70\n",
      "  episodes_total: 2065\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3641.369\n",
      "    load_time_ms: 2.79\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2629517316818237\n",
      "      kl: 0.0053502339869737625\n",
      "      policy_loss: -0.0019586163107305765\n",
      "      total_loss: 59.04083251953125\n",
      "      vf_explained_var: 0.9016920924186707\n",
      "      vf_loss: 59.04278564453125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.313225884932663e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.403181791305542\n",
      "      kl: 0.0028101522475481033\n",
      "      policy_loss: -0.0025483889039605856\n",
      "      total_loss: 718.1109008789062\n",
      "      vf_explained_var: 0.5968457460403442\n",
      "      vf_loss: 718.1134643554688\n",
      "    sample_time_ms: 27601.764\n",
      "    update_time_ms: 8.769\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 90.49000443888536\n",
      "    rl_1: 190.41514640587448\n",
      "  time_since_restore: 1156.1446232795715\n",
      "  time_this_iter_s: 24.224494457244873\n",
      "  time_total_s: 1156.1446232795715\n",
      "  timestamp: 1552573166\n",
      "  timesteps_since_restore: 430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 43\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1156 s, 43 iter, 430000 ts, 281 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-19-50\n",
      "  done: false\n",
      "  episode_len_mean: 139.88\n",
      "  episode_reward_max: 307.49118950269326\n",
      "  episode_reward_mean: 273.0911424729361\n",
      "  episode_reward_min: -146.41807466216596\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 2138\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3613.951\n",
      "    load_time_ms: 2.791\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2731717824935913\n",
      "      kl: 0.009626315906643867\n",
      "      policy_loss: -0.005321219563484192\n",
      "      total_loss: 109.09928131103516\n",
      "      vf_explained_var: 0.8054443001747131\n",
      "      vf_loss: 109.10460662841797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.401182770729065\n",
      "      kl: 0.013153820298612118\n",
      "      policy_loss: -0.004310292191803455\n",
      "      total_loss: 727.0272216796875\n",
      "      vf_explained_var: 0.5467634201049805\n",
      "      vf_loss: 727.0315551757812\n",
      "    sample_time_ms: 27141.937\n",
      "    update_time_ms: 8.631\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 87.98526990139945\n",
      "    rl_1: 185.10587257153654\n",
      "  time_since_restore: 1180.090362071991\n",
      "  time_this_iter_s: 23.945738792419434\n",
      "  time_total_s: 1180.090362071991\n",
      "  timestamp: 1552573190\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 44\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1180 s, 44 iter, 440000 ts, 273 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-20-14\n",
      "  done: false\n",
      "  episode_len_mean: 139.19\n",
      "  episode_reward_max: 303.56098750293785\n",
      "  episode_reward_mean: 280.37871407784843\n",
      "  episode_reward_min: 255.10111813973037\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 2210\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3488.174\n",
      "    load_time_ms: 2.743\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2789556980133057\n",
      "      kl: 0.017654959112405777\n",
      "      policy_loss: -0.0030077595729380846\n",
      "      total_loss: 54.35982131958008\n",
      "      vf_explained_var: 0.9091084003448486\n",
      "      vf_loss: 54.362831115722656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3935050964355469\n",
      "      kl: 0.008516691625118256\n",
      "      policy_loss: -0.0036571139935404062\n",
      "      total_loss: 710.4943237304688\n",
      "      vf_explained_var: 0.6073940992355347\n",
      "      vf_loss: 710.4979248046875\n",
      "    sample_time_ms: 26764.773\n",
      "    update_time_ms: 8.651\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 90.00484604798665\n",
      "    rl_1: 190.37386802986168\n",
      "  time_since_restore: 1203.8979485034943\n",
      "  time_this_iter_s: 23.807586431503296\n",
      "  time_total_s: 1203.8979485034943\n",
      "  timestamp: 1552573214\n",
      "  timesteps_since_restore: 450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 45\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1203 s, 45 iter, 450000 ts, 280 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-20-38\n",
      "  done: false\n",
      "  episode_len_mean: 135.81\n",
      "  episode_reward_max: 306.602137687676\n",
      "  episode_reward_mean: 277.5389687329652\n",
      "  episode_reward_min: 250.53106545499892\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 2283\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3433.53\n",
      "    load_time_ms: 2.73\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2670496702194214\n",
      "      kl: 0.015212983824312687\n",
      "      policy_loss: -0.005587846040725708\n",
      "      total_loss: 47.102699279785156\n",
      "      vf_explained_var: 0.9271605014801025\n",
      "      vf_loss: 47.10828399658203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.399857759475708\n",
      "      kl: 0.002854093909263611\n",
      "      policy_loss: -0.0015997281298041344\n",
      "      total_loss: 702.8535766601562\n",
      "      vf_explained_var: 0.6382749080657959\n",
      "      vf_loss: 702.8551025390625\n",
      "    sample_time_ms: 26258.409\n",
      "    update_time_ms: 8.704\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 88.18816132077717\n",
      "    rl_1: 189.35080741218803\n",
      "  time_since_restore: 1227.7036883831024\n",
      "  time_this_iter_s: 23.805739879608154\n",
      "  time_total_s: 1227.7036883831024\n",
      "  timestamp: 1552573238\n",
      "  timesteps_since_restore: 460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 46\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1227 s, 46 iter, 460000 ts, 278 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-21-02\n",
      "  done: false\n",
      "  episode_len_mean: 136.52\n",
      "  episode_reward_max: 307.6397336365787\n",
      "  episode_reward_mean: 280.1559146183235\n",
      "  episode_reward_min: 250.59795514156906\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 2357\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3419.218\n",
      "    load_time_ms: 2.617\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.274949073791504\n",
      "      kl: 0.014370460994541645\n",
      "      policy_loss: -0.004935977049171925\n",
      "      total_loss: 45.833866119384766\n",
      "      vf_explained_var: 0.9334926009178162\n",
      "      vf_loss: 45.83880615234375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.416374683380127\n",
      "      kl: 0.004997607320547104\n",
      "      policy_loss: -0.001730920746922493\n",
      "      total_loss: 640.9933471679688\n",
      "      vf_explained_var: 0.6706103086471558\n",
      "      vf_loss: 640.9950561523438\n",
      "    sample_time_ms: 24737.63\n",
      "    update_time_ms: 8.667\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 91.01121817123769\n",
      "    rl_1: 189.1446964470858\n",
      "  time_since_restore: 1251.9024105072021\n",
      "  time_this_iter_s: 24.19872212409973\n",
      "  time_total_s: 1251.9024105072021\n",
      "  timestamp: 1552573262\n",
      "  timesteps_since_restore: 470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 47\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1251 s, 47 iter, 470000 ts, 280 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-21-27\n",
      "  done: false\n",
      "  episode_len_mean: 141.17\n",
      "  episode_reward_max: 321.677532495525\n",
      "  episode_reward_mean: 280.3063494541807\n",
      "  episode_reward_min: -143.13940979480918\n",
      "  episodes_this_iter: 69\n",
      "  episodes_total: 2426\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3351.746\n",
      "    load_time_ms: 2.569\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2924597263336182\n",
      "      kl: 0.013175382278859615\n",
      "      policy_loss: -0.004477622453123331\n",
      "      total_loss: 82.14146423339844\n",
      "      vf_explained_var: 0.858385443687439\n",
      "      vf_loss: 82.14593505859375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.439531922340393\n",
      "      kl: 0.00579978758469224\n",
      "      policy_loss: -0.002065856009721756\n",
      "      total_loss: 623.2184448242188\n",
      "      vf_explained_var: 0.6348872780799866\n",
      "      vf_loss: 623.2205200195312\n",
      "    sample_time_ms: 23903.928\n",
      "    update_time_ms: 8.455\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 92.78746887450576\n",
      "    rl_1: 187.51888057967494\n",
      "  time_since_restore: 1276.404953956604\n",
      "  time_this_iter_s: 24.502543449401855\n",
      "  time_total_s: 1276.404953956604\n",
      "  timestamp: 1552573287\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 48\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1276 s, 48 iter, 480000 ts, 280 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-21-53\n",
      "  done: false\n",
      "  episode_len_mean: 141.49\n",
      "  episode_reward_max: 324.0598191319402\n",
      "  episode_reward_mean: 289.1209966862509\n",
      "  episode_reward_min: 263.9943163457659\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 2498\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3302.023\n",
      "    load_time_ms: 2.56\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.287123203277588\n",
      "      kl: 0.0054945130832493305\n",
      "      policy_loss: -0.003307824721559882\n",
      "      total_loss: 55.470245361328125\n",
      "      vf_explained_var: 0.924667477607727\n",
      "      vf_loss: 55.47355651855469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9103830890414573e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4388031959533691\n",
      "      kl: 0.015790818259119987\n",
      "      policy_loss: -0.003687027608975768\n",
      "      total_loss: 594.326904296875\n",
      "      vf_explained_var: 0.6945794820785522\n",
      "      vf_loss: 594.3306274414062\n",
      "    sample_time_ms: 22622.985\n",
      "    update_time_ms: 8.192\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 99.24686546623083\n",
      "    rl_1: 189.8741312200201\n",
      "  time_since_restore: 1302.163890361786\n",
      "  time_this_iter_s: 25.758936405181885\n",
      "  time_total_s: 1302.163890361786\n",
      "  timestamp: 1552573313\n",
      "  timesteps_since_restore: 490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 49\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1302 s, 49 iter, 490000 ts, 289 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-22-17\n",
      "  done: false\n",
      "  episode_len_mean: 134.94\n",
      "  episode_reward_max: 319.369354569989\n",
      "  episode_reward_mean: 291.95916607741884\n",
      "  episode_reward_min: 263.0934669273648\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 2573\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3253.628\n",
      "    load_time_ms: 2.568\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8189894306509108e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.258284091949463\n",
      "      kl: 0.007650570012629032\n",
      "      policy_loss: -0.0021321531385183334\n",
      "      total_loss: 75.3919677734375\n",
      "      vf_explained_var: 0.907874584197998\n",
      "      vf_loss: 75.39409637451172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9103830890414573e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4295052289962769\n",
      "      kl: 0.007842016406357288\n",
      "      policy_loss: -0.0011919653043150902\n",
      "      total_loss: 662.8232421875\n",
      "      vf_explained_var: 0.6824814081192017\n",
      "      vf_loss: 662.8243408203125\n",
      "    sample_time_ms: 21486.914\n",
      "    update_time_ms: 7.709\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 102.90751653255222\n",
      "    rl_1: 189.05164954486662\n",
      "  time_since_restore: 1326.490208864212\n",
      "  time_this_iter_s: 24.326318502426147\n",
      "  time_total_s: 1326.490208864212\n",
      "  timestamp: 1552573337\n",
      "  timesteps_since_restore: 500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 50\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1326 s, 50 iter, 500000 ts, 292 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-22-42\n",
      "  done: false\n",
      "  episode_len_mean: 137.01\n",
      "  episode_reward_max: 319.369354569989\n",
      "  episode_reward_mean: 290.2640256339878\n",
      "  episode_reward_min: 258.4711446486144\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 2645\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3291.387\n",
      "    load_time_ms: 2.61\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.266113042831421\n",
      "      kl: 0.00822914857417345\n",
      "      policy_loss: -0.002220034133642912\n",
      "      total_loss: 58.84499740600586\n",
      "      vf_explained_var: 0.9235020279884338\n",
      "      vf_loss: 58.84721755981445\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4101762771606445\n",
      "      kl: 0.0038333272095769644\n",
      "      policy_loss: -0.002017481252551079\n",
      "      total_loss: 650.0543823242188\n",
      "      vf_explained_var: 0.6842352747917175\n",
      "      vf_loss: 650.056396484375\n",
      "    sample_time_ms: 21044.559\n",
      "    update_time_ms: 7.721\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 100.34087806059134\n",
      "    rl_1: 189.92314757339648\n",
      "  time_since_restore: 1351.465719461441\n",
      "  time_this_iter_s: 24.975510597229004\n",
      "  time_total_s: 1351.465719461441\n",
      "  timestamp: 1552573362\n",
      "  timesteps_since_restore: 510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 51\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1351 s, 51 iter, 510000 ts, 290 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-23-07\n",
      "  done: false\n",
      "  episode_len_mean: 135.01\n",
      "  episode_reward_max: 320.22956978494864\n",
      "  episode_reward_mean: 294.5669348002842\n",
      "  episode_reward_min: 261.8013863015004\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 2720\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3292.445\n",
      "    load_time_ms: 2.618\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2266684770584106\n",
      "      kl: 0.01888032630085945\n",
      "      policy_loss: -0.005550819914788008\n",
      "      total_loss: 72.69938659667969\n",
      "      vf_explained_var: 0.9160798192024231\n",
      "      vf_loss: 72.70494842529297\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4207956790924072\n",
      "      kl: 0.010647066868841648\n",
      "      policy_loss: -0.0037593443412333727\n",
      "      total_loss: 654.6036987304688\n",
      "      vf_explained_var: 0.7049137353897095\n",
      "      vf_loss: 654.6075439453125\n",
      "    sample_time_ms: 21108.594\n",
      "    update_time_ms: 7.789\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 104.10951155456854\n",
      "    rl_1: 190.45742324571563\n",
      "  time_since_restore: 1376.2298347949982\n",
      "  time_this_iter_s: 24.76411533355713\n",
      "  time_total_s: 1376.2298347949982\n",
      "  timestamp: 1552573387\n",
      "  timesteps_since_restore: 520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 52\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1376 s, 52 iter, 520000 ts, 295 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 133.08\n",
      "  episode_reward_max: 323.2593527825024\n",
      "  episode_reward_mean: 294.38403801805606\n",
      "  episode_reward_min: 264.6747406150115\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 2795\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3298.344\n",
      "    load_time_ms: 2.612\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.214893102645874\n",
      "      kl: 0.015468945726752281\n",
      "      policy_loss: -0.0042631616815924644\n",
      "      total_loss: 69.11546325683594\n",
      "      vf_explained_var: 0.9181285500526428\n",
      "      vf_loss: 69.11971282958984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3720332384109497\n",
      "      kl: 0.019073082134127617\n",
      "      policy_loss: -0.004265849478542805\n",
      "      total_loss: 656.3120727539062\n",
      "      vf_explained_var: 0.7012473344802856\n",
      "      vf_loss: 656.3162841796875\n",
      "    sample_time_ms: 21203.698\n",
      "    update_time_ms: 7.866\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 104.16372088747565\n",
      "    rl_1: 190.2203171305804\n",
      "  time_since_restore: 1401.4634613990784\n",
      "  time_this_iter_s: 25.2336266040802\n",
      "  time_total_s: 1401.4634613990784\n",
      "  timestamp: 1552573412\n",
      "  timesteps_since_restore: 530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 53\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1401 s, 53 iter, 530000 ts, 294 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-23-56\n",
      "  done: false\n",
      "  episode_len_mean: 132.05\n",
      "  episode_reward_max: 331.6436170074077\n",
      "  episode_reward_mean: 294.83213003291075\n",
      "  episode_reward_min: 264.6747406150115\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 2871\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3298.569\n",
      "    load_time_ms: 2.59\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.202133297920227\n",
      "      kl: 0.014893617480993271\n",
      "      policy_loss: -0.004623618442565203\n",
      "      total_loss: 72.6943588256836\n",
      "      vf_explained_var: 0.9170500636100769\n",
      "      vf_loss: 72.69895935058594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3605103492736816\n",
      "      kl: 0.006949230562895536\n",
      "      policy_loss: -0.003632159670814872\n",
      "      total_loss: 635.1586303710938\n",
      "      vf_explained_var: 0.7154097557067871\n",
      "      vf_loss: 635.1622314453125\n",
      "    sample_time_ms: 21221.535\n",
      "    update_time_ms: 7.764\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 104.38828366777643\n",
      "    rl_1: 190.44384636513433\n",
      "  time_since_restore: 1425.5886232852936\n",
      "  time_this_iter_s: 24.12516188621521\n",
      "  time_total_s: 1425.5886232852936\n",
      "  timestamp: 1552573436\n",
      "  timesteps_since_restore: 540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 54\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1425 s, 54 iter, 540000 ts, 295 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-24-20\n",
      "  done: false\n",
      "  episode_len_mean: 129.89\n",
      "  episode_reward_max: 333.9256552885023\n",
      "  episode_reward_mean: 301.8222666392396\n",
      "  episode_reward_min: 269.5807270589719\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 2948\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3297.292\n",
      "    load_time_ms: 2.58\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.212695598602295\n",
      "      kl: 0.012561974115669727\n",
      "      policy_loss: -0.003497864119708538\n",
      "      total_loss: 94.02639770507812\n",
      "      vf_explained_var: 0.9123934507369995\n",
      "      vf_loss: 94.02989959716797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3527593612670898\n",
      "      kl: 0.019947757944464684\n",
      "      policy_loss: -0.0037197077181190252\n",
      "      total_loss: 639.6719970703125\n",
      "      vf_explained_var: 0.7302612066268921\n",
      "      vf_loss: 639.6756591796875\n",
      "    sample_time_ms: 21227.866\n",
      "    update_time_ms: 7.803\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 111.11565013845457\n",
      "    rl_1: 190.70661650078503\n",
      "  time_since_restore: 1449.4471998214722\n",
      "  time_this_iter_s: 23.85857653617859\n",
      "  time_total_s: 1449.4471998214722\n",
      "  timestamp: 1552573460\n",
      "  timesteps_since_restore: 550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 55\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1449 s, 55 iter, 550000 ts, 302 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-24-44\n",
      "  done: false\n",
      "  episode_len_mean: 128.12\n",
      "  episode_reward_max: 332.153685485018\n",
      "  episode_reward_mean: 301.2473035184364\n",
      "  episode_reward_min: 265.8974994616579\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 3026\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3293.165\n",
      "    load_time_ms: 2.685\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1926883459091187\n",
      "      kl: 0.012188795953989029\n",
      "      policy_loss: -0.0020720520988106728\n",
      "      total_loss: 78.38890838623047\n",
      "      vf_explained_var: 0.9215434193611145\n",
      "      vf_loss: 78.39097595214844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2980296611785889\n",
      "      kl: 0.012581033632159233\n",
      "      policy_loss: -0.0015247722622007132\n",
      "      total_loss: 623.9238891601562\n",
      "      vf_explained_var: 0.7375417947769165\n",
      "      vf_loss: 623.9254150390625\n",
      "    sample_time_ms: 21266.187\n",
      "    update_time_ms: 7.862\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 110.65694163296354\n",
      "    rl_1: 190.59036188547287\n",
      "  time_since_restore: 1473.598045349121\n",
      "  time_this_iter_s: 24.150845527648926\n",
      "  time_total_s: 1473.598045349121\n",
      "  timestamp: 1552573484\n",
      "  timesteps_since_restore: 560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 56\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1473 s, 56 iter, 560000 ts, 301 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-25-09\n",
      "  done: false\n",
      "  episode_len_mean: 126.27\n",
      "  episode_reward_max: 335.0785513224002\n",
      "  episode_reward_mean: 300.0737523350455\n",
      "  episode_reward_min: 260.63479977598104\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 3106\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3288.704\n",
      "    load_time_ms: 2.711\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.170042634010315\n",
      "      kl: 0.011864827014505863\n",
      "      policy_loss: -0.004674112424254417\n",
      "      total_loss: 78.29151916503906\n",
      "      vf_explained_var: 0.9212631583213806\n",
      "      vf_loss: 78.29619598388672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.261793613433838\n",
      "      kl: 0.010422690771520138\n",
      "      policy_loss: -0.0020496922079473734\n",
      "      total_loss: 632.52685546875\n",
      "      vf_explained_var: 0.743724524974823\n",
      "      vf_loss: 632.5289916992188\n",
      "    sample_time_ms: 21275.66\n",
      "    update_time_ms: 7.791\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 109.59887048232609\n",
      "    rl_1: 190.47488185271934\n",
      "  time_since_restore: 1497.8437387943268\n",
      "  time_this_iter_s: 24.24569344520569\n",
      "  time_total_s: 1497.8437387943268\n",
      "  timestamp: 1552573509\n",
      "  timesteps_since_restore: 570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 57\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1497 s, 57 iter, 570000 ts, 300 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-25-36\n",
      "  done: false\n",
      "  episode_len_mean: 126.6\n",
      "  episode_reward_max: 338.87532975695046\n",
      "  episode_reward_mean: 303.16008490100563\n",
      "  episode_reward_min: 260.63479977598104\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 3185\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3349.372\n",
      "    load_time_ms: 2.725\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1513965129852295\n",
      "      kl: 0.013881281018257141\n",
      "      policy_loss: -0.004594553727656603\n",
      "      total_loss: 86.67510223388672\n",
      "      vf_explained_var: 0.918551504611969\n",
      "      vf_loss: 86.6796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.266235589981079\n",
      "      kl: 0.014660349115729332\n",
      "      policy_loss: -0.0041113547049462795\n",
      "      total_loss: 598.2869873046875\n",
      "      vf_explained_var: 0.7586967945098877\n",
      "      vf_loss: 598.2910766601562\n",
      "    sample_time_ms: 21509.209\n",
      "    update_time_ms: 7.6\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 112.71852923351824\n",
      "    rl_1: 190.44155566748736\n",
      "  time_since_restore: 1525.290892124176\n",
      "  time_this_iter_s: 27.447153329849243\n",
      "  time_total_s: 1525.290892124176\n",
      "  timestamp: 1552573536\n",
      "  timesteps_since_restore: 580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 58\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1525 s, 58 iter, 580000 ts, 303 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-26-09\n",
      "  done: false\n",
      "  episode_len_mean: 126.59\n",
      "  episode_reward_max: 335.49904704077545\n",
      "  episode_reward_mean: 304.41536657696093\n",
      "  episode_reward_min: 271.864155637395\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 3264\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3403.096\n",
      "    load_time_ms: 2.625\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.157075047492981\n",
      "      kl: 0.002055652905255556\n",
      "      policy_loss: -0.0023772576823830605\n",
      "      total_loss: 91.3935317993164\n",
      "      vf_explained_var: 0.9158239960670471\n",
      "      vf_loss: 91.39591217041016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.310153603553772\n",
      "      kl: 0.018427494913339615\n",
      "      policy_loss: -0.0054876781068742275\n",
      "      total_loss: 588.7836303710938\n",
      "      vf_explained_var: 0.7662957310676575\n",
      "      vf_loss: 588.7891845703125\n",
      "    sample_time_ms: 22129.464\n",
      "    update_time_ms: 7.598\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 115.19142531622643\n",
      "    rl_1: 189.22394126073442\n",
      "  time_since_restore: 1557.7877931594849\n",
      "  time_this_iter_s: 32.49690103530884\n",
      "  time_total_s: 1557.7877931594849\n",
      "  timestamp: 1552573569\n",
      "  timesteps_since_restore: 590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 59\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1557 s, 59 iter, 590000 ts, 304 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-26-48\n",
      "  done: false\n",
      "  episode_len_mean: 126.48\n",
      "  episode_reward_max: 338.285089864742\n",
      "  episode_reward_mean: 305.0278891222402\n",
      "  episode_reward_min: 271.2475624672532\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 3343\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3634.957\n",
      "    load_time_ms: 2.749\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1468698978424072\n",
      "      kl: 0.0046281698159873486\n",
      "      policy_loss: -0.0029053145553916693\n",
      "      total_loss: 88.23143768310547\n",
      "      vf_explained_var: 0.9211059808731079\n",
      "      vf_loss: 88.23435974121094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.305782675743103\n",
      "      kl: 0.0039526959881186485\n",
      "      policy_loss: -0.0015027893241494894\n",
      "      total_loss: 563.6791381835938\n",
      "      vf_explained_var: 0.7795290350914001\n",
      "      vf_loss: 563.6806030273438\n",
      "    sample_time_ms: 23430.078\n",
      "    update_time_ms: 7.9\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 115.91932721931101\n",
      "    rl_1: 189.1085619029293\n",
      "  time_since_restore: 1597.4465110301971\n",
      "  time_this_iter_s: 39.65871787071228\n",
      "  time_total_s: 1597.4465110301971\n",
      "  timestamp: 1552573608\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 60\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1597 s, 60 iter, 600000 ts, 305 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-27-24\n",
      "  done: false\n",
      "  episode_len_mean: 127.31\n",
      "  episode_reward_max: 338.1076275331691\n",
      "  episode_reward_mean: 306.7331213624643\n",
      "  episode_reward_min: 270.68150396787775\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 3421\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3625.822\n",
      "    load_time_ms: 2.727\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1368683941568192e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1460996866226196\n",
      "      kl: 0.003801000537350774\n",
      "      policy_loss: -0.001983080292120576\n",
      "      total_loss: 84.35540771484375\n",
      "      vf_explained_var: 0.9270892143249512\n",
      "      vf_loss: 84.3573989868164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8189894306509108e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2859773635864258\n",
      "      kl: 0.00727455597370863\n",
      "      policy_loss: -0.0013984163524582982\n",
      "      total_loss: 566.7307739257812\n",
      "      vf_explained_var: 0.7849999666213989\n",
      "      vf_loss: 566.7320556640625\n",
      "    sample_time_ms: 24541.869\n",
      "    update_time_ms: 8.084\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 116.23518322150892\n",
      "    rl_1: 190.4979381409553\n",
      "  time_since_restore: 1633.4491157531738\n",
      "  time_this_iter_s: 36.002604722976685\n",
      "  time_total_s: 1633.4491157531738\n",
      "  timestamp: 1552573644\n",
      "  timesteps_since_restore: 610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 61\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1633 s, 61 iter, 610000 ts, 307 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-27-58\n",
      "  done: false\n",
      "  episode_len_mean: 126.16\n",
      "  episode_reward_max: 342.9922482130353\n",
      "  episode_reward_mean: 308.18331789648266\n",
      "  episode_reward_min: 274.38482497837674\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 3501\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3797.682\n",
      "    load_time_ms: 2.74\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1249761581420898\n",
      "      kl: 0.004147897474467754\n",
      "      policy_loss: -0.0018802335252985358\n",
      "      total_loss: 84.64818572998047\n",
      "      vf_explained_var: 0.9275823831558228\n",
      "      vf_loss: 84.65007019042969\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2610918283462524\n",
      "      kl: 0.008858942426741123\n",
      "      policy_loss: -0.0022151588927954435\n",
      "      total_loss: 568.728515625\n",
      "      vf_explained_var: 0.7872978448867798\n",
      "      vf_loss: 568.7308349609375\n",
      "    sample_time_ms: 25221.213\n",
      "    update_time_ms: 8.052\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 117.43366184594919\n",
      "    rl_1: 190.7496560505334\n",
      "  time_since_restore: 1666.727706193924\n",
      "  time_this_iter_s: 33.27859044075012\n",
      "  time_total_s: 1666.727706193924\n",
      "  timestamp: 1552573678\n",
      "  timesteps_since_restore: 620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 62\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1666 s, 62 iter, 620000 ts, 308 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-28-27\n",
      "  done: false\n",
      "  episode_len_mean: 124.44\n",
      "  episode_reward_max: 347.31274839482836\n",
      "  episode_reward_mean: 312.283800442002\n",
      "  episode_reward_min: 281.5995400682473\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 3581\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3806.0\n",
      "    load_time_ms: 2.723\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.842170985392048e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1021674871444702\n",
      "      kl: 0.009377366863191128\n",
      "      policy_loss: -0.002190701197832823\n",
      "      total_loss: 101.43754577636719\n",
      "      vf_explained_var: 0.9239113926887512\n",
      "      vf_loss: 101.43972778320312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.284342885017395\n",
      "      kl: 0.008650929667055607\n",
      "      policy_loss: -0.0022830308880656958\n",
      "      total_loss: 568.19287109375\n",
      "      vf_explained_var: 0.795845627784729\n",
      "      vf_loss: 568.1951293945312\n",
      "    sample_time_ms: 25605.552\n",
      "    update_time_ms: 7.998\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 122.05028072353875\n",
      "    rl_1: 190.23351971846324\n",
      "  time_since_restore: 1695.888282775879\n",
      "  time_this_iter_s: 29.160576581954956\n",
      "  time_total_s: 1695.888282775879\n",
      "  timestamp: 1552573707\n",
      "  timesteps_since_restore: 630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 63\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1695 s, 63 iter, 630000 ts, 312 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-28-53\n",
      "  done: false\n",
      "  episode_len_mean: 125.32\n",
      "  episode_reward_max: 341.9215569034874\n",
      "  episode_reward_mean: 312.4581437066265\n",
      "  episode_reward_min: 275.87880284024385\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3662\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3818.664\n",
      "    load_time_ms: 2.804\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.421085492696024e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0996482372283936\n",
      "      kl: 0.017358962446451187\n",
      "      policy_loss: -0.0036907903850078583\n",
      "      total_loss: 95.48603820800781\n",
      "      vf_explained_var: 0.9274294972419739\n",
      "      vf_loss: 95.48973083496094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2867499589920044\n",
      "      kl: 0.00581539748236537\n",
      "      policy_loss: -0.0025143870152533054\n",
      "      total_loss: 525.2694091796875\n",
      "      vf_explained_var: 0.8094920516014099\n",
      "      vf_loss: 525.2719116210938\n",
      "    sample_time_ms: 25730.981\n",
      "    update_time_ms: 7.971\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 122.63234598229784\n",
      "    rl_1: 189.82579772432877\n",
      "  time_since_restore: 1721.396286725998\n",
      "  time_this_iter_s: 25.50800395011902\n",
      "  time_total_s: 1721.396286725998\n",
      "  timestamp: 1552573733\n",
      "  timesteps_since_restore: 640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 64\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1721 s, 64 iter, 640000 ts, 312 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-29-18\n",
      "  done: false\n",
      "  episode_len_mean: 125.62\n",
      "  episode_reward_max: 346.6889135744394\n",
      "  episode_reward_mean: 313.4344862340645\n",
      "  episode_reward_min: 276.34340356988724\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 3741\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3832.209\n",
      "    load_time_ms: 2.897\n",
      "    num_steps_sampled: 650000\n",
      "    num_steps_trained: 650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.421085492696024e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0633472204208374\n",
      "      kl: 0.0074365208856761456\n",
      "      policy_loss: -0.002822505310177803\n",
      "      total_loss: 88.51689910888672\n",
      "      vf_explained_var: 0.9318167567253113\n",
      "      vf_loss: 88.51972198486328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1368683941568192e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2432835102081299\n",
      "      kl: 0.008327077142894268\n",
      "      policy_loss: -0.0017300074687227607\n",
      "      total_loss: 543.9725952148438\n",
      "      vf_explained_var: 0.8106602430343628\n",
      "      vf_loss: 543.9742431640625\n",
      "    sample_time_ms: 25907.87\n",
      "    update_time_ms: 7.928\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 122.8106964461047\n",
      "    rl_1: 190.62378978795977\n",
      "  time_since_restore: 1747.1599378585815\n",
      "  time_this_iter_s: 25.763651132583618\n",
      "  time_total_s: 1747.1599378585815\n",
      "  timestamp: 1552573758\n",
      "  timesteps_since_restore: 650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 650000\n",
      "  training_iteration: 65\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1747 s, 65 iter, 650000 ts, 313 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-29-44\n",
      "  done: false\n",
      "  episode_len_mean: 124.68\n",
      "  episode_reward_max: 347.2544935948925\n",
      "  episode_reward_mean: 314.0330538192834\n",
      "  episode_reward_min: 278.5881948519873\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3822\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3845.34\n",
      "    load_time_ms: 2.805\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0231249332427979\n",
      "      kl: 0.012852458283305168\n",
      "      policy_loss: -0.0036712747532874346\n",
      "      total_loss: 88.47433471679688\n",
      "      vf_explained_var: 0.9335412383079529\n",
      "      vf_loss: 88.4780044555664\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2049206495285034\n",
      "      kl: 0.01254092063754797\n",
      "      policy_loss: -0.003426565555855632\n",
      "      total_loss: 532.208251953125\n",
      "      vf_explained_var: 0.8166936039924622\n",
      "      vf_loss: 532.2117309570312\n",
      "    sample_time_ms: 26048.135\n",
      "    update_time_ms: 7.861\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 122.98933454386074\n",
      "    rl_1: 191.0437192754227\n",
      "  time_since_restore: 1772.8444056510925\n",
      "  time_this_iter_s: 25.684467792510986\n",
      "  time_total_s: 1772.8444056510925\n",
      "  timestamp: 1552573784\n",
      "  timesteps_since_restore: 660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 66\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1772 s, 66 iter, 660000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-30-10\n",
      "  done: false\n",
      "  episode_len_mean: 122.86\n",
      "  episode_reward_max: 344.0644468940634\n",
      "  episode_reward_mean: 313.5901679388393\n",
      "  episode_reward_min: 272.6213497527387\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3903\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3860.743\n",
      "    load_time_ms: 2.801\n",
      "    num_steps_sampled: 670000\n",
      "    num_steps_trained: 670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.008554220199585\n",
      "      kl: 0.01504718791693449\n",
      "      policy_loss: -0.002928977133706212\n",
      "      total_loss: 81.74723052978516\n",
      "      vf_explained_var: 0.9382075667381287\n",
      "      vf_loss: 81.75016021728516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.213820457458496\n",
      "      kl: 0.010755696333944798\n",
      "      policy_loss: -0.0017876607598736882\n",
      "      total_loss: 524.9591674804688\n",
      "      vf_explained_var: 0.8216963410377502\n",
      "      vf_loss: 524.9610595703125\n",
      "    sample_time_ms: 26178.449\n",
      "    update_time_ms: 7.858\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 122.59355620494262\n",
      "    rl_1: 190.99661173389666\n",
      "  time_since_restore: 1798.5482268333435\n",
      "  time_this_iter_s: 25.703821182250977\n",
      "  time_total_s: 1798.5482268333435\n",
      "  timestamp: 1552573810\n",
      "  timesteps_since_restore: 670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 670000\n",
      "  training_iteration: 67\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1798 s, 67 iter, 670000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-30-45\n",
      "  done: false\n",
      "  episode_len_mean: 122.17\n",
      "  episode_reward_max: 344.063616389613\n",
      "  episode_reward_mean: 310.2069366734276\n",
      "  episode_reward_min: 274.5551161124223\n",
      "  episodes_this_iter: 82\n",
      "  episodes_total: 3985\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3806.534\n",
      "    load_time_ms: 2.841\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.99464350938797\n",
      "      kl: 0.008063930086791515\n",
      "      policy_loss: -0.001236226293258369\n",
      "      total_loss: 71.41674041748047\n",
      "      vf_explained_var: 0.9465905427932739\n",
      "      vf_loss: 71.41798400878906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2032719850540161\n",
      "      kl: 0.0038195450324565172\n",
      "      policy_loss: -0.0008031068718992174\n",
      "      total_loss: 498.23431396484375\n",
      "      vf_explained_var: 0.8317425847053528\n",
      "      vf_loss: 498.23516845703125\n",
      "    sample_time_ms: 27035.973\n",
      "    update_time_ms: 8.002\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 121.24031198336016\n",
      "    rl_1: 188.9666246900674\n",
      "  time_since_restore: 1834.030553817749\n",
      "  time_this_iter_s: 35.48232698440552\n",
      "  time_total_s: 1834.030553817749\n",
      "  timestamp: 1552573845\n",
      "  timesteps_since_restore: 680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 68\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1834 s, 68 iter, 680000 ts, 310 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-31-16\n",
      "  done: false\n",
      "  episode_len_mean: 123.28\n",
      "  episode_reward_max: 344.063616389613\n",
      "  episode_reward_mean: 313.0999763789682\n",
      "  episode_reward_min: 281.09753977194026\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 4066\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3781.156\n",
      "    load_time_ms: 2.811\n",
      "    num_steps_sampled: 690000\n",
      "    num_steps_trained: 690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.55271373174006e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9592463374137878\n",
      "      kl: 0.008142907172441483\n",
      "      policy_loss: -0.0017618953716009855\n",
      "      total_loss: 69.20813751220703\n",
      "      vf_explained_var: 0.9481364488601685\n",
      "      vf_loss: 69.20990753173828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.842170985392048e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.146206021308899\n",
      "      kl: 0.015293208882212639\n",
      "      policy_loss: -0.002940160920843482\n",
      "      total_loss: 503.49127197265625\n",
      "      vf_explained_var: 0.832971453666687\n",
      "      vf_loss: 503.494140625\n",
      "    sample_time_ms: 26917.124\n",
      "    update_time_ms: 7.874\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 121.92857790646312\n",
      "    rl_1: 191.1713984725051\n",
      "  time_since_restore: 1865.081701517105\n",
      "  time_this_iter_s: 31.05114769935608\n",
      "  time_total_s: 1865.081701517105\n",
      "  timestamp: 1552573876\n",
      "  timesteps_since_restore: 690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 690000\n",
      "  training_iteration: 69\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1865 s, 69 iter, 690000 ts, 313 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-31-48\n",
      "  done: false\n",
      "  episode_len_mean: 122.03\n",
      "  episode_reward_max: 346.16845472064034\n",
      "  episode_reward_mean: 310.99605858098334\n",
      "  episode_reward_min: 275.18381929286824\n",
      "  episodes_this_iter: 82\n",
      "  episodes_total: 4148\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3734.495\n",
      "    load_time_ms: 2.69\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.77635686587003e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9144368171691895\n",
      "      kl: 0.010570749640464783\n",
      "      policy_loss: -0.003826047293841839\n",
      "      total_loss: 69.87342834472656\n",
      "      vf_explained_var: 0.949072539806366\n",
      "      vf_loss: 69.87725067138672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.842170985392048e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0981851816177368\n",
      "      kl: 0.01884656772017479\n",
      "      policy_loss: -0.0037705195136368275\n",
      "      total_loss: 479.23486328125\n",
      "      vf_explained_var: 0.8414835929870605\n",
      "      vf_loss: 479.2387390136719\n",
      "    sample_time_ms: 26195.566\n",
      "    update_time_ms: 8.119\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 121.23834244013962\n",
      "    rl_1: 189.75771614084368\n",
      "  time_since_restore: 1897.06094455719\n",
      "  time_this_iter_s: 31.97924304008484\n",
      "  time_total_s: 1897.06094455719\n",
      "  timestamp: 1552573908\n",
      "  timesteps_since_restore: 700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 70\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1897 s, 70 iter, 700000 ts, 311 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-32-19\n",
      "  done: false\n",
      "  episode_len_mean: 121.18\n",
      "  episode_reward_max: 346.16845472064034\n",
      "  episode_reward_mean: 312.24784505226006\n",
      "  episode_reward_min: 277.8861487090096\n",
      "  episodes_this_iter: 83\n",
      "  episodes_total: 4231\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3845.733\n",
      "    load_time_ms: 2.805\n",
      "    num_steps_sampled: 710000\n",
      "    num_steps_trained: 710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.77635686587003e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.900344729423523\n",
      "      kl: 0.003141129156574607\n",
      "      policy_loss: -0.0009680268121883273\n",
      "      total_loss: 56.955928802490234\n",
      "      vf_explained_var: 0.9574515223503113\n",
      "      vf_loss: 56.95690155029297\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.842170985392048e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0878418684005737\n",
      "      kl: 0.00803648866713047\n",
      "      policy_loss: -0.0020400467328727245\n",
      "      total_loss: 472.25439453125\n",
      "      vf_explained_var: 0.8472478985786438\n",
      "      vf_loss: 472.2564697265625\n",
      "    sample_time_ms: 25499.436\n",
      "    update_time_ms: 8.28\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 122.43975733716773\n",
      "    rl_1: 189.80808771509234\n",
      "  time_since_restore: 1927.221780538559\n",
      "  time_this_iter_s: 30.16083598136902\n",
      "  time_total_s: 1927.221780538559\n",
      "  timestamp: 1552573939\n",
      "  timesteps_since_restore: 710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 710000\n",
      "  training_iteration: 71\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1927 s, 71 iter, 710000 ts, 312 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-32-50\n",
      "  done: false\n",
      "  episode_len_mean: 123.14\n",
      "  episode_reward_max: 346.67394694163903\n",
      "  episode_reward_mean: 310.6940430150864\n",
      "  episode_reward_min: 281.12933332956356\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 4311\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3679.358\n",
      "    load_time_ms: 2.898\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.88178432935015e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9052608013153076\n",
      "      kl: 0.005785136017948389\n",
      "      policy_loss: -0.0008771157590672374\n",
      "      total_loss: 46.186805725097656\n",
      "      vf_explained_var: 0.9621270895004272\n",
      "      vf_loss: 46.18768310546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.421085492696024e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0681365728378296\n",
      "      kl: 0.016085295006632805\n",
      "      policy_loss: -0.004860198590904474\n",
      "      total_loss: 470.64337158203125\n",
      "      vf_explained_var: 0.8487664461135864\n",
      "      vf_loss: 470.6482238769531\n",
      "    sample_time_ms: 25415.524\n",
      "    update_time_ms: 8.247\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 118.59239515937666\n",
      "    rl_1: 192.1016478557097\n",
      "  time_since_restore: 1957.9960820674896\n",
      "  time_this_iter_s: 30.774301528930664\n",
      "  time_total_s: 1957.9960820674896\n",
      "  timestamp: 1552573970\n",
      "  timesteps_since_restore: 720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 72\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1957 s, 72 iter, 720000 ts, 311 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-33-15\n",
      "  done: false\n",
      "  episode_len_mean: 122.93\n",
      "  episode_reward_max: 346.67394694163903\n",
      "  episode_reward_mean: 311.781623477048\n",
      "  episode_reward_min: 277.48671065751773\n",
      "  episodes_this_iter: 82\n",
      "  episodes_total: 4393\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3675.704\n",
      "    load_time_ms: 2.898\n",
      "    num_steps_sampled: 730000\n",
      "    num_steps_trained: 730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8863610625267029\n",
      "      kl: 0.010939017869532108\n",
      "      policy_loss: -0.0031542680226266384\n",
      "      total_loss: 52.30857849121094\n",
      "      vf_explained_var: 0.9597723484039307\n",
      "      vf_loss: 52.31172561645508\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.421085492696024e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0846914052963257\n",
      "      kl: 0.006701939273625612\n",
      "      policy_loss: -0.001769163878634572\n",
      "      total_loss: 433.5866394042969\n",
      "      vf_explained_var: 0.861944854259491\n",
      "      vf_loss: 433.58837890625\n",
      "    sample_time_ms: 25065.525\n",
      "    update_time_ms: 8.265\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 121.50858835362422\n",
      "    rl_1: 190.27303512342382\n",
      "  time_since_restore: 1983.6213941574097\n",
      "  time_this_iter_s: 25.625312089920044\n",
      "  time_total_s: 1983.6213941574097\n",
      "  timestamp: 1552573995\n",
      "  timesteps_since_restore: 730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 730000\n",
      "  training_iteration: 73\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 1983 s, 73 iter, 730000 ts, 312 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-33-41\n",
      "  done: false\n",
      "  episode_len_mean: 122.23\n",
      "  episode_reward_max: 345.31745818166087\n",
      "  episode_reward_mean: 310.1197808214836\n",
      "  episode_reward_min: 273.0812391950817\n",
      "  episodes_this_iter: 82\n",
      "  episodes_total: 4475\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3671.848\n",
      "    load_time_ms: 2.853\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8562361001968384\n",
      "      kl: 0.008703745901584625\n",
      "      policy_loss: -0.002719527343288064\n",
      "      total_loss: 46.0300178527832\n",
      "      vf_explained_var: 0.9642899632453918\n",
      "      vf_loss: 46.03274154663086\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.096311330795288\n",
      "      kl: 0.024125749245285988\n",
      "      policy_loss: -0.006132134236395359\n",
      "      total_loss: 421.19732666015625\n",
      "      vf_explained_var: 0.8667976260185242\n",
      "      vf_loss: 421.20343017578125\n",
      "    sample_time_ms: 25076.24\n",
      "    update_time_ms: 8.643\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 121.33741369427595\n",
      "    rl_1: 188.7823671272076\n",
      "  time_since_restore: 2009.2000658512115\n",
      "  time_this_iter_s: 25.57867169380188\n",
      "  time_total_s: 2009.2000658512115\n",
      "  timestamp: 1552574021\n",
      "  timesteps_since_restore: 740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 74\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2009 s, 74 iter, 740000 ts, 310 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-34-07\n",
      "  done: false\n",
      "  episode_len_mean: 123.01\n",
      "  episode_reward_max: 343.0279838522836\n",
      "  episode_reward_mean: 313.35142070725857\n",
      "  episode_reward_min: 280.9083265223367\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 4556\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3671.204\n",
      "    load_time_ms: 2.798\n",
      "    num_steps_sampled: 750000\n",
      "    num_steps_trained: 750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2204460823375376e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8577484488487244\n",
      "      kl: 0.005945899989455938\n",
      "      policy_loss: -0.001601989846676588\n",
      "      total_loss: 48.58257293701172\n",
      "      vf_explained_var: 0.9631173610687256\n",
      "      vf_loss: 48.58417510986328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1347684860229492\n",
      "      kl: 0.003978865221142769\n",
      "      policy_loss: -0.000523960858117789\n",
      "      total_loss: 407.6012878417969\n",
      "      vf_explained_var: 0.8712732195854187\n",
      "      vf_loss: 407.6018371582031\n",
      "    sample_time_ms: 25069.254\n",
      "    update_time_ms: 8.619\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 124.17100867453641\n",
      "    rl_1: 189.18041203272216\n",
      "  time_since_restore: 2034.8875532150269\n",
      "  time_this_iter_s: 25.687487363815308\n",
      "  time_total_s: 2034.8875532150269\n",
      "  timestamp: 1552574047\n",
      "  timesteps_since_restore: 750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 750000\n",
      "  training_iteration: 75\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2034 s, 75 iter, 750000 ts, 313 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-34-33\n",
      "  done: false\n",
      "  episode_len_mean: 124.63\n",
      "  episode_reward_max: 351.35746452875816\n",
      "  episode_reward_mean: 313.75093753492183\n",
      "  episode_reward_min: 281.30384465216906\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 4636\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3687.785\n",
      "    load_time_ms: 2.783\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1102230411687688e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8484992980957031\n",
      "      kl: 0.009864941239356995\n",
      "      policy_loss: -0.004292510915547609\n",
      "      total_loss: 39.3851203918457\n",
      "      vf_explained_var: 0.9686363339424133\n",
      "      vf_loss: 39.389408111572266\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.55271373174006e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1460376977920532\n",
      "      kl: 0.0384662039577961\n",
      "      policy_loss: -0.007370724808424711\n",
      "      total_loss: 402.9434509277344\n",
      "      vf_explained_var: 0.8750947117805481\n",
      "      vf_loss: 402.9508361816406\n",
      "    sample_time_ms: 25107.32\n",
      "    update_time_ms: 8.871\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 122.75910996526274\n",
      "    rl_1: 190.9918275696591\n",
      "  time_since_restore: 2061.1205558776855\n",
      "  time_this_iter_s: 26.23300266265869\n",
      "  time_total_s: 2061.1205558776855\n",
      "  timestamp: 1552574073\n",
      "  timesteps_since_restore: 760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 76\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2061 s, 76 iter, 760000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-35-00\n",
      "  done: false\n",
      "  episode_len_mean: 124.64\n",
      "  episode_reward_max: 351.74814900192746\n",
      "  episode_reward_mean: 314.5595136987246\n",
      "  episode_reward_min: 278.410235021905\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 4716\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3690.813\n",
      "    load_time_ms: 2.759\n",
      "    num_steps_sampled: 770000\n",
      "    num_steps_trained: 770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.551115205843844e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8323246836662292\n",
      "      kl: 0.01690436713397503\n",
      "      policy_loss: -0.005431560333818197\n",
      "      total_loss: 42.97610855102539\n",
      "      vf_explained_var: 0.9670185446739197\n",
      "      vf_loss: 42.98154067993164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.55271373174006e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1893893480300903\n",
      "      kl: 0.01075268816202879\n",
      "      policy_loss: -0.0035772169940173626\n",
      "      total_loss: 382.3572082519531\n",
      "      vf_explained_var: 0.8833799362182617\n",
      "      vf_loss: 382.3608093261719\n",
      "    sample_time_ms: 25233.233\n",
      "    update_time_ms: 8.984\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 124.58482987733186\n",
      "    rl_1: 189.97468382139274\n",
      "  time_since_restore: 2088.1133518218994\n",
      "  time_this_iter_s: 26.992795944213867\n",
      "  time_total_s: 2088.1133518218994\n",
      "  timestamp: 1552574100\n",
      "  timesteps_since_restore: 770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 770000\n",
      "  training_iteration: 77\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2088 s, 77 iter, 770000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-35-27\n",
      "  done: false\n",
      "  episode_len_mean: 124.6\n",
      "  episode_reward_max: 350.27054610231534\n",
      "  episode_reward_mean: 316.92231049274494\n",
      "  episode_reward_min: 280.99963043605806\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 4796\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3725.173\n",
      "    load_time_ms: 2.724\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.551115205843844e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8272140622138977\n",
      "      kl: 0.008590794168412685\n",
      "      policy_loss: -0.001598292961716652\n",
      "      total_loss: 52.07351303100586\n",
      "      vf_explained_var: 0.9646735787391663\n",
      "      vf_loss: 52.07511520385742\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.55271373174006e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1955994367599487\n",
      "      kl: 0.018625663593411446\n",
      "      policy_loss: -0.0032703608740121126\n",
      "      total_loss: 367.3101501464844\n",
      "      vf_explained_var: 0.8898282051086426\n",
      "      vf_loss: 367.31341552734375\n",
      "    sample_time_ms: 24386.238\n",
      "    update_time_ms: 9.162\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.85640879071315\n",
      "    rl_1: 189.0659017020317\n",
      "  time_since_restore: 2115.4677109718323\n",
      "  time_this_iter_s: 27.35435914993286\n",
      "  time_total_s: 2115.4677109718323\n",
      "  timestamp: 1552574127\n",
      "  timesteps_since_restore: 780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 78\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2115 s, 78 iter, 780000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-36-06\n",
      "  done: false\n",
      "  episode_len_mean: 124.09\n",
      "  episode_reward_max: 348.35193854162475\n",
      "  episode_reward_mean: 318.91773327250024\n",
      "  episode_reward_min: 284.4807819245588\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 4876\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4013.789\n",
      "    load_time_ms: 2.747\n",
      "    num_steps_sampled: 790000\n",
      "    num_steps_trained: 790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.775557602921922e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7890915274620056\n",
      "      kl: 0.011991196312010288\n",
      "      policy_loss: -8.200338197639212e-05\n",
      "      total_loss: 50.56816864013672\n",
      "      vf_explained_var: 0.9666978120803833\n",
      "      vf_loss: 50.56825256347656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.55271373174006e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1657662391662598\n",
      "      kl: 0.008539333939552307\n",
      "      policy_loss: -0.0008100197883322835\n",
      "      total_loss: 360.47186279296875\n",
      "      vf_explained_var: 0.8932884335517883\n",
      "      vf_loss: 360.47265625\n",
      "    sample_time_ms: 24844.936\n",
      "    update_time_ms: 9.688\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.06836542780488\n",
      "    rl_1: 188.8493678446954\n",
      "  time_since_restore: 2154.0104150772095\n",
      "  time_this_iter_s: 38.5427041053772\n",
      "  time_total_s: 2154.0104150772095\n",
      "  timestamp: 1552574166\n",
      "  timesteps_since_restore: 790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 790000\n",
      "  training_iteration: 79\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2154 s, 79 iter, 790000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-37-13\n",
      "  done: false\n",
      "  episode_len_mean: 123.62\n",
      "  episode_reward_max: 348.35193854162475\n",
      "  episode_reward_mean: 319.1435194543915\n",
      "  episode_reward_min: 285.62221895774127\n",
      "  episodes_this_iter: 82\n",
      "  episodes_total: 4958\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4037.329\n",
      "    load_time_ms: 2.857\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.775557602921922e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7387590408325195\n",
      "      kl: 0.011686421930789948\n",
      "      policy_loss: -0.0022722615394741297\n",
      "      total_loss: 45.546417236328125\n",
      "      vf_explained_var: 0.9703084230422974\n",
      "      vf_loss: 45.54869079589844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.77635686587003e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1356083154678345\n",
      "      kl: 0.0024929887149482965\n",
      "      policy_loss: -0.0013024425134062767\n",
      "      total_loss: 356.1667175292969\n",
      "      vf_explained_var: 0.8965604901313782\n",
      "      vf_loss: 356.1680603027344\n",
      "    sample_time_ms: 28316.629\n",
      "    update_time_ms: 10.047\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.14765338269794\n",
      "    rl_1: 189.9958660716936\n",
      "  time_since_restore: 2220.950737953186\n",
      "  time_this_iter_s: 66.94032287597656\n",
      "  time_total_s: 2220.950737953186\n",
      "  timestamp: 1552574233\n",
      "  timesteps_since_restore: 800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 80\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2220 s, 80 iter, 800000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-37-39\n",
      "  done: false\n",
      "  episode_len_mean: 124.09\n",
      "  episode_reward_max: 349.21862138618945\n",
      "  episode_reward_mean: 315.75287302835903\n",
      "  episode_reward_min: 281.1656127638242\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 5038\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3897.783\n",
      "    load_time_ms: 2.837\n",
      "    num_steps_sampled: 810000\n",
      "    num_steps_trained: 810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.775557602921922e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.736802339553833\n",
      "      kl: 0.007614422123879194\n",
      "      policy_loss: -0.0005473821074701846\n",
      "      total_loss: 33.57624435424805\n",
      "      vf_explained_var: 0.975719153881073\n",
      "      vf_loss: 33.57678985595703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.88178432935015e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1352437734603882\n",
      "      kl: 0.019648056477308273\n",
      "      policy_loss: -0.002834041602909565\n",
      "      total_loss: 347.5311279296875\n",
      "      vf_explained_var: 0.8985193967819214\n",
      "      vf_loss: 347.533935546875\n",
      "    sample_time_ms: 27992.133\n",
      "    update_time_ms: 10.374\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 124.97531343690635\n",
      "    rl_1: 190.7775595914527\n",
      "  time_since_restore: 2246.4724938869476\n",
      "  time_this_iter_s: 25.521755933761597\n",
      "  time_total_s: 2246.4724938869476\n",
      "  timestamp: 1552574259\n",
      "  timesteps_since_restore: 810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 810000\n",
      "  training_iteration: 81\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2246 s, 81 iter, 810000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-38-03\n",
      "  done: false\n",
      "  episode_len_mean: 123.93\n",
      "  episode_reward_max: 355.14569939494146\n",
      "  episode_reward_mean: 315.6449957978327\n",
      "  episode_reward_min: 285.4375969074444\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 5119\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3887.016\n",
      "    load_time_ms: 2.698\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.387778801460961e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7141368985176086\n",
      "      kl: 0.00836455263197422\n",
      "      policy_loss: 0.0015476631233468652\n",
      "      total_loss: 32.01472854614258\n",
      "      vf_explained_var: 0.977304220199585\n",
      "      vf_loss: 32.01317596435547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.88178432935015e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.107742428779602\n",
      "      kl: 0.012261985801160336\n",
      "      policy_loss: -0.0026962505653500557\n",
      "      total_loss: 335.8515930175781\n",
      "      vf_explained_var: 0.9026588201522827\n",
      "      vf_loss: 335.85430908203125\n",
      "    sample_time_ms: 27382.545\n",
      "    update_time_ms: 10.237\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 125.40009491742654\n",
      "    rl_1: 190.2449008804061\n",
      "  time_since_restore: 2271.0391824245453\n",
      "  time_this_iter_s: 24.566688537597656\n",
      "  time_total_s: 2271.0391824245453\n",
      "  timestamp: 1552574283\n",
      "  timesteps_since_restore: 820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 82\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2271 s, 82 iter, 820000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-38-28\n",
      "  done: false\n",
      "  episode_len_mean: 124.59\n",
      "  episode_reward_max: 346.9881988644059\n",
      "  episode_reward_mean: 316.36328928559624\n",
      "  episode_reward_min: 284.2656437772031\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 5200\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3877.965\n",
      "    load_time_ms: 2.702\n",
      "    num_steps_sampled: 830000\n",
      "    num_steps_trained: 830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.938894007304805e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7317394614219666\n",
      "      kl: 0.006056923419237137\n",
      "      policy_loss: -0.0004479279450606555\n",
      "      total_loss: 32.74319839477539\n",
      "      vf_explained_var: 0.9767128229141235\n",
      "      vf_loss: 32.74364471435547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.88178432935015e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1031240224838257\n",
      "      kl: 0.006337221246212721\n",
      "      policy_loss: -0.002252008765935898\n",
      "      total_loss: 310.6127014160156\n",
      "      vf_explained_var: 0.9106964468955994\n",
      "      vf_loss: 310.6149597167969\n",
      "    sample_time_ms: 27268.379\n",
      "    update_time_ms: 10.376\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.97097284599243\n",
      "    rl_1: 189.39231643960377\n",
      "  time_since_restore: 2295.4332983493805\n",
      "  time_this_iter_s: 24.394115924835205\n",
      "  time_total_s: 2295.4332983493805\n",
      "  timestamp: 1552574308\n",
      "  timesteps_since_restore: 830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 830000\n",
      "  training_iteration: 83\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2295 s, 83 iter, 830000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-38-52\n",
      "  done: false\n",
      "  episode_len_mean: 124.74\n",
      "  episode_reward_max: 350.5130494120937\n",
      "  episode_reward_mean: 313.5123374694067\n",
      "  episode_reward_min: 282.4339837625843\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 5279\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3868.659\n",
      "    load_time_ms: 2.653\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.4694470036524025e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6888235807418823\n",
      "      kl: 0.008528325706720352\n",
      "      policy_loss: 0.0012415896635502577\n",
      "      total_loss: 25.348581314086914\n",
      "      vf_explained_var: 0.9803655743598938\n",
      "      vf_loss: 25.34733772277832\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0851640701293945\n",
      "      kl: 0.03370622918009758\n",
      "      policy_loss: -0.0071339537389576435\n",
      "      total_loss: 304.4878234863281\n",
      "      vf_explained_var: 0.9131373763084412\n",
      "      vf_loss: 304.4949951171875\n",
      "    sample_time_ms: 27140.951\n",
      "    update_time_ms: 10.437\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 124.17579906233057\n",
      "    rl_1: 189.33653840707612\n",
      "  time_since_restore: 2319.643185377121\n",
      "  time_this_iter_s: 24.20988702774048\n",
      "  time_total_s: 2319.643185377121\n",
      "  timestamp: 1552574332\n",
      "  timesteps_since_restore: 840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 84\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2319 s, 84 iter, 840000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-39-16\n",
      "  done: false\n",
      "  episode_len_mean: 125.24\n",
      "  episode_reward_max: 350.5130494120937\n",
      "  episode_reward_mean: 316.2166081417861\n",
      "  episode_reward_min: 284.8094890076721\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 5359\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3852.921\n",
      "    load_time_ms: 2.658\n",
      "    num_steps_sampled: 850000\n",
      "    num_steps_trained: 850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6735144853591919\n",
      "      kl: 0.008607061579823494\n",
      "      policy_loss: -0.0013328040950000286\n",
      "      total_loss: 27.88370132446289\n",
      "      vf_explained_var: 0.9804425239562988\n",
      "      vf_loss: 27.885038375854492\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1188666820526123\n",
      "      kl: 0.004811230581253767\n",
      "      policy_loss: -0.0005221595056355\n",
      "      total_loss: 295.91644287109375\n",
      "      vf_explained_var: 0.916046142578125\n",
      "      vf_loss: 295.91693115234375\n",
      "    sample_time_ms: 26999.853\n",
      "    update_time_ms: 10.465\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.35904675840064\n",
      "    rl_1: 189.85756138338547\n",
      "  time_since_restore: 2343.761007785797\n",
      "  time_this_iter_s: 24.117822408676147\n",
      "  time_total_s: 2343.761007785797\n",
      "  timestamp: 1552574356\n",
      "  timesteps_since_restore: 850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 850000\n",
      "  training_iteration: 85\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2343 s, 85 iter, 850000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-39-40\n",
      "  done: false\n",
      "  episode_len_mean: 125.69\n",
      "  episode_reward_max: 354.53525440409805\n",
      "  episode_reward_mean: 315.68483513298526\n",
      "  episode_reward_min: 277.2872156539748\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 5439\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3825.114\n",
      "    load_time_ms: 2.724\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6942243576049805\n",
      "      kl: 0.009017391130328178\n",
      "      policy_loss: -0.0005507310852408409\n",
      "      total_loss: 25.60622215270996\n",
      "      vf_explained_var: 0.9805535674095154\n",
      "      vf_loss: 25.60677146911621\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2204460823375376e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0710222721099854\n",
      "      kl: 0.014959304593503475\n",
      "      policy_loss: -0.002327986992895603\n",
      "      total_loss: 274.63714599609375\n",
      "      vf_explained_var: 0.9219918847084045\n",
      "      vf_loss: 274.63946533203125\n",
      "    sample_time_ms: 26842.103\n",
      "    update_time_ms: 10.376\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.54498083896786\n",
      "    rl_1: 189.13985429401743\n",
      "  time_since_restore: 2368.1410748958588\n",
      "  time_this_iter_s: 24.380067110061646\n",
      "  time_total_s: 2368.1410748958588\n",
      "  timestamp: 1552574380\n",
      "  timesteps_since_restore: 860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 86\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2368 s, 86 iter, 860000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-40-05\n",
      "  done: false\n",
      "  episode_len_mean: 124.98\n",
      "  episode_reward_max: 351.38867808068034\n",
      "  episode_reward_mean: 318.01371020122156\n",
      "  episode_reward_min: 282.8611671696501\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 5519\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3809.325\n",
      "    load_time_ms: 2.781\n",
      "    num_steps_sampled: 870000\n",
      "    num_steps_trained: 870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.336808754565503e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6188979744911194\n",
      "      kl: 0.008393039926886559\n",
      "      policy_loss: -0.0006595453596673906\n",
      "      total_loss: 26.58909034729004\n",
      "      vf_explained_var: 0.9812549948692322\n",
      "      vf_loss: 26.589746475219727\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2204460823375376e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0628904104232788\n",
      "      kl: 0.006140418350696564\n",
      "      policy_loss: -0.0011193060781806707\n",
      "      total_loss: 283.7855529785156\n",
      "      vf_explained_var: 0.921234130859375\n",
      "      vf_loss: 283.78668212890625\n",
      "    sample_time_ms: 26595.564\n",
      "    update_time_ms: 10.212\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.90239128513299\n",
      "    rl_1: 190.11131891608863\n",
      "  time_since_restore: 2392.509440422058\n",
      "  time_this_iter_s: 24.36836552619934\n",
      "  time_total_s: 2392.509440422058\n",
      "  timestamp: 1552574405\n",
      "  timesteps_since_restore: 870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 870000\n",
      "  training_iteration: 87\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2392 s, 87 iter, 870000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-40-29\n",
      "  done: false\n",
      "  episode_len_mean: 124.74\n",
      "  episode_reward_max: 351.23225557123465\n",
      "  episode_reward_mean: 319.02839961327754\n",
      "  episode_reward_min: 280.96398860559674\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 5599\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3753.218\n",
      "    load_time_ms: 2.858\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.1684043772827515e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6238628029823303\n",
      "      kl: 0.010774759575724602\n",
      "      policy_loss: -0.0015068021602928638\n",
      "      total_loss: 28.23008155822754\n",
      "      vf_explained_var: 0.9808767437934875\n",
      "      vf_loss: 28.231586456298828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1102230411687688e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1003810167312622\n",
      "      kl: 0.014274867251515388\n",
      "      policy_loss: -0.0026542835403233767\n",
      "      total_loss: 264.9529724121094\n",
      "      vf_explained_var: 0.9267787337303162\n",
      "      vf_loss: 264.95556640625\n",
      "    sample_time_ms: 26313.87\n",
      "    update_time_ms: 9.992\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.1047981640901\n",
      "    rl_1: 188.92360144918746\n",
      "  time_since_restore: 2416.4860968589783\n",
      "  time_this_iter_s: 23.976656436920166\n",
      "  time_total_s: 2416.4860968589783\n",
      "  timestamp: 1552574429\n",
      "  timesteps_since_restore: 880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 88\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2416 s, 88 iter, 880000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-40-53\n",
      "  done: false\n",
      "  episode_len_mean: 126.97\n",
      "  episode_reward_max: 351.62924874985964\n",
      "  episode_reward_mean: 320.33489274479354\n",
      "  episode_reward_min: 288.1750184607785\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5677\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3418.875\n",
      "    load_time_ms: 2.852\n",
      "    num_steps_sampled: 890000\n",
      "    num_steps_trained: 890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.1684043772827515e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6194603443145752\n",
      "      kl: 0.008759121410548687\n",
      "      policy_loss: -0.0022859054151922464\n",
      "      total_loss: 25.422847747802734\n",
      "      vf_explained_var: 0.9813920855522156\n",
      "      vf_loss: 25.42513084411621\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1102230411687688e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.061923861503601\n",
      "      kl: 0.002686872612684965\n",
      "      policy_loss: -0.00016234867507591844\n",
      "      total_loss: 256.4871826171875\n",
      "      vf_explained_var: 0.9297611713409424\n",
      "      vf_loss: 256.48736572265625\n",
      "    sample_time_ms: 25202.322\n",
      "    update_time_ms: 9.345\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.97846292796794\n",
      "    rl_1: 191.35642981682562\n",
      "  time_since_restore: 2440.5516867637634\n",
      "  time_this_iter_s: 24.065589904785156\n",
      "  time_total_s: 2440.5516867637634\n",
      "  timestamp: 1552574453\n",
      "  timesteps_since_restore: 890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 890000\n",
      "  training_iteration: 89\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2440 s, 89 iter, 890000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-41-17\n",
      "  done: false\n",
      "  episode_len_mean: 126.28\n",
      "  episode_reward_max: 349.87702007821133\n",
      "  episode_reward_mean: 315.82920664260126\n",
      "  episode_reward_min: 278.6916216588692\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 5757\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3206.333\n",
      "    load_time_ms: 2.714\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842021886413758e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6214117407798767\n",
      "      kl: 0.011848149821162224\n",
      "      policy_loss: 0.002416628645732999\n",
      "      total_loss: 20.53456687927246\n",
      "      vf_explained_var: 0.9845991134643555\n",
      "      vf_loss: 20.53215217590332\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.551115205843844e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0902047157287598\n",
      "      kl: 0.013691365718841553\n",
      "      policy_loss: -0.0024200472980737686\n",
      "      total_loss: 233.0458984375\n",
      "      vf_explained_var: 0.9358769655227661\n",
      "      vf_loss: 233.04832458496094\n",
      "    sample_time_ms: 21129.638\n",
      "    update_time_ms: 8.642\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.16806086136518\n",
      "    rl_1: 188.66114578123612\n",
      "  time_since_restore: 2464.622204065323\n",
      "  time_this_iter_s: 24.07051730155945\n",
      "  time_total_s: 2464.622204065323\n",
      "  timestamp: 1552574477\n",
      "  timesteps_since_restore: 900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 90\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2464 s, 90 iter, 900000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-41-42\n",
      "  done: false\n",
      "  episode_len_mean: 125.28\n",
      "  episode_reward_max: 357.0809983372257\n",
      "  episode_reward_mean: 318.36601014605856\n",
      "  episode_reward_min: 281.78703351710305\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 5837\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3206.237\n",
      "    load_time_ms: 2.619\n",
      "    num_steps_sampled: 910000\n",
      "    num_steps_trained: 910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842021886413758e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.54844731092453\n",
      "      kl: 0.012807168066501617\n",
      "      policy_loss: 7.281146827153862e-05\n",
      "      total_loss: 20.12388038635254\n",
      "      vf_explained_var: 0.9861806035041809\n",
      "      vf_loss: 20.123807907104492\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.551115205843844e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0733821392059326\n",
      "      kl: 0.009425578638911247\n",
      "      policy_loss: -0.0017245736671611667\n",
      "      total_loss: 228.5833282470703\n",
      "      vf_explained_var: 0.9372769594192505\n",
      "      vf_loss: 228.5850372314453\n",
      "    sample_time_ms: 21027.293\n",
      "    update_time_ms: 7.971\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.71137399758788\n",
      "    rl_1: 188.65463614847067\n",
      "  time_since_restore: 2489.11048078537\n",
      "  time_this_iter_s: 24.488276720046997\n",
      "  time_total_s: 2489.11048078537\n",
      "  timestamp: 1552574502\n",
      "  timesteps_since_restore: 910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 910000\n",
      "  training_iteration: 91\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2489 s, 91 iter, 910000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-42-06\n",
      "  done: false\n",
      "  episode_len_mean: 126.01\n",
      "  episode_reward_max: 352.1189737185658\n",
      "  episode_reward_mean: 317.863815456268\n",
      "  episode_reward_min: 282.86842858044827\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 5916\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3211.605\n",
      "    load_time_ms: 2.632\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842021886413758e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5589059591293335\n",
      "      kl: 0.007273926865309477\n",
      "      policy_loss: 0.00365494005382061\n",
      "      total_loss: 17.167619705200195\n",
      "      vf_explained_var: 0.987305760383606\n",
      "      vf_loss: 17.163965225219727\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.775557602921922e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0259993076324463\n",
      "      kl: 0.004560060333460569\n",
      "      policy_loss: -0.0009236927726306021\n",
      "      total_loss: 224.10472106933594\n",
      "      vf_explained_var: 0.9401746988296509\n",
      "      vf_loss: 224.1056365966797\n",
      "    sample_time_ms: 21023.704\n",
      "    update_time_ms: 8.171\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.81576117636013\n",
      "    rl_1: 189.04805427990794\n",
      "  time_since_restore: 2513.6959557533264\n",
      "  time_this_iter_s: 24.585474967956543\n",
      "  time_total_s: 2513.6959557533264\n",
      "  timestamp: 1552574526\n",
      "  timesteps_since_restore: 920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 92\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2513 s, 92 iter, 920000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-42-30\n",
      "  done: false\n",
      "  episode_len_mean: 126.38\n",
      "  episode_reward_max: 354.32564064717457\n",
      "  episode_reward_mean: 317.7101753008106\n",
      "  episode_reward_min: 287.5300490461719\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 5995\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3215.817\n",
      "    load_time_ms: 2.665\n",
      "    num_steps_sampled: 930000\n",
      "    num_steps_trained: 930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.421010943206879e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5702468752861023\n",
      "      kl: 0.008084388449788094\n",
      "      policy_loss: 0.0009866803884506226\n",
      "      total_loss: 19.822187423706055\n",
      "      vf_explained_var: 0.9863595366477966\n",
      "      vf_loss: 19.82120132446289\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.387778801460961e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.039894461631775\n",
      "      kl: 0.014517517760396004\n",
      "      policy_loss: -0.0026063895784318447\n",
      "      total_loss: 208.39100646972656\n",
      "      vf_explained_var: 0.9438573122024536\n",
      "      vf_loss: 208.39358520507812\n",
      "    sample_time_ms: 20990.02\n",
      "    update_time_ms: 7.981\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.6202941941132\n",
      "    rl_1: 189.08988110669742\n",
      "  time_since_restore: 2537.79461812973\n",
      "  time_this_iter_s: 24.09866237640381\n",
      "  time_total_s: 2537.79461812973\n",
      "  timestamp: 1552574550\n",
      "  timesteps_since_restore: 930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 930000\n",
      "  training_iteration: 93\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2537 s, 93 iter, 930000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-42-55\n",
      "  done: false\n",
      "  episode_len_mean: 126.07\n",
      "  episode_reward_max: 353.2241976724008\n",
      "  episode_reward_mean: 318.3225067381026\n",
      "  episode_reward_min: 284.5693424119923\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 6075\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3221.395\n",
      "    load_time_ms: 2.706\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.7105054716034394e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5407662391662598\n",
      "      kl: 0.0080093489959836\n",
      "      policy_loss: -0.0011013202602043748\n",
      "      total_loss: 14.920039176940918\n",
      "      vf_explained_var: 0.989276111125946\n",
      "      vf_loss: 14.921140670776367\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.387778801460961e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0225119590759277\n",
      "      kl: 0.003863229649141431\n",
      "      policy_loss: 4.4269439968047664e-05\n",
      "      total_loss: 201.42971801757812\n",
      "      vf_explained_var: 0.9467363357543945\n",
      "      vf_loss: 201.42967224121094\n",
      "    sample_time_ms: 21009.412\n",
      "    update_time_ms: 7.764\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.0618256010444\n",
      "    rl_1: 189.26068113705824\n",
      "  time_since_restore: 2562.2545335292816\n",
      "  time_this_iter_s: 24.45991539955139\n",
      "  time_total_s: 2562.2545335292816\n",
      "  timestamp: 1552574575\n",
      "  timesteps_since_restore: 940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 94\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2562 s, 94 iter, 940000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-43-19\n",
      "  done: false\n",
      "  episode_len_mean: 126.55\n",
      "  episode_reward_max: 351.55891099155707\n",
      "  episode_reward_mean: 318.943997362948\n",
      "  episode_reward_min: 286.14800215869195\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6153\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3225.897\n",
      "    load_time_ms: 2.704\n",
      "    num_steps_sampled: 950000\n",
      "    num_steps_trained: 950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3552527358017197e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4961884319782257\n",
      "      kl: 0.00988792348653078\n",
      "      policy_loss: 0.000776217901147902\n",
      "      total_loss: 10.616984367370605\n",
      "      vf_explained_var: 0.9918438792228699\n",
      "      vf_loss: 10.616209030151367\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.938894007304805e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0052450895309448\n",
      "      kl: 0.026578383520245552\n",
      "      policy_loss: -0.0052179391495883465\n",
      "      total_loss: 212.33946228027344\n",
      "      vf_explained_var: 0.9446114897727966\n",
      "      vf_loss: 212.3446502685547\n",
      "    sample_time_ms: 21032.216\n",
      "    update_time_ms: 7.749\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.87329481115933\n",
      "    rl_1: 191.07070255178863\n",
      "  time_since_restore: 2586.6441061496735\n",
      "  time_this_iter_s: 24.389572620391846\n",
      "  time_total_s: 2586.6441061496735\n",
      "  timestamp: 1552574599\n",
      "  timesteps_since_restore: 950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 950000\n",
      "  training_iteration: 95\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2586 s, 95 iter, 950000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-43-44\n",
      "  done: false\n",
      "  episode_len_mean: 127.29\n",
      "  episode_reward_max: 352.58149523624775\n",
      "  episode_reward_mean: 319.6541981195991\n",
      "  episode_reward_min: 279.7197155342248\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6231\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3222.114\n",
      "    load_time_ms: 2.671\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.776263679008599e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5355355143547058\n",
      "      kl: 0.012478808872401714\n",
      "      policy_loss: 0.0014758873730897903\n",
      "      total_loss: 14.60068130493164\n",
      "      vf_explained_var: 0.9894299507141113\n",
      "      vf_loss: 14.599210739135742\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.938894007304805e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0501792430877686\n",
      "      kl: 0.00495791994035244\n",
      "      policy_loss: -0.0012470657238736749\n",
      "      total_loss: 181.68507385253906\n",
      "      vf_explained_var: 0.9520905613899231\n",
      "      vf_loss: 181.68630981445312\n",
      "    sample_time_ms: 21046.237\n",
      "    update_time_ms: 7.634\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.789441918652\n",
      "    rl_1: 189.864756200947\n",
      "  time_since_restore: 2611.1202721595764\n",
      "  time_this_iter_s: 24.476166009902954\n",
      "  time_total_s: 2611.1202721595764\n",
      "  timestamp: 1552574624\n",
      "  timesteps_since_restore: 960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 96\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2611 s, 96 iter, 960000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-44-09\n",
      "  done: false\n",
      "  episode_len_mean: 127.14\n",
      "  episode_reward_max: 351.45730755823496\n",
      "  episode_reward_mean: 316.43417259093076\n",
      "  episode_reward_min: 278.5350054488951\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 6310\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3221.266\n",
      "    load_time_ms: 2.636\n",
      "    num_steps_sampled: 970000\n",
      "    num_steps_trained: 970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.776263679008599e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5291634798049927\n",
      "      kl: 0.00473406258970499\n",
      "      policy_loss: 0.0014059215318411589\n",
      "      total_loss: 10.245826721191406\n",
      "      vf_explained_var: 0.9921992421150208\n",
      "      vf_loss: 10.244421005249023\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.4694470036524025e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0137577056884766\n",
      "      kl: 0.007374885026365519\n",
      "      policy_loss: -0.001013346598483622\n",
      "      total_loss: 173.1914825439453\n",
      "      vf_explained_var: 0.95379239320755\n",
      "      vf_loss: 173.19248962402344\n",
      "    sample_time_ms: 21110.477\n",
      "    update_time_ms: 7.763\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.88050097890715\n",
      "    rl_1: 188.55367161202358\n",
      "  time_since_restore: 2636.1243240833282\n",
      "  time_this_iter_s: 25.00405192375183\n",
      "  time_total_s: 2636.1243240833282\n",
      "  timestamp: 1552574649\n",
      "  timesteps_since_restore: 970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 970000\n",
      "  training_iteration: 97\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2636 s, 97 iter, 970000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-44-33\n",
      "  done: false\n",
      "  episode_len_mean: 126.92\n",
      "  episode_reward_max: 359.50333288642764\n",
      "  episode_reward_mean: 318.85792728270906\n",
      "  episode_reward_min: 285.0799455984742\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 6390\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3219.09\n",
      "    load_time_ms: 2.59\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.3881318395042993e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.48593002557754517\n",
      "      kl: 0.006106066983193159\n",
      "      policy_loss: 0.0007764643523842096\n",
      "      total_loss: 12.589645385742188\n",
      "      vf_explained_var: 0.9912502765655518\n",
      "      vf_loss: 12.5888671875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9921223521232605\n",
      "      kl: 0.010607381351292133\n",
      "      policy_loss: 9.066549682756886e-05\n",
      "      total_loss: 174.26512145996094\n",
      "      vf_explained_var: 0.9551538228988647\n",
      "      vf_loss: 174.26504516601562\n",
      "    sample_time_ms: 21135.027\n",
      "    update_time_ms: 7.721\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.9035006705961\n",
      "    rl_1: 189.95442661211294\n",
      "  time_since_restore: 2660.3199298381805\n",
      "  time_this_iter_s: 24.195605754852295\n",
      "  time_total_s: 2660.3199298381805\n",
      "  timestamp: 1552574673\n",
      "  timesteps_since_restore: 980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 98\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2660 s, 98 iter, 980000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-44-57\n",
      "  done: false\n",
      "  episode_len_mean: 126.29\n",
      "  episode_reward_max: 348.9090737452615\n",
      "  episode_reward_mean: 318.06001730649075\n",
      "  episode_reward_min: 280.5064000987597\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 6469\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3219.597\n",
      "    load_time_ms: 2.652\n",
      "    num_steps_sampled: 990000\n",
      "    num_steps_trained: 990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6940659197521496e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4422394633293152\n",
      "      kl: 0.010984324850142002\n",
      "      policy_loss: 0.0011385499965399504\n",
      "      total_loss: 11.020697593688965\n",
      "      vf_explained_var: 0.9924055337905884\n",
      "      vf_loss: 11.01955795288086\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9621550440788269\n",
      "      kl: 0.010511592030525208\n",
      "      policy_loss: -0.0009516619029454887\n",
      "      total_loss: 165.04273986816406\n",
      "      vf_explained_var: 0.9574148654937744\n",
      "      vf_loss: 165.043701171875\n",
      "    sample_time_ms: 21136.541\n",
      "    update_time_ms: 7.699\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.37488687286185\n",
      "    rl_1: 189.685130433629\n",
      "  time_since_restore: 2684.4071815013885\n",
      "  time_this_iter_s: 24.087251663208008\n",
      "  time_total_s: 2684.4071815013885\n",
      "  timestamp: 1552574697\n",
      "  timesteps_since_restore: 990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 990000\n",
      "  training_iteration: 99\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2684 s, 99 iter, 990000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-45-22\n",
      "  done: false\n",
      "  episode_len_mean: 127.09\n",
      "  episode_reward_max: 350.24931071400687\n",
      "  episode_reward_mean: 319.0207300167894\n",
      "  episode_reward_min: 284.1041248085353\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 6548\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3220.028\n",
      "    load_time_ms: 2.634\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6940659197521496e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.43828126788139343\n",
      "      kl: 0.005542269442230463\n",
      "      policy_loss: 0.0032982409466058016\n",
      "      total_loss: 6.905789852142334\n",
      "      vf_explained_var: 0.9949273467063904\n",
      "      vf_loss: 6.902493000030518\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9529803395271301\n",
      "      kl: 0.013610722497105598\n",
      "      policy_loss: -0.0008120561833493412\n",
      "      total_loss: 164.62229919433594\n",
      "      vf_explained_var: 0.9578959941864014\n",
      "      vf_loss: 164.62307739257812\n",
      "    sample_time_ms: 21164.54\n",
      "    update_time_ms: 7.431\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.2763448563032\n",
      "    rl_1: 191.74438516048616\n",
      "  time_since_restore: 2708.7612380981445\n",
      "  time_this_iter_s: 24.35405659675598\n",
      "  time_total_s: 2708.7612380981445\n",
      "  timestamp: 1552574722\n",
      "  timesteps_since_restore: 1000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 100\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2708 s, 100 iter, 1000000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 126.54\n",
      "  episode_reward_max: 356.97264682741866\n",
      "  episode_reward_mean: 317.5241721304896\n",
      "  episode_reward_min: 283.16180084256587\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6626\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3219.061\n",
      "    load_time_ms: 2.628\n",
      "    num_steps_sampled: 1010000\n",
      "    num_steps_trained: 1010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.470329598760748e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.44057905673980713\n",
      "      kl: 0.009228764101862907\n",
      "      policy_loss: 0.00237631076015532\n",
      "      total_loss: 7.012019157409668\n",
      "      vf_explained_var: 0.9947234988212585\n",
      "      vf_loss: 7.009642601013184\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.012084722518921\n",
      "      kl: 0.019898539409041405\n",
      "      policy_loss: -0.0008360964129678905\n",
      "      total_loss: 149.8961639404297\n",
      "      vf_explained_var: 0.9616950750350952\n",
      "      vf_loss: 149.89698791503906\n",
      "    sample_time_ms: 21108.26\n",
      "    update_time_ms: 7.627\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.19979811955236\n",
      "    rl_1: 189.32437401093725\n",
      "  time_since_restore: 2732.677651166916\n",
      "  time_this_iter_s: 23.916413068771362\n",
      "  time_total_s: 2732.677651166916\n",
      "  timestamp: 1552574746\n",
      "  timesteps_since_restore: 1010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1010000\n",
      "  training_iteration: 101\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2732 s, 101 iter, 1010000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-46-10\n",
      "  done: false\n",
      "  episode_len_mean: 128.9\n",
      "  episode_reward_max: 362.4638079873114\n",
      "  episode_reward_mean: 316.30838642066794\n",
      "  episode_reward_min: 284.341482369654\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6704\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3216.261\n",
      "    load_time_ms: 2.689\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.235164799380374e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5105772018432617\n",
      "      kl: 0.010237203910946846\n",
      "      policy_loss: 0.001599833252839744\n",
      "      total_loss: 5.929227828979492\n",
      "      vf_explained_var: 0.9952341914176941\n",
      "      vf_loss: 5.927628517150879\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9594075083732605\n",
      "      kl: 0.016352197155356407\n",
      "      policy_loss: 0.0025242920964956284\n",
      "      total_loss: 146.82962036132812\n",
      "      vf_explained_var: 0.9624921083450317\n",
      "      vf_loss: 146.82708740234375\n",
      "    sample_time_ms: 21041.11\n",
      "    update_time_ms: 7.661\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 125.61911501482916\n",
      "    rl_1: 190.68927140583878\n",
      "  time_since_restore: 2756.5655455589294\n",
      "  time_this_iter_s: 23.88789439201355\n",
      "  time_total_s: 2756.5655455589294\n",
      "  timestamp: 1552574770\n",
      "  timesteps_since_restore: 1020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 102\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2756 s, 102 iter, 1020000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-46-34\n",
      "  done: false\n",
      "  episode_len_mean: 128.24\n",
      "  episode_reward_max: 362.4638079873114\n",
      "  episode_reward_mean: 321.5564492331482\n",
      "  episode_reward_min: 290.3779079758772\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6782\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3209.914\n",
      "    load_time_ms: 2.652\n",
      "    num_steps_sampled: 1030000\n",
      "    num_steps_trained: 1030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.235164799380374e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.443645179271698\n",
      "      kl: 0.009257890284061432\n",
      "      policy_loss: 0.0024858189281076193\n",
      "      total_loss: 8.1486177444458\n",
      "      vf_explained_var: 0.9940018057823181\n",
      "      vf_loss: 8.14613151550293\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9608674645423889\n",
      "      kl: 0.021044954657554626\n",
      "      policy_loss: -0.003591192187741399\n",
      "      total_loss: 139.50747680664062\n",
      "      vf_explained_var: 0.9651656746864319\n",
      "      vf_loss: 139.5110626220703\n",
      "    sample_time_ms: 21037.014\n",
      "    update_time_ms: 7.608\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.0835446858192\n",
      "    rl_1: 191.472904547329\n",
      "  time_since_restore: 2780.5581743717194\n",
      "  time_this_iter_s: 23.992628812789917\n",
      "  time_total_s: 2780.5581743717194\n",
      "  timestamp: 1552574794\n",
      "  timesteps_since_restore: 1030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1030000\n",
      "  training_iteration: 103\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2780 s, 103 iter, 1030000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-46-58\n",
      "  done: false\n",
      "  episode_len_mean: 127.77\n",
      "  episode_reward_max: 358.38329584857814\n",
      "  episode_reward_mean: 320.23424875027905\n",
      "  episode_reward_min: 286.6651177627521\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6860\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3204.866\n",
      "    load_time_ms: 2.622\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.117582399690187e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.410403311252594\n",
      "      kl: 0.006703242193907499\n",
      "      policy_loss: 0.00023030406737234443\n",
      "      total_loss: 6.87353515625\n",
      "      vf_explained_var: 0.9949622750282288\n",
      "      vf_loss: 6.87330436706543\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9362242221832275\n",
      "      kl: 0.018944494426250458\n",
      "      policy_loss: 0.0008994376403279603\n",
      "      total_loss: 128.02264404296875\n",
      "      vf_explained_var: 0.9676938056945801\n",
      "      vf_loss: 128.021728515625\n",
      "    sample_time_ms: 21008.163\n",
      "    update_time_ms: 7.608\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.89148122864322\n",
      "    rl_1: 190.34276752163586\n",
      "  time_since_restore: 2804.678655385971\n",
      "  time_this_iter_s: 24.12048101425171\n",
      "  time_total_s: 2804.678655385971\n",
      "  timestamp: 1552574818\n",
      "  timesteps_since_restore: 1040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 104\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2804 s, 104 iter, 1040000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-47-22\n",
      "  done: false\n",
      "  episode_len_mean: 128.04\n",
      "  episode_reward_max: 353.0927211807667\n",
      "  episode_reward_mean: 320.1064397680283\n",
      "  episode_reward_min: 282.31084455440316\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6938\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3200.813\n",
      "    load_time_ms: 2.57\n",
      "    num_steps_sampled: 1050000\n",
      "    num_steps_trained: 1050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4317009747028351\n",
      "      kl: 0.013635065406560898\n",
      "      policy_loss: 0.00501002324745059\n",
      "      total_loss: 6.479308128356934\n",
      "      vf_explained_var: 0.995244026184082\n",
      "      vf_loss: 6.474298000335693\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9219244122505188\n",
      "      kl: 0.010012856684625149\n",
      "      policy_loss: 1.1601413461903576e-05\n",
      "      total_loss: 121.64232635498047\n",
      "      vf_explained_var: 0.9695101976394653\n",
      "      vf_loss: 121.64234161376953\n",
      "    sample_time_ms: 21011.621\n",
      "    update_time_ms: 7.913\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.36397461683728\n",
      "    rl_1: 189.742465151191\n",
      "  time_since_restore: 2829.065058708191\n",
      "  time_this_iter_s: 24.38640332221985\n",
      "  time_total_s: 2829.065058708191\n",
      "  timestamp: 1552574842\n",
      "  timesteps_since_restore: 1050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1050000\n",
      "  training_iteration: 105\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2829 s, 105 iter, 1050000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-47-46\n",
      "  done: false\n",
      "  episode_len_mean: 127.2\n",
      "  episode_reward_max: 353.8336145428643\n",
      "  episode_reward_mean: 320.18644880443867\n",
      "  episode_reward_min: 282.31084455440316\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 7017\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3201.043\n",
      "    load_time_ms: 2.62\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.40324866771698\n",
      "      kl: 0.013557231985032558\n",
      "      policy_loss: 0.006941393483430147\n",
      "      total_loss: 7.193716049194336\n",
      "      vf_explained_var: 0.9950650334358215\n",
      "      vf_loss: 7.186774730682373\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9394001960754395\n",
      "      kl: 0.014364772476255894\n",
      "      policy_loss: -0.002826370531693101\n",
      "      total_loss: 107.32831573486328\n",
      "      vf_explained_var: 0.9722173810005188\n",
      "      vf_loss: 107.33113098144531\n",
      "    sample_time_ms: 20981.116\n",
      "    update_time_ms: 7.98\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.11120199422933\n",
      "    rl_1: 189.0752468102094\n",
      "  time_since_restore: 2853.2399973869324\n",
      "  time_this_iter_s: 24.174938678741455\n",
      "  time_total_s: 2853.2399973869324\n",
      "  timestamp: 1552574866\n",
      "  timesteps_since_restore: 1060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 106\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2853 s, 106 iter, 1060000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-48-11\n",
      "  done: false\n",
      "  episode_len_mean: 128.17\n",
      "  episode_reward_max: 358.907642605878\n",
      "  episode_reward_mean: 320.97661586250285\n",
      "  episode_reward_min: 284.03967754785856\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7095\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3199.851\n",
      "    load_time_ms: 2.659\n",
      "    num_steps_sampled: 1070000\n",
      "    num_steps_trained: 1070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3792702257633209\n",
      "      kl: 0.01312977820634842\n",
      "      policy_loss: 0.006147993728518486\n",
      "      total_loss: 5.102104187011719\n",
      "      vf_explained_var: 0.9962936639785767\n",
      "      vf_loss: 5.095956325531006\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8393890261650085\n",
      "      kl: 0.013362853787839413\n",
      "      policy_loss: 0.0013344347244128585\n",
      "      total_loss: 115.14232635498047\n",
      "      vf_explained_var: 0.9718201756477356\n",
      "      vf_loss: 115.1409912109375\n",
      "    sample_time_ms: 20972.318\n",
      "    update_time_ms: 7.974\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.46439314173335\n",
      "    rl_1: 190.5122227207694\n",
      "  time_since_restore: 2878.143792152405\n",
      "  time_this_iter_s: 24.903794765472412\n",
      "  time_total_s: 2878.143792152405\n",
      "  timestamp: 1552574891\n",
      "  timesteps_since_restore: 1070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1070000\n",
      "  training_iteration: 107\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2878 s, 107 iter, 1070000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-48-36\n",
      "  done: false\n",
      "  episode_len_mean: 127.84\n",
      "  episode_reward_max: 357.47756481704124\n",
      "  episode_reward_mean: 319.8605362472432\n",
      "  episode_reward_min: 284.03967754785856\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7173\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3200.243\n",
      "    load_time_ms: 2.652\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3534882366657257\n",
      "      kl: 0.010540267452597618\n",
      "      policy_loss: 0.006121319252997637\n",
      "      total_loss: 4.97301721572876\n",
      "      vf_explained_var: 0.9964121580123901\n",
      "      vf_loss: 4.96689510345459\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8506255745887756\n",
      "      kl: 0.03199092671275139\n",
      "      policy_loss: -0.003501186380162835\n",
      "      total_loss: 101.01375579833984\n",
      "      vf_explained_var: 0.9746827483177185\n",
      "      vf_loss: 101.01725769042969\n",
      "    sample_time_ms: 21006.542\n",
      "    update_time_ms: 8.18\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.56907703885219\n",
      "    rl_1: 190.29145920839113\n",
      "  time_since_restore: 2902.689847946167\n",
      "  time_this_iter_s: 24.546055793762207\n",
      "  time_total_s: 2902.689847946167\n",
      "  timestamp: 1552574916\n",
      "  timesteps_since_restore: 1080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 108\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2902 s, 108 iter, 1080000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-49-00\n",
      "  done: false\n",
      "  episode_len_mean: 129.32\n",
      "  episode_reward_max: 355.42718178603815\n",
      "  episode_reward_mean: 320.9297003586324\n",
      "  episode_reward_min: 289.49153079456624\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 7249\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3201.18\n",
      "    load_time_ms: 2.622\n",
      "    num_steps_sampled: 1090000\n",
      "    num_steps_trained: 1090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39332836866378784\n",
      "      kl: 0.013997647911310196\n",
      "      policy_loss: -5.717470412491821e-05\n",
      "      total_loss: 3.683391571044922\n",
      "      vf_explained_var: 0.9971121549606323\n",
      "      vf_loss: 3.68344783782959\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8007904291152954\n",
      "      kl: 0.016398487612605095\n",
      "      policy_loss: 0.0023307872470468283\n",
      "      total_loss: 105.18927764892578\n",
      "      vf_explained_var: 0.9743162989616394\n",
      "      vf_loss: 105.18695831298828\n",
      "    sample_time_ms: 21015.207\n",
      "    update_time_ms: 8.617\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.51508684264297\n",
      "    rl_1: 191.41461351598943\n",
      "  time_since_restore: 2926.878496170044\n",
      "  time_this_iter_s: 24.188648223876953\n",
      "  time_total_s: 2926.878496170044\n",
      "  timestamp: 1552574940\n",
      "  timesteps_since_restore: 1090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1090000\n",
      "  training_iteration: 109\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2926 s, 109 iter, 1090000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-49-24\n",
      "  done: false\n",
      "  episode_len_mean: 129.15\n",
      "  episode_reward_max: 360.1907607705892\n",
      "  episode_reward_mean: 318.9074958099684\n",
      "  episode_reward_min: 285.76808058687914\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7327\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3202.014\n",
      "    load_time_ms: 2.639\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.31722551584243774\n",
      "      kl: 0.013085901737213135\n",
      "      policy_loss: 0.0016319149872288108\n",
      "      total_loss: 4.276020526885986\n",
      "      vf_explained_var: 0.996816873550415\n",
      "      vf_loss: 4.274388790130615\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7705186605453491\n",
      "      kl: 0.009330309927463531\n",
      "      policy_loss: 0.003310019848868251\n",
      "      total_loss: 94.43292999267578\n",
      "      vf_explained_var: 0.9766577482223511\n",
      "      vf_loss: 94.42963409423828\n",
      "    sample_time_ms: 20951.553\n",
      "    update_time_ms: 9.203\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.76299690212079\n",
      "    rl_1: 191.14449890784763\n",
      "  time_since_restore: 2950.6098594665527\n",
      "  time_this_iter_s: 23.73136329650879\n",
      "  time_total_s: 2950.6098594665527\n",
      "  timestamp: 1552574964\n",
      "  timesteps_since_restore: 1100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 110\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2950 s, 110 iter, 1100000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-49-49\n",
      "  done: false\n",
      "  episode_len_mean: 127.45\n",
      "  episode_reward_max: 360.1907607705892\n",
      "  episode_reward_mean: 320.87129847039154\n",
      "  episode_reward_min: 285.76808058687914\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7405\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3202.49\n",
      "    load_time_ms: 2.652\n",
      "    num_steps_sampled: 1110000\n",
      "    num_steps_trained: 1110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2990134656429291\n",
      "      kl: 0.009719599038362503\n",
      "      policy_loss: 0.0033697187900543213\n",
      "      total_loss: 3.6078813076019287\n",
      "      vf_explained_var: 0.9974806308746338\n",
      "      vf_loss: 3.604511260986328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8029680252075195\n",
      "      kl: 0.027641816064715385\n",
      "      policy_loss: -0.0003612929431255907\n",
      "      total_loss: 84.09117126464844\n",
      "      vf_explained_var: 0.9790797233581543\n",
      "      vf_loss: 84.09153747558594\n",
      "    sample_time_ms: 21054.879\n",
      "    update_time_ms: 8.998\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.2595290707515\n",
      "    rl_1: 189.61176939964\n",
      "  time_since_restore: 2975.5632708072662\n",
      "  time_this_iter_s: 24.9534113407135\n",
      "  time_total_s: 2975.5632708072662\n",
      "  timestamp: 1552574989\n",
      "  timesteps_since_restore: 1110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1110000\n",
      "  training_iteration: 111\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2975 s, 111 iter, 1110000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-50-13\n",
      "  done: false\n",
      "  episode_len_mean: 128.19\n",
      "  episode_reward_max: 355.3169035759196\n",
      "  episode_reward_mean: 318.98121587930876\n",
      "  episode_reward_min: 288.4710569661321\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 7484\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3200.927\n",
      "    load_time_ms: 2.582\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2769474685192108\n",
      "      kl: 0.011492768302559853\n",
      "      policy_loss: 0.0035262207966297865\n",
      "      total_loss: 4.02479362487793\n",
      "      vf_explained_var: 0.9971278309822083\n",
      "      vf_loss: 4.021267890930176\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7806058526039124\n",
      "      kl: 0.03434692323207855\n",
      "      policy_loss: -0.004129580222070217\n",
      "      total_loss: 81.23638916015625\n",
      "      vf_explained_var: 0.9800074696540833\n",
      "      vf_loss: 81.24051666259766\n",
      "    sample_time_ms: 21105.853\n",
      "    update_time_ms: 8.852\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.20161793081053\n",
      "    rl_1: 189.7795979484982\n",
      "  time_since_restore: 2999.9449887275696\n",
      "  time_this_iter_s: 24.381717920303345\n",
      "  time_total_s: 2999.9449887275696\n",
      "  timestamp: 1552575013\n",
      "  timesteps_since_restore: 1120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 112\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 2999 s, 112 iter, 1120000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-50-38\n",
      "  done: false\n",
      "  episode_len_mean: 129.18\n",
      "  episode_reward_max: 359.49094148933534\n",
      "  episode_reward_mean: 319.27119106913705\n",
      "  episode_reward_min: 283.130859254929\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 7561\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3200.088\n",
      "    load_time_ms: 2.611\n",
      "    num_steps_sampled: 1130000\n",
      "    num_steps_trained: 1130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32417726516723633\n",
      "      kl: 0.021389612928032875\n",
      "      policy_loss: 0.01040850393474102\n",
      "      total_loss: 3.964428186416626\n",
      "      vf_explained_var: 0.9970749020576477\n",
      "      vf_loss: 3.95401930809021\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7482601404190063\n",
      "      kl: 0.014001541770994663\n",
      "      policy_loss: 0.001308378647081554\n",
      "      total_loss: 74.05931854248047\n",
      "      vf_explained_var: 0.9818059206008911\n",
      "      vf_loss: 74.05799865722656\n",
      "    sample_time_ms: 21119.934\n",
      "    update_time_ms: 8.819\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.2033574524859\n",
      "    rl_1: 190.06783361665111\n",
      "  time_since_restore: 3024.0698387622833\n",
      "  time_this_iter_s: 24.124850034713745\n",
      "  time_total_s: 3024.0698387622833\n",
      "  timestamp: 1552575038\n",
      "  timesteps_since_restore: 1130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1130000\n",
      "  training_iteration: 113\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3024 s, 113 iter, 1130000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-51-01\n",
      "  done: false\n",
      "  episode_len_mean: 128.27\n",
      "  episode_reward_max: 352.44150823478725\n",
      "  episode_reward_mean: 321.94163200348373\n",
      "  episode_reward_min: 283.19074838473136\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7639\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3200.379\n",
      "    load_time_ms: 2.663\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2790243923664093\n",
      "      kl: 0.01923166774213314\n",
      "      policy_loss: 0.004719612654298544\n",
      "      total_loss: 3.574289083480835\n",
      "      vf_explained_var: 0.9975126385688782\n",
      "      vf_loss: 3.569568395614624\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7860504984855652\n",
      "      kl: 0.023466866463422775\n",
      "      policy_loss: 0.0005622068420052528\n",
      "      total_loss: 69.30228424072266\n",
      "      vf_explained_var: 0.9829269051551819\n",
      "      vf_loss: 69.30171203613281\n",
      "    sample_time_ms: 21070.878\n",
      "    update_time_ms: 8.67\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.89092505535635\n",
      "    rl_1: 190.0507069481274\n",
      "  time_since_restore: 3047.7078216075897\n",
      "  time_this_iter_s: 23.637982845306396\n",
      "  time_total_s: 3047.7078216075897\n",
      "  timestamp: 1552575061\n",
      "  timesteps_since_restore: 1140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 114\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3047 s, 114 iter, 1140000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-51-26\n",
      "  done: false\n",
      "  episode_len_mean: 129.7\n",
      "  episode_reward_max: 358.10600306364006\n",
      "  episode_reward_mean: 318.8393864130172\n",
      "  episode_reward_min: 283.19074838473136\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 7716\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3211.599\n",
      "    load_time_ms: 2.729\n",
      "    num_steps_sampled: 1150000\n",
      "    num_steps_trained: 1150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.28965887427330017\n",
      "      kl: 0.010879440233111382\n",
      "      policy_loss: 0.0063300770707428455\n",
      "      total_loss: 2.983088254928589\n",
      "      vf_explained_var: 0.9976800680160522\n",
      "      vf_loss: 2.976757764816284\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6916382312774658\n",
      "      kl: 0.026443930342793465\n",
      "      policy_loss: 0.006647728383541107\n",
      "      total_loss: 67.03804779052734\n",
      "      vf_explained_var: 0.9837256073951721\n",
      "      vf_loss: 67.03138732910156\n",
      "    sample_time_ms: 21118.598\n",
      "    update_time_ms: 8.302\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.6904618281581\n",
      "    rl_1: 191.14892458485912\n",
      "  time_since_restore: 3072.6837277412415\n",
      "  time_this_iter_s: 24.975906133651733\n",
      "  time_total_s: 3072.6837277412415\n",
      "  timestamp: 1552575086\n",
      "  timesteps_since_restore: 1150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1150000\n",
      "  training_iteration: 115\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3072 s, 115 iter, 1150000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-51-54\n",
      "  done: false\n",
      "  episode_len_mean: 130.1\n",
      "  episode_reward_max: 358.10600306364006\n",
      "  episode_reward_mean: 320.16480400572834\n",
      "  episode_reward_min: 284.8364402537459\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 7792\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3298.527\n",
      "    load_time_ms: 2.67\n",
      "    num_steps_sampled: 1160000\n",
      "    num_steps_trained: 1160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3060396909713745\n",
      "      kl: 0.02291029319167137\n",
      "      policy_loss: 0.010630914010107517\n",
      "      total_loss: 3.279200315475464\n",
      "      vf_explained_var: 0.9975923299789429\n",
      "      vf_loss: 3.2685694694519043\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7188690304756165\n",
      "      kl: 0.017171846702694893\n",
      "      policy_loss: 0.003692092141136527\n",
      "      total_loss: 61.50885772705078\n",
      "      vf_explained_var: 0.9846445918083191\n",
      "      vf_loss: 61.50516891479492\n",
      "    sample_time_ms: 21350.729\n",
      "    update_time_ms: 8.175\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.70609475459315\n",
      "    rl_1: 190.45870925113516\n",
      "  time_since_restore: 3100.0499424934387\n",
      "  time_this_iter_s: 27.366214752197266\n",
      "  time_total_s: 3100.0499424934387\n",
      "  timestamp: 1552575114\n",
      "  timesteps_since_restore: 1160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1160000\n",
      "  training_iteration: 116\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3100 s, 116 iter, 1160000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-52-22\n",
      "  done: false\n",
      "  episode_len_mean: 129.75\n",
      "  episode_reward_max: 351.43705643295857\n",
      "  episode_reward_mean: 319.6114444561609\n",
      "  episode_reward_min: 284.56826134390786\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7870\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3371.845\n",
      "    load_time_ms: 2.657\n",
      "    num_steps_sampled: 1170000\n",
      "    num_steps_trained: 1170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.26146581768989563\n",
      "      kl: 0.025176996365189552\n",
      "      policy_loss: 0.008692796342074871\n",
      "      total_loss: 2.8266165256500244\n",
      "      vf_explained_var: 0.9978994727134705\n",
      "      vf_loss: 2.8179240226745605\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6828399300575256\n",
      "      kl: 0.02209429256618023\n",
      "      policy_loss: 0.004561069421470165\n",
      "      total_loss: 55.06591796875\n",
      "      vf_explained_var: 0.9864397048950195\n",
      "      vf_loss: 55.06134796142578\n",
      "    sample_time_ms: 21624.717\n",
      "    update_time_ms: 8.326\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.6179218260255\n",
      "    rl_1: 190.99352263013535\n",
      "  time_since_restore: 3128.4294986724854\n",
      "  time_this_iter_s: 28.37955617904663\n",
      "  time_total_s: 3128.4294986724854\n",
      "  timestamp: 1552575142\n",
      "  timesteps_since_restore: 1170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1170000\n",
      "  training_iteration: 117\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3128 s, 117 iter, 1170000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-52-54\n",
      "  done: false\n",
      "  episode_len_mean: 130.23\n",
      "  episode_reward_max: 356.7902831549945\n",
      "  episode_reward_mean: 319.0111035244446\n",
      "  episode_reward_min: 282.6322488455132\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 7946\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3416.79\n",
      "    load_time_ms: 2.645\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2735291123390198\n",
      "      kl: 0.018859215080738068\n",
      "      policy_loss: 0.007444288581609726\n",
      "      total_loss: 2.8111042976379395\n",
      "      vf_explained_var: 0.9978799819946289\n",
      "      vf_loss: 2.8036599159240723\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6831300258636475\n",
      "      kl: 0.03737727552652359\n",
      "      policy_loss: 0.014208014123141766\n",
      "      total_loss: 51.245426177978516\n",
      "      vf_explained_var: 0.9871759414672852\n",
      "      vf_loss: 51.23122787475586\n",
      "    sample_time_ms: 22350.908\n",
      "    update_time_ms: 8.209\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.57971293735824\n",
      "    rl_1: 190.4313905870864\n",
      "  time_since_restore: 3160.6874408721924\n",
      "  time_this_iter_s: 32.25794219970703\n",
      "  time_total_s: 3160.6874408721924\n",
      "  timestamp: 1552575174\n",
      "  timesteps_since_restore: 1180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 118\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3160 s, 118 iter, 1180000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-53-27\n",
      "  done: false\n",
      "  episode_len_mean: 130.5\n",
      "  episode_reward_max: 363.46390969943644\n",
      "  episode_reward_mean: 322.5811975300605\n",
      "  episode_reward_min: 287.8673063311292\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 8023\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3468.875\n",
      "    load_time_ms: 2.615\n",
      "    num_steps_sampled: 1190000\n",
      "    num_steps_trained: 1190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24900108575820923\n",
      "      kl: 0.01905268430709839\n",
      "      policy_loss: 0.008464936167001724\n",
      "      total_loss: 3.187880516052246\n",
      "      vf_explained_var: 0.9977229237556458\n",
      "      vf_loss: 3.179415464401245\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6838041543960571\n",
      "      kl: 0.019477738067507744\n",
      "      policy_loss: 0.00752886151894927\n",
      "      total_loss: 48.842037200927734\n",
      "      vf_explained_var: 0.9880862832069397\n",
      "      vf_loss: 48.834510803222656\n",
      "    sample_time_ms: 23107.471\n",
      "    update_time_ms: 8.11\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.35161140298842\n",
      "    rl_1: 191.22958612707203\n",
      "  time_since_restore: 3192.9575321674347\n",
      "  time_this_iter_s: 32.27009129524231\n",
      "  time_total_s: 3192.9575321674347\n",
      "  timestamp: 1552575207\n",
      "  timesteps_since_restore: 1190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1190000\n",
      "  training_iteration: 119\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3192 s, 119 iter, 1190000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-53-57\n",
      "  done: false\n",
      "  episode_len_mean: 129.55\n",
      "  episode_reward_max: 363.46390969943644\n",
      "  episode_reward_mean: 316.9087487295968\n",
      "  episode_reward_min: 287.0361001436934\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 8100\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3475.632\n",
      "    load_time_ms: 2.6\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2731184959411621\n",
      "      kl: 0.022384418174624443\n",
      "      policy_loss: 0.01098710298538208\n",
      "      total_loss: 2.746673107147217\n",
      "      vf_explained_var: 0.9978190660476685\n",
      "      vf_loss: 2.7356863021850586\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6634174585342407\n",
      "      kl: 0.038589656352996826\n",
      "      policy_loss: 0.006906982511281967\n",
      "      total_loss: 39.27006912231445\n",
      "      vf_explained_var: 0.9901735186576843\n",
      "      vf_loss: 39.263160705566406\n",
      "    sample_time_ms: 23762.273\n",
      "    update_time_ms: 7.679\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.09158141484828\n",
      "    rl_1: 189.81716731474856\n",
      "  time_since_restore: 3223.30038523674\n",
      "  time_this_iter_s: 30.34285306930542\n",
      "  time_total_s: 3223.30038523674\n",
      "  timestamp: 1552575237\n",
      "  timesteps_since_restore: 1200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 120\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3223 s, 120 iter, 1200000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 130.04\n",
      "  episode_reward_max: 355.74598596873886\n",
      "  episode_reward_mean: 320.42322982914703\n",
      "  episode_reward_min: 281.8838499251007\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 8177\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3465.991\n",
      "    load_time_ms: 2.662\n",
      "    num_steps_sampled: 1210000\n",
      "    num_steps_trained: 1210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.25578874349594116\n",
      "      kl: 0.022632954642176628\n",
      "      policy_loss: 0.007817141711711884\n",
      "      total_loss: 3.1256206035614014\n",
      "      vf_explained_var: 0.9977414011955261\n",
      "      vf_loss: 3.1178035736083984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6788236498832703\n",
      "      kl: 0.015592865645885468\n",
      "      policy_loss: 0.005888329818844795\n",
      "      total_loss: 41.61579895019531\n",
      "      vf_explained_var: 0.9897869825363159\n",
      "      vf_loss: 41.60990524291992\n",
      "    sample_time_ms: 23985.844\n",
      "    update_time_ms: 7.956\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.9831019460828\n",
      "    rl_1: 190.44012788306426\n",
      "  time_since_restore: 3250.397680044174\n",
      "  time_this_iter_s: 27.097294807434082\n",
      "  time_total_s: 3250.397680044174\n",
      "  timestamp: 1552575264\n",
      "  timesteps_since_restore: 1210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1210000\n",
      "  training_iteration: 121\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3250 s, 121 iter, 1210000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-54-49\n",
      "  done: false\n",
      "  episode_len_mean: 129.6\n",
      "  episode_reward_max: 355.32513584883486\n",
      "  episode_reward_mean: 320.75012121510224\n",
      "  episode_reward_min: 281.8838499251007\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 8255\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3452.355\n",
      "    load_time_ms: 2.64\n",
      "    num_steps_sampled: 1220000\n",
      "    num_steps_trained: 1220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.25518736243247986\n",
      "      kl: 0.018695103004574776\n",
      "      policy_loss: 0.005388736259192228\n",
      "      total_loss: 3.2872917652130127\n",
      "      vf_explained_var: 0.9975955486297607\n",
      "      vf_loss: 3.281902313232422\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6563127040863037\n",
      "      kl: 0.025993656367063522\n",
      "      policy_loss: 0.006154133938252926\n",
      "      total_loss: 35.80645751953125\n",
      "      vf_explained_var: 0.9910414218902588\n",
      "      vf_loss: 35.8003044128418\n",
      "    sample_time_ms: 24070.76\n",
      "    update_time_ms: 7.824\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.28074022552093\n",
      "    rl_1: 190.4693809895814\n",
      "  time_since_restore: 3275.4867243766785\n",
      "  time_this_iter_s: 25.089044332504272\n",
      "  time_total_s: 3275.4867243766785\n",
      "  timestamp: 1552575289\n",
      "  timesteps_since_restore: 1220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1220000\n",
      "  training_iteration: 122\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3275 s, 122 iter, 1220000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-55-26\n",
      "  done: false\n",
      "  episode_len_mean: 130.12\n",
      "  episode_reward_max: 357.889231347334\n",
      "  episode_reward_mean: 320.00251664767524\n",
      "  episode_reward_min: 286.63749895193496\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 8331\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3677.374\n",
      "    load_time_ms: 2.661\n",
      "    num_steps_sampled: 1230000\n",
      "    num_steps_trained: 1230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.25752416253089905\n",
      "      kl: 0.02744896523654461\n",
      "      policy_loss: 0.005729784723371267\n",
      "      total_loss: 2.423123836517334\n",
      "      vf_explained_var: 0.9982249140739441\n",
      "      vf_loss: 2.417393922805786\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6299983859062195\n",
      "      kl: 0.01702384650707245\n",
      "      policy_loss: 0.010276447981595993\n",
      "      total_loss: 31.94383430480957\n",
      "      vf_explained_var: 0.9919049143791199\n",
      "      vf_loss: 31.93355369567871\n",
      "    sample_time_ms: 25126.716\n",
      "    update_time_ms: 7.757\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.6234372061379\n",
      "    rl_1: 190.37907944153733\n",
      "  time_since_restore: 3312.422349214554\n",
      "  time_this_iter_s: 36.935624837875366\n",
      "  time_total_s: 3312.422349214554\n",
      "  timestamp: 1552575326\n",
      "  timesteps_since_restore: 1230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1230000\n",
      "  training_iteration: 123\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3312 s, 123 iter, 1230000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-56-06\n",
      "  done: false\n",
      "  episode_len_mean: 129.81\n",
      "  episode_reward_max: 364.4185454688442\n",
      "  episode_reward_mean: 321.2987942172612\n",
      "  episode_reward_min: 281.73212304190673\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 8408\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3817.143\n",
      "    load_time_ms: 2.905\n",
      "    num_steps_sampled: 1240000\n",
      "    num_steps_trained: 1240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2754259705543518\n",
      "      kl: 0.055963315069675446\n",
      "      policy_loss: 0.023410670459270477\n",
      "      total_loss: 3.2633440494537354\n",
      "      vf_explained_var: 0.9976555109024048\n",
      "      vf_loss: 3.239933490753174\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6517850160598755\n",
      "      kl: 0.034020937979221344\n",
      "      policy_loss: 0.01029212400317192\n",
      "      total_loss: 26.801815032958984\n",
      "      vf_explained_var: 0.9932535886764526\n",
      "      vf_loss: 26.791522979736328\n",
      "    sample_time_ms: 26593.797\n",
      "    update_time_ms: 7.901\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.55922146880502\n",
      "    rl_1: 189.7395727484562\n",
      "  time_since_restore: 3352.1353714466095\n",
      "  time_this_iter_s: 39.713022232055664\n",
      "  time_total_s: 3352.1353714466095\n",
      "  timestamp: 1552575366\n",
      "  timesteps_since_restore: 1240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1240000\n",
      "  training_iteration: 124\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3352 s, 124 iter, 1240000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-57-03\n",
      "  done: false\n",
      "  episode_len_mean: 130.1\n",
      "  episode_reward_max: 367.8516282212098\n",
      "  episode_reward_mean: 320.6403648847514\n",
      "  episode_reward_min: 285.4066313086209\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 8485\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4664.724\n",
      "    load_time_ms: 4.483\n",
      "    num_steps_sampled: 1250000\n",
      "    num_steps_trained: 1250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.940931829470712e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.279278427362442\n",
      "      kl: 0.027184681966900826\n",
      "      policy_loss: 0.004711077082902193\n",
      "      total_loss: 3.427278518676758\n",
      "      vf_explained_var: 0.9974859356880188\n",
      "      vf_loss: 3.42256760597229\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.660532534122467\n",
      "      kl: 0.02980996109545231\n",
      "      policy_loss: 0.01678336039185524\n",
      "      total_loss: 23.022602081298828\n",
      "      vf_explained_var: 0.9940786957740784\n",
      "      vf_loss: 23.00581932067871\n",
      "    sample_time_ms: 28870.249\n",
      "    update_time_ms: 8.59\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.09670426405685\n",
      "    rl_1: 189.54366062069457\n",
      "  time_since_restore: 3408.4015843868256\n",
      "  time_this_iter_s: 56.266212940216064\n",
      "  time_total_s: 3408.4015843868256\n",
      "  timestamp: 1552575423\n",
      "  timesteps_since_restore: 1250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1250000\n",
      "  training_iteration: 125\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3408 s, 125 iter, 1250000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-57-37\n",
      "  done: false\n",
      "  episode_len_mean: 131.4\n",
      "  episode_reward_max: 367.8516282212098\n",
      "  episode_reward_mean: 319.72266539323306\n",
      "  episode_reward_min: 279.6549809784367\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 8560\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4563.493\n",
      "    load_time_ms: 4.451\n",
      "    num_steps_sampled: 1260000\n",
      "    num_steps_trained: 1260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.940931829470712e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22836722433567047\n",
      "      kl: 0.026286082342267036\n",
      "      policy_loss: 0.007003249134868383\n",
      "      total_loss: 2.7119853496551514\n",
      "      vf_explained_var: 0.9980127215385437\n",
      "      vf_loss: 2.704982280731201\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5710727572441101\n",
      "      kl: 0.01425330899655819\n",
      "      policy_loss: 0.006242094561457634\n",
      "      total_loss: 23.56379508972168\n",
      "      vf_explained_var: 0.9939854741096497\n",
      "      vf_loss: 23.557552337646484\n",
      "    sample_time_ms: 29637.092\n",
      "    update_time_ms: 9.363\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.4420873259366\n",
      "    rl_1: 190.2805780672964\n",
      "  time_since_restore: 3442.4266669750214\n",
      "  time_this_iter_s: 34.0250825881958\n",
      "  time_total_s: 3442.4266669750214\n",
      "  timestamp: 1552575457\n",
      "  timesteps_since_restore: 1260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1260000\n",
      "  training_iteration: 126\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3442 s, 126 iter, 1260000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-58-00\n",
      "  done: false\n",
      "  episode_len_mean: 129.57\n",
      "  episode_reward_max: 358.3255881781577\n",
      "  episode_reward_mean: 322.2325428004943\n",
      "  episode_reward_min: 279.6549809784367\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 8638\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4477.745\n",
      "    load_time_ms: 4.435\n",
      "    num_steps_sampled: 1270000\n",
      "    num_steps_trained: 1270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.940931829470712e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24142272770404816\n",
      "      kl: 12.512256622314453\n",
      "      policy_loss: 0.04715842008590698\n",
      "      total_loss: 3.050017833709717\n",
      "      vf_explained_var: 0.9978851079940796\n",
      "      vf_loss: 3.002859592437744\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6201565861701965\n",
      "      kl: 0.036765482276678085\n",
      "      policy_loss: 0.026748577132821083\n",
      "      total_loss: 19.395601272583008\n",
      "      vf_explained_var: 0.9949519634246826\n",
      "      vf_loss: 19.368852615356445\n",
      "    sample_time_ms: 29249.976\n",
      "    update_time_ms: 9.404\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.4999125516052\n",
      "    rl_1: 189.732630248889\n",
      "  time_since_restore: 3466.0766768455505\n",
      "  time_this_iter_s: 23.650009870529175\n",
      "  time_total_s: 3466.0766768455505\n",
      "  timestamp: 1552575480\n",
      "  timesteps_since_restore: 1270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1270000\n",
      "  training_iteration: 127\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3466 s, 127 iter, 1270000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-58-24\n",
      "  done: false\n",
      "  episode_len_mean: 129.96\n",
      "  episode_reward_max: 362.7001924505572\n",
      "  episode_reward_mean: 320.3882721423821\n",
      "  episode_reward_min: 279.441094650432\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 8714\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4420.787\n",
      "    load_time_ms: 4.381\n",
      "    num_steps_sampled: 1280000\n",
      "    num_steps_trained: 1280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1911402871801952e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.19813235104084015\n",
      "      kl: 0.026507526636123657\n",
      "      policy_loss: 0.008134113624691963\n",
      "      total_loss: 3.435127019882202\n",
      "      vf_explained_var: 0.9974732995033264\n",
      "      vf_loss: 3.426992893218994\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5888961553573608\n",
      "      kl: 0.029440226033329964\n",
      "      policy_loss: 0.009384187869727612\n",
      "      total_loss: 18.50264549255371\n",
      "      vf_explained_var: 0.995180070400238\n",
      "      vf_loss: 18.493263244628906\n",
      "    sample_time_ms: 28450.644\n",
      "    update_time_ms: 9.396\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.0114393288243\n",
      "    rl_1: 190.37683281355783\n",
      "  time_since_restore: 3489.768075466156\n",
      "  time_this_iter_s: 23.69139862060547\n",
      "  time_total_s: 3489.768075466156\n",
      "  timestamp: 1552575504\n",
      "  timesteps_since_restore: 1280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1280000\n",
      "  training_iteration: 128\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3489 s, 128 iter, 1280000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-58-52\n",
      "  done: false\n",
      "  episode_len_mean: 129.42\n",
      "  episode_reward_max: 362.7001924505572\n",
      "  episode_reward_mean: 319.80536811682634\n",
      "  episode_reward_min: 279.441094650432\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 8792\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4505.864\n",
      "    load_time_ms: 4.441\n",
      "    num_steps_sampled: 1290000\n",
      "    num_steps_trained: 1290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1911402871801952e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.25018781423568726\n",
      "      kl: 0.015788400545716286\n",
      "      policy_loss: 0.0030890298075973988\n",
      "      total_loss: 2.701491355895996\n",
      "      vf_explained_var: 0.998103678226471\n",
      "      vf_loss: 2.698402166366577\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6270161867141724\n",
      "      kl: 0.05919281020760536\n",
      "      policy_loss: 0.03155987709760666\n",
      "      total_loss: 14.581618309020996\n",
      "      vf_explained_var: 0.9960165023803711\n",
      "      vf_loss: 14.550056457519531\n",
      "    sample_time_ms: 27888.047\n",
      "    update_time_ms: 9.016\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.2554186253258\n",
      "    rl_1: 188.5499494915005\n",
      "  time_since_restore: 3517.2670137882233\n",
      "  time_this_iter_s: 27.49893832206726\n",
      "  time_total_s: 3517.2670137882233\n",
      "  timestamp: 1552575532\n",
      "  timesteps_since_restore: 1290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1290000\n",
      "  training_iteration: 129\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3517 s, 129 iter, 1290000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-59-21\n",
      "  done: false\n",
      "  episode_len_mean: 130.24\n",
      "  episode_reward_max: 361.5330222600573\n",
      "  episode_reward_mean: 321.5106852723445\n",
      "  episode_reward_min: 286.69050301741686\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 8869\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4563.125\n",
      "    load_time_ms: 4.427\n",
      "    num_steps_sampled: 1300000\n",
      "    num_steps_trained: 1300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1911402871801952e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2197788506746292\n",
      "      kl: 0.023089325055480003\n",
      "      policy_loss: 0.00540499622002244\n",
      "      total_loss: 3.2357053756713867\n",
      "      vf_explained_var: 0.997704029083252\n",
      "      vf_loss: 3.2303004264831543\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.3010422709404815e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6124274134635925\n",
      "      kl: 0.04882708936929703\n",
      "      policy_loss: 0.037925656884908676\n",
      "      total_loss: 14.871020317077637\n",
      "      vf_explained_var: 0.9960206151008606\n",
      "      vf_loss: 14.83309555053711\n",
      "    sample_time_ms: 27725.362\n",
      "    update_time_ms: 8.927\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.0160561727635\n",
      "    rl_1: 190.494629099581\n",
      "  time_since_restore: 3546.5518119335175\n",
      "  time_this_iter_s: 29.28479814529419\n",
      "  time_total_s: 3546.5518119335175\n",
      "  timestamp: 1552575561\n",
      "  timesteps_since_restore: 1300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1300000\n",
      "  training_iteration: 130\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3546 s, 130 iter, 1300000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_15-59-53\n",
      "  done: false\n",
      "  episode_len_mean: 131.42\n",
      "  episode_reward_max: 358.31145645598656\n",
      "  episode_reward_mean: 323.43505463779746\n",
      "  episode_reward_min: 286.21987837757905\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 8946\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4727.692\n",
      "    load_time_ms: 4.465\n",
      "    num_steps_sampled: 1310000\n",
      "    num_steps_trained: 1310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1911402871801952e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2611647844314575\n",
      "      kl: 0.01711978018283844\n",
      "      policy_loss: 0.0034824467729777098\n",
      "      total_loss: 3.539240598678589\n",
      "      vf_explained_var: 0.9975486397743225\n",
      "      vf_loss: 3.5357580184936523\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9515642465160318e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6218432188034058\n",
      "      kl: 0.040636345744132996\n",
      "      policy_loss: 0.027126792818307877\n",
      "      total_loss: 12.30634880065918\n",
      "      vf_explained_var: 0.9966937303543091\n",
      "      vf_loss: 12.279221534729004\n",
      "    sample_time_ms: 27991.246\n",
      "    update_time_ms: 8.908\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.78854360720905\n",
      "    rl_1: 190.6465110305884\n",
      "  time_since_restore: 3577.9634828567505\n",
      "  time_this_iter_s: 31.411670923233032\n",
      "  time_total_s: 3577.9634828567505\n",
      "  timestamp: 1552575593\n",
      "  timesteps_since_restore: 1310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1310000\n",
      "  training_iteration: 131\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3577 s, 131 iter, 1310000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-00-25\n",
      "  done: false\n",
      "  episode_len_mean: 130.14\n",
      "  episode_reward_max: 357.68488673023904\n",
      "  episode_reward_mean: 321.30039335010923\n",
      "  episode_reward_min: 282.1632691879854\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 9021\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4756.385\n",
      "    load_time_ms: 4.513\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1911402871801952e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2126639187335968\n",
      "      kl: 0.04702865332365036\n",
      "      policy_loss: 0.011096940375864506\n",
      "      total_loss: 2.9991912841796875\n",
      "      vf_explained_var: 0.9977339506149292\n",
      "      vf_loss: 2.9880943298339844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9273459174096503e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4839820861816406\n",
      "      kl: 0.05484742298722267\n",
      "      policy_loss: 0.03607529401779175\n",
      "      total_loss: 11.23237419128418\n",
      "      vf_explained_var: 0.9969255328178406\n",
      "      vf_loss: 11.196298599243164\n",
      "    sample_time_ms: 28656.672\n",
      "    update_time_ms: 9.9\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.54760335401537\n",
      "    rl_1: 190.7527899960939\n",
      "  time_since_restore: 3610.0059700012207\n",
      "  time_this_iter_s: 32.042487144470215\n",
      "  time_total_s: 3610.0059700012207\n",
      "  timestamp: 1552575625\n",
      "  timesteps_since_restore: 1320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 132\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3610 s, 132 iter, 1320000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-00-56\n",
      "  done: false\n",
      "  episode_len_mean: 130.08\n",
      "  episode_reward_max: 365.05531795611637\n",
      "  episode_reward_mean: 320.08195819241024\n",
      "  episode_reward_min: 282.1632691879854\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 9099\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4642.989\n",
      "    load_time_ms: 4.607\n",
      "    num_steps_sampled: 1330000\n",
      "    num_steps_trained: 1330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.786710154668976e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.20812983810901642\n",
      "      kl: 0.04008471220731735\n",
      "      policy_loss: 0.009260596707463264\n",
      "      total_loss: 3.2613980770111084\n",
      "      vf_explained_var: 0.9977258443832397\n",
      "      vf_loss: 3.2521374225616455\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.391017454397798e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5783472061157227\n",
      "      kl: 0.03030712530016899\n",
      "      policy_loss: 0.01826835237443447\n",
      "      total_loss: 8.481208801269531\n",
      "      vf_explained_var: 0.9975849986076355\n",
      "      vf_loss: 8.46294116973877\n",
      "    sample_time_ms: 28224.136\n",
      "    update_time_ms: 10.176\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.6677225550906\n",
      "    rl_1: 189.41423563731965\n",
      "  time_since_restore: 3641.4861068725586\n",
      "  time_this_iter_s: 31.48013687133789\n",
      "  time_total_s: 3641.4861068725586\n",
      "  timestamp: 1552575656\n",
      "  timesteps_since_restore: 1330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1330000\n",
      "  training_iteration: 133\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3641 s, 133 iter, 1330000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 129.17\n",
      "  episode_reward_max: 356.2977969158705\n",
      "  episode_reward_mean: 321.91543877831765\n",
      "  episode_reward_min: 281.0260132923256\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 9176\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4522.668\n",
      "    load_time_ms: 4.379\n",
      "    num_steps_sampled: 1340000\n",
      "    num_steps_trained: 1340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.680064364256468e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24377499520778656\n",
      "      kl: 5.414758682250977\n",
      "      policy_loss: 0.034881092607975006\n",
      "      total_loss: 2.8930723667144775\n",
      "      vf_explained_var: 0.9978986978530884\n",
      "      vf_loss: 2.8581910133361816\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.391017454397798e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.564905047416687\n",
      "      kl: 0.04963282123208046\n",
      "      policy_loss: 0.038554929196834564\n",
      "      total_loss: 6.74949836730957\n",
      "      vf_explained_var: 0.9980958700180054\n",
      "      vf_loss: 6.710943698883057\n",
      "    sample_time_ms: 27523.186\n",
      "    update_time_ms: 10.069\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.62691126156037\n",
      "    rl_1: 189.28852751675728\n",
      "  time_since_restore: 3672.9718832969666\n",
      "  time_this_iter_s: 31.48577642440796\n",
      "  time_total_s: 3672.9718832969666\n",
      "  timestamp: 1552575688\n",
      "  timesteps_since_restore: 1340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1340000\n",
      "  training_iteration: 134\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3672 s, 134 iter, 1340000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-01-58\n",
      "  done: false\n",
      "  episode_len_mean: 130.77\n",
      "  episode_reward_max: 363.0940171055902\n",
      "  episode_reward_mean: 318.27082205786064\n",
      "  episode_reward_min: 281.0260132923256\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 9252\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3724.477\n",
      "    load_time_ms: 2.993\n",
      "    num_steps_sampled: 1350000\n",
      "    num_steps_trained: 1350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.020098597423056e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.21968834102153778\n",
      "      kl: 0.02682606317102909\n",
      "      policy_loss: 0.0067974720150232315\n",
      "      total_loss: 2.984286069869995\n",
      "      vf_explained_var: 0.9977438449859619\n",
      "      vf_loss: 2.9774885177612305\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.586529542017935e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5747970342636108\n",
      "      kl: 0.0535406656563282\n",
      "      policy_loss: 0.03159511834383011\n",
      "      total_loss: 6.459957599639893\n",
      "      vf_explained_var: 0.9981260299682617\n",
      "      vf_loss: 6.4283623695373535\n",
      "    sample_time_ms: 25694.033\n",
      "    update_time_ms: 9.566\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.4842988008423\n",
      "    rl_1: 189.78652325701833\n",
      "  time_since_restore: 3702.915463209152\n",
      "  time_this_iter_s: 29.94357991218567\n",
      "  time_total_s: 3702.915463209152\n",
      "  timestamp: 1552575718\n",
      "  timesteps_since_restore: 1350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1350000\n",
      "  training_iteration: 135\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3702 s, 135 iter, 1350000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-02-31\n",
      "  done: false\n",
      "  episode_len_mean: 131.27\n",
      "  episode_reward_max: 353.35107024944574\n",
      "  episode_reward_mean: 319.84861260346656\n",
      "  episode_reward_min: 279.9387831425132\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 9329\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3828.027\n",
      "    load_time_ms: 3.073\n",
      "    num_steps_sampled: 1360000\n",
      "    num_steps_trained: 1360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.020098597423056e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22473396360874176\n",
      "      kl: 0.022758454084396362\n",
      "      policy_loss: 0.003932770807296038\n",
      "      total_loss: 3.4550907611846924\n",
      "      vf_explained_var: 0.9974653720855713\n",
      "      vf_loss: 3.451158285140991\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.879791469593547e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5477342009544373\n",
      "      kl: 0.08406835049390793\n",
      "      policy_loss: 0.05340830609202385\n",
      "      total_loss: 5.47860050201416\n",
      "      vf_explained_var: 0.9984295964241028\n",
      "      vf_loss: 5.425192356109619\n",
      "    sample_time_ms: 25475.073\n",
      "    update_time_ms: 8.946\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.33927020192579\n",
      "    rl_1: 190.50934240154078\n",
      "  time_since_restore: 3735.7830929756165\n",
      "  time_this_iter_s: 32.86762976646423\n",
      "  time_total_s: 3735.7830929756165\n",
      "  timestamp: 1552575751\n",
      "  timesteps_since_restore: 1360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1360000\n",
      "  training_iteration: 136\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3735 s, 136 iter, 1360000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-03-00\n",
      "  done: false\n",
      "  episode_len_mean: 129.55\n",
      "  episode_reward_max: 360.40350727656494\n",
      "  episode_reward_mean: 320.51110471001954\n",
      "  episode_reward_min: 279.9387831425132\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 9406\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3835.28\n",
      "    load_time_ms: 3.038\n",
      "    num_steps_sampled: 1370000\n",
      "    num_steps_trained: 1370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.020098597423056e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.15438641607761383\n",
      "      kl: 0.017570411786437035\n",
      "      policy_loss: 0.005529029294848442\n",
      "      total_loss: 2.856987953186035\n",
      "      vf_explained_var: 0.997971773147583\n",
      "      vf_loss: 2.851458787918091\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4819685136438789e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6018025279045105\n",
      "      kl: 0.05478551611304283\n",
      "      policy_loss: 0.02810748666524887\n",
      "      total_loss: 4.6879377365112305\n",
      "      vf_explained_var: 0.9986125230789185\n",
      "      vf_loss: 4.659830570220947\n",
      "    sample_time_ms: 26068.361\n",
      "    update_time_ms: 8.846\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.07275176212426\n",
      "    rl_1: 189.43835294789525\n",
      "  time_since_restore: 3765.4345996379852\n",
      "  time_this_iter_s: 29.651506662368774\n",
      "  time_total_s: 3765.4345996379852\n",
      "  timestamp: 1552575780\n",
      "  timesteps_since_restore: 1370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1370000\n",
      "  training_iteration: 137\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3765 s, 137 iter, 1370000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-03-34\n",
      "  done: false\n",
      "  episode_len_mean: 131.62\n",
      "  episode_reward_max: 360.40350727656494\n",
      "  episode_reward_mean: 321.105205876765\n",
      "  episode_reward_min: 279.83030458309463\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 9481\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3972.193\n",
      "    load_time_ms: 3.225\n",
      "    num_steps_sampled: 1380000\n",
      "    num_steps_trained: 1380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.020098597423056e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.26093408465385437\n",
      "      kl: 0.02240585908293724\n",
      "      policy_loss: 0.0035366201773285866\n",
      "      total_loss: 2.8686599731445312\n",
      "      vf_explained_var: 0.997830331325531\n",
      "      vf_loss: 2.8651235103607178\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.222953804441584e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5724428296089172\n",
      "      kl: 0.3332062065601349\n",
      "      policy_loss: 0.08459887653589249\n",
      "      total_loss: 5.186947345733643\n",
      "      vf_explained_var: 0.9985255002975464\n",
      "      vf_loss: 5.1023478507995605\n",
      "    sample_time_ms: 26921.237\n",
      "    update_time_ms: 8.897\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.560128990821\n",
      "    rl_1: 190.54507688594396\n",
      "  time_since_restore: 3799.0271015167236\n",
      "  time_this_iter_s: 33.5925018787384\n",
      "  time_total_s: 3799.0271015167236\n",
      "  timestamp: 1552575814\n",
      "  timesteps_since_restore: 1380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1380000\n",
      "  training_iteration: 138\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3799 s, 138 iter, 1380000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-04-03\n",
      "  done: false\n",
      "  episode_len_mean: 131.3\n",
      "  episode_reward_max: 360.6494561346659\n",
      "  episode_reward_mean: 317.63919025800277\n",
      "  episode_reward_min: 279.83030458309463\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 9558\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3861.231\n",
      "    load_time_ms: 3.169\n",
      "    num_steps_sampled: 1390000\n",
      "    num_steps_trained: 1390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.020098597423056e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2728717625141144\n",
      "      kl: 0.011312292888760567\n",
      "      policy_loss: 0.002604796551167965\n",
      "      total_loss: 2.768094301223755\n",
      "      vf_explained_var: 0.9978328347206116\n",
      "      vf_loss: 2.7654898166656494\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.3344293624938806e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5398406386375427\n",
      "      kl: 0.06561864912509918\n",
      "      policy_loss: 0.045342590659856796\n",
      "      total_loss: 3.7273008823394775\n",
      "      vf_explained_var: 0.9988769292831421\n",
      "      vf_loss: 3.681957960128784\n",
      "    sample_time_ms: 27201.714\n",
      "    update_time_ms: 9.123\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.45466750365074\n",
      "    rl_1: 190.1845227543521\n",
      "  time_since_restore: 3828.2156705856323\n",
      "  time_this_iter_s: 29.18856906890869\n",
      "  time_total_s: 3828.2156705856323\n",
      "  timestamp: 1552575843\n",
      "  timesteps_since_restore: 1390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1390000\n",
      "  training_iteration: 139\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3828 s, 139 iter, 1390000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-04-29\n",
      "  done: false\n",
      "  episode_len_mean: 132.7\n",
      "  episode_reward_max: 358.187602548145\n",
      "  episode_reward_mean: 317.62074203736097\n",
      "  episode_reward_min: 283.55452322493943\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 9633\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3804.099\n",
      "    load_time_ms: 3.249\n",
      "    num_steps_sampled: 1400000\n",
      "    num_steps_trained: 1400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.020098597423056e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.20338605344295502\n",
      "      kl: 0.015923021361231804\n",
      "      policy_loss: 0.0009787207236513495\n",
      "      total_loss: 2.518389940261841\n",
      "      vf_explained_var: 0.9980958104133606\n",
      "      vf_loss: 2.517411231994629\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.001644767523857e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5223662853240967\n",
      "      kl: 0.035228364169597626\n",
      "      policy_loss: 0.02646193467080593\n",
      "      total_loss: 3.108983278274536\n",
      "      vf_explained_var: 0.9990850687026978\n",
      "      vf_loss: 3.0825212001800537\n",
      "    sample_time_ms: 26904.875\n",
      "    update_time_ms: 9.049\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.69741423153162\n",
      "    rl_1: 190.92332780582936\n",
      "  time_since_restore: 3853.960421562195\n",
      "  time_this_iter_s: 25.7447509765625\n",
      "  time_total_s: 3853.960421562195\n",
      "  timestamp: 1552575869\n",
      "  timesteps_since_restore: 1400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1400000\n",
      "  training_iteration: 140\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3853 s, 140 iter, 1400000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-04-53\n",
      "  done: false\n",
      "  episode_len_mean: 130.9\n",
      "  episode_reward_max: 355.94531071911007\n",
      "  episode_reward_mean: 319.54334605431694\n",
      "  episode_reward_min: 275.8608847830594\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 9710\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3637.063\n",
      "    load_time_ms: 3.102\n",
      "    num_steps_sampled: 1410000\n",
      "    num_steps_trained: 1410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.020098597423056e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2033711075782776\n",
      "      kl: 0.06523238867521286\n",
      "      policy_loss: 0.013603424653410912\n",
      "      total_loss: 2.128182888031006\n",
      "      vf_explained_var: 0.9984562397003174\n",
      "      vf_loss: 2.114579439163208\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.001644767523857e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5985108613967896\n",
      "      kl: 0.05267518758773804\n",
      "      policy_loss: 0.03437066823244095\n",
      "      total_loss: 2.5430526733398438\n",
      "      vf_explained_var: 0.9992340207099915\n",
      "      vf_loss: 2.5086817741394043\n",
      "    sample_time_ms: 26296.4\n",
      "    update_time_ms: 8.989\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.44727737989749\n",
      "    rl_1: 190.09606867441948\n",
      "  time_since_restore: 3877.601914167404\n",
      "  time_this_iter_s: 23.64149260520935\n",
      "  time_total_s: 3877.601914167404\n",
      "  timestamp: 1552575893\n",
      "  timesteps_since_restore: 1410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1410000\n",
      "  training_iteration: 141\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3877 s, 141 iter, 1410000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-05-24\n",
      "  done: false\n",
      "  episode_len_mean: 131.39\n",
      "  episode_reward_max: 362.8100155102197\n",
      "  episode_reward_mean: 316.94008533750565\n",
      "  episode_reward_min: 275.8608847830594\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 9785\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3757.217\n",
      "    load_time_ms: 3.152\n",
      "    num_steps_sampled: 1420000\n",
      "    num_steps_trained: 1420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.030146160640592e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.19895197451114655\n",
      "      kl: 0.014458063058555126\n",
      "      policy_loss: 0.0033666123636066914\n",
      "      total_loss: 2.1049587726593018\n",
      "      vf_explained_var: 0.9983333945274353\n",
      "      vf_loss: 2.1015923023223877\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.50246942603247e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5295353531837463\n",
      "      kl: 0.12000197917222977\n",
      "      policy_loss: 0.0611489936709404\n",
      "      total_loss: 1.9950668811798096\n",
      "      vf_explained_var: 0.9993895292282104\n",
      "      vf_loss: 1.933917760848999\n",
      "    sample_time_ms: 26060.872\n",
      "    update_time_ms: 8.263\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.4415388816171\n",
      "    rl_1: 190.49854645588857\n",
      "  time_since_restore: 3908.482361316681\n",
      "  time_this_iter_s: 30.880447149276733\n",
      "  time_total_s: 3908.482361316681\n",
      "  timestamp: 1552575924\n",
      "  timesteps_since_restore: 1420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1420000\n",
      "  training_iteration: 142\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3908 s, 142 iter, 1420000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-06-17\n",
      "  done: false\n",
      "  episode_len_mean: 130.75\n",
      "  episode_reward_max: 357.3630279958662\n",
      "  episode_reward_mean: 319.37309554624227\n",
      "  episode_reward_min: 284.2639067933513\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 9862\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4212.392\n",
      "    load_time_ms: 3.146\n",
      "    num_steps_sampled: 1430000\n",
      "    num_steps_trained: 1430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.030146160640592e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.18880659341812134\n",
      "      kl: 0.8979978561401367\n",
      "      policy_loss: 0.026019763201475143\n",
      "      total_loss: 2.1762185096740723\n",
      "      vf_explained_var: 0.998449444770813\n",
      "      vf_loss: 2.150198459625244\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1253698762374723e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6191802024841309\n",
      "      kl: 0.03904925286769867\n",
      "      policy_loss: 0.026544829830527306\n",
      "      total_loss: 2.14003849029541\n",
      "      vf_explained_var: 0.9993568062782288\n",
      "      vf_loss: 2.1134934425354004\n",
      "    sample_time_ms: 27770.098\n",
      "    update_time_ms: 8.147\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.14052968614345\n",
      "    rl_1: 190.23256586009873\n",
      "  time_since_restore: 3961.609284877777\n",
      "  time_this_iter_s: 53.12692356109619\n",
      "  time_total_s: 3961.609284877777\n",
      "  timestamp: 1552575977\n",
      "  timesteps_since_restore: 1430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1430000\n",
      "  training_iteration: 143\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 3961 s, 143 iter, 1430000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-07-22\n",
      "  done: false\n",
      "  episode_len_mean: 131.68\n",
      "  episode_reward_max: 351.79354323386053\n",
      "  episode_reward_mean: 317.6885554992211\n",
      "  episode_reward_min: 278.19922424579045\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 9938\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4554.139\n",
      "    load_time_ms: 3.317\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.04521797878344e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.21832314133644104\n",
      "      kl: 1.8367263078689575\n",
      "      policy_loss: 0.045501649379730225\n",
      "      total_loss: 2.662566661834717\n",
      "      vf_explained_var: 0.9980083107948303\n",
      "      vf_loss: 2.6170647144317627\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1253698762374723e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5918039083480835\n",
      "      kl: 211.49989318847656\n",
      "      policy_loss: 0.23920173943042755\n",
      "      total_loss: 1.8613418340682983\n",
      "      vf_explained_var: 0.9994917511940002\n",
      "      vf_loss: 1.6221402883529663\n",
      "    sample_time_ms: 30827.881\n",
      "    update_time_ms: 8.414\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.95982734990861\n",
      "    rl_1: 189.72872814931245\n",
      "  time_since_restore: 4027.106525182724\n",
      "  time_this_iter_s: 65.4972403049469\n",
      "  time_total_s: 4027.106525182724\n",
      "  timestamp: 1552576042\n",
      "  timesteps_since_restore: 1440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 144\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4027 s, 144 iter, 1440000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 131.93\n",
      "  episode_reward_max: 360.47915012770557\n",
      "  episode_reward_mean: 318.82473809989494\n",
      "  episode_reward_min: 280.8859661768723\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 10014\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4631.473\n",
      "    load_time_ms: 3.238\n",
      "    num_steps_sampled: 1450000\n",
      "    num_steps_trained: 1450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3567833279062402e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2594224512577057\n",
      "      kl: 0.028265738859772682\n",
      "      policy_loss: 0.003208385780453682\n",
      "      total_loss: 2.562704563140869\n",
      "      vf_explained_var: 0.9980589747428894\n",
      "      vf_loss: 2.5594961643218994\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.6880547729971778e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5971484780311584\n",
      "      kl: 0.04893102869391441\n",
      "      policy_loss: 0.02935413457453251\n",
      "      total_loss: 1.237726092338562\n",
      "      vf_explained_var: 0.9996201992034912\n",
      "      vf_loss: 1.208371877670288\n",
      "    sample_time_ms: 31523.297\n",
      "    update_time_ms: 8.776\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.07957408749118\n",
      "    rl_1: 189.74516401240388\n",
      "  time_since_restore: 4064.791485786438\n",
      "  time_this_iter_s: 37.68496060371399\n",
      "  time_total_s: 4064.791485786438\n",
      "  timestamp: 1552576080\n",
      "  timesteps_since_restore: 1450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1450000\n",
      "  training_iteration: 145\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4064 s, 145 iter, 1450000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-08-31\n",
      "  done: false\n",
      "  episode_len_mean: 133.48\n",
      "  episode_reward_max: 360.47915012770557\n",
      "  episode_reward_mean: 321.14812570446634\n",
      "  episode_reward_min: 280.8362510916647\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 10089\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4544.116\n",
      "    load_time_ms: 3.236\n",
      "    num_steps_sampled: 1460000\n",
      "    num_steps_trained: 1460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3567833279062402e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3061002492904663\n",
      "      kl: 0.009108448401093483\n",
      "      policy_loss: 0.0015056332340463996\n",
      "      total_loss: 2.110276937484741\n",
      "      vf_explained_var: 0.9983711242675781\n",
      "      vf_loss: 2.108771324157715\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.532082573086073e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5680665373802185\n",
      "      kl: 0.032705433666706085\n",
      "      policy_loss: 0.02273603528738022\n",
      "      total_loss: 1.3204963207244873\n",
      "      vf_explained_var: 0.9995985627174377\n",
      "      vf_loss: 1.297760248184204\n",
      "    sample_time_ms: 31393.423\n",
      "    update_time_ms: 9.415\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.40025136455077\n",
      "    rl_1: 190.74787433991554\n",
      "  time_since_restore: 4095.491630554199\n",
      "  time_this_iter_s: 30.70014476776123\n",
      "  time_total_s: 4095.491630554199\n",
      "  timestamp: 1552576111\n",
      "  timesteps_since_restore: 1460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1460000\n",
      "  training_iteration: 146\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4095 s, 146 iter, 1460000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-08-55\n",
      "  done: false\n",
      "  episode_len_mean: 136.15\n",
      "  episode_reward_max: 360.09225298625626\n",
      "  episode_reward_mean: 318.9504918617889\n",
      "  episode_reward_min: 283.16738929615934\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 10162\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4539.294\n",
      "    load_time_ms: 3.292\n",
      "    num_steps_sampled: 1470000\n",
      "    num_steps_trained: 1470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.783916639531201e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.29945677518844604\n",
      "      kl: 0.014700221829116344\n",
      "      policy_loss: 0.003583604237064719\n",
      "      total_loss: 2.2953336238861084\n",
      "      vf_explained_var: 0.9981237649917603\n",
      "      vf_loss: 2.291750192642212\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.532082573086073e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5242880582809448\n",
      "      kl: 0.0941627025604248\n",
      "      policy_loss: 0.056937456130981445\n",
      "      total_loss: 1.1867362260818481\n",
      "      vf_explained_var: 0.9996470808982849\n",
      "      vf_loss: 1.1297986507415771\n",
      "    sample_time_ms: 30819.258\n",
      "    update_time_ms: 9.18\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.90934233454703\n",
      "    rl_1: 192.0411495272419\n",
      "  time_since_restore: 4119.354577064514\n",
      "  time_this_iter_s: 23.86294651031494\n",
      "  time_total_s: 4119.354577064514\n",
      "  timestamp: 1552576135\n",
      "  timesteps_since_restore: 1470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1470000\n",
      "  training_iteration: 147\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4119 s, 147 iter, 1470000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-09-19\n",
      "  done: false\n",
      "  episode_len_mean: 134.23\n",
      "  episode_reward_max: 355.7280785156859\n",
      "  episode_reward_mean: 317.53945636490266\n",
      "  episode_reward_min: 281.8775879485196\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 10238\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4414.022\n",
      "    load_time_ms: 3.137\n",
      "    num_steps_sampled: 1480000\n",
      "    num_steps_trained: 1480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.783916639531201e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.25217345356941223\n",
      "      kl: 0.011269341222941875\n",
      "      policy_loss: -0.0013780564768239856\n",
      "      total_loss: 2.5708107948303223\n",
      "      vf_explained_var: 0.9979971051216125\n",
      "      vf_loss: 2.5721888542175293\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7981238596291095e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5499374866485596\n",
      "      kl: 0.032032985240221024\n",
      "      policy_loss: 0.024808309972286224\n",
      "      total_loss: 1.0434379577636719\n",
      "      vf_explained_var: 0.9996812343597412\n",
      "      vf_loss: 1.0186296701431274\n",
      "    sample_time_ms: 30033.232\n",
      "    update_time_ms: 9.06\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.81777881762797\n",
      "    rl_1: 190.72167754727474\n",
      "  time_since_restore: 4143.829802274704\n",
      "  time_this_iter_s: 24.47522521018982\n",
      "  time_total_s: 4143.829802274704\n",
      "  timestamp: 1552576159\n",
      "  timesteps_since_restore: 1480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1480000\n",
      "  training_iteration: 148\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4143 s, 148 iter, 1480000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-09-43\n",
      "  done: false\n",
      "  episode_len_mean: 133.47\n",
      "  episode_reward_max: 355.7280785156859\n",
      "  episode_reward_mean: 318.2084265598065\n",
      "  episode_reward_min: 285.03619850120754\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 10312\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4381.706\n",
      "    load_time_ms: 3.257\n",
      "    num_steps_sampled: 1490000\n",
      "    num_steps_trained: 1490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.783916639531201e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.27688485383987427\n",
      "      kl: 0.00995518546551466\n",
      "      policy_loss: 0.0017835944890975952\n",
      "      total_loss: 2.874559164047241\n",
      "      vf_explained_var: 0.9978297352790833\n",
      "      vf_loss: 2.8727760314941406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7981238596291095e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6103568077087402\n",
      "      kl: 0.04145427048206329\n",
      "      policy_loss: 0.02584025263786316\n",
      "      total_loss: 0.7426961064338684\n",
      "      vf_explained_var: 0.9997729659080505\n",
      "      vf_loss: 0.7168559432029724\n",
      "    sample_time_ms: 29548.516\n",
      "    update_time_ms: 8.844\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.52553785541528\n",
      "    rl_1: 189.68288870439122\n",
      "  time_since_restore: 4167.845766782761\n",
      "  time_this_iter_s: 24.01596450805664\n",
      "  time_total_s: 4167.845766782761\n",
      "  timestamp: 1552576183\n",
      "  timesteps_since_restore: 1490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1490000\n",
      "  training_iteration: 149\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4167 s, 149 iter, 1490000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-10-08\n",
      "  done: false\n",
      "  episode_len_mean: 131.58\n",
      "  episode_reward_max: 354.7747898679599\n",
      "  episode_reward_mean: 321.0298876683201\n",
      "  episode_reward_min: 287.3893799166414\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 10389\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4360.197\n",
      "    load_time_ms: 3.251\n",
      "    num_steps_sampled: 1500000\n",
      "    num_steps_trained: 1500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.3919583197656005e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.17033007740974426\n",
      "      kl: 0.009174066595733166\n",
      "      policy_loss: 0.00016366060299333185\n",
      "      total_loss: 3.112274646759033\n",
      "      vf_explained_var: 0.9978085160255432\n",
      "      vf_loss: 3.1121110916137695\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.697185954879787e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.654102087020874\n",
      "      kl: 20.3466739654541\n",
      "      policy_loss: 0.19513972103595734\n",
      "      total_loss: 0.8107603788375854\n",
      "      vf_explained_var: 0.9998043775558472\n",
      "      vf_loss: 0.6156207323074341\n",
      "    sample_time_ms: 29427.664\n",
      "    update_time_ms: 9.065\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.55679601544927\n",
      "    rl_1: 189.4730916528709\n",
      "  time_since_restore: 4192.168920040131\n",
      "  time_this_iter_s: 24.323153257369995\n",
      "  time_total_s: 4192.168920040131\n",
      "  timestamp: 1552576208\n",
      "  timesteps_since_restore: 1500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1500000\n",
      "  training_iteration: 150\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4192 s, 150 iter, 1500000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-10-31\n",
      "  done: false\n",
      "  episode_len_mean: 131.51\n",
      "  episode_reward_max: 350.63970909872324\n",
      "  episode_reward_mean: 318.3127420554459\n",
      "  episode_reward_min: 277.48785166242016\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 10465\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4363.476\n",
      "    load_time_ms: 3.248\n",
      "    num_steps_sampled: 1510000\n",
      "    num_steps_trained: 1510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6959791598828003e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.20885375142097473\n",
      "      kl: 0.01761734113097191\n",
      "      policy_loss: 0.006209996063262224\n",
      "      total_loss: 2.5757811069488525\n",
      "      vf_explained_var: 0.9980517625808716\n",
      "      vf_loss: 2.569570779800415\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.54578025580866e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5746781826019287\n",
      "      kl: 30.190067291259766\n",
      "      policy_loss: 0.22693964838981628\n",
      "      total_loss: 0.7700544595718384\n",
      "      vf_explained_var: 0.999829113483429\n",
      "      vf_loss: 0.5431148409843445\n",
      "    sample_time_ms: 29375.096\n",
      "    update_time_ms: 8.871\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.89836506585\n",
      "    rl_1: 189.41437698959584\n",
      "  time_since_restore: 4215.315643548965\n",
      "  time_this_iter_s: 23.14672350883484\n",
      "  time_total_s: 4215.315643548965\n",
      "  timestamp: 1552576231\n",
      "  timesteps_since_restore: 1510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1510000\n",
      "  training_iteration: 151\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4215 s, 151 iter, 1510000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-10-57\n",
      "  done: false\n",
      "  episode_len_mean: 132.3\n",
      "  episode_reward_max: 354.6458923354405\n",
      "  episode_reward_mean: 320.0007040501061\n",
      "  episode_reward_min: 277.48785166242016\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 10540\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4222.237\n",
      "    load_time_ms: 3.239\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6959791598828003e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1872096210718155\n",
      "      kl: 0.011480218730866909\n",
      "      policy_loss: 0.0007718584383837879\n",
      "      total_loss: 2.5174949169158936\n",
      "      vf_explained_var: 0.9981233477592468\n",
      "      vf_loss: 2.516723155975342\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.281866707499054e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6010021567344666\n",
      "      kl: 0.08538832515478134\n",
      "      policy_loss: 0.040482427924871445\n",
      "      total_loss: 0.6383921504020691\n",
      "      vf_explained_var: 0.9998098611831665\n",
      "      vf_loss: 0.5979098081588745\n",
      "    sample_time_ms: 28994.638\n",
      "    update_time_ms: 8.628\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.5452164431009\n",
      "    rl_1: 190.45548760700524\n",
      "  time_since_restore: 4240.979154825211\n",
      "  time_this_iter_s: 25.663511276245117\n",
      "  time_total_s: 4240.979154825211\n",
      "  timestamp: 1552576257\n",
      "  timesteps_since_restore: 1520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 152\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4240 s, 152 iter, 1520000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-11-21\n",
      "  done: false\n",
      "  episode_len_mean: 132.9\n",
      "  episode_reward_max: 350.3009004934055\n",
      "  episode_reward_mean: 315.52881013311134\n",
      "  episode_reward_min: 275.6733583718504\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 10615\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3648.791\n",
      "    load_time_ms: 3.103\n",
      "    num_steps_sampled: 1530000\n",
      "    num_steps_trained: 1530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6959791598828003e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22237631678581238\n",
      "      kl: 0.021095391362905502\n",
      "      policy_loss: 0.0031359049025923014\n",
      "      total_loss: 2.3961338996887207\n",
      "      vf_explained_var: 0.9981518983840942\n",
      "      vf_loss: 2.392997980117798\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.922800392120826e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6204344630241394\n",
      "      kl: 0.03755658492445946\n",
      "      policy_loss: 0.022266320884227753\n",
      "      total_loss: 0.4203927516937256\n",
      "      vf_explained_var: 0.9998705983161926\n",
      "      vf_loss: 0.39812639355659485\n",
      "    sample_time_ms: 26711.638\n",
      "    update_time_ms: 8.639\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.01430439063229\n",
      "    rl_1: 188.51450574247906\n",
      "  time_since_restore: 4265.532274007797\n",
      "  time_this_iter_s: 24.55311918258667\n",
      "  time_total_s: 4265.532274007797\n",
      "  timestamp: 1552576281\n",
      "  timesteps_since_restore: 1530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1530000\n",
      "  training_iteration: 153\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4265 s, 153 iter, 1530000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-11-44\n",
      "  done: false\n",
      "  episode_len_mean: 133.41\n",
      "  episode_reward_max: 352.3179935288737\n",
      "  episode_reward_mean: 316.2449663333714\n",
      "  episode_reward_min: 275.6733583718504\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 10691\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3272.973\n",
      "    load_time_ms: 2.881\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6959791598828003e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24941837787628174\n",
      "      kl: 0.016219275072216988\n",
      "      policy_loss: 0.0003077510045841336\n",
      "      total_loss: 2.6490626335144043\n",
      "      vf_explained_var: 0.9978893995285034\n",
      "      vf_loss: 2.648754596710205\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.922800392120826e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5926044583320618\n",
      "      kl: 0.02685640938580036\n",
      "      policy_loss: 0.014590593054890633\n",
      "      total_loss: 0.4513304829597473\n",
      "      vf_explained_var: 0.9998608827590942\n",
      "      vf_loss: 0.43673989176750183\n",
      "    sample_time_ms: 22859.484\n",
      "    update_time_ms: 8.277\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.49544480786966\n",
      "    rl_1: 189.74952152550173\n",
      "  time_since_restore: 4288.731840610504\n",
      "  time_this_iter_s: 23.19956660270691\n",
      "  time_total_s: 4288.731840610504\n",
      "  timestamp: 1552576304\n",
      "  timesteps_since_restore: 1540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 154\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4288 s, 154 iter, 1540000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-12-07\n",
      "  done: false\n",
      "  episode_len_mean: 134.46\n",
      "  episode_reward_max: 352.3179935288737\n",
      "  episode_reward_mean: 316.1724650697139\n",
      "  episode_reward_min: 278.4456618978549\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 10765\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3123.919\n",
      "    load_time_ms: 2.714\n",
      "    num_steps_sampled: 1550000\n",
      "    num_steps_trained: 1550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6959791598828003e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.30316218733787537\n",
      "      kl: 0.01622210070490837\n",
      "      policy_loss: 0.001000070944428444\n",
      "      total_loss: 2.1806483268737793\n",
      "      vf_explained_var: 0.9982229471206665\n",
      "      vf_loss: 2.1796481609344482\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.922800392120826e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6170470118522644\n",
      "      kl: 40.07661437988281\n",
      "      policy_loss: 0.2168205976486206\n",
      "      total_loss: 0.6115078330039978\n",
      "      vf_explained_var: 0.9998722672462463\n",
      "      vf_loss: 0.3946872651576996\n",
      "    sample_time_ms: 21532.873\n",
      "    update_time_ms: 7.667\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.4405633373334\n",
      "    rl_1: 189.73190173238046\n",
      "  time_since_restore: 4311.637854099274\n",
      "  time_this_iter_s: 22.90601348876953\n",
      "  time_total_s: 4311.637854099274\n",
      "  timestamp: 1552576327\n",
      "  timesteps_since_restore: 1550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1550000\n",
      "  training_iteration: 155\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4311 s, 155 iter, 1550000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-12-31\n",
      "  done: false\n",
      "  episode_len_mean: 135.64\n",
      "  episode_reward_max: 358.89418696889675\n",
      "  episode_reward_mean: 319.7059345247792\n",
      "  episode_reward_min: 278.4226273622018\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 10837\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3113.082\n",
      "    load_time_ms: 2.754\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6959791598828003e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.23612873256206512\n",
      "      kl: 0.023396464064717293\n",
      "      policy_loss: 0.002682365709915757\n",
      "      total_loss: 2.406808853149414\n",
      "      vf_explained_var: 0.998127281665802\n",
      "      vf_loss: 2.4041271209716797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.884200124960096e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5832074284553528\n",
      "      kl: 0.02738299034535885\n",
      "      policy_loss: 0.015258020721375942\n",
      "      total_loss: 0.4605002701282501\n",
      "      vf_explained_var: 0.9998571872711182\n",
      "      vf_loss: 0.44524216651916504\n",
      "    sample_time_ms: 20795.148\n",
      "    update_time_ms: 6.906\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.1332502620088\n",
      "    rl_1: 190.57268426277045\n",
      "  time_since_restore: 4334.845945596695\n",
      "  time_this_iter_s: 23.208091497421265\n",
      "  time_total_s: 4334.845945596695\n",
      "  timestamp: 1552576351\n",
      "  timesteps_since_restore: 1560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 156\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4334 s, 156 iter, 1560000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-12-57\n",
      "  done: false\n",
      "  episode_len_mean: 136.32\n",
      "  episode_reward_max: 352.9666129319543\n",
      "  episode_reward_mean: 316.4785751103856\n",
      "  episode_reward_min: 278.4226273622018\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 10911\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3107.92\n",
      "    load_time_ms: 2.684\n",
      "    num_steps_sampled: 1570000\n",
      "    num_steps_trained: 1570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6959791598828003e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3001156747341156\n",
      "      kl: 0.04003746807575226\n",
      "      policy_loss: 0.008836803957819939\n",
      "      total_loss: 2.556379795074463\n",
      "      vf_explained_var: 0.9979191422462463\n",
      "      vf_loss: 2.5475432872772217\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.884200124960096e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6142413020133972\n",
      "      kl: 0.034701019525527954\n",
      "      policy_loss: 0.018865790218114853\n",
      "      total_loss: 0.391996830701828\n",
      "      vf_explained_var: 0.9998783469200134\n",
      "      vf_loss: 0.3731310963630676\n",
      "    sample_time_ms: 21008.578\n",
      "    update_time_ms: 6.809\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.8135047718794\n",
      "    rl_1: 189.66507033850615\n",
      "  time_since_restore: 4360.787916660309\n",
      "  time_this_iter_s: 25.94197106361389\n",
      "  time_total_s: 4360.787916660309\n",
      "  timestamp: 1552576377\n",
      "  timesteps_since_restore: 1570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1570000\n",
      "  training_iteration: 157\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4360 s, 157 iter, 1570000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-13-20\n",
      "  done: false\n",
      "  episode_len_mean: 134.96\n",
      "  episode_reward_max: 353.1346041134921\n",
      "  episode_reward_mean: 316.2555558127288\n",
      "  episode_reward_min: 284.9466790956523\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 10985\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3094.775\n",
      "    load_time_ms: 2.701\n",
      "    num_steps_sampled: 1580000\n",
      "    num_steps_trained: 1580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5439677143050236e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2718050479888916\n",
      "      kl: 0.02046995796263218\n",
      "      policy_loss: 0.0008850499871186912\n",
      "      total_loss: 2.1861989498138428\n",
      "      vf_explained_var: 0.9983006119728088\n",
      "      vf_loss: 2.1853137016296387\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.884200124960096e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6528573036193848\n",
      "      kl: 0.03707917034626007\n",
      "      policy_loss: 0.020637640729546547\n",
      "      total_loss: 0.402229905128479\n",
      "      vf_explained_var: 0.9998753070831299\n",
      "      vf_loss: 0.3815922141075134\n",
      "    sample_time_ms: 20908.133\n",
      "    update_time_ms: 6.743\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.90600993040495\n",
      "    rl_1: 188.3495458823238\n",
      "  time_since_restore: 4384.125417470932\n",
      "  time_this_iter_s: 23.33750081062317\n",
      "  time_total_s: 4384.125417470932\n",
      "  timestamp: 1552576400\n",
      "  timesteps_since_restore: 1580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1580000\n",
      "  training_iteration: 158\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4384 s, 158 iter, 1580000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-13-43\n",
      "  done: false\n",
      "  episode_len_mean: 135.75\n",
      "  episode_reward_max: 360.71119539164346\n",
      "  episode_reward_mean: 319.7396997100479\n",
      "  episode_reward_min: 281.89317961227033\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 11058\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3090.91\n",
      "    load_time_ms: 2.563\n",
      "    num_steps_sampled: 1590000\n",
      "    num_steps_trained: 1590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5439677143050236e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.27415740489959717\n",
      "      kl: 0.015662360936403275\n",
      "      policy_loss: -0.0012275222688913345\n",
      "      total_loss: 2.718708038330078\n",
      "      vf_explained_var: 0.9978628754615784\n",
      "      vf_loss: 2.719935417175293\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.884200124960096e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5907725095748901\n",
      "      kl: 0.057847097516059875\n",
      "      policy_loss: 0.029538139700889587\n",
      "      total_loss: 0.4390455484390259\n",
      "      vf_explained_var: 0.9998696446418762\n",
      "      vf_loss: 0.4095073938369751\n",
      "    sample_time_ms: 20821.988\n",
      "    update_time_ms: 6.939\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.80796526106596\n",
      "    rl_1: 189.93173444898196\n",
      "  time_since_restore: 4407.2425763607025\n",
      "  time_this_iter_s: 23.117158889770508\n",
      "  time_total_s: 4407.2425763607025\n",
      "  timestamp: 1552576423\n",
      "  timesteps_since_restore: 1590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1590000\n",
      "  training_iteration: 159\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4407 s, 159 iter, 1590000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-14-06\n",
      "  done: false\n",
      "  episode_len_mean: 136.71\n",
      "  episode_reward_max: 360.71119539164346\n",
      "  episode_reward_mean: 318.5556147065802\n",
      "  episode_reward_min: 280.0225547472259\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 11132\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3092.118\n",
      "    load_time_ms: 2.495\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5439677143050236e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.26094529032707214\n",
      "      kl: 0.02733551897108555\n",
      "      policy_loss: 0.0031220854725688696\n",
      "      total_loss: 2.356025457382202\n",
      "      vf_explained_var: 0.9982211589813232\n",
      "      vf_loss: 2.352903366088867\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.326301246231328e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6512918472290039\n",
      "      kl: 0.014272344298660755\n",
      "      policy_loss: 0.010450474917888641\n",
      "      total_loss: 0.3877573013305664\n",
      "      vf_explained_var: 0.9998785853385925\n",
      "      vf_loss: 0.37730687856674194\n",
      "    sample_time_ms: 20694.712\n",
      "    update_time_ms: 6.699\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.0433648070768\n",
      "    rl_1: 189.51224989950336\n",
      "  time_since_restore: 4430.301416635513\n",
      "  time_this_iter_s: 23.05884027481079\n",
      "  time_total_s: 4430.301416635513\n",
      "  timestamp: 1552576446\n",
      "  timesteps_since_restore: 1600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 160\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4430 s, 160 iter, 1600000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-14-29\n",
      "  done: false\n",
      "  episode_len_mean: 135.63\n",
      "  episode_reward_max: 357.0886651101932\n",
      "  episode_reward_mean: 318.93119352646244\n",
      "  episode_reward_min: 277.7819240524074\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 11206\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3090.321\n",
      "    load_time_ms: 2.543\n",
      "    num_steps_sampled: 1610000\n",
      "    num_steps_trained: 1610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5439677143050236e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.19228830933570862\n",
      "      kl: 0.012495742179453373\n",
      "      policy_loss: 0.000424320314778015\n",
      "      total_loss: 2.431248188018799\n",
      "      vf_explained_var: 0.9980624318122864\n",
      "      vf_loss: 2.430823802947998\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.326301246231328e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5737019777297974\n",
      "      kl: 0.0683574229478836\n",
      "      policy_loss: 0.026304125785827637\n",
      "      total_loss: 0.3820413649082184\n",
      "      vf_explained_var: 0.9998863339424133\n",
      "      vf_loss: 0.35573720932006836\n",
      "    sample_time_ms: 20662.546\n",
      "    update_time_ms: 6.796\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.32294913067773\n",
      "    rl_1: 190.60824439578468\n",
      "  time_since_restore: 4453.111777067184\n",
      "  time_this_iter_s: 22.810360431671143\n",
      "  time_total_s: 4453.111777067184\n",
      "  timestamp: 1552576469\n",
      "  timesteps_since_restore: 1610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1610000\n",
      "  training_iteration: 161\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4453 s, 161 iter, 1610000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-14-56\n",
      "  done: false\n",
      "  episode_len_mean: 136.0\n",
      "  episode_reward_max: 363.17095283977693\n",
      "  episode_reward_mean: 321.9658167657076\n",
      "  episode_reward_min: 286.0805616994832\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 11279\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.198\n",
      "    load_time_ms: 2.446\n",
      "    num_steps_sampled: 1620000\n",
      "    num_steps_trained: 1620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5439677143050236e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24028000235557556\n",
      "      kl: 0.037634752690792084\n",
      "      policy_loss: 0.0036891575437039137\n",
      "      total_loss: 2.3462562561035156\n",
      "      vf_explained_var: 0.9981719851493835\n",
      "      vf_loss: 2.342566967010498\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.489450810555808e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5888434648513794\n",
      "      kl: 0.03412596881389618\n",
      "      policy_loss: 0.018213873729109764\n",
      "      total_loss: 0.35709279775619507\n",
      "      vf_explained_var: 0.9998927712440491\n",
      "      vf_loss: 0.33887889981269836\n",
      "    sample_time_ms: 20743.985\n",
      "    update_time_ms: 6.883\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.1008712309604\n",
      "    rl_1: 191.86494553474714\n",
      "  time_since_restore: 4480.094517469406\n",
      "  time_this_iter_s: 26.98274040222168\n",
      "  time_total_s: 4480.094517469406\n",
      "  timestamp: 1552576496\n",
      "  timesteps_since_restore: 1620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1620000\n",
      "  training_iteration: 162\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4480 s, 162 iter, 1620000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-15-22\n",
      "  done: false\n",
      "  episode_len_mean: 137.86\n",
      "  episode_reward_max: 363.17095283977693\n",
      "  episode_reward_mean: 317.3799128114883\n",
      "  episode_reward_min: 281.7924345053201\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 11352\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.872\n",
      "    load_time_ms: 2.429\n",
      "    num_steps_sampled: 1630000\n",
      "    num_steps_trained: 1630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5439677143050236e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2771177887916565\n",
      "      kl: 0.38696396350860596\n",
      "      policy_loss: 0.017345890402793884\n",
      "      total_loss: 2.1157493591308594\n",
      "      vf_explained_var: 0.9981949925422668\n",
      "      vf_loss: 2.0984034538269043\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.489450810555808e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.523285984992981\n",
      "      kl: 0.01746445894241333\n",
      "      policy_loss: 0.008572692051529884\n",
      "      total_loss: 0.3602698743343353\n",
      "      vf_explained_var: 0.9998867511749268\n",
      "      vf_loss: 0.3516972064971924\n",
      "    sample_time_ms: 20842.791\n",
      "    update_time_ms: 6.724\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.0738305108709\n",
      "    rl_1: 191.30608230061742\n",
      "  time_since_restore: 4505.571594238281\n",
      "  time_this_iter_s: 25.477076768875122\n",
      "  time_total_s: 4505.571594238281\n",
      "  timestamp: 1552576522\n",
      "  timesteps_since_restore: 1630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1630000\n",
      "  training_iteration: 163\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4505 s, 163 iter, 1630000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-15-45\n",
      "  done: false\n",
      "  episode_len_mean: 135.91\n",
      "  episode_reward_max: 351.4495944266508\n",
      "  episode_reward_mean: 318.8270435458835\n",
      "  episode_reward_min: 283.4274240951088\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 11427\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.557\n",
      "    load_time_ms: 2.415\n",
      "    num_steps_sampled: 1640000\n",
      "    num_steps_trained: 1640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.815952123660169e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22659553587436676\n",
      "      kl: 0.020840579643845558\n",
      "      policy_loss: 0.003402500180527568\n",
      "      total_loss: 2.3468031883239746\n",
      "      vf_explained_var: 0.9981944561004639\n",
      "      vf_loss: 2.343400478363037\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.489450810555808e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6820470094680786\n",
      "      kl: 76.84160614013672\n",
      "      policy_loss: 0.17483735084533691\n",
      "      total_loss: 0.4996632933616638\n",
      "      vf_explained_var: 0.9998965263366699\n",
      "      vf_loss: 0.3248259127140045\n",
      "    sample_time_ms: 20845.809\n",
      "    update_time_ms: 6.774\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.8211971229682\n",
      "    rl_1: 190.00584642291528\n",
      "  time_since_restore: 4528.78870010376\n",
      "  time_this_iter_s: 23.217105865478516\n",
      "  time_total_s: 4528.78870010376\n",
      "  timestamp: 1552576545\n",
      "  timesteps_since_restore: 1640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1640000\n",
      "  training_iteration: 164\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4528 s, 164 iter, 1640000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-16-08\n",
      "  done: false\n",
      "  episode_len_mean: 134.01\n",
      "  episode_reward_max: 356.72427673060804\n",
      "  episode_reward_mean: 318.79452436329024\n",
      "  episode_reward_min: 283.4274240951088\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 11501\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.685\n",
      "    load_time_ms: 2.39\n",
      "    num_steps_sampled: 1650000\n",
      "    num_steps_trained: 1650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.815952123660169e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.18664288520812988\n",
      "      kl: 0.016583584249019623\n",
      "      policy_loss: 0.0028547022957354784\n",
      "      total_loss: 2.458233594894409\n",
      "      vf_explained_var: 0.9980581402778625\n",
      "      vf_loss: 2.455378532409668\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.734177804020488e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6803751587867737\n",
      "      kl: 52.54427719116211\n",
      "      policy_loss: 0.16534423828125\n",
      "      total_loss: 0.4686121344566345\n",
      "      vf_explained_var: 0.99990314245224\n",
      "      vf_loss: 0.3032679259777069\n",
      "    sample_time_ms: 20890.6\n",
      "    update_time_ms: 6.784\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.21931095147735\n",
      "    rl_1: 190.57521341181283\n",
      "  time_since_restore: 4552.144838094711\n",
      "  time_this_iter_s: 23.356137990951538\n",
      "  time_total_s: 4552.144838094711\n",
      "  timestamp: 1552576568\n",
      "  timesteps_since_restore: 1650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1650000\n",
      "  training_iteration: 165\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4552 s, 165 iter, 1650000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-16-31\n",
      "  done: false\n",
      "  episode_len_mean: 137.93\n",
      "  episode_reward_max: 356.72427673060804\n",
      "  episode_reward_mean: 317.776545035646\n",
      "  episode_reward_min: 284.6365278971343\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 11573\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.965\n",
      "    load_time_ms: 2.379\n",
      "    num_steps_sampled: 1660000\n",
      "    num_steps_trained: 1660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.815952123660169e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.29388436675071716\n",
      "      kl: 0.017870185896754265\n",
      "      policy_loss: 0.003315599635243416\n",
      "      total_loss: 1.9302246570587158\n",
      "      vf_explained_var: 0.9983412623405457\n",
      "      vf_loss: 1.9269092082977295\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4601267235426325e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5715757012367249\n",
      "      kl: 0.008275909349322319\n",
      "      policy_loss: 0.00282291229814291\n",
      "      total_loss: 0.3234666585922241\n",
      "      vf_explained_var: 0.9998974800109863\n",
      "      vf_loss: 0.32064375281333923\n",
      "    sample_time_ms: 20869.69\n",
      "    update_time_ms: 6.841\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 125.98682585420607\n",
      "    rl_1: 191.78971918143986\n",
      "  time_since_restore: 4575.089347362518\n",
      "  time_this_iter_s: 22.944509267807007\n",
      "  time_total_s: 4575.089347362518\n",
      "  timestamp: 1552576591\n",
      "  timesteps_since_restore: 1660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1660000\n",
      "  training_iteration: 166\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4575 s, 166 iter, 1660000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-16-57\n",
      "  done: false\n",
      "  episode_len_mean: 136.68\n",
      "  episode_reward_max: 361.48465244395635\n",
      "  episode_reward_mean: 318.6651973593209\n",
      "  episode_reward_min: 287.09772005877903\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 11646\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3185.025\n",
      "    load_time_ms: 2.438\n",
      "    num_steps_sampled: 1670000\n",
      "    num_steps_trained: 1670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.815952123660169e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24594077467918396\n",
      "      kl: 0.1422804594039917\n",
      "      policy_loss: 0.012955879792571068\n",
      "      total_loss: 2.5336227416992188\n",
      "      vf_explained_var: 0.9980061054229736\n",
      "      vf_loss: 2.5206668376922607\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.300633617713162e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6943304538726807\n",
      "      kl: 0.012254557572305202\n",
      "      policy_loss: 0.0032411550637334585\n",
      "      total_loss: 0.2812970280647278\n",
      "      vf_explained_var: 0.9999099373817444\n",
      "      vf_loss: 0.2780558466911316\n",
      "    sample_time_ms: 20743.953\n",
      "    update_time_ms: 7.48\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.86272595781557\n",
      "    rl_1: 190.8024714015054\n",
      "  time_since_restore: 4600.353052139282\n",
      "  time_this_iter_s: 25.263704776763916\n",
      "  time_total_s: 4600.353052139282\n",
      "  timestamp: 1552576617\n",
      "  timesteps_since_restore: 1670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1670000\n",
      "  training_iteration: 167\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4600 s, 167 iter, 1670000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-17-22\n",
      "  done: false\n",
      "  episode_len_mean: 134.69\n",
      "  episode_reward_max: 352.294872523148\n",
      "  episode_reward_mean: 320.4801045064071\n",
      "  episode_reward_min: 285.25029585315895\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 11721\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3198.635\n",
      "    load_time_ms: 2.447\n",
      "    num_steps_sampled: 1680000\n",
      "    num_steps_trained: 1680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.723929920984245e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.20799042284488678\n",
      "      kl: 0.011387640610337257\n",
      "      policy_loss: -0.0009125354699790478\n",
      "      total_loss: 2.6912052631378174\n",
      "      vf_explained_var: 0.9980028867721558\n",
      "      vf_loss: 2.69211745262146\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.300633617713162e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7553011775016785\n",
      "      kl: 0.01134678814560175\n",
      "      policy_loss: 0.004520621616393328\n",
      "      total_loss: 0.3038310408592224\n",
      "      vf_explained_var: 0.9999033808708191\n",
      "      vf_loss: 0.29931047558784485\n",
      "    sample_time_ms: 20888.672\n",
      "    update_time_ms: 7.782\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.8253355169175\n",
      "    rl_1: 189.65476898948967\n",
      "  time_since_restore: 4625.2782735824585\n",
      "  time_this_iter_s: 24.92522144317627\n",
      "  time_total_s: 4625.2782735824585\n",
      "  timestamp: 1552576642\n",
      "  timesteps_since_restore: 1680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1680000\n",
      "  training_iteration: 168\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4625 s, 168 iter, 1680000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-17-50\n",
      "  done: false\n",
      "  episode_len_mean: 137.5\n",
      "  episode_reward_max: 351.7992525408155\n",
      "  episode_reward_mean: 319.48732729779454\n",
      "  episode_reward_min: 286.22481186353355\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 11794\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3383.554\n",
      "    load_time_ms: 2.432\n",
      "    num_steps_sampled: 1690000\n",
      "    num_steps_trained: 1690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.723929920984245e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2497696578502655\n",
      "      kl: 0.018086783587932587\n",
      "      policy_loss: 0.003082429291680455\n",
      "      total_loss: 2.3978474140167236\n",
      "      vf_explained_var: 0.9980738759040833\n",
      "      vf_loss: 2.3947646617889404\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.300633617713162e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.666275680065155\n",
      "      kl: 0.014100181870162487\n",
      "      policy_loss: 0.0055818986147642136\n",
      "      total_loss: 0.3236578404903412\n",
      "      vf_explained_var: 0.9998987913131714\n",
      "      vf_loss: 0.31807586550712585\n",
      "    sample_time_ms: 21259.338\n",
      "    update_time_ms: 7.634\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.99791401089155\n",
      "    rl_1: 190.4894132869029\n",
      "  time_since_restore: 4653.952717781067\n",
      "  time_this_iter_s: 28.6744441986084\n",
      "  time_total_s: 4653.952717781067\n",
      "  timestamp: 1552576670\n",
      "  timesteps_since_restore: 1690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1690000\n",
      "  training_iteration: 169\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4653 s, 169 iter, 1690000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-18-14\n",
      "  done: false\n",
      "  episode_len_mean: 132.6\n",
      "  episode_reward_max: 358.2735176618807\n",
      "  episode_reward_mean: 321.9342831904561\n",
      "  episode_reward_min: 275.3589962830865\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 11869\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3381.374\n",
      "    load_time_ms: 2.457\n",
      "    num_steps_sampled: 1700000\n",
      "    num_steps_trained: 1700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.723929920984245e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.12601986527442932\n",
      "      kl: 0.020427528768777847\n",
      "      policy_loss: 0.0011489071184769273\n",
      "      total_loss: 2.497570276260376\n",
      "      vf_explained_var: 0.9981718063354492\n",
      "      vf_loss: 2.4964213371276855\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.300633617713162e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7049083113670349\n",
      "      kl: 0.03389182686805725\n",
      "      policy_loss: 0.013333601877093315\n",
      "      total_loss: 0.2839687168598175\n",
      "      vf_explained_var: 0.9999123811721802\n",
      "      vf_loss: 0.2706351578235626\n",
      "    sample_time_ms: 21336.62\n",
      "    update_time_ms: 8.474\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.79123041369834\n",
      "    rl_1: 190.1430527767577\n",
      "  time_since_restore: 4677.774416446686\n",
      "  time_this_iter_s: 23.821698665618896\n",
      "  time_total_s: 4677.774416446686\n",
      "  timestamp: 1552576694\n",
      "  timesteps_since_restore: 1700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1700000\n",
      "  training_iteration: 170\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4677 s, 170 iter, 1700000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-18-38\n",
      "  done: false\n",
      "  episode_len_mean: 134.86\n",
      "  episode_reward_max: 357.2726381904184\n",
      "  episode_reward_mean: 318.52949809447\n",
      "  episode_reward_min: 275.3589962830865\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 11943\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3379.009\n",
      "    load_time_ms: 2.42\n",
      "    num_steps_sampled: 1710000\n",
      "    num_steps_trained: 1710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.723929920984245e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2725156247615814\n",
      "      kl: 0.5103540420532227\n",
      "      policy_loss: 0.00429909722879529\n",
      "      total_loss: 1.8710205554962158\n",
      "      vf_explained_var: 0.9984977841377258\n",
      "      vf_loss: 1.866721749305725\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.300633617713162e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6392101645469666\n",
      "      kl: 0.037369851022958755\n",
      "      policy_loss: 0.018631180748343468\n",
      "      total_loss: 0.30670657753944397\n",
      "      vf_explained_var: 0.999907374382019\n",
      "      vf_loss: 0.28807538747787476\n",
      "    sample_time_ms: 21455.95\n",
      "    update_time_ms: 8.525\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.9788493368222\n",
      "    rl_1: 189.55064875764788\n",
      "  time_since_restore: 4701.753085136414\n",
      "  time_this_iter_s: 23.978668689727783\n",
      "  time_total_s: 4701.753085136414\n",
      "  timestamp: 1552576718\n",
      "  timesteps_since_restore: 1710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1710000\n",
      "  training_iteration: 171\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4701 s, 171 iter, 1710000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-19-02\n",
      "  done: false\n",
      "  episode_len_mean: 134.76\n",
      "  episode_reward_max: 358.5998216044008\n",
      "  episode_reward_mean: 316.2799190873266\n",
      "  episode_reward_min: 284.86582849236686\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 12017\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3341.785\n",
      "    load_time_ms: 2.432\n",
      "    num_steps_sampled: 1720000\n",
      "    num_steps_trained: 1720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.58589077939966e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1995617002248764\n",
      "      kl: 0.014464471489191055\n",
      "      policy_loss: 0.0022668601013720036\n",
      "      total_loss: 2.0472898483276367\n",
      "      vf_explained_var: 0.9983553886413574\n",
      "      vf_loss: 2.045022964477539\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.300633617713162e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6323891282081604\n",
      "      kl: 0.021619105711579323\n",
      "      policy_loss: 0.010673056356608868\n",
      "      total_loss: 0.29580649733543396\n",
      "      vf_explained_var: 0.9999067783355713\n",
      "      vf_loss: 0.285133421421051\n",
      "    sample_time_ms: 21199.494\n",
      "    update_time_ms: 8.517\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.94986735386769\n",
      "    rl_1: 189.33005173345896\n",
      "  time_since_restore: 4725.798936128616\n",
      "  time_this_iter_s: 24.04585099220276\n",
      "  time_total_s: 4725.798936128616\n",
      "  timestamp: 1552576742\n",
      "  timesteps_since_restore: 1720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1720000\n",
      "  training_iteration: 172\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4725 s, 172 iter, 1720000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-19-26\n",
      "  done: false\n",
      "  episode_len_mean: 135.01\n",
      "  episode_reward_max: 354.1133256187885\n",
      "  episode_reward_mean: 320.18747818473827\n",
      "  episode_reward_min: 285.4696755403382\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 12091\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3341.344\n",
      "    load_time_ms: 2.436\n",
      "    num_steps_sampled: 1730000\n",
      "    num_steps_trained: 1730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.58589077939966e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.18112540245056152\n",
      "      kl: 0.021736832335591316\n",
      "      policy_loss: 0.005278352182358503\n",
      "      total_loss: 2.1506896018981934\n",
      "      vf_explained_var: 0.9984024167060852\n",
      "      vf_loss: 2.145411252975464\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.300633617713162e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6572533249855042\n",
      "      kl: 0.030872173607349396\n",
      "      policy_loss: 0.006880911532789469\n",
      "      total_loss: 0.3364396095275879\n",
      "      vf_explained_var: 0.9998949766159058\n",
      "      vf_loss: 0.32955873012542725\n",
      "    sample_time_ms: 21029.001\n",
      "    update_time_ms: 8.697\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.05678755141807\n",
      "    rl_1: 190.1306906333202\n",
      "  time_since_restore: 4749.567035198212\n",
      "  time_this_iter_s: 23.768099069595337\n",
      "  time_total_s: 4749.567035198212\n",
      "  timestamp: 1552576766\n",
      "  timesteps_since_restore: 1730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1730000\n",
      "  training_iteration: 173\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4749 s, 173 iter, 1730000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-19-50\n",
      "  done: false\n",
      "  episode_len_mean: 134.29\n",
      "  episode_reward_max: 355.1923158888531\n",
      "  episode_reward_mean: 321.30330379054413\n",
      "  episode_reward_min: 278.7850754050226\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 12165\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3342.795\n",
      "    load_time_ms: 2.45\n",
      "    num_steps_sampled: 1740000\n",
      "    num_steps_trained: 1740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.58589077939966e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.20242315530776978\n",
      "      kl: 0.02105846256017685\n",
      "      policy_loss: 0.004684344865381718\n",
      "      total_loss: 2.1029553413391113\n",
      "      vf_explained_var: 0.9983662366867065\n",
      "      vf_loss: 2.098271131515503\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.300633617713162e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6427164673805237\n",
      "      kl: 78.96466064453125\n",
      "      policy_loss: 0.1766434609889984\n",
      "      total_loss: 0.4940670132637024\n",
      "      vf_explained_var: 0.9998985528945923\n",
      "      vf_loss: 0.3174234926700592\n",
      "    sample_time_ms: 21042.99\n",
      "    update_time_ms: 8.672\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.11924881789116\n",
      "    rl_1: 190.18405497265294\n",
      "  time_since_restore: 4772.9378271102905\n",
      "  time_this_iter_s: 23.370791912078857\n",
      "  time_total_s: 4772.9378271102905\n",
      "  timestamp: 1552576790\n",
      "  timesteps_since_restore: 1740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1740000\n",
      "  training_iteration: 174\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4772 s, 174 iter, 1740000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-20-13\n",
      "  done: false\n",
      "  episode_len_mean: 137.45\n",
      "  episode_reward_max: 355.1923158888531\n",
      "  episode_reward_mean: 315.95691711348235\n",
      "  episode_reward_min: 279.6732908961781\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 12238\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3365.035\n",
      "    load_time_ms: 2.46\n",
      "    num_steps_sampled: 1750000\n",
      "    num_steps_trained: 1750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.58589077939966e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2696291506290436\n",
      "      kl: 0.027404919266700745\n",
      "      policy_loss: 0.003363653551787138\n",
      "      total_loss: 2.1714730262756348\n",
      "      vf_explained_var: 0.9982385039329529\n",
      "      vf_loss: 2.16810941696167\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0950946985498395e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.605803370475769\n",
      "      kl: 0.024072745814919472\n",
      "      policy_loss: 0.013792546465992928\n",
      "      total_loss: 0.3054462969303131\n",
      "      vf_explained_var: 0.9999026656150818\n",
      "      vf_loss: 0.29165372252464294\n",
      "    sample_time_ms: 21031.572\n",
      "    update_time_ms: 8.741\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.60266774451776\n",
      "    rl_1: 188.35424936896456\n",
      "  time_since_restore: 4796.402879714966\n",
      "  time_this_iter_s: 23.465052604675293\n",
      "  time_total_s: 4796.402879714966\n",
      "  timestamp: 1552576813\n",
      "  timesteps_since_restore: 1750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1750000\n",
      "  training_iteration: 175\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4796 s, 175 iter, 1750000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-20-37\n",
      "  done: false\n",
      "  episode_len_mean: 143.37\n",
      "  episode_reward_max: 352.5146981999352\n",
      "  episode_reward_mean: 315.61536968710135\n",
      "  episode_reward_min: 283.08277528354677\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 12306\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3391.849\n",
      "    load_time_ms: 2.363\n",
      "    num_steps_sampled: 1760000\n",
      "    num_steps_trained: 1760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.58589077939966e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.35399094223976135\n",
      "      kl: 0.03575759753584862\n",
      "      policy_loss: 0.004474646877497435\n",
      "      total_loss: 2.0741448402404785\n",
      "      vf_explained_var: 0.9981797337532043\n",
      "      vf_loss: 2.069669723510742\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0950946985498395e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5195689797401428\n",
      "      kl: 0.03610212728381157\n",
      "      policy_loss: 0.013373171910643578\n",
      "      total_loss: 0.3938456177711487\n",
      "      vf_explained_var: 0.9998739957809448\n",
      "      vf_loss: 0.38047248125076294\n",
      "    sample_time_ms: 21108.691\n",
      "    update_time_ms: 8.753\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.58274235266624\n",
      "    rl_1: 189.03262733443518\n",
      "  time_since_restore: 4820.38122177124\n",
      "  time_this_iter_s: 23.978342056274414\n",
      "  time_total_s: 4820.38122177124\n",
      "  timestamp: 1552576837\n",
      "  timesteps_since_restore: 1760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1760000\n",
      "  training_iteration: 176\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4820 s, 176 iter, 1760000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-21-01\n",
      "  done: false\n",
      "  episode_len_mean: 139.49\n",
      "  episode_reward_max: 351.5127855400178\n",
      "  episode_reward_mean: 318.67918258446116\n",
      "  episode_reward_min: 276.8842469909898\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 12379\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3352.166\n",
      "    load_time_ms: 2.347\n",
      "    num_steps_sampled: 1770000\n",
      "    num_steps_trained: 1770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.58589077939966e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.18729721009731293\n",
      "      kl: 0.010198227129876614\n",
      "      policy_loss: -0.00024198301252909005\n",
      "      total_loss: 2.3379435539245605\n",
      "      vf_explained_var: 0.9981421232223511\n",
      "      vf_loss: 2.3381855487823486\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0950946985498395e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5717006325721741\n",
      "      kl: 0.04632776975631714\n",
      "      policy_loss: 0.02799130417406559\n",
      "      total_loss: 0.34938329458236694\n",
      "      vf_explained_var: 0.9998942613601685\n",
      "      vf_loss: 0.3213919997215271\n",
      "    sample_time_ms: 21031.308\n",
      "    update_time_ms: 8.147\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 129.2277790356989\n",
      "    rl_1: 189.45140354876224\n",
      "  time_since_restore: 4844.4672768116\n",
      "  time_this_iter_s: 24.086055040359497\n",
      "  time_total_s: 4844.4672768116\n",
      "  timestamp: 1552576861\n",
      "  timesteps_since_restore: 1770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1770000\n",
      "  training_iteration: 177\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4844 s, 177 iter, 1770000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-21-33\n",
      "  done: false\n",
      "  episode_len_mean: 139.29\n",
      "  episode_reward_max: 352.47674171300946\n",
      "  episode_reward_mean: 316.34212599542246\n",
      "  episode_reward_min: 281.07816801565434\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 12451\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3467.793\n",
      "    load_time_ms: 2.456\n",
      "    num_steps_sampled: 1780000\n",
      "    num_steps_trained: 1780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.58589077939966e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.25783199071884155\n",
      "      kl: 0.006588620133697987\n",
      "      policy_loss: 0.00015570275718346238\n",
      "      total_loss: 2.2564516067504883\n",
      "      vf_explained_var: 0.9980708360671997\n",
      "      vf_loss: 2.256295680999756\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.642642736039029e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5664194822311401\n",
      "      kl: 0.021010788157582283\n",
      "      policy_loss: 0.01018346007913351\n",
      "      total_loss: 0.32219722867012024\n",
      "      vf_explained_var: 0.9998964071273804\n",
      "      vf_loss: 0.3120137155056\n",
      "    sample_time_ms: 21573.743\n",
      "    update_time_ms: 8.058\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.24251428186291\n",
      "    rl_1: 189.09961171355954\n",
      "  time_since_restore: 4875.975140333176\n",
      "  time_this_iter_s: 31.507863521575928\n",
      "  time_total_s: 4875.975140333176\n",
      "  timestamp: 1552576893\n",
      "  timesteps_since_restore: 1780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1780000\n",
      "  training_iteration: 178\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4875 s, 178 iter, 1780000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-21-56\n",
      "  done: false\n",
      "  episode_len_mean: 136.16\n",
      "  episode_reward_max: 351.1968090346899\n",
      "  episode_reward_mean: 314.32600983247426\n",
      "  episode_reward_min: 281.1281420931982\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 12524\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3277.692\n",
      "    load_time_ms: 2.492\n",
      "    num_steps_sampled: 1790000\n",
      "    num_steps_trained: 1790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.29294538969983e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.21905356645584106\n",
      "      kl: 0.020477360114455223\n",
      "      policy_loss: 0.00544139975681901\n",
      "      total_loss: 1.7429791688919067\n",
      "      vf_explained_var: 0.9985790848731995\n",
      "      vf_loss: 1.7375376224517822\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.642642736039029e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5500507354736328\n",
      "      kl: 0.03769276663661003\n",
      "      policy_loss: 0.019306685775518417\n",
      "      total_loss: 0.3311448097229004\n",
      "      vf_explained_var: 0.9998968243598938\n",
      "      vf_loss: 0.3118381202220917\n",
      "    sample_time_ms: 21215.31\n",
      "    update_time_ms: 8.018\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.38747068406323\n",
      "    rl_1: 187.9385391484111\n",
      "  time_since_restore: 4899.15945315361\n",
      "  time_this_iter_s: 23.18431282043457\n",
      "  time_total_s: 4899.15945315361\n",
      "  timestamp: 1552576916\n",
      "  timesteps_since_restore: 1790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1790000\n",
      "  training_iteration: 179\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4899 s, 179 iter, 1790000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-22-20\n",
      "  done: false\n",
      "  episode_len_mean: 137.51\n",
      "  episode_reward_max: 351.1968090346899\n",
      "  episode_reward_mean: 315.7129316022581\n",
      "  episode_reward_min: 278.1023573176204\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 12596\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3284.572\n",
      "    load_time_ms: 2.493\n",
      "    num_steps_sampled: 1800000\n",
      "    num_steps_trained: 1800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.29294538969983e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2589666545391083\n",
      "      kl: 0.006507233716547489\n",
      "      policy_loss: 0.001275925082154572\n",
      "      total_loss: 2.0692944526672363\n",
      "      vf_explained_var: 0.9982230067253113\n",
      "      vf_loss: 2.068018913269043\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.642642736039029e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5041378140449524\n",
      "      kl: 0.02413230389356613\n",
      "      policy_loss: 0.01543350052088499\n",
      "      total_loss: 0.385189950466156\n",
      "      vf_explained_var: 0.9998788237571716\n",
      "      vf_loss: 0.3697565197944641\n",
      "    sample_time_ms: 21203.816\n",
      "    update_time_ms: 7.203\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.8048417581477\n",
      "    rl_1: 188.9080898441104\n",
      "  time_since_restore: 4922.926382064819\n",
      "  time_this_iter_s: 23.766928911209106\n",
      "  time_total_s: 4922.926382064819\n",
      "  timestamp: 1552576940\n",
      "  timesteps_since_restore: 1800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1800000\n",
      "  training_iteration: 180\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4922 s, 180 iter, 1800000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-22-43\n",
      "  done: false\n",
      "  episode_len_mean: 140.06\n",
      "  episode_reward_max: 348.23757196205844\n",
      "  episode_reward_mean: 315.08107832362714\n",
      "  episode_reward_min: 278.1023573176204\n",
      "  episodes_this_iter: 70\n",
      "  episodes_total: 12666\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3296.906\n",
      "    load_time_ms: 2.492\n",
      "    num_steps_sampled: 1810000\n",
      "    num_steps_trained: 1810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.146472694849915e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2394000142812729\n",
      "      kl: 0.012766940519213676\n",
      "      policy_loss: 0.003767323913052678\n",
      "      total_loss: 1.5775928497314453\n",
      "      vf_explained_var: 0.9986187815666199\n",
      "      vf_loss: 1.573825478553772\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.642642736039029e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4613548219203949\n",
      "      kl: 0.07729027420282364\n",
      "      policy_loss: 0.03274093568325043\n",
      "      total_loss: 0.32354769110679626\n",
      "      vf_explained_var: 0.9999040365219116\n",
      "      vf_loss: 0.29080671072006226\n",
      "    sample_time_ms: 21140.824\n",
      "    update_time_ms: 7.09\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 124.81968134700753\n",
      "    rl_1: 190.26139697661958\n",
      "  time_since_restore: 4946.398457288742\n",
      "  time_this_iter_s: 23.47207522392273\n",
      "  time_total_s: 4946.398457288742\n",
      "  timestamp: 1552576963\n",
      "  timesteps_since_restore: 1810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1810000\n",
      "  training_iteration: 181\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4946 s, 181 iter, 1810000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-23-08\n",
      "  done: false\n",
      "  episode_len_mean: 139.38\n",
      "  episode_reward_max: 350.735846013434\n",
      "  episode_reward_mean: 316.0100515195972\n",
      "  episode_reward_min: 286.4289467940506\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 12738\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3285.701\n",
      "    load_time_ms: 2.603\n",
      "    num_steps_sampled: 1820000\n",
      "    num_steps_trained: 1820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.146472694849915e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22340930998325348\n",
      "      kl: 0.018924526870250702\n",
      "      policy_loss: 0.0006581234629265964\n",
      "      total_loss: 1.5967209339141846\n",
      "      vf_explained_var: 0.9985849261283875\n",
      "      vf_loss: 1.5960627794265747\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.463962727630004e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.47867196798324585\n",
      "      kl: 0.020466750487685204\n",
      "      policy_loss: 0.006378197111189365\n",
      "      total_loss: 0.3956744968891144\n",
      "      vf_explained_var: 0.9998724460601807\n",
      "      vf_loss: 0.3892962336540222\n",
      "    sample_time_ms: 21171.091\n",
      "    update_time_ms: 6.995\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 125.42488114560452\n",
      "    rl_1: 190.5851703739927\n",
      "  time_since_restore: 4970.640235185623\n",
      "  time_this_iter_s: 24.241777896881104\n",
      "  time_total_s: 4970.640235185623\n",
      "  timestamp: 1552576988\n",
      "  timesteps_since_restore: 1820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1820000\n",
      "  training_iteration: 182\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4970 s, 182 iter, 1820000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 139.15\n",
      "  episode_reward_max: 350.4667810946417\n",
      "  episode_reward_mean: 315.6469315352101\n",
      "  episode_reward_min: 286.4289467940506\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 12810\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3290.682\n",
      "    load_time_ms: 2.601\n",
      "    num_steps_sampled: 1830000\n",
      "    num_steps_trained: 1830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.146472694849915e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.244118794798851\n",
      "      kl: 0.009874409064650536\n",
      "      policy_loss: -0.0003022610617335886\n",
      "      total_loss: 1.4927520751953125\n",
      "      vf_explained_var: 0.9987204074859619\n",
      "      vf_loss: 1.4930540323257446\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.463962727630004e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.49035903811454773\n",
      "      kl: 0.02135876566171646\n",
      "      policy_loss: 0.010213575325906277\n",
      "      total_loss: 0.2997198700904846\n",
      "      vf_explained_var: 0.999904453754425\n",
      "      vf_loss: 0.2895062565803528\n",
      "    sample_time_ms: 21187.707\n",
      "    update_time_ms: 6.792\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 125.4121411659425\n",
      "    rl_1: 190.23479036926753\n",
      "  time_since_restore: 4994.625236034393\n",
      "  time_this_iter_s: 23.98500084877014\n",
      "  time_total_s: 4994.625236034393\n",
      "  timestamp: 1552577012\n",
      "  timesteps_since_restore: 1830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1830000\n",
      "  training_iteration: 183\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 4994 s, 183 iter, 1830000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-23-56\n",
      "  done: false\n",
      "  episode_len_mean: 138.11\n",
      "  episode_reward_max: 345.5081026364607\n",
      "  episode_reward_mean: 318.28277120856023\n",
      "  episode_reward_min: 279.74752402977816\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 12882\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3303.435\n",
      "    load_time_ms: 2.58\n",
      "    num_steps_sampled: 1840000\n",
      "    num_steps_trained: 1840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0732363474249576e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1638050675392151\n",
      "      kl: 0.01054112333804369\n",
      "      policy_loss: 0.0009808583417907357\n",
      "      total_loss: 1.714577555656433\n",
      "      vf_explained_var: 0.998563826084137\n",
      "      vf_loss: 1.7135965824127197\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.463962727630004e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4917992353439331\n",
      "      kl: 0.03825591504573822\n",
      "      policy_loss: 0.01967102661728859\n",
      "      total_loss: 0.325262188911438\n",
      "      vf_explained_var: 0.9998997449874878\n",
      "      vf_loss: 0.3055911064147949\n",
      "    sample_time_ms: 21254.058\n",
      "    update_time_ms: 6.759\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 127.70986986855127\n",
      "    rl_1: 190.5729013400089\n",
      "  time_since_restore: 5018.788122177124\n",
      "  time_this_iter_s: 24.162886142730713\n",
      "  time_total_s: 5018.788122177124\n",
      "  timestamp: 1552577036\n",
      "  timesteps_since_restore: 1840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1840000\n",
      "  training_iteration: 184\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 5018 s, 184 iter, 1840000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-24-22\n",
      "  done: false\n",
      "  episode_len_mean: 140.62\n",
      "  episode_reward_max: 346.29698737655207\n",
      "  episode_reward_mean: 315.6784571765328\n",
      "  episode_reward_min: 279.74752402977816\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 12954\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3533.787\n",
      "    load_time_ms: 2.583\n",
      "    num_steps_sampled: 1850000\n",
      "    num_steps_trained: 1850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0732363474249576e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.21886497735977173\n",
      "      kl: 0.015416436828672886\n",
      "      policy_loss: -0.001124072354286909\n",
      "      total_loss: 1.5014610290527344\n",
      "      vf_explained_var: 0.9986562728881836\n",
      "      vf_loss: 1.5025850534439087\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.463962727630004e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5067159533500671\n",
      "      kl: 7.052387714385986\n",
      "      policy_loss: 0.11163211613893509\n",
      "      total_loss: 0.45362111926078796\n",
      "      vf_explained_var: 0.9998875856399536\n",
      "      vf_loss: 0.34198904037475586\n",
      "    sample_time_ms: 21299.124\n",
      "    update_time_ms: 6.872\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 124.78178732013878\n",
      "    rl_1: 190.89666985639403\n",
      "  time_since_restore: 5045.011693954468\n",
      "  time_this_iter_s: 26.22357177734375\n",
      "  time_total_s: 5045.011693954468\n",
      "  timestamp: 1552577062\n",
      "  timesteps_since_restore: 1850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1850000\n",
      "  training_iteration: 185\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 5045 s, 185 iter, 1850000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-24-48\n",
      "  done: false\n",
      "  episode_len_mean: 144.68\n",
      "  episode_reward_max: 346.61037176112325\n",
      "  episode_reward_mean: 315.4376505917588\n",
      "  episode_reward_min: 278.97546433229957\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 13022\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3585.552\n",
      "    load_time_ms: 2.632\n",
      "    num_steps_sampled: 1860000\n",
      "    num_steps_trained: 1860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0732363474249576e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.30331987142562866\n",
      "      kl: 0.013309485279023647\n",
      "      policy_loss: -0.0005811771843582392\n",
      "      total_loss: 2.086196184158325\n",
      "      vf_explained_var: 0.9981202483177185\n",
      "      vf_loss: 2.0867772102355957\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.695944409082361e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.447441041469574\n",
      "      kl: 0.030514655634760857\n",
      "      policy_loss: 0.008778935298323631\n",
      "      total_loss: 0.3740856349468231\n",
      "      vf_explained_var: 0.9998759031295776\n",
      "      vf_loss: 0.36530664563179016\n",
      "    sample_time_ms: 21391.064\n",
      "    update_time_ms: 6.884\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 124.74293344575977\n",
      "    rl_1: 190.6947171459991\n",
      "  time_since_restore: 5070.430574178696\n",
      "  time_this_iter_s: 25.418880224227905\n",
      "  time_total_s: 5070.430574178696\n",
      "  timestamp: 1552577088\n",
      "  timesteps_since_restore: 1860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1860000\n",
      "  training_iteration: 186\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 5070 s, 186 iter, 1860000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-25-12\n",
      "  done: false\n",
      "  episode_len_mean: 149.08\n",
      "  episode_reward_max: 344.3082666478564\n",
      "  episode_reward_mean: 308.5262612433473\n",
      "  episode_reward_min: 278.97546433229957\n",
      "  episodes_this_iter: 67\n",
      "  episodes_total: 13089\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3580.748\n",
      "    load_time_ms: 2.589\n",
      "    num_steps_sampled: 1870000\n",
      "    num_steps_trained: 1870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0732363474249576e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3943055272102356\n",
      "      kl: 0.021918533369898796\n",
      "      policy_loss: 0.0012805780861526728\n",
      "      total_loss: 1.9705487489700317\n",
      "      vf_explained_var: 0.998008131980896\n",
      "      vf_loss: 1.9692684412002563\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.695944409082361e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4185481369495392\n",
      "      kl: 0.013685631565749645\n",
      "      policy_loss: 0.0037131928838789463\n",
      "      total_loss: 0.5762169361114502\n",
      "      vf_explained_var: 0.9998008608818054\n",
      "      vf_loss: 0.5725037455558777\n",
      "    sample_time_ms: 21399.005\n",
      "    update_time_ms: 6.97\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 119.02433192904724\n",
      "    rl_1: 189.50192931429999\n",
      "  time_since_restore: 5094.547306776047\n",
      "  time_this_iter_s: 24.116732597351074\n",
      "  time_total_s: 5094.547306776047\n",
      "  timestamp: 1552577112\n",
      "  timesteps_since_restore: 1870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1870000\n",
      "  training_iteration: 187\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 5094 s, 187 iter, 1870000 ts, 309 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-25-36\n",
      "  done: false\n",
      "  episode_len_mean: 147.47\n",
      "  episode_reward_max: 350.35129119929286\n",
      "  episode_reward_mean: 310.7308362927668\n",
      "  episode_reward_min: 10.488791554535364\n",
      "  episodes_this_iter: 70\n",
      "  episodes_total: 13159\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3458.959\n",
      "    load_time_ms: 2.464\n",
      "    num_steps_sampled: 1880000\n",
      "    num_steps_trained: 1880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0732363474249576e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2442498505115509\n",
      "      kl: 0.013157517649233341\n",
      "      policy_loss: -0.0037220281083136797\n",
      "      total_loss: 4.788316249847412\n",
      "      vf_explained_var: 0.9957464337348938\n",
      "      vf_loss: 4.7920379638671875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.695944409082361e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.42782899737358093\n",
      "      kl: 0.030963445082306862\n",
      "      policy_loss: 0.0024782137479633093\n",
      "      total_loss: 32.96244430541992\n",
      "      vf_explained_var: 0.9890235662460327\n",
      "      vf_loss: 32.95996856689453\n",
      "    sample_time_ms: 20756.581\n",
      "    update_time_ms: 6.763\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 121.9916540055386\n",
      "    rl_1: 188.7391822872282\n",
      "  time_since_restore: 5118.408704280853\n",
      "  time_this_iter_s: 23.86139750480652\n",
      "  time_total_s: 5118.408704280853\n",
      "  timestamp: 1552577136\n",
      "  timesteps_since_restore: 1880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1880000\n",
      "  training_iteration: 188\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 5118 s, 188 iter, 1880000 ts, 311 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-25-59\n",
      "  done: false\n",
      "  episode_len_mean: 144.39\n",
      "  episode_reward_max: 353.10604862527913\n",
      "  episode_reward_mean: 312.56348665613746\n",
      "  episode_reward_min: 10.400580736107893\n",
      "  episodes_this_iter: 67\n",
      "  episodes_total: 13226\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3461.403\n",
      "    load_time_ms: 2.406\n",
      "    num_steps_sampled: 1890000\n",
      "    num_steps_trained: 1890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0732363474249576e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2905527651309967\n",
      "      kl: 0.00864788331091404\n",
      "      policy_loss: -2.021103682636749e-05\n",
      "      total_loss: 2.158548355102539\n",
      "      vf_explained_var: 0.998120129108429\n",
      "      vf_loss: 2.158569097518921\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.695944409082361e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39612093567848206\n",
      "      kl: 0.03537376970052719\n",
      "      policy_loss: 0.00228447956033051\n",
      "      total_loss: 29.328022003173828\n",
      "      vf_explained_var: 0.9903318881988525\n",
      "      vf_loss: 29.32573699951172\n",
      "    sample_time_ms: 20790.717\n",
      "    update_time_ms: 7.233\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 124.00180328818661\n",
      "    rl_1: 188.5616833679508\n",
      "  time_since_restore: 5141.964240789413\n",
      "  time_this_iter_s: 23.55553650856018\n",
      "  time_total_s: 5141.964240789413\n",
      "  timestamp: 1552577159\n",
      "  timesteps_since_restore: 1890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1890000\n",
      "  training_iteration: 189\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 5141 s, 189 iter, 1890000 ts, 313 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-26-23\n",
      "  done: false\n",
      "  episode_len_mean: 145.24\n",
      "  episode_reward_max: 350.1908647660648\n",
      "  episode_reward_mean: 314.38821084049965\n",
      "  episode_reward_min: 10.400580736107893\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 13298\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3458.755\n",
      "    load_time_ms: 2.444\n",
      "    num_steps_sampled: 1900000\n",
      "    num_steps_trained: 1900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.366181737124788e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.15816277265548706\n",
      "      kl: 0.014654543250799179\n",
      "      policy_loss: 0.00043008619104512036\n",
      "      total_loss: 1.7386797666549683\n",
      "      vf_explained_var: 0.9985274076461792\n",
      "      vf_loss: 1.7382495403289795\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.695944409082361e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4468838572502136\n",
      "      kl: 151.75762939453125\n",
      "      policy_loss: 0.17197224497795105\n",
      "      total_loss: 0.6031932830810547\n",
      "      vf_explained_var: 0.9998562932014465\n",
      "      vf_loss: 0.431221067905426\n",
      "    sample_time_ms: 20756.963\n",
      "    update_time_ms: 7.231\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 125.32153844377845\n",
      "    rl_1: 189.0666723967213\n",
      "  time_since_restore: 5165.366711139679\n",
      "  time_this_iter_s: 23.402470350265503\n",
      "  time_total_s: 5165.366711139679\n",
      "  timestamp: 1552577183\n",
      "  timesteps_since_restore: 1900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1900000\n",
      "  training_iteration: 190\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 5165 s, 190 iter, 1900000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-14_16-26-46\n",
      "  done: false\n",
      "  episode_len_mean: 139.26\n",
      "  episode_reward_max: 352.725148774613\n",
      "  episode_reward_mean: 315.5418766361675\n",
      "  episode_reward_min: 279.86186329500157\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 13370\n",
      "  experiment_id: a07979e1ce864d8ea1fbe22a9b6262d7\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3465.75\n",
      "    load_time_ms: 2.508\n",
      "    num_steps_sampled: 1910000\n",
      "    num_steps_trained: 1910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.366181737124788e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2529289126396179\n",
      "      kl: 0.010446731932461262\n",
      "      policy_loss: -0.0018192071001976728\n",
      "      total_loss: 2.027759552001953\n",
      "      vf_explained_var: 0.9980742335319519\n",
      "      vf_loss: 2.029578685760498\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.543917460656489e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4630173444747925\n",
      "      kl: 1.87822425365448\n",
      "      policy_loss: 0.09303353726863861\n",
      "      total_loss: 0.5952600240707397\n",
      "      vf_explained_var: 0.9998310804367065\n",
      "      vf_loss: 0.5022264719009399\n",
      "    sample_time_ms: 20755.544\n",
      "    update_time_ms: 7.164\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 15100\n",
      "  policy_reward_mean:\n",
      "    rl_0: 124.03699738646166\n",
      "    rl_1: 191.50487924970582\n",
      "  time_since_restore: 5188.893274307251\n",
      "  time_this_iter_s: 23.52656316757202\n",
      "  time_total_s: 5188.893274307251\n",
      "  timestamp: 1552577206\n",
      "  timesteps_since_restore: 1910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1910000\n",
      "  training_iteration: 191\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=15100], 5188 s, 191 iter, 1910000 ts, 316 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,  # RL algorithm to run\n",
    "        \"env\": gym_name,  # environment name generated earlier\n",
    "        \"config\": {  # configuration params (must match \"run\" value)\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1000,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow_2)",
   "language": "python",
   "name": "flow_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
