{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING I3W\n",
    "\n",
    "\n",
    "# A) Create Envorinment, Vehicles etc\n",
    "\n",
    "### General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scenarios:\n",
      "['Scenario', 'BayBridgeScenario', 'BayBridgeTollScenario', 'BottleneckScenario', 'Figure8Scenario', 'SimpleGridScenario', 'HighwayScenario', 'LoopScenario', 'MergeScenario', 'TwoLoopsOneMergingScenario', 'MultiLoopScenario', 'IntersectionScenarioTW']\n",
      "\n",
      "Available environments:\n",
      "['MultiEnv', 'MultiAgentAccelEnv', 'MultiWaveAttenuationPOEnv', 'MultiAgentIntersectionEnv', 'MultiAgentTeamSpiritIntersectionEnv']\n"
     ]
    }
   ],
   "source": [
    "# Define horizon as a variable to ensure consistent use across notebook (length of one rollout)\n",
    "HORIZON=500                                 #103 max Horizon, wenn es vor verlassen abbrechen soll!, default war 500\n",
    "\n",
    "# name of the experiment\n",
    "experiment_name = \"IntersectionExample\"\n",
    "\n",
    "# scenario class\n",
    "import flow.scenarios as scenarios\n",
    "print(\"Available scenarios:\")\n",
    "print(scenarios.__all__)\n",
    "scenario_name = \"IntersectionTWScenario\"\n",
    "\n",
    "# environment class\n",
    "import flow.multiagent_envs as flowenvs\n",
    "print(\"\\nAvailable environments:\")\n",
    "print(flowenvs.__all__)\n",
    "env_name = \"MultiAgentIntersectionEnv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "from flow.scenarios.intersection import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "additionalNetParams = {\n",
    "            \"edge_length\": 40,\n",
    "            \"lanes\": 1,\n",
    "            \"speed_limit\": 30\n",
    "        }\n",
    "\n",
    "net_params = NetParams( no_internal_links=False,                  #default: True   !! damit Kreuzungen nicht Ã¼berspr. werden\n",
    "                        inflows=None,                             #default: None\n",
    "                        osm_path=None,                            #default: None\n",
    "                        netfile=None,                             #default: None\n",
    "                        additional_params=additionalNetParams     #default: None   !!\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InitialConfig Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "\n",
    "initial_config = InitialConfig( shuffle=True,                            #default: False         !!\n",
    "                                spacing=\"custom\",                        #default: \"uniform\"     !!\n",
    "                                min_gap=10,                              #default: 0\n",
    "                                perturbation=29.99,                      #default: 0.0            !!        \n",
    "                                x0=0,                                    #default: 0\n",
    "                                bunching=0,                              #default: 0\n",
    "                                lanes_distribution=float(\"inf\"),         #default: float(\"inf\")\n",
    "                                edges_distribution=\"all\",                #default: \"all\"\n",
    "                                additional_params=None )                 #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams( port = None,                  #default: None\n",
    "                          sim_step=0.1,                 #default: 0.1\n",
    "                          emission_path=None,           #default: None\n",
    "                          lateral_resolution=None,      #default: None\n",
    "                          no_step_log=True,             #default: True\n",
    "                          render=False,                 #default: False\n",
    "                          save_render=False,            #default: False\n",
    "                          sight_radius=25,              #default: 25\n",
    "                          show_radius=False,            #default: False\n",
    "                          pxpm=2,                       #default: 2\n",
    "                          overtake_right=False,         #default: False    \n",
    "                          seed=None,                    #default: None\n",
    "                          restart_instance=False,       #default: False\n",
    "                          print_warnings=True,          #default: True\n",
    "                          teleport_time=-1,             #default: -1\n",
    "                          num_clients=1,                #default: 1\n",
    "                          sumo_binary=None )            #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "additionalEnvParams = {\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 3,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 3,\n",
    "        \"target_velocity\": 30\n",
    "    }\n",
    "\n",
    "env_params = EnvParams( additional_params=additionalEnvParams, #default: None    !!\n",
    "                        horizon=HORIZON,                       #default: 500     !!\n",
    "                        warmup_steps=0,                        #default: 0       \n",
    "                        sims_per_step=1,                       #default: 1\n",
    "                        evaluate=False )                       #default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# import vehicles dynamics models\n",
    "#from flow.controllers import SumoCarFollowingController\n",
    "from flow.controllers import ContinuousRouter\n",
    "#from flow.controllers.lane_change_controllers import SumoLaneChangeController\n",
    "from flow.controllers.lane_change_controllers import StaticLaneChanger\n",
    "from flow.controllers import RLController\n",
    "from flow.core.params import SumoLaneChangeParams\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "from random import *\n",
    "\n",
    "vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RL-Agent controlled vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car following parameters, default: None\n",
    "cf_parameter = SumoCarFollowingParams(\n",
    "                speed_mode=\"aggressive\")\n",
    "# lane change parameters, default: None\n",
    "lc_parameter =  None\n",
    "\n",
    "vehicles.add( # name of the vehicle\n",
    "                veh_id = \"rl\",\n",
    "              # acceleration controller, default: (SumoCarFollowingController, {})\n",
    "                acceleration_controller=(RLController, {}),\n",
    "              # lane_change_controller, default: (SumoLaneChangeController, {})\n",
    "                lane_change_controller=(StaticLaneChanger,{}),\n",
    "              # routing controller, default: None\n",
    "                routing_controller=(ContinuousRouter, {}),\n",
    "              # initial speed, default: 0\n",
    "                initial_speed=0,\n",
    "              # number of vehicles, default: 1 \n",
    "                num_vehicles=2,\n",
    "                \n",
    "                car_following_params=cf_parameter\n",
    "              # speed mode, default: \"right_of_way\"\n",
    "                #speed_mode=\"aggressive\",\n",
    "              # lane change mode, default: \"no_lat_collide\"\n",
    "                #lane_change_mode=\"aggressive\", \n",
    "              # car following parameter, default: None\n",
    "                #sumo_car_following_params=cf_parameter,\n",
    "              # lane change parameter, default: None\n",
    "                #sumo_lc_params=lc_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict( # name of the experiment\n",
    "                      exp_tag=experiment_name,\n",
    "                    # name of the flow environment the experiment is running on\n",
    "                      env_name=env_name,\n",
    "                    # name of the scenario class the experiment uses\n",
    "                      scenario=scenario_name,\n",
    "                    # simulator that is used by the experiment\n",
    "                      simulator='traci',\n",
    "                    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "                      sim=sumo_params,\n",
    "                    # environment related parameters (see flow.core.params.EnvParams)\n",
    "                      env=env_params,\n",
    "                    # network-related parameters (see flow.core.params.NetParams and\n",
    "                    # the scenario's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "                      net=net_params,\n",
    "                    # vehicles to be placed in the network at the start of a rollout \n",
    "                    # (see flow.core.vehicles.Vehicles)\n",
    "                      veh=vehicles,\n",
    "                   # (optional) parameters affecting the positioning of vehicles upon \n",
    "                   # initialization/reset (see flow.core.params.InitialConfig)\n",
    "                      initial=initial_config\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-03-12_14-34-58_18114/logs.\n",
      "Waiting for redis server at 127.0.0.1:35001 to respond...\n",
      "Waiting for redis server at 127.0.0.1:17531 to respond...\n",
      "Starting the Plasma object store with 6.554658406 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=7fbdae9c001dcdf318cbc08369cccbca47552e3da1baed9c\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.16.123.117',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-03-12_14-34-58_18114/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-03-12_14-34-58_18114/sockets/raylet'],\n",
       " 'redis_address': '172.16.123.117:35001',\n",
       " 'webui_url': 'http://localhost:8889/notebooks/ray_ui.ipynb?token=7fbdae9c001dcdf318cbc08369cccbca47552e3da1baed9c'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "\n",
    "ray.init(redirect_output=True, num_cpus=N_CPUS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [64, 32]})  # size of hidden layers in network\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "#config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "#config[\"sample_batch_size\"] = config[\"train_batch_size\"]/config[\"num_workers\"] # 200 default, trotzdem zu hoch?\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 42261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.196257408802122\n",
      "3.55673754868485\n"
     ]
    }
   ],
   "source": [
    "# multi agent policy mapping\n",
    "test_env = create_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "\n",
    "def gen_policy():\n",
    "    return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "# Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "policy_graphs = {'rl_0': gen_policy(), 'rl_1': gen_policy()}\n",
    "    \n",
    "def policy_mapping_fn(agent_id):\n",
    "    return agent_id\n",
    "\n",
    "config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn)\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "\n",
      "Created LogSyncer for /home/thorsten/ray_results/IntersectionExample/PPO_MultiAgentIntersectionEnv-v0_0_2019-03-12_14-35-01ownx4tyw -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-36-35\n",
      "  done: false\n",
      "  episode_len_mean: 462.0952380952381\n",
      "  episode_reward_max: 177.99807200266918\n",
      "  episode_reward_mean: 51.85029512876206\n",
      "  episode_reward_min: -141.171780641903\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 21\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4574.424\n",
      "    load_time_ms: 159.314\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4203084707260132\n",
      "      kl: 0.0020100194960832596\n",
      "      policy_loss: -0.0028228494338691235\n",
      "      total_loss: 65.3825912475586\n",
      "      vf_explained_var: -0.0009856453398242593\n",
      "      vf_loss: 65.38501739501953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.421116590499878\n",
      "      kl: 0.002224639058113098\n",
      "      policy_loss: -0.002272198675200343\n",
      "      total_loss: 70.36119842529297\n",
      "      vf_explained_var: 0.022267362102866173\n",
      "      vf_loss: 70.36302947998047\n",
      "    sample_time_ms: 42219.689\n",
      "    update_time_ms: 3203.944\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 19.458875151664543\n",
      "    rl_1: 32.39141997709754\n",
      "  time_since_restore: 50.290170192718506\n",
      "  time_this_iter_s: 50.290170192718506\n",
      "  time_total_s: 50.290170192718506\n",
      "  timestamp: 1552397795\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 50 s, 1 iter, 10000 ts, 51.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-37-00\n",
      "  done: false\n",
      "  episode_len_mean: 452.4651162790698\n",
      "  episode_reward_max: 177.99807200266918\n",
      "  episode_reward_mean: 41.00475272701886\n",
      "  episode_reward_min: -154.21663104131054\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 43\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3896.577\n",
      "    load_time_ms: 80.787\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4136780500411987\n",
      "      kl: 0.0011593750678002834\n",
      "      policy_loss: -0.0009084278717637062\n",
      "      total_loss: 105.07447052001953\n",
      "      vf_explained_var: 0.04057629033923149\n",
      "      vf_loss: 105.07524871826172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.41893470287323\n",
      "      kl: 0.004817398265004158\n",
      "      policy_loss: -0.002790606115013361\n",
      "      total_loss: 107.4573974609375\n",
      "      vf_explained_var: 0.07982049882411957\n",
      "      vf_loss: 107.45968627929688\n",
      "    sample_time_ms: 32092.473\n",
      "    update_time_ms: 1605.516\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.001511658213143\n",
      "    rl_1: 25.00324106880574\n",
      "  time_since_restore: 75.50011801719666\n",
      "  time_this_iter_s: 25.20994782447815\n",
      "  time_total_s: 75.50011801719666\n",
      "  timestamp: 1552397820\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 75 s, 2 iter, 20000 ts, 41 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-37-27\n",
      "  done: false\n",
      "  episode_len_mean: 442.6865671641791\n",
      "  episode_reward_max: 244.5135043665767\n",
      "  episode_reward_mean: 56.578861988037644\n",
      "  episode_reward_min: -157.35891022829117\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 67\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3688.973\n",
      "    load_time_ms: 54.64\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.418462872505188\n",
      "      kl: 0.009837759658694267\n",
      "      policy_loss: -0.006151365116238594\n",
      "      total_loss: 102.0279541015625\n",
      "      vf_explained_var: 0.05130567401647568\n",
      "      vf_loss: 102.03360748291016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4194419384002686\n",
      "      kl: 0.002012198092415929\n",
      "      policy_loss: -0.0015211814315989614\n",
      "      total_loss: 130.6625213623047\n",
      "      vf_explained_var: 0.27300140261650085\n",
      "      vf_loss: 130.66390991210938\n",
      "    sample_time_ms: 29222.797\n",
      "    update_time_ms: 1074.889\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.623235059143006\n",
      "    rl_1: 39.95562692889469\n",
      "  time_since_restore: 102.28856420516968\n",
      "  time_this_iter_s: 26.788446187973022\n",
      "  time_total_s: 102.28856420516968\n",
      "  timestamp: 1552397847\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 102 s, 3 iter, 30000 ts, 56.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-37-51\n",
      "  done: false\n",
      "  episode_len_mean: 443.65168539325845\n",
      "  episode_reward_max: 250.38823021362302\n",
      "  episode_reward_mean: 68.07112261362205\n",
      "  episode_reward_min: -161.12027829879614\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 89\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3568.241\n",
      "    load_time_ms: 41.521\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4232869148254395\n",
      "      kl: 0.0019882412161678076\n",
      "      policy_loss: -0.0018211717251688242\n",
      "      total_loss: 66.03504943847656\n",
      "      vf_explained_var: 0.1883552521467209\n",
      "      vf_loss: 66.03681945800781\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4184365272521973\n",
      "      kl: 0.005021692253649235\n",
      "      policy_loss: -0.0032183516304939985\n",
      "      total_loss: 104.05774688720703\n",
      "      vf_explained_var: 0.3827824294567108\n",
      "      vf_loss: 104.06084442138672\n",
      "    sample_time_ms: 27137.224\n",
      "    update_time_ms: 808.287\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 18.885171663288396\n",
      "    rl_1: 49.18595095033368\n",
      "  time_since_restore: 126.401043176651\n",
      "  time_this_iter_s: 24.112478971481323\n",
      "  time_total_s: 126.401043176651\n",
      "  timestamp: 1552397871\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 126 s, 4 iter, 40000 ts, 68.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-38-15\n",
      "  done: false\n",
      "  episode_len_mean: 443.56\n",
      "  episode_reward_max: 271.2659360999774\n",
      "  episode_reward_mean: 97.34473175555858\n",
      "  episode_reward_min: -161.12027829879614\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 112\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3497.572\n",
      "    load_time_ms: 33.665\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4250469207763672\n",
      "      kl: 0.0028582620434463024\n",
      "      policy_loss: -0.0028415319975465536\n",
      "      total_loss: 36.61734390258789\n",
      "      vf_explained_var: 0.40489599108695984\n",
      "      vf_loss: 36.62015151977539\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.41482675075531\n",
      "      kl: 0.00896541029214859\n",
      "      policy_loss: -0.003199500497430563\n",
      "      total_loss: 116.6922607421875\n",
      "      vf_explained_var: 0.5445774793624878\n",
      "      vf_loss: 116.69536590576172\n",
      "    sample_time_ms: 25791.994\n",
      "    update_time_ms: 648.434\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.95415340555379\n",
      "    rl_1: 70.39057835000483\n",
      "  time_since_restore: 150.0544147491455\n",
      "  time_this_iter_s: 23.653371572494507\n",
      "  time_total_s: 150.0544147491455\n",
      "  timestamp: 1552397895\n",
      "  timesteps_since_restore: 50000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 5\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 150 s, 5 iter, 50000 ts, 97.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-38-40\n",
      "  done: false\n",
      "  episode_len_mean: 416.55\n",
      "  episode_reward_max: 271.2659360999774\n",
      "  episode_reward_mean: 132.0436259129416\n",
      "  episode_reward_min: -165.23170337734985\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 140\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3546.157\n",
      "    load_time_ms: 28.417\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4136534929275513\n",
      "      kl: 0.006396283861249685\n",
      "      policy_loss: -0.0018972709076479077\n",
      "      total_loss: 37.11015701293945\n",
      "      vf_explained_var: 0.42036840319633484\n",
      "      vf_loss: 37.11201477050781\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3981736898422241\n",
      "      kl: 0.002573129255324602\n",
      "      policy_loss: -0.0017098936950787902\n",
      "      total_loss: 190.3069610595703\n",
      "      vf_explained_var: 0.5250268578529358\n",
      "      vf_loss: 190.3086395263672\n",
      "    sample_time_ms: 24996.176\n",
      "    update_time_ms: 541.629\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.988315518068116\n",
      "    rl_1: 100.05531039487349\n",
      "  time_since_restore: 174.88625240325928\n",
      "  time_this_iter_s: 24.83183765411377\n",
      "  time_total_s: 174.88625240325928\n",
      "  timestamp: 1552397920\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 174 s, 6 iter, 60000 ts, 132 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-39-07\n",
      "  done: false\n",
      "  episode_len_mean: 389.88\n",
      "  episode_reward_max: 271.2659360999774\n",
      "  episode_reward_mean: 158.26682325991777\n",
      "  episode_reward_min: -165.23170337734985\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 169\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3536.954\n",
      "    load_time_ms: 24.696\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.410622477531433\n",
      "      kl: 0.02333689294755459\n",
      "      policy_loss: -0.00635403860360384\n",
      "      total_loss: 54.73685836791992\n",
      "      vf_explained_var: 0.4719657301902771\n",
      "      vf_loss: 54.7431526184082\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3994109630584717\n",
      "      kl: 0.008258488029241562\n",
      "      policy_loss: -0.002153570530936122\n",
      "      total_loss: 214.02870178222656\n",
      "      vf_explained_var: 0.5712922811508179\n",
      "      vf_loss: 214.03082275390625\n",
      "    sample_time_ms: 24826.148\n",
      "    update_time_ms: 465.281\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.48772190177405\n",
      "    rl_1: 119.7791013581437\n",
      "  time_since_restore: 202.20235133171082\n",
      "  time_this_iter_s: 27.316098928451538\n",
      "  time_total_s: 202.20235133171082\n",
      "  timestamp: 1552397947\n",
      "  timesteps_since_restore: 70000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 202 s, 7 iter, 70000 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-39-35\n",
      "  done: false\n",
      "  episode_len_mean: 347.42\n",
      "  episode_reward_max: 269.6495996763386\n",
      "  episode_reward_mean: 184.1329504044457\n",
      "  episode_reward_min: -165.23170337734985\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 201\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3491.853\n",
      "    load_time_ms: 21.882\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4230512380599976\n",
      "      kl: 0.0013395454734563828\n",
      "      policy_loss: -0.0006639821804128587\n",
      "      total_loss: 44.381744384765625\n",
      "      vf_explained_var: 0.5771566033363342\n",
      "      vf_loss: 44.38240051269531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.396723747253418\n",
      "      kl: 0.007267711218446493\n",
      "      policy_loss: -0.0024165844079107046\n",
      "      total_loss: 250.1754608154297\n",
      "      vf_explained_var: 0.5271787643432617\n",
      "      vf_loss: 250.1778106689453\n",
      "    sample_time_ms: 24850.479\n",
      "    update_time_ms: 407.863\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.28829585194671\n",
      "    rl_1: 140.844654552499\n",
      "  time_since_restore: 230.42289662361145\n",
      "  time_this_iter_s: 28.220545291900635\n",
      "  time_total_s: 230.42289662361145\n",
      "  timestamp: 1552397975\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 230 s, 8 iter, 80000 ts, 184 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-40-02\n",
      "  done: false\n",
      "  episode_len_mean: 316.04\n",
      "  episode_reward_max: 278.89512259050395\n",
      "  episode_reward_mean: 204.4679364601342\n",
      "  episode_reward_min: -161.6549615926662\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 236\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3456.288\n",
      "    load_time_ms: 19.744\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.415627360343933\n",
      "      kl: 0.003316056216135621\n",
      "      policy_loss: -0.0021722428500652313\n",
      "      total_loss: 56.299041748046875\n",
      "      vf_explained_var: 0.48870980739593506\n",
      "      vf_loss: 56.30120849609375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3909775018692017\n",
      "      kl: 0.0050976029597222805\n",
      "      policy_loss: -0.002711926819756627\n",
      "      total_loss: 344.5398254394531\n",
      "      vf_explained_var: 0.4189096987247467\n",
      "      vf_loss: 344.54254150390625\n",
      "    sample_time_ms: 24664.361\n",
      "    update_time_ms: 363.921\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.02708119286737\n",
      "    rl_1: 155.4408552672669\n",
      "  time_since_restore: 256.8025095462799\n",
      "  time_this_iter_s: 26.379612922668457\n",
      "  time_total_s: 256.8025095462799\n",
      "  timestamp: 1552398002\n",
      "  timesteps_since_restore: 90000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 9\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 256 s, 9 iter, 90000 ts, 204 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-40-27\n",
      "  done: false\n",
      "  episode_len_mean: 294.47\n",
      "  episode_reward_max: 278.89512259050395\n",
      "  episode_reward_mean: 215.40306944313127\n",
      "  episode_reward_min: -153.57586899538597\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 272\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3428.637\n",
      "    load_time_ms: 18.061\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.417761206626892\n",
      "      kl: 0.0025372228119522333\n",
      "      policy_loss: -0.0008997512049973011\n",
      "      total_loss: 70.6082763671875\n",
      "      vf_explained_var: 0.5928749442100525\n",
      "      vf_loss: 70.60917663574219\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4072749614715576\n",
      "      kl: 0.003449181327596307\n",
      "      policy_loss: -0.0013197404332458973\n",
      "      total_loss: 357.339111328125\n",
      "      vf_explained_var: 0.5407754778862\n",
      "      vf_loss: 357.3404846191406\n",
      "    sample_time_ms: 24339.666\n",
      "    update_time_ms: 328.216\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.328533730672135\n",
      "    rl_1: 164.07453571245918\n",
      "  time_since_restore: 281.42707204818726\n",
      "  time_this_iter_s: 24.62456250190735\n",
      "  time_total_s: 281.42707204818726\n",
      "  timestamp: 1552398027\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 281 s, 10 iter, 100000 ts, 215 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-40-51\n",
      "  done: false\n",
      "  episode_len_mean: 270.43\n",
      "  episode_reward_max: 278.89512259050395\n",
      "  episode_reward_mean: 208.8871976888167\n",
      "  episode_reward_min: -158.58859194153015\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 312\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3288.056\n",
      "    load_time_ms: 2.344\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3993749618530273\n",
      "      kl: 0.0017284556524828076\n",
      "      policy_loss: -0.0007239271653816104\n",
      "      total_loss: 93.02238464355469\n",
      "      vf_explained_var: 0.4496018886566162\n",
      "      vf_loss: 93.02310943603516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.388777732849121\n",
      "      kl: 0.009284268133342266\n",
      "      policy_loss: -0.00312087987549603\n",
      "      total_loss: 465.4696044921875\n",
      "      vf_explained_var: 0.4859824478626251\n",
      "      vf_loss: 465.4727478027344\n",
      "    sample_time_ms: 22192.425\n",
      "    update_time_ms: 8.542\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.12378495378187\n",
      "    rl_1: 160.76341273503488\n",
      "  time_since_restore: 305.3685007095337\n",
      "  time_this_iter_s: 23.941428661346436\n",
      "  time_total_s: 305.3685007095337\n",
      "  timestamp: 1552398051\n",
      "  timesteps_since_restore: 110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 11\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 305 s, 11 iter, 110000 ts, 209 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-41-15\n",
      "  done: false\n",
      "  episode_len_mean: 240.7\n",
      "  episode_reward_max: 276.4466590736997\n",
      "  episode_reward_mean: 209.83549274896134\n",
      "  episode_reward_min: -158.58859194153015\n",
      "  episodes_this_iter: 47\n",
      "  episodes_total: 359\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3284.031\n",
      "    load_time_ms: 2.375\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3763478994369507\n",
      "      kl: 0.005420477129518986\n",
      "      policy_loss: -0.0015054718824103475\n",
      "      total_loss: 72.44705200195312\n",
      "      vf_explained_var: 0.4704969823360443\n",
      "      vf_loss: 72.44856262207031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3497323989868164\n",
      "      kl: 0.005128193646669388\n",
      "      policy_loss: -0.0013722237199544907\n",
      "      total_loss: 538.8407592773438\n",
      "      vf_explained_var: 0.3051246702671051\n",
      "      vf_loss: 538.8422241210938\n",
      "    sample_time_ms: 22081.796\n",
      "    update_time_ms: 8.552\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.79176984824342\n",
      "    rl_1: 164.0437229007179\n",
      "  time_since_restore: 329.43414330482483\n",
      "  time_this_iter_s: 24.065642595291138\n",
      "  time_total_s: 329.43414330482483\n",
      "  timestamp: 1552398075\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 329 s, 12 iter, 120000 ts, 210 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-41-39\n",
      "  done: false\n",
      "  episode_len_mean: 227.22\n",
      "  episode_reward_max: 276.4466590736997\n",
      "  episode_reward_mean: 213.60819924180296\n",
      "  episode_reward_min: -166.9967295674996\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 400\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3273.456\n",
      "    load_time_ms: 2.356\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3909536600112915\n",
      "      kl: 0.0026879431679844856\n",
      "      policy_loss: -0.0008896338986232877\n",
      "      total_loss: 65.93761444091797\n",
      "      vf_explained_var: 0.6581522822380066\n",
      "      vf_loss: 65.9384994506836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3367383480072021\n",
      "      kl: 0.018447216600179672\n",
      "      policy_loss: -0.0033188480883836746\n",
      "      total_loss: 464.86920166015625\n",
      "      vf_explained_var: 0.5737199783325195\n",
      "      vf_loss: 464.87249755859375\n",
      "    sample_time_ms: 21852.252\n",
      "    update_time_ms: 7.802\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 46.08007932913426\n",
      "    rl_1: 167.52811991266879\n",
      "  time_since_restore: 353.81418633461\n",
      "  time_this_iter_s: 24.380043029785156\n",
      "  time_total_s: 353.81418633461\n",
      "  timestamp: 1552398099\n",
      "  timesteps_since_restore: 130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 13\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 353 s, 13 iter, 130000 ts, 214 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-42-05\n",
      "  done: false\n",
      "  episode_len_mean: 219.27\n",
      "  episode_reward_max: 274.3828659016818\n",
      "  episode_reward_mean: 196.7744308631423\n",
      "  episode_reward_min: -166.9967295674996\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 450\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3269.184\n",
      "    load_time_ms: 2.358\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3534352779388428\n",
      "      kl: 0.01909422129392624\n",
      "      policy_loss: -0.003769516246393323\n",
      "      total_loss: 136.33963012695312\n",
      "      vf_explained_var: 0.47951650619506836\n",
      "      vf_loss: 136.3433837890625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3076355457305908\n",
      "      kl: 0.003053460968658328\n",
      "      policy_loss: -0.002230497309938073\n",
      "      total_loss: 597.4736328125\n",
      "      vf_explained_var: 0.4381767809391022\n",
      "      vf_loss: 597.4758911132812\n",
      "    sample_time_ms: 22008.433\n",
      "    update_time_ms: 7.612\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.861530389260025\n",
      "    rl_1: 156.91290047388227\n",
      "  time_since_restore: 379.44673204421997\n",
      "  time_this_iter_s: 25.632545709609985\n",
      "  time_total_s: 379.44673204421997\n",
      "  timestamp: 1552398125\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 14\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 379 s, 14 iter, 140000 ts, 197 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-42-30\n",
      "  done: false\n",
      "  episode_len_mean: 207.46\n",
      "  episode_reward_max: 275.6967459544674\n",
      "  episode_reward_mean: 183.66448722667204\n",
      "  episode_reward_min: -150.69026983776095\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 498\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3261.901\n",
      "    load_time_ms: 2.43\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3759373426437378\n",
      "      kl: 0.0054731774143874645\n",
      "      policy_loss: -0.0013969348510727286\n",
      "      total_loss: 123.9571533203125\n",
      "      vf_explained_var: 0.5947438478469849\n",
      "      vf_loss: 123.95857238769531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2952054738998413\n",
      "      kl: 0.01305621862411499\n",
      "      policy_loss: -0.0032860797364264727\n",
      "      total_loss: 516.5904541015625\n",
      "      vf_explained_var: 0.481974720954895\n",
      "      vf_loss: 516.5936889648438\n",
      "    sample_time_ms: 22203.214\n",
      "    update_time_ms: 7.315\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.586860243976\n",
      "    rl_1: 148.07762698269605\n",
      "  time_since_restore: 404.9747431278229\n",
      "  time_this_iter_s: 25.528011083602905\n",
      "  time_total_s: 404.9747431278229\n",
      "  timestamp: 1552398150\n",
      "  timesteps_since_restore: 150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 15\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 404 s, 15 iter, 150000 ts, 184 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-42-56\n",
      "  done: false\n",
      "  episode_len_mean: 232.08\n",
      "  episode_reward_max: 284.19984033290035\n",
      "  episode_reward_mean: 195.53037724545936\n",
      "  episode_reward_min: -164.78027090443703\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 535\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3270.168\n",
      "    load_time_ms: 2.431\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4263074398040771\n",
      "      kl: 0.0042649609968066216\n",
      "      policy_loss: -0.0019940724596381187\n",
      "      total_loss: 54.379581451416016\n",
      "      vf_explained_var: 0.821628749370575\n",
      "      vf_loss: 54.38157272338867\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.363125205039978\n",
      "      kl: 0.017363552004098892\n",
      "      policy_loss: -0.004980253987014294\n",
      "      total_loss: 399.7890625\n",
      "      vf_explained_var: 0.7467034459114075\n",
      "      vf_loss: 399.7940673828125\n",
      "    sample_time_ms: 22238.626\n",
      "    update_time_ms: 7.162\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.21735121628768\n",
      "    rl_1: 152.3130260291717\n",
      "  time_since_restore: 430.2419738769531\n",
      "  time_this_iter_s: 25.26723074913025\n",
      "  time_total_s: 430.2419738769531\n",
      "  timestamp: 1552398176\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 16\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 430 s, 16 iter, 160000 ts, 196 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-43-20\n",
      "  done: false\n",
      "  episode_len_mean: 235.03\n",
      "  episode_reward_max: 284.19984033290035\n",
      "  episode_reward_mean: 211.75733293984854\n",
      "  episode_reward_min: -164.78027090443703\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 580\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3250.609\n",
      "    load_time_ms: 2.488\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3838984966278076\n",
      "      kl: 0.002459279028698802\n",
      "      policy_loss: -0.0012161030899733305\n",
      "      total_loss: 47.84977722167969\n",
      "      vf_explained_var: 0.8176055550575256\n",
      "      vf_loss: 47.85099411010742\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2785569429397583\n",
      "      kl: 0.0043205576948821545\n",
      "      policy_loss: -0.0013564035762101412\n",
      "      total_loss: 480.7049255371094\n",
      "      vf_explained_var: 0.6942083239555359\n",
      "      vf_loss: 480.7062683105469\n",
      "    sample_time_ms: 21994.859\n",
      "    update_time_ms: 7.216\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.064906289944595\n",
      "    rl_1: 161.6924266499039\n",
      "  time_since_restore: 454.9276466369629\n",
      "  time_this_iter_s: 24.685672760009766\n",
      "  time_total_s: 454.9276466369629\n",
      "  timestamp: 1552398200\n",
      "  timesteps_since_restore: 170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 17\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 454 s, 17 iter, 170000 ts, 212 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-43-45\n",
      "  done: false\n",
      "  episode_len_mean: 223.8\n",
      "  episode_reward_max: 279.1229116779168\n",
      "  episode_reward_mean: 232.99905694476692\n",
      "  episode_reward_min: -135.04329798017892\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 626\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3271.251\n",
      "    load_time_ms: 2.486\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3530298471450806\n",
      "      kl: 0.007985660806298256\n",
      "      policy_loss: -0.004467647057026625\n",
      "      total_loss: 35.45383071899414\n",
      "      vf_explained_var: 0.838084876537323\n",
      "      vf_loss: 35.45829391479492\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.245173692703247\n",
      "      kl: 0.0029732114635407925\n",
      "      policy_loss: -0.0005939183756709099\n",
      "      total_loss: 522.3418579101562\n",
      "      vf_explained_var: 0.6411138772964478\n",
      "      vf_loss: 522.3424682617188\n",
      "    sample_time_ms: 21574.682\n",
      "    update_time_ms: 7.299\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.03068315320904\n",
      "    rl_1: 176.96837379155798\n",
      "  time_since_restore: 479.15798830986023\n",
      "  time_this_iter_s: 24.23034167289734\n",
      "  time_total_s: 479.15798830986023\n",
      "  timestamp: 1552398225\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 18\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 479 s, 18 iter, 180000 ts, 233 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 221.91\n",
      "  episode_reward_max: 276.8713119626218\n",
      "  episode_reward_mean: 235.20599002542403\n",
      "  episode_reward_min: -153.38145899130322\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 670\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3316.556\n",
      "    load_time_ms: 2.517\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3960760831832886\n",
      "      kl: 0.0032820533961057663\n",
      "      policy_loss: -0.0012778382515534759\n",
      "      total_loss: 28.22602081298828\n",
      "      vf_explained_var: 0.8754867911338806\n",
      "      vf_loss: 28.227294921875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.28053879737854\n",
      "      kl: 0.013124710880219936\n",
      "      policy_loss: -0.0019170105224475265\n",
      "      total_loss: 485.61810302734375\n",
      "      vf_explained_var: 0.7169646620750427\n",
      "      vf_loss: 485.62005615234375\n",
      "    sample_time_ms: 21670.397\n",
      "    update_time_ms: 6.729\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.46633932599448\n",
      "    rl_1: 178.73965069942957\n",
      "  time_since_restore: 506.9553711414337\n",
      "  time_this_iter_s: 27.797382831573486\n",
      "  time_total_s: 506.9553711414337\n",
      "  timestamp: 1552398253\n",
      "  timesteps_since_restore: 190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 19\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 506 s, 19 iter, 190000 ts, 235 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-44-52\n",
      "  done: false\n",
      "  episode_len_mean: 225.37\n",
      "  episode_reward_max: 276.8713119626218\n",
      "  episode_reward_mean: 227.00223891505678\n",
      "  episode_reward_min: -154.72176574546071\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 715\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3943.722\n",
      "    load_time_ms: 2.55\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.395506501197815\n",
      "      kl: 0.0090343551710248\n",
      "      policy_loss: -0.003080629510805011\n",
      "      total_loss: 83.39285278320312\n",
      "      vf_explained_var: 0.6969831585884094\n",
      "      vf_loss: 83.39593505859375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2826682329177856\n",
      "      kl: 0.0010756445117294788\n",
      "      policy_loss: -0.0014830866130068898\n",
      "      total_loss: 513.3446044921875\n",
      "      vf_explained_var: 0.6361474990844727\n",
      "      vf_loss: 513.3461303710938\n",
      "    sample_time_ms: 22521.923\n",
      "    update_time_ms: 8.704\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.38629716490011\n",
      "    rl_1: 172.61594175015668\n",
      "  time_since_restore: 546.4021282196045\n",
      "  time_this_iter_s: 39.446757078170776\n",
      "  time_total_s: 546.4021282196045\n",
      "  timestamp: 1552398292\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 20\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 546 s, 20 iter, 200000 ts, 227 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 220.56\n",
      "  episode_reward_max: 276.15960461104146\n",
      "  episode_reward_mean: 230.3833222755041\n",
      "  episode_reward_min: -154.72176574546071\n",
      "  episodes_this_iter: 47\n",
      "  episodes_total: 762\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4162.418\n",
      "    load_time_ms: 2.678\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3393938541412354\n",
      "      kl: 0.015278058126568794\n",
      "      policy_loss: -0.005240389611572027\n",
      "      total_loss: 22.33716583251953\n",
      "      vf_explained_var: 0.876435399055481\n",
      "      vf_loss: 22.342409133911133\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2175153493881226\n",
      "      kl: 0.002844866830855608\n",
      "      policy_loss: -0.0010113234166055918\n",
      "      total_loss: 504.3460998535156\n",
      "      vf_explained_var: 0.6225559711456299\n",
      "      vf_loss: 504.3470458984375\n",
      "    sample_time_ms: 24126.869\n",
      "    update_time_ms: 9.667\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.864826188292774\n",
      "    rl_1: 175.5184960872113\n",
      "  time_since_restore: 588.5941212177277\n",
      "  time_this_iter_s: 42.19199299812317\n",
      "  time_total_s: 588.5941212177277\n",
      "  timestamp: 1552398334\n",
      "  timesteps_since_restore: 210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 21\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 588 s, 21 iter, 210000 ts, 230 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-46-10\n",
      "  done: false\n",
      "  episode_len_mean: 207.15\n",
      "  episode_reward_max: 276.15960461104146\n",
      "  episode_reward_mean: 238.14934404705028\n",
      "  episode_reward_min: -150.87395820686902\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 812\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4418.68\n",
      "    load_time_ms: 3.048\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.347275733947754\n",
      "      kl: 0.001171817653812468\n",
      "      policy_loss: -0.0003591243003029376\n",
      "      total_loss: 87.89240264892578\n",
      "      vf_explained_var: 0.6108362674713135\n",
      "      vf_loss: 87.89276123046875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2175917625427246\n",
      "      kl: 0.0029568285681307316\n",
      "      policy_loss: -0.0007583676488138735\n",
      "      total_loss: 576.8800048828125\n",
      "      vf_explained_var: 0.4152129292488098\n",
      "      vf_loss: 576.8807373046875\n",
      "    sample_time_ms: 25037.079\n",
      "    update_time_ms: 9.827\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.60252446551199\n",
      "    rl_1: 181.54681958153824\n",
      "  time_since_restore: 624.3340585231781\n",
      "  time_this_iter_s: 35.73993730545044\n",
      "  time_total_s: 624.3340585231781\n",
      "  timestamp: 1552398370\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 22\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 624 s, 22 iter, 220000 ts, 238 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-46-40\n",
      "  done: false\n",
      "  episode_len_mean: 207.3\n",
      "  episode_reward_max: 277.25138244082325\n",
      "  episode_reward_mean: 236.37734338645691\n",
      "  episode_reward_min: -150.87395820686902\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 860\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4438.704\n",
      "    load_time_ms: 3.088\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3261677026748657\n",
      "      kl: 0.007636478170752525\n",
      "      policy_loss: -0.001979199005290866\n",
      "      total_loss: 43.614959716796875\n",
      "      vf_explained_var: 0.7585138082504272\n",
      "      vf_loss: 43.616939544677734\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2071661949157715\n",
      "      kl: 0.003772462485358119\n",
      "      policy_loss: -0.002141183242201805\n",
      "      total_loss: 527.4848022460938\n",
      "      vf_explained_var: 0.48958921432495117\n",
      "      vf_loss: 527.4868774414062\n",
      "    sample_time_ms: 25576.638\n",
      "    update_time_ms: 10.949\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.16186560657467\n",
      "    rl_1: 180.21547777988226\n",
      "  time_since_restore: 654.323456287384\n",
      "  time_this_iter_s: 29.989397764205933\n",
      "  time_total_s: 654.323456287384\n",
      "  timestamp: 1552398400\n",
      "  timesteps_since_restore: 230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 23\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 654 s, 23 iter, 230000 ts, 236 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-47-08\n",
      "  done: false\n",
      "  episode_len_mean: 214.65\n",
      "  episode_reward_max: 281.89678741148253\n",
      "  episode_reward_mean: 238.64366072824367\n",
      "  episode_reward_min: -148.5130223696808\n",
      "  episodes_this_iter: 43\n",
      "  episodes_total: 903\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4454.044\n",
      "    load_time_ms: 3.115\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.379624366760254\n",
      "      kl: 0.006826899945735931\n",
      "      policy_loss: -0.0016932174330577254\n",
      "      total_loss: 62.79336166381836\n",
      "      vf_explained_var: 0.7395099997520447\n",
      "      vf_loss: 62.795047760009766\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2860119342803955\n",
      "      kl: 0.009199314750730991\n",
      "      policy_loss: -0.0033770722802728415\n",
      "      total_loss: 462.7239685058594\n",
      "      vf_explained_var: 0.6363245844841003\n",
      "      vf_loss: 462.7273254394531\n",
      "    sample_time_ms: 25784.86\n",
      "    update_time_ms: 10.906\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.86531574733196\n",
      "    rl_1: 180.77834498091167\n",
      "  time_since_restore: 682.1920328140259\n",
      "  time_this_iter_s: 27.868576526641846\n",
      "  time_total_s: 682.1920328140259\n",
      "  timestamp: 1552398428\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 24\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 682 s, 24 iter, 240000 ts, 239 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-47-36\n",
      "  done: false\n",
      "  episode_len_mean: 225.23\n",
      "  episode_reward_max: 328.06272142414383\n",
      "  episode_reward_mean: 241.12618376142424\n",
      "  episode_reward_min: -148.5130223696808\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 948\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4480.388\n",
      "    load_time_ms: 3.11\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3614243268966675\n",
      "      kl: 0.001630291691981256\n",
      "      policy_loss: -0.001613517990335822\n",
      "      total_loss: 22.779687881469727\n",
      "      vf_explained_var: 0.8559950590133667\n",
      "      vf_loss: 22.78130340576172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2598235607147217\n",
      "      kl: 0.0038433794397860765\n",
      "      policy_loss: -0.0012270845472812653\n",
      "      total_loss: 481.29693603515625\n",
      "      vf_explained_var: 0.6625934839248657\n",
      "      vf_loss: 481.2981262207031\n",
      "    sample_time_ms: 25995.18\n",
      "    update_time_ms: 11.123\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.22969627373125\n",
      "    rl_1: 180.896487487693\n",
      "  time_since_restore: 710.0882844924927\n",
      "  time_this_iter_s: 27.896251678466797\n",
      "  time_total_s: 710.0882844924927\n",
      "  timestamp: 1552398456\n",
      "  timesteps_since_restore: 250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 25\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 710 s, 25 iter, 250000 ts, 241 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-48-08\n",
      "  done: false\n",
      "  episode_len_mean: 203.66\n",
      "  episode_reward_max: 328.06272142414383\n",
      "  episode_reward_mean: 236.9482956833141\n",
      "  episode_reward_min: -154.16148843906913\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 1002\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4439.27\n",
      "    load_time_ms: 3.132\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2474439144134521\n",
      "      kl: 0.007740604691207409\n",
      "      policy_loss: -0.0016538107302039862\n",
      "      total_loss: 66.0319595336914\n",
      "      vf_explained_var: 0.6725601553916931\n",
      "      vf_loss: 66.03359985351562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1418650150299072\n",
      "      kl: 0.0022595031186938286\n",
      "      policy_loss: -0.001248069922439754\n",
      "      total_loss: 711.7498779296875\n",
      "      vf_explained_var: 0.18779617547988892\n",
      "      vf_loss: 711.7510986328125\n",
      "    sample_time_ms: 26724.395\n",
      "    update_time_ms: 11.205\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.06425734203438\n",
      "    rl_1: 180.8840383412797\n",
      "  time_since_restore: 742.2396140098572\n",
      "  time_this_iter_s: 32.1513295173645\n",
      "  time_total_s: 742.2396140098572\n",
      "  timestamp: 1552398488\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 26\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 742 s, 26 iter, 260000 ts, 237 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-48-39\n",
      "  done: false\n",
      "  episode_len_mean: 186.55\n",
      "  episode_reward_max: 309.29109227554034\n",
      "  episode_reward_mean: 230.65840782340626\n",
      "  episode_reward_min: -154.16148843906913\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 1054\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4575.527\n",
      "    load_time_ms: 3.189\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.25593101978302\n",
      "      kl: 0.0020300219766795635\n",
      "      policy_loss: -0.0017889570444822311\n",
      "      total_loss: 62.86018753051758\n",
      "      vf_explained_var: 0.6774702072143555\n",
      "      vf_loss: 62.86198043823242\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1741859912872314\n",
      "      kl: 0.002509003272280097\n",
      "      policy_loss: -0.0009272855240851641\n",
      "      total_loss: 623.4906616210938\n",
      "      vf_explained_var: 0.43740081787109375\n",
      "      vf_loss: 623.4915771484375\n",
      "    sample_time_ms: 27147.08\n",
      "    update_time_ms: 11.314\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.77782933726537\n",
      "    rl_1: 179.88057848614096\n",
      "  time_since_restore: 772.5192241668701\n",
      "  time_this_iter_s: 30.27961015701294\n",
      "  time_total_s: 772.5192241668701\n",
      "  timestamp: 1552398519\n",
      "  timesteps_since_restore: 270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 27\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 772 s, 27 iter, 270000 ts, 231 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-49-11\n",
      "  done: false\n",
      "  episode_len_mean: 185.71\n",
      "  episode_reward_max: 272.3123312719666\n",
      "  episode_reward_mean: 235.65597358900996\n",
      "  episode_reward_min: -151.62542682411396\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 1107\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4630.561\n",
      "    load_time_ms: 3.311\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2522138357162476\n",
      "      kl: 0.0022942875511944294\n",
      "      policy_loss: -0.0005422057583928108\n",
      "      total_loss: 36.054908752441406\n",
      "      vf_explained_var: 0.7963257431983948\n",
      "      vf_loss: 36.055450439453125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1464241743087769\n",
      "      kl: 0.005159288644790649\n",
      "      policy_loss: -0.0022849009837955236\n",
      "      total_loss: 624.1792602539062\n",
      "      vf_explained_var: 0.2491740882396698\n",
      "      vf_loss: 624.1815795898438\n",
      "    sample_time_ms: 27896.707\n",
      "    update_time_ms: 11.752\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.31780616356172\n",
      "    rl_1: 184.33816742544826\n",
      "  time_since_restore: 804.7998781204224\n",
      "  time_this_iter_s: 32.280653953552246\n",
      "  time_total_s: 804.7998781204224\n",
      "  timestamp: 1552398551\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 28\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 804 s, 28 iter, 280000 ts, 236 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-49-38\n",
      "  done: false\n",
      "  episode_len_mean: 202.51\n",
      "  episode_reward_max: 322.5477415676803\n",
      "  episode_reward_mean: 242.49342219927084\n",
      "  episode_reward_min: -151.62542682411396\n",
      "  episodes_this_iter: 47\n",
      "  episodes_total: 1154\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4594.481\n",
      "    load_time_ms: 3.249\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3453919887542725\n",
      "      kl: 0.006628498900681734\n",
      "      policy_loss: -0.0027999859303236008\n",
      "      total_loss: 14.778196334838867\n",
      "      vf_explained_var: 0.8925837278366089\n",
      "      vf_loss: 14.781001091003418\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2497440576553345\n",
      "      kl: 0.004284857306629419\n",
      "      policy_loss: -0.002730380743741989\n",
      "      total_loss: 506.16998291015625\n",
      "      vf_explained_var: 0.6460928320884705\n",
      "      vf_loss: 506.1727294921875\n",
      "    sample_time_ms: 27831.038\n",
      "    update_time_ms: 11.831\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.28723657395144\n",
      "    rl_1: 186.20618562531934\n",
      "  time_since_restore: 831.5680868625641\n",
      "  time_this_iter_s: 26.768208742141724\n",
      "  time_total_s: 831.5680868625641\n",
      "  timestamp: 1552398578\n",
      "  timesteps_since_restore: 290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 29\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 831 s, 29 iter, 290000 ts, 242 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-50-02\n",
      "  done: false\n",
      "  episode_len_mean: 202.48\n",
      "  episode_reward_max: 322.5477415676803\n",
      "  episode_reward_mean: 236.70171432230157\n",
      "  episode_reward_min: -151.62542682411396\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 1205\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3973.295\n",
      "    load_time_ms: 3.148\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9802322831784522e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.268086314201355\n",
      "      kl: 0.005785956513136625\n",
      "      policy_loss: -0.0024502102751284838\n",
      "      total_loss: 38.29454040527344\n",
      "      vf_explained_var: 0.7795091271400452\n",
      "      vf_loss: 38.29698944091797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1506421566009521\n",
      "      kl: 0.009530925191938877\n",
      "      policy_loss: -0.0031407272908836603\n",
      "      total_loss: 624.3252563476562\n",
      "      vf_explained_var: 0.40859365463256836\n",
      "      vf_loss: 624.3284301757812\n",
      "    sample_time_ms: 26930.551\n",
      "    update_time_ms: 10.043\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.83341756351592\n",
      "    rl_1: 182.8682967587856\n",
      "  time_since_restore: 855.7627832889557\n",
      "  time_this_iter_s: 24.1946964263916\n",
      "  time_total_s: 855.7627832889557\n",
      "  timestamp: 1552398602\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 30\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 855 s, 30 iter, 300000 ts, 237 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-50-30\n",
      "  done: false\n",
      "  episode_len_mean: 188.87\n",
      "  episode_reward_max: 275.2885638567085\n",
      "  episode_reward_mean: 219.26289583521236\n",
      "  episode_reward_min: -154.06376546314647\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 1259\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3780.208\n",
      "    load_time_ms: 3.03\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2952643632888794\n",
      "      kl: 0.0014846252743154764\n",
      "      policy_loss: -0.0021473781671375036\n",
      "      total_loss: 132.21522521972656\n",
      "      vf_explained_var: 0.5859295129776001\n",
      "      vf_loss: 132.21739196777344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9802322831784522e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1645567417144775\n",
      "      kl: 0.007719880435615778\n",
      "      policy_loss: -0.00279157399199903\n",
      "      total_loss: 751.0494995117188\n",
      "      vf_explained_var: 0.35350385308265686\n",
      "      vf_loss: 751.0523681640625\n",
      "    sample_time_ms: 25719.359\n",
      "    update_time_ms: 9.374\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 46.330875733077306\n",
      "    rl_1: 172.93202010213503\n",
      "  time_since_restore: 883.9038953781128\n",
      "  time_this_iter_s: 28.141112089157104\n",
      "  time_total_s: 883.9038953781128\n",
      "  timestamp: 1552398630\n",
      "  timesteps_since_restore: 310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 31\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 883 s, 31 iter, 310000 ts, 219 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-51-05\n",
      "  done: false\n",
      "  episode_len_mean: 201.65\n",
      "  episode_reward_max: 311.75633790352276\n",
      "  episode_reward_mean: 219.8990970321346\n",
      "  episode_reward_min: -154.06376546314647\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 1304\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3654.471\n",
      "    load_time_ms: 2.746\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3792815208435059\n",
      "      kl: 0.009039792232215405\n",
      "      policy_loss: -0.0025336442049592733\n",
      "      total_loss: 66.62792205810547\n",
      "      vf_explained_var: 0.7445352077484131\n",
      "      vf_loss: 66.63046264648438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2596012353897095\n",
      "      kl: 0.003980011213570833\n",
      "      policy_loss: -0.0009891288354992867\n",
      "      total_loss: 544.5985107421875\n",
      "      vf_explained_var: 0.6864728927612305\n",
      "      vf_loss: 544.5994873046875\n",
      "    sample_time_ms: 25753.426\n",
      "    update_time_ms: 9.322\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.25739571957498\n",
      "    rl_1: 170.64170131255966\n",
      "  time_since_restore: 918.7263674736023\n",
      "  time_this_iter_s: 34.8224720954895\n",
      "  time_total_s: 918.7263674736023\n",
      "  timestamp: 1552398665\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 32\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 918 s, 32 iter, 320000 ts, 220 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-51-34\n",
      "  done: false\n",
      "  episode_len_mean: 208.05\n",
      "  episode_reward_max: 311.75633790352276\n",
      "  episode_reward_mean: 231.66965140136523\n",
      "  episode_reward_min: -154.06376546314647\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1354\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3637.347\n",
      "    load_time_ms: 2.698\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3064992427825928\n",
      "      kl: 0.00636692252010107\n",
      "      policy_loss: -0.0005749459960497916\n",
      "      total_loss: 20.19207763671875\n",
      "      vf_explained_var: 0.9008628726005554\n",
      "      vf_loss: 20.192649841308594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1793347597122192\n",
      "      kl: 0.008387962356209755\n",
      "      policy_loss: -0.004088052082806826\n",
      "      total_loss: 523.289794921875\n",
      "      vf_explained_var: 0.527145266532898\n",
      "      vf_loss: 523.2938842773438\n",
      "    sample_time_ms: 25649.875\n",
      "    update_time_ms: 8.539\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.239046672193375\n",
      "    rl_1: 176.43060472917188\n",
      "  time_since_restore: 947.4996950626373\n",
      "  time_this_iter_s: 28.773327589035034\n",
      "  time_total_s: 947.4996950626373\n",
      "  timestamp: 1552398694\n",
      "  timesteps_since_restore: 330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 33\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 947 s, 33 iter, 330000 ts, 232 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-51-59\n",
      "  done: false\n",
      "  episode_len_mean: 211.7\n",
      "  episode_reward_max: 324.75212806111193\n",
      "  episode_reward_mean: 247.63122013464323\n",
      "  episode_reward_min: -13.26876076605916\n",
      "  episodes_this_iter: 47\n",
      "  episodes_total: 1401\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3639.468\n",
      "    load_time_ms: 2.665\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3763387203216553\n",
      "      kl: 0.008866488933563232\n",
      "      policy_loss: -0.004430967848747969\n",
      "      total_loss: 11.799957275390625\n",
      "      vf_explained_var: 0.9461023211479187\n",
      "      vf_loss: 11.804388046264648\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2276519536972046\n",
      "      kl: 0.01034197025001049\n",
      "      policy_loss: -0.0027532808016985655\n",
      "      total_loss: 511.9876403808594\n",
      "      vf_explained_var: 0.694574773311615\n",
      "      vf_loss: 511.9903259277344\n",
      "    sample_time_ms: 25346.916\n",
      "    update_time_ms: 8.618\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.04083983186038\n",
      "    rl_1: 184.59038030278282\n",
      "  time_since_restore: 972.3583681583405\n",
      "  time_this_iter_s: 24.858673095703125\n",
      "  time_total_s: 972.3583681583405\n",
      "  timestamp: 1552398719\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 34\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 972 s, 34 iter, 340000 ts, 248 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-52-30\n",
      "  done: false\n",
      "  episode_len_mean: 203.2\n",
      "  episode_reward_max: 324.75212806111193\n",
      "  episode_reward_mean: 249.77232760140296\n",
      "  episode_reward_min: 88.8095784616653\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 1453\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3782.933\n",
      "    load_time_ms: 2.776\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.313225884932663e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3279123306274414\n",
      "      kl: 0.008350067771971226\n",
      "      policy_loss: -0.0032423455268144608\n",
      "      total_loss: 39.906524658203125\n",
      "      vf_explained_var: 0.8525277972221375\n",
      "      vf_loss: 39.90977096557617\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1553620100021362\n",
      "      kl: 0.0077114710584282875\n",
      "      policy_loss: -0.0017117410898208618\n",
      "      total_loss: 585.6167602539062\n",
      "      vf_explained_var: 0.5593605637550354\n",
      "      vf_loss: 585.6184692382812\n",
      "    sample_time_ms: 25565.867\n",
      "    update_time_ms: 8.729\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.22736699711974\n",
      "    rl_1: 185.54496060428323\n",
      "  time_since_restore: 1003.8847365379333\n",
      "  time_this_iter_s: 31.526368379592896\n",
      "  time_total_s: 1003.8847365379333\n",
      "  timestamp: 1552398750\n",
      "  timesteps_since_restore: 350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 35\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1003 s, 35 iter, 350000 ts, 250 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-53-03\n",
      "  done: false\n",
      "  episode_len_mean: 190.47\n",
      "  episode_reward_max: 325.5051653231647\n",
      "  episode_reward_mean: 249.53412153790953\n",
      "  episode_reward_min: 88.8095784616653\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 1506\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3807.755\n",
      "    load_time_ms: 2.769\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3099544048309326\n",
      "      kl: 0.0073943291790783405\n",
      "      policy_loss: -0.004028436727821827\n",
      "      total_loss: 35.602500915527344\n",
      "      vf_explained_var: 0.8695663809776306\n",
      "      vf_loss: 35.606529235839844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1326173543930054\n",
      "      kl: 0.002418253570795059\n",
      "      policy_loss: -0.0008949310868047178\n",
      "      total_loss: 592.4508666992188\n",
      "      vf_explained_var: 0.5228285193443298\n",
      "      vf_loss: 592.4517822265625\n",
      "    sample_time_ms: 25563.796\n",
      "    update_time_ms: 8.977\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.87173274284324\n",
      "    rl_1: 185.66238879506628\n",
      "  time_since_restore: 1036.271159172058\n",
      "  time_this_iter_s: 32.386422634124756\n",
      "  time_total_s: 1036.271159172058\n",
      "  timestamp: 1552398783\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 36\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1036 s, 36 iter, 360000 ts, 250 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-53-33\n",
      "  done: false\n",
      "  episode_len_mean: 184.25\n",
      "  episode_reward_max: 325.5051653231647\n",
      "  episode_reward_mean: 256.36445169886383\n",
      "  episode_reward_min: 129.71685246563482\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 1562\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3698.583\n",
      "    load_time_ms: 2.697\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3023065328598022\n",
      "      kl: 0.010728738270699978\n",
      "      policy_loss: -0.006360596977174282\n",
      "      total_loss: 13.87967586517334\n",
      "      vf_explained_var: 0.9335007071495056\n",
      "      vf_loss: 13.88603687286377\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.313225884932663e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.104243278503418\n",
      "      kl: 0.009677414782345295\n",
      "      policy_loss: -0.0023833252489566803\n",
      "      total_loss: 588.2984619140625\n",
      "      vf_explained_var: 0.37449127435684204\n",
      "      vf_loss: 588.30078125\n",
      "    sample_time_ms: 25692.44\n",
      "    update_time_ms: 8.893\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.18537988558433\n",
      "    rl_1: 188.1790718132795\n",
      "  time_since_restore: 1066.7411940097809\n",
      "  time_this_iter_s: 30.47003483772278\n",
      "  time_total_s: 1066.7411940097809\n",
      "  timestamp: 1552398813\n",
      "  timesteps_since_restore: 370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 37\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1066 s, 37 iter, 370000 ts, 256 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-54-20\n",
      "  done: false\n",
      "  episode_len_mean: 180.88\n",
      "  episode_reward_max: 317.5458323213254\n",
      "  episode_reward_mean: 264.7605760383756\n",
      "  episode_reward_min: 234.23490013545694\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 1616\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4231.932\n",
      "    load_time_ms: 2.872\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2983038425445557\n",
      "      kl: 0.012384863570332527\n",
      "      policy_loss: -0.0054478393867611885\n",
      "      total_loss: 15.092876434326172\n",
      "      vf_explained_var: 0.9407529830932617\n",
      "      vf_loss: 15.098322868347168\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.101441502571106\n",
      "      kl: 0.007279943209141493\n",
      "      policy_loss: -0.002070340560749173\n",
      "      total_loss: 564.4182739257812\n",
      "      vf_explained_var: 0.4677726626396179\n",
      "      vf_loss: 564.42041015625\n",
      "    sample_time_ms: 26598.594\n",
      "    update_time_ms: 8.614\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.73869567734029\n",
      "    rl_1: 191.02188036103522\n",
      "  time_since_restore: 1113.4369246959686\n",
      "  time_this_iter_s: 46.695730686187744\n",
      "  time_total_s: 1113.4369246959686\n",
      "  timestamp: 1552398860\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 38\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1113 s, 38 iter, 380000 ts, 265 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-55-00\n",
      "  done: false\n",
      "  episode_len_mean: 184.1\n",
      "  episode_reward_max: 317.5458323213254\n",
      "  episode_reward_mean: 269.36952241434335\n",
      "  episode_reward_min: 169.39291687128122\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 1670\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4240.959\n",
      "    load_time_ms: 3.023\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.291929841041565\n",
      "      kl: 0.02657618187367916\n",
      "      policy_loss: -0.009164750576019287\n",
      "      total_loss: 31.911163330078125\n",
      "      vf_explained_var: 0.9181156754493713\n",
      "      vf_loss: 31.920331954956055\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1062543392181396\n",
      "      kl: 0.01461753062903881\n",
      "      policy_loss: -0.004658338148146868\n",
      "      total_loss: 524.3327026367188\n",
      "      vf_explained_var: 0.6250795125961304\n",
      "      vf_loss: 524.3373413085938\n",
      "    sample_time_ms: 27887.859\n",
      "    update_time_ms: 9.658\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 80.09191354213706\n",
      "    rl_1: 189.27760887220632\n",
      "  time_since_restore: 1153.2013483047485\n",
      "  time_this_iter_s: 39.76442360877991\n",
      "  time_total_s: 1153.2013483047485\n",
      "  timestamp: 1552398900\n",
      "  timesteps_since_restore: 390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 39\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1153 s, 39 iter, 390000 ts, 269 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-55-34\n",
      "  done: false\n",
      "  episode_len_mean: 189.46\n",
      "  episode_reward_max: 329.6852325561404\n",
      "  episode_reward_mean: 278.20435758088865\n",
      "  episode_reward_min: 169.39291687128122\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 1722\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4304.162\n",
      "    load_time_ms: 3.042\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3190593719482422\n",
      "      kl: 0.006510613951832056\n",
      "      policy_loss: -0.0009651337168179452\n",
      "      total_loss: 34.150909423828125\n",
      "      vf_explained_var: 0.9022318720817566\n",
      "      vf_loss: 34.15188217163086\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1044222116470337\n",
      "      kl: 0.00952186994254589\n",
      "      policy_loss: -0.002415681490674615\n",
      "      total_loss: 365.33837890625\n",
      "      vf_explained_var: 0.649368405342102\n",
      "      vf_loss: 365.34088134765625\n",
      "    sample_time_ms: 28765.452\n",
      "    update_time_ms: 9.472\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 89.08040781209361\n",
      "    rl_1: 189.12394976879506\n",
      "  time_since_restore: 1186.8111655712128\n",
      "  time_this_iter_s: 33.60981726646423\n",
      "  time_total_s: 1186.8111655712128\n",
      "  timestamp: 1552398934\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 40\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1186 s, 40 iter, 400000 ts, 278 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-56-14\n",
      "  done: false\n",
      "  episode_len_mean: 188.79\n",
      "  episode_reward_max: 329.6852325561404\n",
      "  episode_reward_mean: 282.4367048188418\n",
      "  episode_reward_min: 248.0930068249331\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 1776\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4592.332\n",
      "    load_time_ms: 3.123\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2397526502609253\n",
      "      kl: 0.01432378962635994\n",
      "      policy_loss: -0.004675844684243202\n",
      "      total_loss: 38.92141342163086\n",
      "      vf_explained_var: 0.897659182548523\n",
      "      vf_loss: 38.926090240478516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9968116879463196\n",
      "      kl: 0.014902324415743351\n",
      "      policy_loss: -0.0029112803749740124\n",
      "      total_loss: 481.9280700683594\n",
      "      vf_explained_var: 0.5818707942962646\n",
      "      vf_loss: 481.9309387207031\n",
      "    sample_time_ms: 29699.494\n",
      "    update_time_ms: 9.505\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 92.98482757257321\n",
      "    rl_1: 189.45187724626857\n",
      "  time_since_restore: 1227.1780693531036\n",
      "  time_this_iter_s: 40.36690378189087\n",
      "  time_total_s: 1227.1780693531036\n",
      "  timestamp: 1552398974\n",
      "  timesteps_since_restore: 410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 41\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1227 s, 41 iter, 410000 ts, 282 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-57-03\n",
      "  done: false\n",
      "  episode_len_mean: 182.91\n",
      "  episode_reward_max: 322.4061847991568\n",
      "  episode_reward_mean: 284.00023426097897\n",
      "  episode_reward_min: 249.11073370583023\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 1831\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4845.768\n",
      "    load_time_ms: 3.224\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2510201930999756\n",
      "      kl: 0.007355603855103254\n",
      "      policy_loss: -0.0032839919440448284\n",
      "      total_loss: 42.97474670410156\n",
      "      vf_explained_var: 0.898614764213562\n",
      "      vf_loss: 42.97802734375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0064146518707275\n",
      "      kl: 0.005370469763875008\n",
      "      policy_loss: -0.0019740904681384563\n",
      "      total_loss: 461.0797424316406\n",
      "      vf_explained_var: 0.6264209151268005\n",
      "      vf_loss: 461.08172607421875\n",
      "    sample_time_ms: 30821.895\n",
      "    update_time_ms: 9.517\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 94.85548584678763\n",
      "    rl_1: 189.1447484141913\n",
      "  time_since_restore: 1275.7587020397186\n",
      "  time_this_iter_s: 48.58063268661499\n",
      "  time_total_s: 1275.7587020397186\n",
      "  timestamp: 1552399023\n",
      "  timesteps_since_restore: 420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 42\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1275 s, 42 iter, 420000 ts, 284 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-57-39\n",
      "  done: false\n",
      "  episode_len_mean: 188.02\n",
      "  episode_reward_max: 350.9386273167341\n",
      "  episode_reward_mean: 291.0344469062775\n",
      "  episode_reward_min: 252.02953712314283\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 1883\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4950.204\n",
      "    load_time_ms: 3.419\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2529833316802979\n",
      "      kl: 0.01603018119931221\n",
      "      policy_loss: -0.0030654321890324354\n",
      "      total_loss: 48.34779357910156\n",
      "      vf_explained_var: 0.870531439781189\n",
      "      vf_loss: 48.35085678100586\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0253169536590576\n",
      "      kl: 0.009095678105950356\n",
      "      policy_loss: -0.0029255026020109653\n",
      "      total_loss: 415.5679626464844\n",
      "      vf_explained_var: 0.6610721349716187\n",
      "      vf_loss: 415.5708923339844\n",
      "    sample_time_ms: 31473.157\n",
      "    update_time_ms: 9.378\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 99.82902879845383\n",
      "    rl_1: 191.20541810782365\n",
      "  time_since_restore: 1312.0960919857025\n",
      "  time_this_iter_s: 36.33738994598389\n",
      "  time_total_s: 1312.0960919857025\n",
      "  timestamp: 1552399059\n",
      "  timesteps_since_restore: 430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 43\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1312 s, 43 iter, 430000 ts, 291 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-58-07\n",
      "  done: false\n",
      "  episode_len_mean: 186.63\n",
      "  episode_reward_max: 324.63429039445083\n",
      "  episode_reward_mean: 288.130189567555\n",
      "  episode_reward_min: 247.686411142333\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 1938\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4985.506\n",
      "    load_time_ms: 3.434\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.17452871799469\n",
      "      kl: 0.016762327402830124\n",
      "      policy_loss: -0.004098999314010143\n",
      "      total_loss: 44.12839126586914\n",
      "      vf_explained_var: 0.8888717293739319\n",
      "      vf_loss: 44.13248825073242\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9103830890414573e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9702711701393127\n",
      "      kl: 0.0031936950981616974\n",
      "      policy_loss: -0.0017694727284833789\n",
      "      total_loss: 521.2518310546875\n",
      "      vf_explained_var: 0.6119096875190735\n",
      "      vf_loss: 521.2536010742188\n",
      "    sample_time_ms: 31702.185\n",
      "    update_time_ms: 9.538\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 97.05782817245239\n",
      "    rl_1: 191.07236139510266\n",
      "  time_since_restore: 1339.6027591228485\n",
      "  time_this_iter_s: 27.506667137145996\n",
      "  time_total_s: 1339.6027591228485\n",
      "  timestamp: 1552399087\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 44\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1339 s, 44 iter, 440000 ts, 288 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-58-36\n",
      "  done: false\n",
      "  episode_len_mean: 183.29\n",
      "  episode_reward_max: 326.76897307073756\n",
      "  episode_reward_mean: 287.5130025602434\n",
      "  episode_reward_min: 247.686411142333\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 1992\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4819.092\n",
      "    load_time_ms: 3.351\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1875951290130615\n",
      "      kl: 0.009444466792047024\n",
      "      policy_loss: -0.0028271740302443504\n",
      "      total_loss: 46.55593490600586\n",
      "      vf_explained_var: 0.8827902674674988\n",
      "      vf_loss: 46.55875778198242\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0012940168380737\n",
      "      kl: 0.00998175609856844\n",
      "      policy_loss: -0.0025744515005499125\n",
      "      total_loss: 433.53411865234375\n",
      "      vf_explained_var: 0.6702563762664795\n",
      "      vf_loss: 433.5366516113281\n",
      "    sample_time_ms: 31699.373\n",
      "    update_time_ms: 9.438\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 96.65118915580952\n",
      "    rl_1: 190.8618134044339\n",
      "  time_since_restore: 1369.4324362277985\n",
      "  time_this_iter_s: 29.82967710494995\n",
      "  time_total_s: 1369.4324362277985\n",
      "  timestamp: 1552399116\n",
      "  timesteps_since_restore: 450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 45\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1369 s, 45 iter, 450000 ts, 288 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-59-00\n",
      "  done: false\n",
      "  episode_len_mean: 180.64\n",
      "  episode_reward_max: 338.048541951435\n",
      "  episode_reward_mean: 289.0711094133307\n",
      "  episode_reward_min: 153.1501067388583\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 2049\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4762.859\n",
      "    load_time_ms: 3.398\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9103830890414573e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.164262294769287\n",
      "      kl: 0.0074107167311012745\n",
      "      policy_loss: -0.0015864080050960183\n",
      "      total_loss: 61.13681411743164\n",
      "      vf_explained_var: 0.8785586357116699\n",
      "      vf_loss: 61.138389587402344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9640442132949829\n",
      "      kl: 0.02567647024989128\n",
      "      policy_loss: -0.005339887458831072\n",
      "      total_loss: 487.8279724121094\n",
      "      vf_explained_var: 0.6811026334762573\n",
      "      vf_loss: 487.8332214355469\n",
      "    sample_time_ms: 30913.187\n",
      "    update_time_ms: 9.411\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 99.72337968416048\n",
      "    rl_1: 189.34772972917025\n",
      "  time_since_restore: 1393.3892273902893\n",
      "  time_this_iter_s: 23.956791162490845\n",
      "  time_total_s: 1393.3892273902893\n",
      "  timestamp: 1552399140\n",
      "  timesteps_since_restore: 460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 46\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1393 s, 46 iter, 460000 ts, 289 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-59-25\n",
      "  done: false\n",
      "  episode_len_mean: 178.24\n",
      "  episode_reward_max: 349.3125119271729\n",
      "  episode_reward_mean: 290.61812633961245\n",
      "  episode_reward_min: 153.1501067388583\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 2104\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4724.337\n",
      "    load_time_ms: 3.345\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1406413316726685\n",
      "      kl: 0.012679564766585827\n",
      "      policy_loss: -0.005255073308944702\n",
      "      total_loss: 65.20929718017578\n",
      "      vf_explained_var: 0.8507053852081299\n",
      "      vf_loss: 65.21454620361328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9575400948524475\n",
      "      kl: 0.005473709665238857\n",
      "      policy_loss: -0.0011447948636487126\n",
      "      total_loss: 485.700927734375\n",
      "      vf_explained_var: 0.6702654361724854\n",
      "      vf_loss: 485.7020263671875\n",
      "    sample_time_ms: 30338.741\n",
      "    update_time_ms: 9.346\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 101.00309556796526\n",
      "    rl_1: 189.61503077164716\n",
      "  time_since_restore: 1417.7237167358398\n",
      "  time_this_iter_s: 24.334489345550537\n",
      "  time_total_s: 1417.7237167358398\n",
      "  timestamp: 1552399165\n",
      "  timesteps_since_restore: 470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 47\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1417 s, 47 iter, 470000 ts, 291 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_14-59-49\n",
      "  done: false\n",
      "  episode_len_mean: 180.51\n",
      "  episode_reward_max: 349.3125119271729\n",
      "  episode_reward_mean: 295.0929371819136\n",
      "  episode_reward_min: 253.15820213943866\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 2160\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4114.961\n",
      "    load_time_ms: 3.21\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1292057037353516\n",
      "      kl: 0.005677137989550829\n",
      "      policy_loss: -0.0016563251847401261\n",
      "      total_loss: 60.98223876953125\n",
      "      vf_explained_var: 0.8671244382858276\n",
      "      vf_loss: 60.98389434814453\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.974346935749054\n",
      "      kl: 0.011851144023239613\n",
      "      policy_loss: -0.0025686759036034346\n",
      "      total_loss: 465.6011657714844\n",
      "      vf_explained_var: 0.6824285387992859\n",
      "      vf_loss: 465.60382080078125\n",
      "    sample_time_ms: 28647.312\n",
      "    update_time_ms: 9.565\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 103.05925486599207\n",
      "    rl_1: 192.03368231592154\n",
      "  time_since_restore: 1441.3931810855865\n",
      "  time_this_iter_s: 23.669464349746704\n",
      "  time_total_s: 1441.3931810855865\n",
      "  timestamp: 1552399189\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 48\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1441 s, 48 iter, 480000 ts, 295 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-00-13\n",
      "  done: false\n",
      "  episode_len_mean: 179.9\n",
      "  episode_reward_max: 356.27103615819846\n",
      "  episode_reward_mean: 298.1065886894861\n",
      "  episode_reward_min: 257.60440762919217\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 2216\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4097.301\n",
      "    load_time_ms: 3.086\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1268445253372192\n",
      "      kl: 0.01978987827897072\n",
      "      policy_loss: -0.005594935733824968\n",
      "      total_loss: 65.3354263305664\n",
      "      vf_explained_var: 0.8607908487319946\n",
      "      vf_loss: 65.34101867675781\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9846729636192322\n",
      "      kl: 0.002802834613248706\n",
      "      policy_loss: -0.001283238991163671\n",
      "      total_loss: 429.6421813964844\n",
      "      vf_explained_var: 0.7274231910705566\n",
      "      vf_loss: 429.6434326171875\n",
      "    sample_time_ms: 27091.796\n",
      "    update_time_ms: 8.561\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 106.02525059123366\n",
      "    rl_1: 192.08133809825253\n",
      "  time_since_restore: 1465.411616563797\n",
      "  time_this_iter_s: 24.01843547821045\n",
      "  time_total_s: 1465.411616563797\n",
      "  timestamp: 1552399213\n",
      "  timesteps_since_restore: 490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 49\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1465 s, 49 iter, 490000 ts, 298 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-00-37\n",
      "  done: false\n",
      "  episode_len_mean: 177.79\n",
      "  episode_reward_max: 369.7586313505695\n",
      "  episode_reward_mean: 295.9323559952172\n",
      "  episode_reward_min: 250.96454134261717\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 2272\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4028.483\n",
      "    load_time_ms: 3.174\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1007241010665894\n",
      "      kl: 0.013715546578168869\n",
      "      policy_loss: -0.004488083068281412\n",
      "      total_loss: 63.18849563598633\n",
      "      vf_explained_var: 0.8755407929420471\n",
      "      vf_loss: 63.19297790527344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8189894306509108e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9676949977874756\n",
      "      kl: 0.006946473382413387\n",
      "      policy_loss: -0.0013233147328719497\n",
      "      total_loss: 476.51446533203125\n",
      "      vf_explained_var: 0.7081940770149231\n",
      "      vf_loss: 476.5157775878906\n",
      "    sample_time_ms: 26203.069\n",
      "    update_time_ms: 8.511\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 105.20285327864418\n",
      "    rl_1: 190.72950271657305\n",
      "  time_since_restore: 1489.438931465149\n",
      "  time_this_iter_s: 24.02731490135193\n",
      "  time_total_s: 1489.438931465149\n",
      "  timestamp: 1552399237\n",
      "  timesteps_since_restore: 500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 50\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1489 s, 50 iter, 500000 ts, 296 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 175.49\n",
      "  episode_reward_max: 334.9263078024293\n",
      "  episode_reward_mean: 292.0422400933882\n",
      "  episode_reward_min: 160.63127255075682\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 2330\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3716.27\n",
      "    load_time_ms: 3.089\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.093165397644043\n",
      "      kl: 0.005943029187619686\n",
      "      policy_loss: -0.0019328740891069174\n",
      "      total_loss: 73.828857421875\n",
      "      vf_explained_var: 0.8687460422515869\n",
      "      vf_loss: 73.8307876586914\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9704327583312988\n",
      "      kl: 0.001351639861240983\n",
      "      policy_loss: -1.49403249452007e-05\n",
      "      total_loss: 448.6965026855469\n",
      "      vf_explained_var: 0.7473344802856445\n",
      "      vf_loss: 448.6964111328125\n",
      "    sample_time_ms: 24881.158\n",
      "    update_time_ms: 8.037\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 103.35328030007358\n",
      "    rl_1: 188.68895979331458\n",
      "  time_since_restore: 1513.4558236598969\n",
      "  time_this_iter_s: 24.016892194747925\n",
      "  time_total_s: 1513.4558236598969\n",
      "  timestamp: 1552399261\n",
      "  timesteps_since_restore: 510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 51\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1513 s, 51 iter, 510000 ts, 292 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-01-25\n",
      "  done: false\n",
      "  episode_len_mean: 174.58\n",
      "  episode_reward_max: 334.9263078024293\n",
      "  episode_reward_mean: 292.58852889384684\n",
      "  episode_reward_min: 160.63127255075682\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 2387\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3330.999\n",
      "    load_time_ms: 2.87\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0586276054382324\n",
      "      kl: 0.012017510831356049\n",
      "      policy_loss: -0.003918908536434174\n",
      "      total_loss: 59.562137603759766\n",
      "      vf_explained_var: 0.8900106549263\n",
      "      vf_loss: 59.56606674194336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9442275762557983\n",
      "      kl: 0.004256092011928558\n",
      "      policy_loss: -0.0016079327324405313\n",
      "      total_loss: 432.41131591796875\n",
      "      vf_explained_var: 0.7581085562705994\n",
      "      vf_loss: 432.4129638671875\n",
      "    sample_time_ms: 22861.663\n",
      "    update_time_ms: 7.895\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 104.45036458995351\n",
      "    rl_1: 188.13816430389332\n",
      "  time_since_restore: 1537.9799976348877\n",
      "  time_this_iter_s: 24.524173974990845\n",
      "  time_total_s: 1537.9799976348877\n",
      "  timestamp: 1552399285\n",
      "  timesteps_since_restore: 520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 52\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1537 s, 52 iter, 520000 ts, 293 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-01-49\n",
      "  done: false\n",
      "  episode_len_mean: 174.89\n",
      "  episode_reward_max: 353.78573256968946\n",
      "  episode_reward_mean: 297.5797048718778\n",
      "  episode_reward_min: 258.66758685253336\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 2444\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3222.897\n",
      "    load_time_ms: 2.677\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0978353023529053\n",
      "      kl: 0.003462111810222268\n",
      "      policy_loss: -0.0019769652280956507\n",
      "      total_loss: 59.93429946899414\n",
      "      vf_explained_var: 0.8931267857551575\n",
      "      vf_loss: 59.93628692626953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.955412745475769\n",
      "      kl: 0.02037060633301735\n",
      "      policy_loss: -0.0034467242658138275\n",
      "      total_loss: 392.267333984375\n",
      "      vf_explained_var: 0.7818698883056641\n",
      "      vf_loss: 392.2708435058594\n",
      "    sample_time_ms: 21739.736\n",
      "    update_time_ms: 7.927\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 108.08596377975007\n",
      "    rl_1: 189.49374109212764\n",
      "  time_since_restore: 1562.0108542442322\n",
      "  time_this_iter_s: 24.030856609344482\n",
      "  time_total_s: 1562.0108542442322\n",
      "  timestamp: 1552399309\n",
      "  timesteps_since_restore: 530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 53\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1562 s, 53 iter, 530000 ts, 298 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-02-13\n",
      "  done: false\n",
      "  episode_len_mean: 174.89\n",
      "  episode_reward_max: 353.2775815539768\n",
      "  episode_reward_mean: 302.2943597472758\n",
      "  episode_reward_min: 258.66758685253336\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 2502\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3171.998\n",
      "    load_time_ms: 2.666\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8189894306509108e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.044795274734497\n",
      "      kl: 0.003738158382475376\n",
      "      policy_loss: -0.0013003223575651646\n",
      "      total_loss: 58.10966873168945\n",
      "      vf_explained_var: 0.9009568691253662\n",
      "      vf_loss: 58.11096954345703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.869379997253418\n",
      "      kl: 0.019353317096829414\n",
      "      policy_loss: -0.0031672592740505934\n",
      "      total_loss: 409.1100769042969\n",
      "      vf_explained_var: 0.7863073348999023\n",
      "      vf_loss: 409.11322021484375\n",
      "    sample_time_ms: 21415.263\n",
      "    update_time_ms: 7.747\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 111.02422536182783\n",
      "    rl_1: 191.27013438544793\n",
      "  time_since_restore: 1585.7618443965912\n",
      "  time_this_iter_s: 23.75099015235901\n",
      "  time_total_s: 1585.7618443965912\n",
      "  timestamp: 1552399333\n",
      "  timesteps_since_restore: 540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 54\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1585 s, 54 iter, 540000 ts, 302 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-02-38\n",
      "  done: false\n",
      "  episode_len_mean: 175.15\n",
      "  episode_reward_max: 346.07470761838005\n",
      "  episode_reward_mean: 301.89289099719474\n",
      "  episode_reward_min: 261.5908994366934\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 2558\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3173.461\n",
      "    load_time_ms: 2.554\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.084411859512329\n",
      "      kl: 0.0036614048294723034\n",
      "      policy_loss: -0.001939520239830017\n",
      "      total_loss: 61.86683654785156\n",
      "      vf_explained_var: 0.8934019207954407\n",
      "      vf_loss: 61.86878204345703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9188456535339355\n",
      "      kl: 0.005436254665255547\n",
      "      policy_loss: -0.00026423102826811373\n",
      "      total_loss: 382.1843566894531\n",
      "      vf_explained_var: 0.7911282777786255\n",
      "      vf_loss: 382.1846008300781\n",
      "    sample_time_ms: 20867.146\n",
      "    update_time_ms: 7.885\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 111.19760531105484\n",
      "    rl_1: 190.69528568613987\n",
      "  time_since_restore: 1610.1227717399597\n",
      "  time_this_iter_s: 24.36092734336853\n",
      "  time_total_s: 1610.1227717399597\n",
      "  timestamp: 1552399358\n",
      "  timesteps_since_restore: 550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 55\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1610 s, 55 iter, 550000 ts, 302 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-03-02\n",
      "  done: false\n",
      "  episode_len_mean: 176.34\n",
      "  episode_reward_max: 359.5471506134814\n",
      "  episode_reward_mean: 304.4616092380968\n",
      "  episode_reward_min: 261.5908994366934\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 2615\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3174.869\n",
      "    load_time_ms: 2.529\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0651041269302368\n",
      "      kl: 0.00953751988708973\n",
      "      policy_loss: -0.0022431630641222\n",
      "      total_loss: 62.24889373779297\n",
      "      vf_explained_var: 0.900457501411438\n",
      "      vf_loss: 62.25114440917969\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1368683941568192e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9121188521385193\n",
      "      kl: 0.009103040210902691\n",
      "      policy_loss: -0.0008639052393846214\n",
      "      total_loss: 393.47967529296875\n",
      "      vf_explained_var: 0.8076749444007874\n",
      "      vf_loss: 393.48052978515625\n",
      "    sample_time_ms: 20894.296\n",
      "    update_time_ms: 7.691\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 113.20492342553435\n",
      "    rl_1: 191.2566858125625\n",
      "  time_since_restore: 1634.3620491027832\n",
      "  time_this_iter_s: 24.239277362823486\n",
      "  time_total_s: 1634.3620491027832\n",
      "  timestamp: 1552399382\n",
      "  timesteps_since_restore: 560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 56\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1634 s, 56 iter, 560000 ts, 304 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-03-26\n",
      "  done: false\n",
      "  episode_len_mean: 175.94\n",
      "  episode_reward_max: 359.5471506134814\n",
      "  episode_reward_mean: 301.75828825369325\n",
      "  episode_reward_min: 258.58366783027145\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 2672\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3176.066\n",
      "    load_time_ms: 2.529\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0180796384811401\n",
      "      kl: 0.006864327006042004\n",
      "      policy_loss: -0.0019951690919697285\n",
      "      total_loss: 59.25552749633789\n",
      "      vf_explained_var: 0.901887059211731\n",
      "      vf_loss: 59.257530212402344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8996607661247253\n",
      "      kl: 0.014137315563857555\n",
      "      policy_loss: -0.0026239524595439434\n",
      "      total_loss: 393.08843994140625\n",
      "      vf_explained_var: 0.8004136681556702\n",
      "      vf_loss: 393.091064453125\n",
      "    sample_time_ms: 20880.178\n",
      "    update_time_ms: 7.62\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 111.29340245549254\n",
      "    rl_1: 190.4648857982007\n",
      "  time_since_restore: 1658.567051410675\n",
      "  time_this_iter_s: 24.205002307891846\n",
      "  time_total_s: 1658.567051410675\n",
      "  timestamp: 1552399406\n",
      "  timesteps_since_restore: 570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 57\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1658 s, 57 iter, 570000 ts, 302 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-03-51\n",
      "  done: false\n",
      "  episode_len_mean: 170.54\n",
      "  episode_reward_max: 337.20260767081\n",
      "  episode_reward_mean: 294.8924469892379\n",
      "  episode_reward_min: 258.58366783027145\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 2731\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3174.772\n",
      "    load_time_ms: 2.441\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1368683941568192e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8898953199386597\n",
      "      kl: 0.007719280663877726\n",
      "      policy_loss: -0.0018781423568725586\n",
      "      total_loss: 44.97233963012695\n",
      "      vf_explained_var: 0.9225967526435852\n",
      "      vf_loss: 44.97420883178711\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.817356526851654\n",
      "      kl: 0.04622865468263626\n",
      "      policy_loss: -0.009306213818490505\n",
      "      total_loss: 411.2845153808594\n",
      "      vf_explained_var: 0.8044916987419128\n",
      "      vf_loss: 411.29376220703125\n",
      "    sample_time_ms: 20975.323\n",
      "    update_time_ms: 7.155\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 105.44960926622858\n",
      "    rl_1: 189.44283772300935\n",
      "  time_since_restore: 1683.1673412322998\n",
      "  time_this_iter_s: 24.600289821624756\n",
      "  time_total_s: 1683.1673412322998\n",
      "  timestamp: 1552399431\n",
      "  timesteps_since_restore: 580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 58\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1683 s, 58 iter, 580000 ts, 295 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-04-15\n",
      "  done: false\n",
      "  episode_len_mean: 168.52\n",
      "  episode_reward_max: 348.5690796723423\n",
      "  episode_reward_mean: 292.54142752763875\n",
      "  episode_reward_min: 252.0155995897892\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 2790\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3178.337\n",
      "    load_time_ms: 2.414\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.915655791759491\n",
      "      kl: 0.008240005001425743\n",
      "      policy_loss: -0.0019072231370955706\n",
      "      total_loss: 49.024818420410156\n",
      "      vf_explained_var: 0.9144414067268372\n",
      "      vf_loss: 49.0267219543457\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.52651062683554e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.862219512462616\n",
      "      kl: 0.025855841115117073\n",
      "      policy_loss: -0.0023396641481667757\n",
      "      total_loss: 389.3420715332031\n",
      "      vf_explained_var: 0.815902054309845\n",
      "      vf_loss: 389.3443908691406\n",
      "    sample_time_ms: 20965.829\n",
      "    update_time_ms: 6.992\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 103.1085963013543\n",
      "    rl_1: 189.43283122628452\n",
      "  time_since_restore: 1707.1248972415924\n",
      "  time_this_iter_s: 23.957556009292603\n",
      "  time_total_s: 1707.1248972415924\n",
      "  timestamp: 1552399455\n",
      "  timesteps_since_restore: 590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 59\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1707 s, 59 iter, 590000 ts, 293 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-04-40\n",
      "  done: false\n",
      "  episode_len_mean: 171.34\n",
      "  episode_reward_max: 357.9686365196818\n",
      "  episode_reward_mean: 297.7465960710316\n",
      "  episode_reward_min: 259.5394013234067\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 2849\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3178.715\n",
      "    load_time_ms: 2.372\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.842170985392048e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8940294981002808\n",
      "      kl: 0.007583055645227432\n",
      "      policy_loss: -0.0017124693840742111\n",
      "      total_loss: 49.450416564941406\n",
      "      vf_explained_var: 0.9195440411567688\n",
      "      vf_loss: 49.452125549316406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.52651062683554e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8358674645423889\n",
      "      kl: 0.06048671528697014\n",
      "      policy_loss: -0.009817146696150303\n",
      "      total_loss: 375.2520751953125\n",
      "      vf_explained_var: 0.8297105431556702\n",
      "      vf_loss: 375.261962890625\n",
      "    sample_time_ms: 21044.028\n",
      "    update_time_ms: 7.003\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 106.68647341019306\n",
      "    rl_1: 191.0601226608385\n",
      "  time_since_restore: 1731.9389684200287\n",
      "  time_this_iter_s: 24.81407117843628\n",
      "  time_total_s: 1731.9389684200287\n",
      "  timestamp: 1552399480\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 60\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1731 s, 60 iter, 600000 ts, 298 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-05-05\n",
      "  done: false\n",
      "  episode_len_mean: 170.56\n",
      "  episode_reward_max: 361.4622948920455\n",
      "  episode_reward_mean: 293.2136636613534\n",
      "  episode_reward_min: 254.7793234559658\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 2907\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3219.734\n",
      "    load_time_ms: 2.451\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.421085492696024e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8967106938362122\n",
      "      kl: 0.0036759632639586926\n",
      "      policy_loss: -0.0019375171978026628\n",
      "      total_loss: 44.30312728881836\n",
      "      vf_explained_var: 0.9239022731781006\n",
      "      vf_loss: 44.30506896972656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2789771445967466e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8204253315925598\n",
      "      kl: 0.01114669255912304\n",
      "      policy_loss: -0.0021698276977986097\n",
      "      total_loss: 350.68402099609375\n",
      "      vf_explained_var: 0.8351448178291321\n",
      "      vf_loss: 350.6861877441406\n",
      "    sample_time_ms: 21176.635\n",
      "    update_time_ms: 7.14\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 103.80284843229737\n",
      "    rl_1: 189.41081522905608\n",
      "  time_since_restore: 1757.6944813728333\n",
      "  time_this_iter_s: 25.755512952804565\n",
      "  time_total_s: 1757.6944813728333\n",
      "  timestamp: 1552399505\n",
      "  timesteps_since_restore: 610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 61\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1757 s, 61 iter, 610000 ts, 293 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-05-32\n",
      "  done: false\n",
      "  episode_len_mean: 171.63\n",
      "  episode_reward_max: 361.4622948920455\n",
      "  episode_reward_mean: 293.6283206092896\n",
      "  episode_reward_min: 255.80340988915032\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 2966\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3275.391\n",
      "    load_time_ms: 2.448\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9057878255844116\n",
      "      kl: 0.006560344714671373\n",
      "      policy_loss: -0.0018546328647062182\n",
      "      total_loss: 36.53333282470703\n",
      "      vf_explained_var: 0.9371153116226196\n",
      "      vf_loss: 36.53518295288086\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2789771445967466e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8168185353279114\n",
      "      kl: 0.010209711268544197\n",
      "      policy_loss: -0.0035450144205242395\n",
      "      total_loss: 364.2404479980469\n",
      "      vf_explained_var: 0.8339106440544128\n",
      "      vf_loss: 364.24407958984375\n",
      "    sample_time_ms: 21328.823\n",
      "    update_time_ms: 7.582\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 103.28090271954709\n",
      "    rl_1: 190.3474178897425\n",
      "  time_since_restore: 1784.3010029792786\n",
      "  time_this_iter_s: 26.606521606445312\n",
      "  time_total_s: 1784.3010029792786\n",
      "  timestamp: 1552399532\n",
      "  timesteps_since_restore: 620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 62\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1784 s, 62 iter, 620000 ts, 294 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-06-00\n",
      "  done: false\n",
      "  episode_len_mean: 169.58\n",
      "  episode_reward_max: 363.70652605525345\n",
      "  episode_reward_mean: 292.48416623030926\n",
      "  episode_reward_min: 252.91335912428565\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 3024\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3284.637\n",
      "    load_time_ms: 2.452\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.55271373174006e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8553999066352844\n",
      "      kl: 0.0062704309821128845\n",
      "      policy_loss: -0.003804916748777032\n",
      "      total_loss: 33.651241302490234\n",
      "      vf_explained_var: 0.9418386816978455\n",
      "      vf_loss: 33.65504837036133\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2789771445967466e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7733519077301025\n",
      "      kl: 0.0034733451902866364\n",
      "      policy_loss: 0.0004914271412417293\n",
      "      total_loss: 346.13861083984375\n",
      "      vf_explained_var: 0.8491804599761963\n",
      "      vf_loss: 346.13818359375\n",
      "    sample_time_ms: 21720.77\n",
      "    update_time_ms: 7.402\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 101.79824782405913\n",
      "    rl_1: 190.68591840625015\n",
      "  time_since_restore: 1812.3412029743195\n",
      "  time_this_iter_s: 28.040199995040894\n",
      "  time_total_s: 1812.3412029743195\n",
      "  timestamp: 1552399560\n",
      "  timesteps_since_restore: 630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 63\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1812 s, 63 iter, 630000 ts, 292 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-06-27\n",
      "  done: false\n",
      "  episode_len_mean: 171.45\n",
      "  episode_reward_max: 363.70652605525345\n",
      "  episode_reward_mean: 296.52494847049013\n",
      "  episode_reward_min: 252.91335912428565\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 3083\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3358.396\n",
      "    load_time_ms: 2.457\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.77635686587003e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9175621271133423\n",
      "      kl: 0.0035589539911597967\n",
      "      policy_loss: 4.8748050176072866e-05\n",
      "      total_loss: 34.48545455932617\n",
      "      vf_explained_var: 0.9425039887428284\n",
      "      vf_loss: 34.48540496826172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.394885722983733e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.832330584526062\n",
      "      kl: 0.00660277483984828\n",
      "      policy_loss: -0.0017595734680071473\n",
      "      total_loss: 331.8362121582031\n",
      "      vf_explained_var: 0.8602344989776611\n",
      "      vf_loss: 331.837890625\n",
      "    sample_time_ms: 21968.773\n",
      "    update_time_ms: 7.465\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 105.35942166956156\n",
      "    rl_1: 191.16552680092857\n",
      "  time_since_restore: 1839.3111364841461\n",
      "  time_this_iter_s: 26.96993350982666\n",
      "  time_total_s: 1839.3111364841461\n",
      "  timestamp: 1552399587\n",
      "  timesteps_since_restore: 640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 64\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1839 s, 64 iter, 640000 ts, 297 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-07-03\n",
      "  done: false\n",
      "  episode_len_mean: 170.29\n",
      "  episode_reward_max: 357.68039085343815\n",
      "  episode_reward_mean: 294.61856995423915\n",
      "  episode_reward_min: 255.8573460805399\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 3141\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3365.523\n",
      "    load_time_ms: 2.548\n",
      "    num_steps_sampled: 650000\n",
      "    num_steps_trained: 650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.88178432935015e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8284774422645569\n",
      "      kl: 0.009317923337221146\n",
      "      policy_loss: -0.002731515094637871\n",
      "      total_loss: 30.15762710571289\n",
      "      vf_explained_var: 0.9488850831985474\n",
      "      vf_loss: 30.16035270690918\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.1974428614918666e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7285681366920471\n",
      "      kl: 0.009208017028868198\n",
      "      policy_loss: -0.003164492081850767\n",
      "      total_loss: 328.3739013671875\n",
      "      vf_explained_var: 0.8602346777915955\n",
      "      vf_loss: 328.3770446777344\n",
      "    sample_time_ms: 23072.046\n",
      "    update_time_ms: 8.324\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 104.02608242633103\n",
      "    rl_1: 190.5924875279082\n",
      "  time_since_restore: 1874.7881112098694\n",
      "  time_this_iter_s: 35.47697472572327\n",
      "  time_total_s: 1874.7881112098694\n",
      "  timestamp: 1552399623\n",
      "  timesteps_since_restore: 650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 650000\n",
      "  training_iteration: 65\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1874 s, 65 iter, 650000 ts, 295 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-07-30\n",
      "  done: false\n",
      "  episode_len_mean: 168.29\n",
      "  episode_reward_max: 330.91895625917755\n",
      "  episode_reward_mean: 291.0977237467845\n",
      "  episode_reward_min: 250.71087693234244\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 3201\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3416.989\n",
      "    load_time_ms: 2.596\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.784001350402832\n",
      "      kl: 0.018053606152534485\n",
      "      policy_loss: -0.0034810297656804323\n",
      "      total_loss: 23.877439498901367\n",
      "      vf_explained_var: 0.9598797559738159\n",
      "      vf_loss: 23.880916595458984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5987214307459333e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7205147743225098\n",
      "      kl: 0.007607373874634504\n",
      "      policy_loss: -0.0010806542122736573\n",
      "      total_loss: 332.89068603515625\n",
      "      vf_explained_var: 0.8595874309539795\n",
      "      vf_loss: 332.8917236328125\n",
      "    sample_time_ms: 23357.597\n",
      "    update_time_ms: 8.345\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 100.72469233247341\n",
      "    rl_1: 190.37303141431116\n",
      "  time_since_restore: 1902.3982264995575\n",
      "  time_this_iter_s: 27.61011528968811\n",
      "  time_total_s: 1902.3982264995575\n",
      "  timestamp: 1552399650\n",
      "  timesteps_since_restore: 660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 66\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1902 s, 66 iter, 660000 ts, 291 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-07-59\n",
      "  done: false\n",
      "  episode_len_mean: 168.68\n",
      "  episode_reward_max: 336.22797123327587\n",
      "  episode_reward_mean: 290.9241632629768\n",
      "  episode_reward_min: 250.71087693234244\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 3261\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3422.244\n",
      "    load_time_ms: 2.733\n",
      "    num_steps_sampled: 670000\n",
      "    num_steps_trained: 670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8298283219337463\n",
      "      kl: 0.003790829796344042\n",
      "      policy_loss: -0.0017107067396864295\n",
      "      total_loss: 24.926803588867188\n",
      "      vf_explained_var: 0.9601115584373474\n",
      "      vf_loss: 24.92851448059082\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.993607153729666e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7709978818893433\n",
      "      kl: 0.005577361676841974\n",
      "      policy_loss: -0.002169647952541709\n",
      "      total_loss: 315.35943603515625\n",
      "      vf_explained_var: 0.8715165257453918\n",
      "      vf_loss: 315.3615417480469\n",
      "    sample_time_ms: 23834.949\n",
      "    update_time_ms: 8.411\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 101.22956225474891\n",
      "    rl_1: 189.69460100822792\n",
      "  time_since_restore: 1931.434207201004\n",
      "  time_this_iter_s: 29.035980701446533\n",
      "  time_total_s: 1931.434207201004\n",
      "  timestamp: 1552399679\n",
      "  timesteps_since_restore: 670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 670000\n",
      "  training_iteration: 67\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1931 s, 67 iter, 670000 ts, 291 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-08-29\n",
      "  done: false\n",
      "  episode_len_mean: 167.05\n",
      "  episode_reward_max: 336.22797123327587\n",
      "  episode_reward_mean: 290.3945765950895\n",
      "  episode_reward_min: 254.5860223732947\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 3321\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3434.477\n",
      "    load_time_ms: 2.665\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2204460823375376e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7760946750640869\n",
      "      kl: 0.004495858680456877\n",
      "      policy_loss: -0.0015966410282999277\n",
      "      total_loss: 20.2056884765625\n",
      "      vf_explained_var: 0.9672490358352661\n",
      "      vf_loss: 20.207284927368164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.996803576864833e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7372326254844666\n",
      "      kl: 0.04668233171105385\n",
      "      policy_loss: -0.008257543668150902\n",
      "      total_loss: 302.93994140625\n",
      "      vf_explained_var: 0.8762627840042114\n",
      "      vf_loss: 302.9482116699219\n",
      "    sample_time_ms: 24317.106\n",
      "    update_time_ms: 8.643\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 101.31680921018709\n",
      "    rl_1: 189.07776738490247\n",
      "  time_since_restore: 1960.9803051948547\n",
      "  time_this_iter_s: 29.546097993850708\n",
      "  time_total_s: 1960.9803051948547\n",
      "  timestamp: 1552399709\n",
      "  timesteps_since_restore: 680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 68\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1960 s, 68 iter, 680000 ts, 290 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-08-58\n",
      "  done: false\n",
      "  episode_len_mean: 166.54\n",
      "  episode_reward_max: 362.9665700222359\n",
      "  episode_reward_mean: 290.3089946655259\n",
      "  episode_reward_min: 253.94655919959675\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 3381\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3470.982\n",
      "    load_time_ms: 2.657\n",
      "    num_steps_sampled: 690000\n",
      "    num_steps_trained: 690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1102230411687688e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7856123447418213\n",
      "      kl: 0.004201518837362528\n",
      "      policy_loss: -0.001668555662035942\n",
      "      total_loss: 18.288951873779297\n",
      "      vf_explained_var: 0.9707857966423035\n",
      "      vf_loss: 18.290620803833008\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.995204438854964e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7395402193069458\n",
      "      kl: 0.0031891765538603067\n",
      "      policy_loss: -0.0003075193671975285\n",
      "      total_loss: 296.9566955566406\n",
      "      vf_explained_var: 0.8819459080696106\n",
      "      vf_loss: 296.95697021484375\n",
      "    sample_time_ms: 24758.275\n",
      "    update_time_ms: 8.991\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 101.19511573847221\n",
      "    rl_1: 189.11387892705366\n",
      "  time_since_restore: 1989.7199640274048\n",
      "  time_this_iter_s: 28.73965883255005\n",
      "  time_total_s: 1989.7199640274048\n",
      "  timestamp: 1552399738\n",
      "  timesteps_since_restore: 690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 690000\n",
      "  training_iteration: 69\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 1989 s, 69 iter, 690000 ts, 290 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-09-30\n",
      "  done: false\n",
      "  episode_len_mean: 167.63\n",
      "  episode_reward_max: 362.9665700222359\n",
      "  episode_reward_mean: 292.21924576544643\n",
      "  episode_reward_min: 259.1365502006783\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 3440\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3483.628\n",
      "    load_time_ms: 2.636\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.551115205843844e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.761993944644928\n",
      "      kl: 0.012266441248357296\n",
      "      policy_loss: -0.0034351865760982037\n",
      "      total_loss: 18.76188850402832\n",
      "      vf_explained_var: 0.9695829749107361\n",
      "      vf_loss: 18.765323638916016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.997602219427482e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7386922836303711\n",
      "      kl: 0.007436707150191069\n",
      "      policy_loss: -0.0014046874130144715\n",
      "      total_loss: 287.25299072265625\n",
      "      vf_explained_var: 0.8851666450500488\n",
      "      vf_loss: 287.25439453125\n",
      "    sample_time_ms: 25496.454\n",
      "    update_time_ms: 9.133\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 102.41773262090473\n",
      "    rl_1: 189.8015131445417\n",
      "  time_since_restore: 2022.0436766147614\n",
      "  time_this_iter_s: 32.32371258735657\n",
      "  time_total_s: 2022.0436766147614\n",
      "  timestamp: 1552399770\n",
      "  timesteps_since_restore: 700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 70\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2022 s, 70 iter, 700000 ts, 292 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-09-59\n",
      "  done: false\n",
      "  episode_len_mean: 167.57\n",
      "  episode_reward_max: 329.49726024468856\n",
      "  episode_reward_mean: 292.0015355702073\n",
      "  episode_reward_min: 261.7204810053219\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 3500\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3502.432\n",
      "    load_time_ms: 2.589\n",
      "    num_steps_sampled: 710000\n",
      "    num_steps_trained: 710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.551115205843844e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7515249252319336\n",
      "      kl: 0.010637563653290272\n",
      "      policy_loss: -0.0023900794330984354\n",
      "      total_loss: 14.401908874511719\n",
      "      vf_explained_var: 0.9770746231079102\n",
      "      vf_loss: 14.404298782348633\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.498801109713741e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7417550683021545\n",
      "      kl: 0.00842469185590744\n",
      "      policy_loss: -0.0015211879508569837\n",
      "      total_loss: 285.7307434082031\n",
      "      vf_explained_var: 0.8916714787483215\n",
      "      vf_loss: 285.7322692871094\n",
      "    sample_time_ms: 25725.824\n",
      "    update_time_ms: 9.01\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 101.81003191747104\n",
      "    rl_1: 190.19150365273626\n",
      "  time_since_restore: 2050.276876449585\n",
      "  time_this_iter_s: 28.23319983482361\n",
      "  time_total_s: 2050.276876449585\n",
      "  timestamp: 1552399799\n",
      "  timesteps_since_restore: 710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 710000\n",
      "  training_iteration: 71\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2050 s, 71 iter, 710000 ts, 292 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-10-27\n",
      "  done: false\n",
      "  episode_len_mean: 166.18\n",
      "  episode_reward_max: 331.6814110825576\n",
      "  episode_reward_mean: 292.3400899180139\n",
      "  episode_reward_min: 261.7204810053219\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 3561\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3518.242\n",
      "    load_time_ms: 2.608\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.551115205843844e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6605620980262756\n",
      "      kl: 0.009471138939261436\n",
      "      policy_loss: -0.0017356238095089793\n",
      "      total_loss: 14.256001472473145\n",
      "      vf_explained_var: 0.977424144744873\n",
      "      vf_loss: 14.257737159729004\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.494005548568705e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6961381435394287\n",
      "      kl: 0.05886367708444595\n",
      "      policy_loss: -0.010955310426652431\n",
      "      total_loss: 271.30169677734375\n",
      "      vf_explained_var: 0.8951725959777832\n",
      "      vf_loss: 271.31268310546875\n",
      "    sample_time_ms: 25919.038\n",
      "    update_time_ms: 8.587\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 102.04089933378715\n",
      "    rl_1: 190.29919058422675\n",
      "  time_since_restore: 2078.971858739853\n",
      "  time_this_iter_s: 28.694982290267944\n",
      "  time_total_s: 2078.971858739853\n",
      "  timestamp: 1552399827\n",
      "  timesteps_since_restore: 720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 72\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2078 s, 72 iter, 720000 ts, 292 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-10-58\n",
      "  done: false\n",
      "  episode_len_mean: 165.25\n",
      "  episode_reward_max: 331.6814110825576\n",
      "  episode_reward_mean: 290.99859537889154\n",
      "  episode_reward_min: 257.0451266078745\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 3621\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3561.687\n",
      "    load_time_ms: 2.761\n",
      "    num_steps_sampled: 730000\n",
      "    num_steps_trained: 730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.775557602921922e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6600322127342224\n",
      "      kl: 0.005274529103189707\n",
      "      policy_loss: -0.0005715573206543922\n",
      "      total_loss: 15.175089836120605\n",
      "      vf_explained_var: 0.976729691028595\n",
      "      vf_loss: 15.175661087036133\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1241004683258362e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6691872477531433\n",
      "      kl: 0.024051347747445107\n",
      "      policy_loss: -0.0031290464103221893\n",
      "      total_loss: 267.3475036621094\n",
      "      vf_explained_var: 0.9003689289093018\n",
      "      vf_loss: 267.35064697265625\n",
      "    sample_time_ms: 26163.277\n",
      "    update_time_ms: 9.252\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 101.86559926000571\n",
      "    rl_1: 189.13299611888579\n",
      "  time_since_restore: 2109.9020578861237\n",
      "  time_this_iter_s: 30.930199146270752\n",
      "  time_total_s: 2109.9020578861237\n",
      "  timestamp: 1552399858\n",
      "  timesteps_since_restore: 730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 730000\n",
      "  training_iteration: 73\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2109 s, 73 iter, 730000 ts, 291 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-11-32\n",
      "  done: false\n",
      "  episode_len_mean: 164.94\n",
      "  episode_reward_max: 328.5798131730271\n",
      "  episode_reward_mean: 291.1403589604035\n",
      "  episode_reward_min: 257.0451266078745\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 3682\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3578.407\n",
      "    load_time_ms: 2.77\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.387778801460961e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7081836462020874\n",
      "      kl: 0.0035032029263675213\n",
      "      policy_loss: -0.001347996643744409\n",
      "      total_loss: 11.854470252990723\n",
      "      vf_explained_var: 0.9822812080383301\n",
      "      vf_loss: 11.855819702148438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1241004683258362e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7478058934211731\n",
      "      kl: 0.023188073188066483\n",
      "      policy_loss: -0.0024468526244163513\n",
      "      total_loss: 252.4579620361328\n",
      "      vf_explained_var: 0.9067733883857727\n",
      "      vf_loss: 252.46035766601562\n",
      "    sample_time_ms: 26783.375\n",
      "    update_time_ms: 9.449\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 101.88267208026538\n",
      "    rl_1: 189.2576868801381\n",
      "  time_since_restore: 2143.241539955139\n",
      "  time_this_iter_s: 33.3394820690155\n",
      "  time_total_s: 2143.241539955139\n",
      "  timestamp: 1552399892\n",
      "  timesteps_since_restore: 740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 74\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2143 s, 74 iter, 740000 ts, 291 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-12-02\n",
      "  done: false\n",
      "  episode_len_mean: 166.22\n",
      "  episode_reward_max: 337.9411959587842\n",
      "  episode_reward_mean: 293.5653057693308\n",
      "  episode_reward_min: 257.9395943183645\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 3741\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3606.46\n",
      "    load_time_ms: 2.719\n",
      "    num_steps_sampled: 750000\n",
      "    num_steps_trained: 750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.938894007304805e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7095910310745239\n",
      "      kl: 0.004280410706996918\n",
      "      policy_loss: -0.0016166850691661239\n",
      "      total_loss: 11.566655158996582\n",
      "      vf_explained_var: 0.9828253388404846\n",
      "      vf_loss: 11.568270683288574\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1241004683258362e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7787098288536072\n",
      "      kl: 0.032966408878564835\n",
      "      policy_loss: -0.005283222068101168\n",
      "      total_loss: 251.17703247070312\n",
      "      vf_explained_var: 0.9104609489440918\n",
      "      vf_loss: 251.1822967529297\n",
      "    sample_time_ms: 26281.398\n",
      "    update_time_ms: 8.459\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 103.28749696605193\n",
      "    rl_1: 190.2778088032789\n",
      "  time_since_restore: 2173.967464208603\n",
      "  time_this_iter_s: 30.725924253463745\n",
      "  time_total_s: 2173.967464208603\n",
      "  timestamp: 1552399922\n",
      "  timesteps_since_restore: 750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 750000\n",
      "  training_iteration: 75\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2173 s, 75 iter, 750000 ts, 294 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-12-35\n",
      "  done: false\n",
      "  episode_len_mean: 168.02\n",
      "  episode_reward_max: 350.74120134737353\n",
      "  episode_reward_mean: 296.15121407808357\n",
      "  episode_reward_min: 257.9395943183645\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 3801\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3603.996\n",
      "    load_time_ms: 2.646\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.4694470036524025e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.732623815536499\n",
      "      kl: 0.006429834291338921\n",
      "      policy_loss: -0.0011862237006425858\n",
      "      total_loss: 15.122965812683105\n",
      "      vf_explained_var: 0.9776182770729065\n",
      "      vf_loss: 15.124153137207031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1241004683258362e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8276785612106323\n",
      "      kl: 0.01072854083031416\n",
      "      policy_loss: -0.002236317377537489\n",
      "      total_loss: 238.1357879638672\n",
      "      vf_explained_var: 0.9141661524772644\n",
      "      vf_loss: 238.13807678222656\n",
      "    sample_time_ms: 26746.838\n",
      "    update_time_ms: 8.435\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 104.80336739696237\n",
      "    rl_1: 191.34784668112124\n",
      "  time_since_restore: 2206.209520339966\n",
      "  time_this_iter_s: 32.242056131362915\n",
      "  time_total_s: 2206.209520339966\n",
      "  timestamp: 1552399955\n",
      "  timesteps_since_restore: 760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 76\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2206 s, 76 iter, 760000 ts, 296 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-13-09\n",
      "  done: false\n",
      "  episode_len_mean: 168.15\n",
      "  episode_reward_max: 350.74120134737353\n",
      "  episode_reward_mean: 296.5879572959886\n",
      "  episode_reward_min: 256.84034960476345\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 3860\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3686.984\n",
      "    load_time_ms: 2.539\n",
      "    num_steps_sampled: 770000\n",
      "    num_steps_trained: 770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7146077156066895\n",
      "      kl: 0.002394165378063917\n",
      "      policy_loss: -0.00030550669180229306\n",
      "      total_loss: 13.702516555786133\n",
      "      vf_explained_var: 0.9798078536987305\n",
      "      vf_loss: 13.702821731567383\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1241004683258362e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7965980768203735\n",
      "      kl: 0.006124872248619795\n",
      "      policy_loss: -0.0012969081290066242\n",
      "      total_loss: 236.24436950683594\n",
      "      vf_explained_var: 0.9159427285194397\n",
      "      vf_loss: 236.24563598632812\n",
      "    sample_time_ms: 27169.095\n",
      "    update_time_ms: 8.482\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 105.35559977208091\n",
      "    rl_1: 191.23235752390767\n",
      "  time_since_restore: 2240.2970972061157\n",
      "  time_this_iter_s: 34.0875768661499\n",
      "  time_total_s: 2240.2970972061157\n",
      "  timestamp: 1552399989\n",
      "  timesteps_since_restore: 770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 770000\n",
      "  training_iteration: 77\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2240 s, 77 iter, 770000 ts, 297 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-13-45\n",
      "  done: false\n",
      "  episode_len_mean: 166.19\n",
      "  episode_reward_max: 334.73958650672154\n",
      "  episode_reward_mean: 294.48623522459684\n",
      "  episode_reward_min: 256.84034960476345\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 3920\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3858.174\n",
      "    load_time_ms: 2.595\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6490191221237183\n",
      "      kl: 0.009554748423397541\n",
      "      policy_loss: -0.0016804446931928396\n",
      "      total_loss: 11.26296329498291\n",
      "      vf_explained_var: 0.9836046099662781\n",
      "      vf_loss: 11.264642715454102\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.620502341629181e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7512456178665161\n",
      "      kl: 0.004310662392526865\n",
      "      policy_loss: -0.00090250582434237\n",
      "      total_loss: 236.17803955078125\n",
      "      vf_explained_var: 0.9164706468582153\n",
      "      vf_loss: 236.178955078125\n",
      "    sample_time_ms: 27620.712\n",
      "    update_time_ms: 8.54\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 103.94924924419975\n",
      "    rl_1: 190.5369859803971\n",
      "  time_since_restore: 2276.0752391815186\n",
      "  time_this_iter_s: 35.77814197540283\n",
      "  time_total_s: 2276.0752391815186\n",
      "  timestamp: 1552400025\n",
      "  timesteps_since_restore: 780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 78\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2276 s, 78 iter, 780000 ts, 294 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-14-42\n",
      "  done: false\n",
      "  episode_len_mean: 167.34\n",
      "  episode_reward_max: 354.594246221557\n",
      "  episode_reward_mean: 295.0724619721473\n",
      "  episode_reward_min: 258.88580352803604\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 3980\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4411.191\n",
      "    load_time_ms: 2.813\n",
      "    num_steps_sampled: 790000\n",
      "    num_steps_trained: 790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.336808754565503e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7459118962287903\n",
      "      kl: 0.004416835494339466\n",
      "      policy_loss: -0.0012697216589003801\n",
      "      total_loss: 11.485847473144531\n",
      "      vf_explained_var: 0.9839369654655457\n",
      "      vf_loss: 11.4871187210083\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.8102511708145904e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8267117142677307\n",
      "      kl: 0.011549500748515129\n",
      "      policy_loss: -0.0028229516465216875\n",
      "      total_loss: 221.39285278320312\n",
      "      vf_explained_var: 0.9252783060073853\n",
      "      vf_loss: 221.39566040039062\n",
      "    sample_time_ms: 29956.663\n",
      "    update_time_ms: 8.373\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 104.64539228809049\n",
      "    rl_1: 190.42706968405687\n",
      "  time_since_restore: 2333.7201681137085\n",
      "  time_this_iter_s: 57.64492893218994\n",
      "  time_total_s: 2333.7201681137085\n",
      "  timestamp: 1552400082\n",
      "  timesteps_since_restore: 790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 790000\n",
      "  training_iteration: 79\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2333 s, 79 iter, 790000 ts, 295 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-15-19\n",
      "  done: false\n",
      "  episode_len_mean: 167.47\n",
      "  episode_reward_max: 331.5605833393372\n",
      "  episode_reward_mean: 295.86572467909775\n",
      "  episode_reward_min: 258.88580352803604\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 4040\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4623.853\n",
      "    load_time_ms: 2.801\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.1684043772827515e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.671798825263977\n",
      "      kl: 0.005866638384759426\n",
      "      policy_loss: -0.0026449565775692463\n",
      "      total_loss: 11.064804077148438\n",
      "      vf_explained_var: 0.9843893647193909\n",
      "      vf_loss: 11.067450523376465\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.8102511708145904e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7442216277122498\n",
      "      kl: 0.009076831862330437\n",
      "      policy_loss: -0.002096375683322549\n",
      "      total_loss: 217.90670776367188\n",
      "      vf_explained_var: 0.9243246912956238\n",
      "      vf_loss: 217.90878295898438\n",
      "    sample_time_ms: 30131.557\n",
      "    update_time_ms: 9.249\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 104.98829065316542\n",
      "    rl_1: 190.8774340259323\n",
      "  time_since_restore: 2369.934458255768\n",
      "  time_this_iter_s: 36.214290142059326\n",
      "  time_total_s: 2369.934458255768\n",
      "  timestamp: 1552400119\n",
      "  timesteps_since_restore: 800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 80\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2369 s, 80 iter, 800000 ts, 296 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-16-04\n",
      "  done: false\n",
      "  episode_len_mean: 166.2\n",
      "  episode_reward_max: 334.22617143007113\n",
      "  episode_reward_mean: 294.53144802680384\n",
      "  episode_reward_min: 264.0774482989178\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 4100\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4835.956\n",
      "    load_time_ms: 2.839\n",
      "    num_steps_sampled: 810000\n",
      "    num_steps_trained: 810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842021886413758e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6407111883163452\n",
      "      kl: 0.010182770900428295\n",
      "      policy_loss: -0.0013655399670824409\n",
      "      total_loss: 10.959839820861816\n",
      "      vf_explained_var: 0.984619140625\n",
      "      vf_loss: 10.961206436157227\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4051255854072952e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7473092079162598\n",
      "      kl: 0.017268160358071327\n",
      "      policy_loss: -0.0015144015196710825\n",
      "      total_loss: 208.70803833007812\n",
      "      vf_explained_var: 0.9290530681610107\n",
      "      vf_loss: 208.7095489501953\n",
      "    sample_time_ms: 31614.57\n",
      "    update_time_ms: 9.922\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 104.00376440203432\n",
      "    rl_1: 190.52768362476962\n",
      "  time_since_restore: 2415.1279368400574\n",
      "  time_this_iter_s: 45.19347858428955\n",
      "  time_total_s: 2415.1279368400574\n",
      "  timestamp: 1552400164\n",
      "  timesteps_since_restore: 810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 810000\n",
      "  training_iteration: 81\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2415 s, 81 iter, 810000 ts, 295 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 166.09\n",
      "  episode_reward_max: 346.85517915940807\n",
      "  episode_reward_mean: 293.755964296676\n",
      "  episode_reward_min: 264.63545878043004\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 4160\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4884.237\n",
      "    load_time_ms: 2.84\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842021886413758e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6602378487586975\n",
      "      kl: 0.007098411675542593\n",
      "      policy_loss: -0.0017101468984037638\n",
      "      total_loss: 11.802281379699707\n",
      "      vf_explained_var: 0.9837443232536316\n",
      "      vf_loss: 11.80399227142334\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4051255854072952e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7597513198852539\n",
      "      kl: 0.021257620304822922\n",
      "      policy_loss: -0.001671982347033918\n",
      "      total_loss: 201.70103454589844\n",
      "      vf_explained_var: 0.93230801820755\n",
      "      vf_loss: 201.7026824951172\n",
      "    sample_time_ms: 31871.278\n",
      "    update_time_ms: 9.992\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 103.96990844818407\n",
      "    rl_1: 189.78605584849188\n",
      "  time_since_restore: 2446.8749816417694\n",
      "  time_this_iter_s: 31.747044801712036\n",
      "  time_total_s: 2446.8749816417694\n",
      "  timestamp: 1552400196\n",
      "  timesteps_since_restore: 820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 82\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2446 s, 82 iter, 820000 ts, 294 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-17-30\n",
      "  done: false\n",
      "  episode_len_mean: 165.85\n",
      "  episode_reward_max: 334.02286152413603\n",
      "  episode_reward_mean: 293.5908988371145\n",
      "  episode_reward_min: 260.0031503360905\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 4221\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5321.625\n",
      "    load_time_ms: 3.147\n",
      "    num_steps_sampled: 830000\n",
      "    num_steps_trained: 830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.421010943206879e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6301889419555664\n",
      "      kl: 0.011834247969090939\n",
      "      policy_loss: -0.003165325615555048\n",
      "      total_loss: 10.952620506286621\n",
      "      vf_explained_var: 0.9850751161575317\n",
      "      vf_loss: 10.95578670501709\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4051255854072952e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7195850610733032\n",
      "      kl: 0.007759426720440388\n",
      "      policy_loss: -0.0016226532170549035\n",
      "      total_loss: 197.6360321044922\n",
      "      vf_explained_var: 0.9356042742729187\n",
      "      vf_loss: 197.63766479492188\n",
      "    sample_time_ms: 33716.356\n",
      "    update_time_ms: 10.488\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 103.73231557632823\n",
      "    rl_1: 189.85858326078622\n",
      "  time_since_restore: 2500.6512274742126\n",
      "  time_this_iter_s: 53.77624583244324\n",
      "  time_total_s: 2500.6512274742126\n",
      "  timestamp: 1552400250\n",
      "  timesteps_since_restore: 830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 830000\n",
      "  training_iteration: 83\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2500 s, 83 iter, 830000 ts, 294 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 165.99\n",
      "  episode_reward_max: 349.66288124105705\n",
      "  episode_reward_mean: 292.48230190768334\n",
      "  episode_reward_min: 260.0031503360905\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 4280\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5805.673\n",
      "    load_time_ms: 3.183\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.421010943206879e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6658964157104492\n",
      "      kl: 0.009612848982214928\n",
      "      policy_loss: -0.0022332544904202223\n",
      "      total_loss: 10.278928756713867\n",
      "      vf_explained_var: 0.9863592982292175\n",
      "      vf_loss: 10.28116226196289\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.025627927036476e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7613590359687805\n",
      "      kl: 0.023116743192076683\n",
      "      policy_loss: -0.002182662021368742\n",
      "      total_loss: 182.32139587402344\n",
      "      vf_explained_var: 0.9405519366264343\n",
      "      vf_loss: 182.3235626220703\n",
      "    sample_time_ms: 33968.463\n",
      "    update_time_ms: 11.31\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 103.4305177870598\n",
      "    rl_1: 189.0517841206235\n",
      "  time_since_restore: 2541.38587808609\n",
      "  time_this_iter_s: 40.73465061187744\n",
      "  time_total_s: 2541.38587808609\n",
      "  timestamp: 1552400290\n",
      "  timesteps_since_restore: 840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 84\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2541 s, 84 iter, 840000 ts, 292 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-19-11\n",
      "  done: false\n",
      "  episode_len_mean: 168.36\n",
      "  episode_reward_max: 338.4835989774198\n",
      "  episode_reward_mean: 296.51448100072923\n",
      "  episode_reward_min: 264.4152966622141\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 4340\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6337.415\n",
      "    load_time_ms: 3.434\n",
      "    num_steps_sampled: 850000\n",
      "    num_steps_trained: 850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.7105054716034394e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7108908891677856\n",
      "      kl: 0.0055423942394554615\n",
      "      policy_loss: -0.001231831032782793\n",
      "      total_loss: 10.847134590148926\n",
      "      vf_explained_var: 0.9858939051628113\n",
      "      vf_loss: 10.84836483001709\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.025627927036476e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8223118782043457\n",
      "      kl: 0.004431634210050106\n",
      "      policy_loss: 0.0007404613425023854\n",
      "      total_loss: 173.8119354248047\n",
      "      vf_explained_var: 0.944022536277771\n",
      "      vf_loss: 173.8112030029297\n",
      "    sample_time_ms: 36414.73\n",
      "    update_time_ms: 12.794\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 105.87451779828649\n",
      "    rl_1: 190.6399632024428\n",
      "  time_since_restore: 2601.929337978363\n",
      "  time_this_iter_s: 60.54345989227295\n",
      "  time_total_s: 2601.929337978363\n",
      "  timestamp: 1552400351\n",
      "  timesteps_since_restore: 850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 850000\n",
      "  training_iteration: 85\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2601 s, 85 iter, 850000 ts, 297 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-19-51\n",
      "  done: false\n",
      "  episode_len_mean: 167.45\n",
      "  episode_reward_max: 333.7946178070857\n",
      "  episode_reward_mean: 295.9115790394029\n",
      "  episode_reward_min: 259.1613079182536\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 4399\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6608.522\n",
      "    load_time_ms: 3.458\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3552527358017197e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6665879487991333\n",
      "      kl: 0.004506103694438934\n",
      "      policy_loss: -0.0012222126824781299\n",
      "      total_loss: 11.268400192260742\n",
      "      vf_explained_var: 0.9851133823394775\n",
      "      vf_loss: 11.269622802734375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.512813963518238e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7719791531562805\n",
      "      kl: 0.03264014050364494\n",
      "      policy_loss: -0.00548155652359128\n",
      "      total_loss: 174.15420532226562\n",
      "      vf_explained_var: 0.9440026879310608\n",
      "      vf_loss: 174.15968322753906\n",
      "    sample_time_ms: 36863.941\n",
      "    update_time_ms: 13.675\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 105.20812961988322\n",
      "    rl_1: 190.7034494195197\n",
      "  time_since_restore: 2641.388938188553\n",
      "  time_this_iter_s: 39.45960021018982\n",
      "  time_total_s: 2641.388938188553\n",
      "  timestamp: 1552400391\n",
      "  timesteps_since_restore: 860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 86\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2641 s, 86 iter, 860000 ts, 296 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-20-32\n",
      "  done: false\n",
      "  episode_len_mean: 167.06\n",
      "  episode_reward_max: 338.90797013858395\n",
      "  episode_reward_mean: 295.93920819708495\n",
      "  episode_reward_min: 259.1613079182536\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 4459\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6529.49\n",
      "    load_time_ms: 3.561\n",
      "    num_steps_sampled: 870000\n",
      "    num_steps_trained: 870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.776263679008599e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6334971189498901\n",
      "      kl: 0.008336871862411499\n",
      "      policy_loss: -0.002205182332545519\n",
      "      total_loss: 9.469061851501465\n",
      "      vf_explained_var: 0.9876089096069336\n",
      "      vf_loss: 9.471266746520996\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.512813963518238e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.745951771736145\n",
      "      kl: 0.0038420348428189754\n",
      "      policy_loss: 0.0011083311401307583\n",
      "      total_loss: 165.90438842773438\n",
      "      vf_explained_var: 0.9479808211326599\n",
      "      vf_loss: 165.90330505371094\n",
      "    sample_time_ms: 37685.866\n",
      "    update_time_ms: 14.178\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 105.50405846216466\n",
      "    rl_1: 190.43514973492032\n",
      "  time_since_restore: 2682.9101552963257\n",
      "  time_this_iter_s: 41.52121710777283\n",
      "  time_total_s: 2682.9101552963257\n",
      "  timestamp: 1552400432\n",
      "  timesteps_since_restore: 870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 870000\n",
      "  training_iteration: 87\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2682 s, 87 iter, 870000 ts, 296 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-21-14\n",
      "  done: false\n",
      "  episode_len_mean: 167.51\n",
      "  episode_reward_max: 338.90797013858395\n",
      "  episode_reward_mean: 297.1550956839683\n",
      "  episode_reward_min: 259.8740324327647\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 4519\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6782.701\n",
      "    load_time_ms: 3.938\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.3881318395042993e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6687678694725037\n",
      "      kl: 0.005941764917224646\n",
      "      policy_loss: -0.0018505713669583201\n",
      "      total_loss: 10.284948348999023\n",
      "      vf_explained_var: 0.9867947101593018\n",
      "      vf_loss: 10.286799430847168\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.756406981759119e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7515711784362793\n",
      "      kl: 0.02467145025730133\n",
      "      policy_loss: -0.0023944431450217962\n",
      "      total_loss: 162.17051696777344\n",
      "      vf_explained_var: 0.9481015205383301\n",
      "      vf_loss: 162.17291259765625\n",
      "    sample_time_ms: 38055.67\n",
      "    update_time_ms: 13.996\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 106.64648764313434\n",
      "    rl_1: 190.50860804083393\n",
      "  time_since_restore: 2724.9271697998047\n",
      "  time_this_iter_s: 42.017014503479004\n",
      "  time_total_s: 2724.9271697998047\n",
      "  timestamp: 1552400474\n",
      "  timesteps_since_restore: 880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 88\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2724 s, 88 iter, 880000 ts, 297 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-22-00\n",
      "  done: false\n",
      "  episode_len_mean: 167.37\n",
      "  episode_reward_max: 333.04466563170905\n",
      "  episode_reward_mean: 296.7108812922252\n",
      "  episode_reward_min: 264.70435431707335\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 4580\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6650.639\n",
      "    load_time_ms: 3.906\n",
      "    num_steps_sampled: 890000\n",
      "    num_steps_trained: 890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6940659197521496e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6667159199714661\n",
      "      kl: 0.010252469219267368\n",
      "      policy_loss: -0.0019152021268382668\n",
      "      total_loss: 10.500675201416016\n",
      "      vf_explained_var: 0.9868460297584534\n",
      "      vf_loss: 10.50259017944336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.756406981759119e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7470845580101013\n",
      "      kl: 0.00833059847354889\n",
      "      policy_loss: -0.0011512907221913338\n",
      "      total_loss: 155.24110412597656\n",
      "      vf_explained_var: 0.9503944516181946\n",
      "      vf_loss: 155.2422332763672\n",
      "    sample_time_ms: 37010.69\n",
      "    update_time_ms: 14.362\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 106.49383792789244\n",
      "    rl_1: 190.2170433643327\n",
      "  time_since_restore: 2770.80499625206\n",
      "  time_this_iter_s: 45.87782645225525\n",
      "  time_total_s: 2770.80499625206\n",
      "  timestamp: 1552400520\n",
      "  timesteps_since_restore: 890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 890000\n",
      "  training_iteration: 89\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2770 s, 89 iter, 890000 ts, 297 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-22-49\n",
      "  done: false\n",
      "  episode_len_mean: 167.26\n",
      "  episode_reward_max: 341.6774101742171\n",
      "  episode_reward_mean: 297.0976816076761\n",
      "  episode_reward_min: 262.5435243910061\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 4638\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6674.66\n",
      "    load_time_ms: 4.069\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6940659197521496e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6747797727584839\n",
      "      kl: 0.007173905149102211\n",
      "      policy_loss: -0.0035354720894247293\n",
      "      total_loss: 8.210165977478027\n",
      "      vf_explained_var: 0.9897543787956238\n",
      "      vf_loss: 8.213700294494629\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.782034908795595e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7722111940383911\n",
      "      kl: 0.00923539511859417\n",
      "      policy_loss: -0.0036509698256850243\n",
      "      total_loss: 144.68653869628906\n",
      "      vf_explained_var: 0.9553590416908264\n",
      "      vf_loss: 144.690185546875\n",
      "    sample_time_ms: 38272.473\n",
      "    update_time_ms: 13.759\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 107.41249002252657\n",
      "    rl_1: 189.68519158514954\n",
      "  time_since_restore: 2819.889901638031\n",
      "  time_this_iter_s: 49.08490538597107\n",
      "  time_total_s: 2819.889901638031\n",
      "  timestamp: 1552400569\n",
      "  timesteps_since_restore: 900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 90\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2819 s, 90 iter, 900000 ts, 297 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-23-33\n",
      "  done: false\n",
      "  episode_len_mean: 169.38\n",
      "  episode_reward_max: 341.6774101742171\n",
      "  episode_reward_mean: 301.51965167939613\n",
      "  episode_reward_min: 267.18783702660613\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 4697\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6562.851\n",
      "    load_time_ms: 4.034\n",
      "    num_steps_sampled: 910000\n",
      "    num_steps_trained: 910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.470329598760748e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7427564859390259\n",
      "      kl: 0.005868471693247557\n",
      "      policy_loss: -0.0014218381838873029\n",
      "      total_loss: 12.195755004882812\n",
      "      vf_explained_var: 0.984982430934906\n",
      "      vf_loss: 12.197175979614258\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.391017454397798e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8140296339988708\n",
      "      kl: 0.010354630649089813\n",
      "      policy_loss: -4.0980296034831554e-05\n",
      "      total_loss: 138.67872619628906\n",
      "      vf_explained_var: 0.956457257270813\n",
      "      vf_loss: 138.67880249023438\n",
      "    sample_time_ms: 38218.507\n",
      "    update_time_ms: 13.472\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 110.17521030097909\n",
      "    rl_1: 191.34444137841703\n",
      "  time_since_restore: 2863.4278461933136\n",
      "  time_this_iter_s: 43.53794455528259\n",
      "  time_total_s: 2863.4278461933136\n",
      "  timestamp: 1552400613\n",
      "  timesteps_since_restore: 910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 910000\n",
      "  training_iteration: 91\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2863 s, 91 iter, 910000 ts, 302 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-24-27\n",
      "  done: false\n",
      "  episode_len_mean: 168.51\n",
      "  episode_reward_max: 353.9830635495872\n",
      "  episode_reward_mean: 298.8849208236241\n",
      "  episode_reward_min: 265.9001587225413\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 4757\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6675.726\n",
      "    load_time_ms: 4.135\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.235164799380374e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6870354413986206\n",
      "      kl: 0.004720750730484724\n",
      "      policy_loss: -0.00089832249796018\n",
      "      total_loss: 12.227490425109863\n",
      "      vf_explained_var: 0.9846982955932617\n",
      "      vf_loss: 12.228386878967285\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.391017454397798e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7387464046478271\n",
      "      kl: 0.011357747949659824\n",
      "      policy_loss: -0.00016137250349856913\n",
      "      total_loss: 136.34915161132812\n",
      "      vf_explained_var: 0.9568667411804199\n",
      "      vf_loss: 136.3492889404297\n",
      "    sample_time_ms: 40364.008\n",
      "    update_time_ms: 13.756\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 109.02919769032431\n",
      "    rl_1: 189.85572313329976\n",
      "  time_since_restore: 2917.7624468803406\n",
      "  time_this_iter_s: 54.33460068702698\n",
      "  time_total_s: 2917.7624468803406\n",
      "  timestamp: 1552400667\n",
      "  timesteps_since_restore: 920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 92\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2917 s, 92 iter, 920000 ts, 299 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-24-59\n",
      "  done: false\n",
      "  episode_len_mean: 167.67\n",
      "  episode_reward_max: 342.84039276070445\n",
      "  episode_reward_mean: 297.20405029787685\n",
      "  episode_reward_min: 263.8206132620501\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 4817\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6207.068\n",
      "    load_time_ms: 3.673\n",
      "    num_steps_sampled: 930000\n",
      "    num_steps_trained: 930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.117582399690187e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.698323667049408\n",
      "      kl: 0.005473625380545855\n",
      "      policy_loss: -0.0017389914719387889\n",
      "      total_loss: 11.78339672088623\n",
      "      vf_explained_var: 0.9855078458786011\n",
      "      vf_loss: 11.785137176513672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.391017454397798e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7506601810455322\n",
      "      kl: 0.01364979799836874\n",
      "      policy_loss: -0.00209256773814559\n",
      "      total_loss: 125.66587829589844\n",
      "      vf_explained_var: 0.9608572721481323\n",
      "      vf_loss: 125.66796875\n",
      "    sample_time_ms: 38622.953\n",
      "    update_time_ms: 12.912\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 108.37639496183175\n",
      "    rl_1: 188.82765533604504\n",
      "  time_since_restore: 2949.4102535247803\n",
      "  time_this_iter_s: 31.647806644439697\n",
      "  time_total_s: 2949.4102535247803\n",
      "  timestamp: 1552400699\n",
      "  timesteps_since_restore: 930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 930000\n",
      "  training_iteration: 93\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2949 s, 93 iter, 930000 ts, 297 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-25-30\n",
      "  done: false\n",
      "  episode_len_mean: 169.16\n",
      "  episode_reward_max: 342.84039276070445\n",
      "  episode_reward_mean: 300.51002080037216\n",
      "  episode_reward_min: 260.3957083823064\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 4876\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5658.966\n",
      "    load_time_ms: 3.629\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.741279661655426\n",
      "      kl: 0.0057044243440032005\n",
      "      policy_loss: -0.0010637709638103843\n",
      "      total_loss: 11.797598838806152\n",
      "      vf_explained_var: 0.9859228134155273\n",
      "      vf_loss: 11.798662185668945\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.391017454397798e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8445985317230225\n",
      "      kl: 0.03163250535726547\n",
      "      policy_loss: -0.005960817914456129\n",
      "      total_loss: 119.8847885131836\n",
      "      vf_explained_var: 0.9629822969436646\n",
      "      vf_loss: 119.8907470703125\n",
      "    sample_time_ms: 38162.171\n",
      "    update_time_ms: 12.054\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 110.43014661196274\n",
      "    rl_1: 190.07987418840938\n",
      "  time_since_restore: 2980.0224499702454\n",
      "  time_this_iter_s: 30.612196445465088\n",
      "  time_total_s: 2980.0224499702454\n",
      "  timestamp: 1552400730\n",
      "  timesteps_since_restore: 940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 94\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 2980 s, 94 iter, 940000 ts, 301 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-26-12\n",
      "  done: false\n",
      "  episode_len_mean: 170.79\n",
      "  episode_reward_max: 344.48003420246135\n",
      "  episode_reward_mean: 303.5896374260337\n",
      "  episode_reward_min: 260.3957083823064\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 4934\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5153.878\n",
      "    load_time_ms: 3.394\n",
      "    num_steps_sampled: 950000\n",
      "    num_steps_trained: 950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7158182859420776\n",
      "      kl: 0.0039252652786672115\n",
      "      policy_loss: -0.0017201772425323725\n",
      "      total_loss: 11.727751731872559\n",
      "      vf_explained_var: 0.9861570596694946\n",
      "      vf_loss: 11.729472160339355\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.391017454397798e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8010802865028381\n",
      "      kl: 0.004224632401019335\n",
      "      policy_loss: 0.0006009434582665563\n",
      "      total_loss: 115.5640869140625\n",
      "      vf_explained_var: 0.9645180106163025\n",
      "      vf_loss: 115.56349182128906\n",
      "    sample_time_ms: 36860.621\n",
      "    update_time_ms: 10.551\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 112.34169502913487\n",
      "    rl_1: 191.24794239689876\n",
      "  time_since_restore: 3022.470879793167\n",
      "  time_this_iter_s: 42.44842982292175\n",
      "  time_total_s: 3022.470879793167\n",
      "  timestamp: 1552400772\n",
      "  timesteps_since_restore: 950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 950000\n",
      "  training_iteration: 95\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3022 s, 95 iter, 950000 ts, 304 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-26-52\n",
      "  done: false\n",
      "  episode_len_mean: 169.58\n",
      "  episode_reward_max: 344.48003420246135\n",
      "  episode_reward_mean: 300.3616087240651\n",
      "  episode_reward_min: 262.0512564993097\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 4994\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5110.142\n",
      "    load_time_ms: 3.532\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.646977999612734e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7309709191322327\n",
      "      kl: 0.006994388997554779\n",
      "      policy_loss: -0.0014602887677028775\n",
      "      total_loss: 10.3229398727417\n",
      "      vf_explained_var: 0.9879202246665955\n",
      "      vf_loss: 10.324399948120117\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.195508727198899e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8514919877052307\n",
      "      kl: 0.04484771192073822\n",
      "      policy_loss: -0.005312853027135134\n",
      "      total_loss: 111.14466857910156\n",
      "      vf_explained_var: 0.9660850167274475\n",
      "      vf_loss: 111.14997863769531\n",
      "    sample_time_ms: 36961.863\n",
      "    update_time_ms: 10.391\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 110.56524110344009\n",
      "    rl_1: 189.79636762062498\n",
      "  time_since_restore: 3062.503668308258\n",
      "  time_this_iter_s: 40.03278851509094\n",
      "  time_total_s: 3062.503668308258\n",
      "  timestamp: 1552400812\n",
      "  timesteps_since_restore: 960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 96\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3062 s, 96 iter, 960000 ts, 300 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-27-24\n",
      "  done: false\n",
      "  episode_len_mean: 170.49\n",
      "  episode_reward_max: 346.5995509351789\n",
      "  episode_reward_mean: 302.15707734880294\n",
      "  episode_reward_min: 262.0512564993097\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 5052\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5139.751\n",
      "    load_time_ms: 3.436\n",
      "    num_steps_sampled: 970000\n",
      "    num_steps_trained: 970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.323488999806367e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7218820452690125\n",
      "      kl: 0.006665005348622799\n",
      "      policy_loss: -0.0026905436534434557\n",
      "      total_loss: 12.552239418029785\n",
      "      vf_explained_var: 0.9855462312698364\n",
      "      vf_loss: 12.554929733276367\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.2932647710089674e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.857348620891571\n",
      "      kl: 0.009495006874203682\n",
      "      policy_loss: 0.001535196672193706\n",
      "      total_loss: 105.9981689453125\n",
      "      vf_explained_var: 0.9682937860488892\n",
      "      vf_loss: 105.99663543701172\n",
      "    sample_time_ms: 35940.1\n",
      "    update_time_ms: 9.902\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 112.03552013744383\n",
      "    rl_1: 190.12155721135906\n",
      "  time_since_restore: 3094.0995678901672\n",
      "  time_this_iter_s: 31.59589958190918\n",
      "  time_total_s: 3094.0995678901672\n",
      "  timestamp: 1552400844\n",
      "  timesteps_since_restore: 970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 970000\n",
      "  training_iteration: 97\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3094 s, 97 iter, 970000 ts, 302 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-27-55\n",
      "  done: false\n",
      "  episode_len_mean: 171.35\n",
      "  episode_reward_max: 351.77063430492234\n",
      "  episode_reward_mean: 304.5980237173009\n",
      "  episode_reward_min: 270.0247943571997\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 5110\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4771.567\n",
      "    load_time_ms: 3.075\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.617444999031835e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7236818671226501\n",
      "      kl: 0.011969356797635555\n",
      "      policy_loss: -0.0034218616783618927\n",
      "      total_loss: 10.744155883789062\n",
      "      vf_explained_var: 0.9877985715866089\n",
      "      vf_loss: 10.747577667236328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.6466323855044837e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8402125239372253\n",
      "      kl: 0.012420651502907276\n",
      "      policy_loss: -0.0012511584209278226\n",
      "      total_loss: 101.39720153808594\n",
      "      vf_explained_var: 0.9685534834861755\n",
      "      vf_loss: 101.39845275878906\n",
      "    sample_time_ms: 35176.592\n",
      "    update_time_ms: 9.985\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 113.91296424376895\n",
      "    rl_1: 190.6850594735319\n",
      "  time_since_restore: 3124.7902290821075\n",
      "  time_this_iter_s: 30.690661191940308\n",
      "  time_total_s: 3124.7902290821075\n",
      "  timestamp: 1552400875\n",
      "  timesteps_since_restore: 980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 98\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3124 s, 98 iter, 980000 ts, 305 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 171.65\n",
      "  episode_reward_max: 355.2978858276576\n",
      "  episode_reward_mean: 304.7198586577081\n",
      "  episode_reward_min: 269.0281102964032\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 5168\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4338.938\n",
      "    load_time_ms: 2.917\n",
      "    num_steps_sampled: 990000\n",
      "    num_steps_trained: 990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.617444999031835e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7587990164756775\n",
      "      kl: 0.007649426814168692\n",
      "      policy_loss: -0.0021560722962021828\n",
      "      total_loss: 11.329343795776367\n",
      "      vf_explained_var: 0.9874255657196045\n",
      "      vf_loss: 11.331500053405762\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.6466323855044837e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9034292697906494\n",
      "      kl: 0.00838153064250946\n",
      "      policy_loss: 0.0006916194106452167\n",
      "      total_loss: 94.0591049194336\n",
      "      vf_explained_var: 0.9718573093414307\n",
      "      vf_loss: 94.05841064453125\n",
      "    sample_time_ms: 34480.022\n",
      "    update_time_ms: 9.525\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 114.19470636590867\n",
      "    rl_1: 190.52515229179937\n",
      "  time_since_restore: 3159.3547587394714\n",
      "  time_this_iter_s: 34.56452965736389\n",
      "  time_total_s: 3159.3547587394714\n",
      "  timestamp: 1552400909\n",
      "  timesteps_since_restore: 990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 990000\n",
      "  training_iteration: 99\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3159 s, 99 iter, 990000 ts, 305 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-29-10\n",
      "  done: false\n",
      "  episode_len_mean: 171.92\n",
      "  episode_reward_max: 355.2978858276576\n",
      "  episode_reward_mean: 304.66001073951514\n",
      "  episode_reward_min: 267.779006155095\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 5226\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4440.522\n",
      "    load_time_ms: 2.944\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.3087224995159173e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6874036192893982\n",
      "      kl: 0.007725963369011879\n",
      "      policy_loss: -0.0015180177288129926\n",
      "      total_loss: 11.87037181854248\n",
      "      vf_explained_var: 0.9870959520339966\n",
      "      vf_loss: 11.871889114379883\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8577293157577515\n",
      "      kl: 0.02505495212972164\n",
      "      policy_loss: -0.00329504138790071\n",
      "      total_loss: 91.83212280273438\n",
      "      vf_explained_var: 0.9722002744674683\n",
      "      vf_loss: 91.83541107177734\n",
      "    sample_time_ms: 33513.288\n",
      "    update_time_ms: 9.043\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 114.36527858254644\n",
      "    rl_1: 190.2947321569687\n",
      "  time_since_restore: 3199.7679014205933\n",
      "  time_this_iter_s: 40.413142681121826\n",
      "  time_total_s: 3199.7679014205933\n",
      "  timestamp: 1552400950\n",
      "  timesteps_since_restore: 1000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 100\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3199 s, 100 iter, 1000000 ts, 305 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-29-58\n",
      "  done: false\n",
      "  episode_len_mean: 172.73\n",
      "  episode_reward_max: 344.68514335226104\n",
      "  episode_reward_mean: 308.1282745195875\n",
      "  episode_reward_min: 270.20726524530056\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 5284\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4526.794\n",
      "    load_time_ms: 3.041\n",
      "    num_steps_sampled: 1010000\n",
      "    num_steps_trained: 1010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6543612497579586e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7348337173461914\n",
      "      kl: 0.005419608671218157\n",
      "      policy_loss: -0.003381171962246299\n",
      "      total_loss: 12.177813529968262\n",
      "      vf_explained_var: 0.987273097038269\n",
      "      vf_loss: 12.181195259094238\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9362177848815918\n",
      "      kl: 0.012488765642046928\n",
      "      policy_loss: -0.002155119087547064\n",
      "      total_loss: 85.53838348388672\n",
      "      vf_explained_var: 0.9744152426719666\n",
      "      vf_loss: 85.54054260253906\n",
      "    sample_time_ms: 33886.717\n",
      "    update_time_ms: 9.567\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 116.83961606515719\n",
      "    rl_1: 191.2886584544303\n",
      "  time_since_restore: 3247.905647277832\n",
      "  time_this_iter_s: 48.13774585723877\n",
      "  time_total_s: 3247.905647277832\n",
      "  timestamp: 1552400998\n",
      "  timesteps_since_restore: 1010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1010000\n",
      "  training_iteration: 101\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3247 s, 101 iter, 1010000 ts, 308 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-30-44\n",
      "  done: false\n",
      "  episode_len_mean: 174.73\n",
      "  episode_reward_max: 378.33766659285186\n",
      "  episode_reward_mean: 309.6138134824836\n",
      "  episode_reward_min: 269.9203704559321\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 5340\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4444.471\n",
      "    load_time_ms: 3.028\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.271806248789793e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.765193521976471\n",
      "      kl: 0.005453057121485472\n",
      "      policy_loss: -0.0016793914837762713\n",
      "      total_loss: 16.054447174072266\n",
      "      vf_explained_var: 0.9826456904411316\n",
      "      vf_loss: 16.056129455566406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9463064670562744\n",
      "      kl: 0.01218657661229372\n",
      "      policy_loss: -0.001671935897320509\n",
      "      total_loss: 85.35687255859375\n",
      "      vf_explained_var: 0.9749171733856201\n",
      "      vf_loss: 85.35855865478516\n",
      "    sample_time_ms: 33118.243\n",
      "    update_time_ms: 9.389\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 118.34100412406569\n",
      "    rl_1: 191.2728093584179\n",
      "  time_since_restore: 3293.7292952537537\n",
      "  time_this_iter_s: 45.82364797592163\n",
      "  time_total_s: 3293.7292952537537\n",
      "  timestamp: 1552401044\n",
      "  timesteps_since_restore: 1020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 102\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3293 s, 102 iter, 1020000 ts, 310 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-31-39\n",
      "  done: false\n",
      "  episode_len_mean: 172.44\n",
      "  episode_reward_max: 378.33766659285186\n",
      "  episode_reward_mean: 306.30824862398714\n",
      "  episode_reward_min: 266.16438763719566\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 5399\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4907.366\n",
      "    load_time_ms: 3.127\n",
      "    num_steps_sampled: 1030000\n",
      "    num_steps_trained: 1030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.1359031243948966e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6798012256622314\n",
      "      kl: 0.006573827005922794\n",
      "      policy_loss: -0.001712099532596767\n",
      "      total_loss: 14.720474243164062\n",
      "      vf_explained_var: 0.9843374490737915\n",
      "      vf_loss: 14.722185134887695\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8532242178916931\n",
      "      kl: 0.017024299129843712\n",
      "      policy_loss: -8.874138438841328e-05\n",
      "      total_loss: 85.10839080810547\n",
      "      vf_explained_var: 0.9747083783149719\n",
      "      vf_loss: 85.10848236083984\n",
      "    sample_time_ms: 35019.725\n",
      "    update_time_ms: 9.773\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 116.55285885285757\n",
      "    rl_1: 189.7553897711296\n",
      "  time_since_restore: 3349.0417194366455\n",
      "  time_this_iter_s: 55.312424182891846\n",
      "  time_total_s: 3349.0417194366455\n",
      "  timestamp: 1552401099\n",
      "  timesteps_since_restore: 1030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1030000\n",
      "  training_iteration: 103\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3349 s, 103 iter, 1030000 ts, 306 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-32-30\n",
      "  done: false\n",
      "  episode_len_mean: 171.35\n",
      "  episode_reward_max: 350.0998939130799\n",
      "  episode_reward_mean: 304.9360495971546\n",
      "  episode_reward_min: 264.2170043618642\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 5457\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5274.065\n",
      "    load_time_ms: 3.25\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.0679515621974483e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6811420917510986\n",
      "      kl: 0.005096888169646263\n",
      "      policy_loss: -0.001301672076806426\n",
      "      total_loss: 13.562658309936523\n",
      "      vf_explained_var: 0.9860909581184387\n",
      "      vf_loss: 13.563960075378418\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8623363971710205\n",
      "      kl: 0.019067173823714256\n",
      "      policy_loss: 0.0013931584544479847\n",
      "      total_loss: 76.07294464111328\n",
      "      vf_explained_var: 0.9777618646621704\n",
      "      vf_loss: 76.07154846191406\n",
      "    sample_time_ms: 36654.2\n",
      "    update_time_ms: 10.595\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 115.52290277350703\n",
      "    rl_1: 189.4131468236475\n",
      "  time_since_restore: 3399.6771943569183\n",
      "  time_this_iter_s: 50.63547492027283\n",
      "  time_total_s: 3399.6771943569183\n",
      "  timestamp: 1552401150\n",
      "  timesteps_since_restore: 1040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 104\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3399 s, 104 iter, 1040000 ts, 305 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-33-07\n",
      "  done: false\n",
      "  episode_len_mean: 174.0\n",
      "  episode_reward_max: 349.0441708779232\n",
      "  episode_reward_mean: 307.9314200974431\n",
      "  episode_reward_min: 269.05567585850935\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 5514\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5336.197\n",
      "    load_time_ms: 3.24\n",
      "    num_steps_sampled: 1050000\n",
      "    num_steps_trained: 1050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0339757810987241e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8054108619689941\n",
      "      kl: 0.0028933945577591658\n",
      "      policy_loss: -0.001090855454094708\n",
      "      total_loss: 13.701483726501465\n",
      "      vf_explained_var: 0.9863015413284302\n",
      "      vf_loss: 13.702573776245117\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0189810991287231\n",
      "      kl: 0.0148586081340909\n",
      "      policy_loss: 0.0004838524619117379\n",
      "      total_loss: 68.47777557373047\n",
      "      vf_explained_var: 0.9802190065383911\n",
      "      vf_loss: 68.47727966308594\n",
      "    sample_time_ms: 36078.793\n",
      "    update_time_ms: 10.983\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 117.44666805481289\n",
      "    rl_1: 190.48475204263022\n",
      "  time_since_restore: 3436.9877665042877\n",
      "  time_this_iter_s: 37.310572147369385\n",
      "  time_total_s: 3436.9877665042877\n",
      "  timestamp: 1552401187\n",
      "  timesteps_since_restore: 1050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1050000\n",
      "  training_iteration: 105\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3436 s, 105 iter, 1050000 ts, 308 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-33-45\n",
      "  done: false\n",
      "  episode_len_mean: 175.36\n",
      "  episode_reward_max: 346.61783418840656\n",
      "  episode_reward_mean: 308.46284084765267\n",
      "  episode_reward_min: 148.16954499333127\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 5571\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5323.119\n",
      "    load_time_ms: 3.082\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.169878905493621e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7240746021270752\n",
      "      kl: 0.004275020677596331\n",
      "      policy_loss: -0.0006563206552527845\n",
      "      total_loss: 22.985605239868164\n",
      "      vf_explained_var: 0.9797621369361877\n",
      "      vf_loss: 22.98625946044922\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.913794219493866\n",
      "      kl: 0.026226380839943886\n",
      "      policy_loss: -0.001523457933217287\n",
      "      total_loss: 77.3033447265625\n",
      "      vf_explained_var: 0.9768090844154358\n",
      "      vf_loss: 77.30486297607422\n",
      "    sample_time_ms: 35826.958\n",
      "    update_time_ms: 10.274\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 117.84548996004592\n",
      "    rl_1: 190.61735088760673\n",
      "  time_since_restore: 3474.362398147583\n",
      "  time_this_iter_s: 37.37463164329529\n",
      "  time_total_s: 3474.362398147583\n",
      "  timestamp: 1552401225\n",
      "  timesteps_since_restore: 1060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 106\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3474 s, 106 iter, 1060000 ts, 308 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-34-31\n",
      "  done: false\n",
      "  episode_len_mean: 175.21\n",
      "  episode_reward_max: 370.12962137264026\n",
      "  episode_reward_mean: 309.87586934041286\n",
      "  episode_reward_min: 271.6961743246046\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 5628\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5754.623\n",
      "    load_time_ms: 3.274\n",
      "    num_steps_sampled: 1070000\n",
      "    num_steps_trained: 1070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5849394527468104e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7779583930969238\n",
      "      kl: 0.005096365697681904\n",
      "      policy_loss: -0.0010681620333343744\n",
      "      total_loss: 13.26092529296875\n",
      "      vf_explained_var: 0.9867268800735474\n",
      "      vf_loss: 13.261994361877441\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9925994873046875\n",
      "      kl: 0.01616506464779377\n",
      "      policy_loss: 0.0027141987811774015\n",
      "      total_loss: 64.98529815673828\n",
      "      vf_explained_var: 0.9810686111450195\n",
      "      vf_loss: 64.98258209228516\n",
      "    sample_time_ms: 36845.747\n",
      "    update_time_ms: 10.431\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 118.53287643342142\n",
      "    rl_1: 191.3429929069914\n",
      "  time_since_restore: 3520.4725408554077\n",
      "  time_this_iter_s: 46.11014270782471\n",
      "  time_total_s: 3520.4725408554077\n",
      "  timestamp: 1552401271\n",
      "  timesteps_since_restore: 1070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1070000\n",
      "  training_iteration: 107\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3520 s, 107 iter, 1070000 ts, 310 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-35-02\n",
      "  done: false\n",
      "  episode_len_mean: 174.92\n",
      "  episode_reward_max: 370.12962137264026\n",
      "  episode_reward_mean: 310.49611079154124\n",
      "  episode_reward_min: 271.6961743246046\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 5686\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5689.223\n",
      "    load_time_ms: 3.198\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2924697263734052e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6795447468757629\n",
      "      kl: 0.005295688286423683\n",
      "      policy_loss: -0.0009420078713446856\n",
      "      total_loss: 12.781753540039062\n",
      "      vf_explained_var: 0.9871370196342468\n",
      "      vf_loss: 12.782694816589355\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8714975118637085\n",
      "      kl: 0.016315432265400887\n",
      "      policy_loss: 0.0027808723971247673\n",
      "      total_loss: 62.42856216430664\n",
      "      vf_explained_var: 0.9818328619003296\n",
      "      vf_loss: 62.42578125\n",
      "    sample_time_ms: 36980.934\n",
      "    update_time_ms: 11.401\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 119.58146908016754\n",
      "    rl_1: 190.9146417113737\n",
      "  time_since_restore: 3551.866835832596\n",
      "  time_this_iter_s: 31.39429497718811\n",
      "  time_total_s: 3551.866835832596\n",
      "  timestamp: 1552401302\n",
      "  timesteps_since_restore: 1080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 108\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3551 s, 108 iter, 1080000 ts, 310 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-35-27\n",
      "  done: false\n",
      "  episode_len_mean: 173.81\n",
      "  episode_reward_max: 371.1097641786338\n",
      "  episode_reward_mean: 311.04281179030204\n",
      "  episode_reward_min: 271.9604680373875\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 5744\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5657.461\n",
      "    load_time_ms: 3.173\n",
      "    num_steps_sampled: 1090000\n",
      "    num_steps_trained: 1090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.462348631867026e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7066510319709778\n",
      "      kl: 0.006019134074449539\n",
      "      policy_loss: -0.0009549255482852459\n",
      "      total_loss: 15.538134574890137\n",
      "      vf_explained_var: 0.9847895503044128\n",
      "      vf_loss: 15.539087295532227\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9136742949485779\n",
      "      kl: 0.011425127275288105\n",
      "      policy_loss: 0.0027138744480907917\n",
      "      total_loss: 58.944854736328125\n",
      "      vf_explained_var: 0.9830224514007568\n",
      "      vf_loss: 58.942138671875\n",
      "    sample_time_ms: 36045.688\n",
      "    update_time_ms: 11.553\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 120.55205668815462\n",
      "    rl_1: 190.49075510214738\n",
      "  time_since_restore: 3576.7607324123383\n",
      "  time_this_iter_s: 24.89389657974243\n",
      "  time_total_s: 3576.7607324123383\n",
      "  timestamp: 1552401327\n",
      "  timesteps_since_restore: 1090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1090000\n",
      "  training_iteration: 109\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3576 s, 109 iter, 1090000 ts, 311 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-35-52\n",
      "  done: false\n",
      "  episode_len_mean: 174.81\n",
      "  episode_reward_max: 371.1097641786338\n",
      "  episode_reward_mean: 311.7494991388955\n",
      "  episode_reward_min: 279.1712832550117\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 5800\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5304.598\n",
      "    load_time_ms: 2.941\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.231174315933513e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6886907815933228\n",
      "      kl: 0.007605917751789093\n",
      "      policy_loss: -0.0013670892221853137\n",
      "      total_loss: 11.862899780273438\n",
      "      vf_explained_var: 0.9885865449905396\n",
      "      vf_loss: 11.864265441894531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8765473961830139\n",
      "      kl: 0.01323180366307497\n",
      "      policy_loss: 0.0035629128105938435\n",
      "      total_loss: 52.80670166015625\n",
      "      vf_explained_var: 0.9848735332489014\n",
      "      vf_loss: 52.80314254760742\n",
      "    sample_time_ms: 34803.478\n",
      "    update_time_ms: 11.591\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 121.14478851740279\n",
      "    rl_1: 190.60471062149267\n",
      "  time_since_restore: 3601.210371017456\n",
      "  time_this_iter_s: 24.449638605117798\n",
      "  time_total_s: 3601.210371017456\n",
      "  timestamp: 1552401352\n",
      "  timesteps_since_restore: 1100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 110\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3601 s, 110 iter, 1100000 ts, 312 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-36-17\n",
      "  done: false\n",
      "  episode_len_mean: 173.36\n",
      "  episode_reward_max: 351.22077670057473\n",
      "  episode_reward_mean: 311.8610170516442\n",
      "  episode_reward_min: 272.85797324827894\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 5859\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5058.781\n",
      "    load_time_ms: 2.888\n",
      "    num_steps_sampled: 1110000\n",
      "    num_steps_trained: 1110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6155871579667565e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6483463048934937\n",
      "      kl: 0.008890392258763313\n",
      "      policy_loss: -0.00213070260360837\n",
      "      total_loss: 12.65577220916748\n",
      "      vf_explained_var: 0.9881452918052673\n",
      "      vf_loss: 12.657903671264648\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8750473260879517\n",
      "      kl: 0.029493756592273712\n",
      "      policy_loss: -0.0018335619242861867\n",
      "      total_loss: 52.17449951171875\n",
      "      vf_explained_var: 0.9845388531684875\n",
      "      vf_loss: 52.17633056640625\n",
      "    sample_time_ms: 32699.049\n",
      "    update_time_ms: 10.874\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 120.69698472930375\n",
      "    rl_1: 191.1640323223404\n",
      "  time_since_restore: 3625.834286212921\n",
      "  time_this_iter_s: 24.623915195465088\n",
      "  time_total_s: 3625.834286212921\n",
      "  timestamp: 1552401377\n",
      "  timesteps_since_restore: 1110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1110000\n",
      "  training_iteration: 111\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3625 s, 111 iter, 1110000 ts, 312 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-36-54\n",
      "  done: false\n",
      "  episode_len_mean: 173.3\n",
      "  episode_reward_max: 367.9082883423825\n",
      "  episode_reward_mean: 311.71046903203387\n",
      "  episode_reward_min: 272.6725679727002\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 5916\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5204.9\n",
      "    load_time_ms: 2.822\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.077935789833782e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6735574007034302\n",
      "      kl: 0.00655004708096385\n",
      "      policy_loss: -0.00053868890972808\n",
      "      total_loss: 12.501134872436523\n",
      "      vf_explained_var: 0.9883166551589966\n",
      "      vf_loss: 12.50167465209961\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9426676630973816\n",
      "      kl: 0.03163185343146324\n",
      "      policy_loss: -0.00449834018945694\n",
      "      total_loss: 43.589603424072266\n",
      "      vf_explained_var: 0.9872955679893494\n",
      "      vf_loss: 43.5941047668457\n",
      "    sample_time_ms: 31737.607\n",
      "    update_time_ms: 10.673\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 120.66085553247615\n",
      "    rl_1: 191.04961349955772\n",
      "  time_since_restore: 3663.5047283172607\n",
      "  time_this_iter_s: 37.6704421043396\n",
      "  time_total_s: 3663.5047283172607\n",
      "  timestamp: 1552401414\n",
      "  timesteps_since_restore: 1120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 112\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3663 s, 112 iter, 1120000 ts, 312 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-37-55\n",
      "  done: false\n",
      "  episode_len_mean: 174.36\n",
      "  episode_reward_max: 376.3127304937026\n",
      "  episode_reward_mean: 313.57949224788723\n",
      "  episode_reward_min: 272.6725679727002\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 5974\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5554.08\n",
      "    load_time_ms: 2.836\n",
      "    num_steps_sampled: 1130000\n",
      "    num_steps_trained: 1130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.038967894916891e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7138200402259827\n",
      "      kl: 0.007506462279707193\n",
      "      policy_loss: -0.003136734012514353\n",
      "      total_loss: 13.65243911743164\n",
      "      vf_explained_var: 0.9874160885810852\n",
      "      vf_loss: 13.655576705932617\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0038279294967651\n",
      "      kl: 0.01191154308617115\n",
      "      policy_loss: 0.0019530507270246744\n",
      "      total_loss: 42.3099250793457\n",
      "      vf_explained_var: 0.9875849485397339\n",
      "      vf_loss: 42.30797576904297\n",
      "    sample_time_ms: 31890.72\n",
      "    update_time_ms: 10.054\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 122.50620856915477\n",
      "    rl_1: 191.0732836787325\n",
      "  time_since_restore: 3723.825091600418\n",
      "  time_this_iter_s: 60.32036328315735\n",
      "  time_total_s: 3723.825091600418\n",
      "  timestamp: 1552401475\n",
      "  timesteps_since_restore: 1130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1130000\n",
      "  training_iteration: 113\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3723 s, 113 iter, 1130000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-38-47\n",
      "  done: false\n",
      "  episode_len_mean: 178.15\n",
      "  episode_reward_max: 376.3127304937026\n",
      "  episode_reward_mean: 315.28111314713914\n",
      "  episode_reward_min: 268.1963629876752\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 6028\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5254.741\n",
      "    load_time_ms: 2.964\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.0194839474584456e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8359136581420898\n",
      "      kl: 0.003758492646738887\n",
      "      policy_loss: -0.00047469011042267084\n",
      "      total_loss: 14.324033737182617\n",
      "      vf_explained_var: 0.9868495464324951\n",
      "      vf_loss: 14.324511528015137\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1518386602401733\n",
      "      kl: 0.025058992207050323\n",
      "      policy_loss: 0.0035900259390473366\n",
      "      total_loss: 36.648502349853516\n",
      "      vf_explained_var: 0.9891563057899475\n",
      "      vf_loss: 36.64491271972656\n",
      "    sample_time_ms: 32317.478\n",
      "    update_time_ms: 9.57\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 123.64354299865985\n",
      "    rl_1: 191.63757014847926\n",
      "  time_since_restore: 3775.729063272476\n",
      "  time_this_iter_s: 51.903971672058105\n",
      "  time_total_s: 3775.729063272476\n",
      "  timestamp: 1552401527\n",
      "  timesteps_since_restore: 1140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 114\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3775 s, 114 iter, 1140000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-39-11\n",
      "  done: false\n",
      "  episode_len_mean: 177.96\n",
      "  episode_reward_max: 362.5128529448928\n",
      "  episode_reward_mean: 314.81115329688413\n",
      "  episode_reward_min: 268.1963629876752\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 6086\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5126.813\n",
      "    load_time_ms: 2.917\n",
      "    num_steps_sampled: 1150000\n",
      "    num_steps_trained: 1150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0097419737292228e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.761408269405365\n",
      "      kl: 0.005423174239695072\n",
      "      policy_loss: -0.0007734191603958607\n",
      "      total_loss: 14.076237678527832\n",
      "      vf_explained_var: 0.9870731830596924\n",
      "      vf_loss: 14.077012062072754\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0616146326065063\n",
      "      kl: 0.02071128599345684\n",
      "      policy_loss: 0.0017672079848125577\n",
      "      total_loss: 34.66317367553711\n",
      "      vf_explained_var: 0.9897649884223938\n",
      "      vf_loss: 34.66140365600586\n",
      "    sample_time_ms: 31143.691\n",
      "    update_time_ms: 9.122\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 124.08775298461809\n",
      "    rl_1: 190.72340031226605\n",
      "  time_since_restore: 3800.016448020935\n",
      "  time_this_iter_s: 24.287384748458862\n",
      "  time_total_s: 3800.016448020935\n",
      "  timestamp: 1552401551\n",
      "  timesteps_since_restore: 1150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1150000\n",
      "  training_iteration: 115\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3800 s, 115 iter, 1150000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-39-51\n",
      "  done: false\n",
      "  episode_len_mean: 176.87\n",
      "  episode_reward_max: 375.16037685707624\n",
      "  episode_reward_mean: 316.50079558169466\n",
      "  episode_reward_min: 275.49945063481715\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 6141\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5164.394\n",
      "    load_time_ms: 2.967\n",
      "    num_steps_sampled: 1160000\n",
      "    num_steps_trained: 1160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.048709868646114e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7398433685302734\n",
      "      kl: 0.004813023377209902\n",
      "      policy_loss: -0.0019175204215571284\n",
      "      total_loss: 16.464893341064453\n",
      "      vf_explained_var: 0.9848179817199707\n",
      "      vf_loss: 16.46681022644043\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0625309944152832\n",
      "      kl: 0.013201519846916199\n",
      "      policy_loss: 0.0013810638338327408\n",
      "      total_loss: 32.92219924926758\n",
      "      vf_explained_var: 0.9899536371231079\n",
      "      vf_loss: 32.92082214355469\n",
      "    sample_time_ms: 31339.672\n",
      "    update_time_ms: 9.217\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.08098781476103\n",
      "    rl_1: 190.41980776693362\n",
      "  time_since_restore: 3839.7415812015533\n",
      "  time_this_iter_s: 39.725133180618286\n",
      "  time_total_s: 3839.7415812015533\n",
      "  timestamp: 1552401591\n",
      "  timesteps_since_restore: 1160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1160000\n",
      "  training_iteration: 116\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3839 s, 116 iter, 1160000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-40-44\n",
      "  done: false\n",
      "  episode_len_mean: 178.24\n",
      "  episode_reward_max: 375.16037685707624\n",
      "  episode_reward_mean: 320.58492026335864\n",
      "  episode_reward_min: 273.84293199725766\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6197\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5245.92\n",
      "    load_time_ms: 2.762\n",
      "    num_steps_sampled: 1170000\n",
      "    num_steps_trained: 1170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.524354934323057e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7667356729507446\n",
      "      kl: 0.007339012809097767\n",
      "      policy_loss: -0.0030056980904191732\n",
      "      total_loss: 17.356807708740234\n",
      "      vf_explained_var: 0.9845277667045593\n",
      "      vf_loss: 17.35981559753418\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.118573546409607\n",
      "      kl: 0.013839704915881157\n",
      "      policy_loss: 0.006061396095901728\n",
      "      total_loss: 30.207881927490234\n",
      "      vf_explained_var: 0.9911848902702332\n",
      "      vf_loss: 30.20182228088379\n",
      "    sample_time_ms: 31987.951\n",
      "    update_time_ms: 11.097\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.8848249954781\n",
      "    rl_1: 191.70009526788053\n",
      "  time_since_restore: 3893.166146993637\n",
      "  time_this_iter_s: 53.42456579208374\n",
      "  time_total_s: 3893.166146993637\n",
      "  timestamp: 1552401644\n",
      "  timesteps_since_restore: 1170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1170000\n",
      "  training_iteration: 117\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3893 s, 117 iter, 1170000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-41-58\n",
      "  done: false\n",
      "  episode_len_mean: 179.45\n",
      "  episode_reward_max: 365.9139692236094\n",
      "  episode_reward_mean: 319.9215030110415\n",
      "  episode_reward_min: 272.4101455438884\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6253\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5812.282\n",
      "    load_time_ms: 3.164\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2621774671615285e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7902237176895142\n",
      "      kl: 0.005238502286374569\n",
      "      policy_loss: -0.0015765833668410778\n",
      "      total_loss: 19.368547439575195\n",
      "      vf_explained_var: 0.9830968976020813\n",
      "      vf_loss: 19.3701229095459\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1569478511810303\n",
      "      kl: 0.015764692798256874\n",
      "      policy_loss: 0.0018034798558801413\n",
      "      total_loss: 29.49558448791504\n",
      "      vf_explained_var: 0.9909530878067017\n",
      "      vf_loss: 29.49378776550293\n",
      "    sample_time_ms: 35666.525\n",
      "    update_time_ms: 10.994\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.36210577354285\n",
      "    rl_1: 191.55939723749873\n",
      "  time_since_restore: 3967.028255701065\n",
      "  time_this_iter_s: 73.86210870742798\n",
      "  time_total_s: 3967.028255701065\n",
      "  timestamp: 1552401718\n",
      "  timesteps_since_restore: 1180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 118\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 3967 s, 118 iter, 1180000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 178.97\n",
      "  episode_reward_max: 366.9274619050381\n",
      "  episode_reward_mean: 321.8784944025447\n",
      "  episode_reward_min: 272.4101455438884\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6309\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7401.529\n",
      "    load_time_ms: 3.879\n",
      "    num_steps_sampled: 1190000\n",
      "    num_steps_trained: 1190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.3108873358076425e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7773590683937073\n",
      "      kl: 0.00443275785073638\n",
      "      policy_loss: -0.0016795956762507558\n",
      "      total_loss: 17.39069938659668\n",
      "      vf_explained_var: 0.9848962426185608\n",
      "      vf_loss: 17.392383575439453\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.173983097076416\n",
      "      kl: 0.018498346209526062\n",
      "      policy_loss: 0.00766565790399909\n",
      "      total_loss: 24.814334869384766\n",
      "      vf_explained_var: 0.9925869703292847\n",
      "      vf_loss: 24.806665420532227\n",
      "    sample_time_ms: 41116.677\n",
      "    update_time_ms: 11.704\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.6662043487459\n",
      "    rl_1: 191.2122900537988\n",
      "  time_since_restore: 4062.3703570365906\n",
      "  time_this_iter_s: 95.34210133552551\n",
      "  time_total_s: 4062.3703570365906\n",
      "  timestamp: 1552401814\n",
      "  timesteps_since_restore: 1190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1190000\n",
      "  training_iteration: 119\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4062 s, 119 iter, 1190000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-45-16\n",
      "  done: false\n",
      "  episode_len_mean: 178.83\n",
      "  episode_reward_max: 389.7159784452137\n",
      "  episode_reward_mean: 324.186787717234\n",
      "  episode_reward_min: 275.25314171876636\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6365\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8665.799\n",
      "    load_time_ms: 4.241\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.1554436679038213e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.714218258857727\n",
      "      kl: 0.004073218442499638\n",
      "      policy_loss: -0.0005258841556496918\n",
      "      total_loss: 20.336345672607422\n",
      "      vf_explained_var: 0.982106626033783\n",
      "      vf_loss: 20.336868286132812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0853191614151\n",
      "      kl: 0.016099005937576294\n",
      "      policy_loss: 0.00403487216681242\n",
      "      total_loss: 25.923969268798828\n",
      "      vf_explained_var: 0.991977870464325\n",
      "      vf_loss: 25.919931411743164\n",
      "    sample_time_ms: 47627.781\n",
      "    update_time_ms: 13.938\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 133.4616214714927\n",
      "    rl_1: 190.72516624574132\n",
      "  time_since_restore: 4164.621343135834\n",
      "  time_this_iter_s: 102.25098609924316\n",
      "  time_total_s: 4164.621343135834\n",
      "  timestamp: 1552401916\n",
      "  timesteps_since_restore: 1200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 120\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4164 s, 120 iter, 1200000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-46-47\n",
      "  done: false\n",
      "  episode_len_mean: 176.65\n",
      "  episode_reward_max: 389.7159784452137\n",
      "  episode_reward_mean: 320.2425896390811\n",
      "  episode_reward_min: 102.7171840710426\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6421\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 10032.011\n",
      "    load_time_ms: 4.483\n",
      "    num_steps_sampled: 1210000\n",
      "    num_steps_trained: 1210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5777218339519106e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6897810697555542\n",
      "      kl: 0.0042258515022695065\n",
      "      policy_loss: -0.001390090212225914\n",
      "      total_loss: 44.61127471923828\n",
      "      vf_explained_var: 0.9654912948608398\n",
      "      vf_loss: 44.6126594543457\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0883114337921143\n",
      "      kl: 0.012112713418900967\n",
      "      policy_loss: 0.003451042342931032\n",
      "      total_loss: 41.02064514160156\n",
      "      vf_explained_var: 0.9872086644172668\n",
      "      vf_loss: 41.01719284057617\n",
      "    sample_time_ms: 52861.339\n",
      "    update_time_ms: 15.581\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.53537339417423\n",
      "    rl_1: 189.70721624490673\n",
      "  time_since_restore: 4255.299775838852\n",
      "  time_this_iter_s: 90.67843270301819\n",
      "  time_total_s: 4255.299775838852\n",
      "  timestamp: 1552402007\n",
      "  timesteps_since_restore: 1210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1210000\n",
      "  training_iteration: 121\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4255 s, 121 iter, 1210000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-48-26\n",
      "  done: false\n",
      "  episode_len_mean: 178.71\n",
      "  episode_reward_max: 377.6025960356871\n",
      "  episode_reward_mean: 321.1624244988798\n",
      "  episode_reward_min: 102.7171840710426\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6477\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 10920.368\n",
      "    load_time_ms: 7.46\n",
      "    num_steps_sampled: 1220000\n",
      "    num_steps_trained: 1220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.888609169759553e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7610493898391724\n",
      "      kl: 0.005905077792704105\n",
      "      policy_loss: -0.0004646673914976418\n",
      "      total_loss: 16.658550262451172\n",
      "      vf_explained_var: 0.9864590167999268\n",
      "      vf_loss: 16.65901756286621\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1519638299942017\n",
      "      kl: 0.028802910819649696\n",
      "      policy_loss: 0.013612193986773491\n",
      "      total_loss: 21.885643005371094\n",
      "      vf_explained_var: 0.9934222102165222\n",
      "      vf_loss: 21.87203025817871\n",
      "    sample_time_ms: 58139.629\n",
      "    update_time_ms: 19.054\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 130.87979529241585\n",
      "    rl_1: 190.28262920646392\n",
      "  time_since_restore: 4354.716200590134\n",
      "  time_this_iter_s: 99.41642475128174\n",
      "  time_total_s: 4354.716200590134\n",
      "  timestamp: 1552402106\n",
      "  timesteps_since_restore: 1220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1220000\n",
      "  training_iteration: 122\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4354 s, 122 iter, 1220000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-49-18\n",
      "  done: false\n",
      "  episode_len_mean: 178.08\n",
      "  episode_reward_max: 377.6025960356871\n",
      "  episode_reward_mean: 321.21216625069866\n",
      "  episode_reward_min: 270.75118038625124\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6533\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 10143.249\n",
      "    load_time_ms: 7.387\n",
      "    num_steps_sampled: 1230000\n",
      "    num_steps_trained: 1230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.9443045848797766e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7150845527648926\n",
      "      kl: 0.004555084276944399\n",
      "      policy_loss: -0.0006866234471090138\n",
      "      total_loss: 17.99761390686035\n",
      "      vf_explained_var: 0.9854206442832947\n",
      "      vf_loss: 17.998300552368164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.122766137123108\n",
      "      kl: 0.020977118983864784\n",
      "      policy_loss: 0.011685926467180252\n",
      "      total_loss: 18.76181983947754\n",
      "      vf_explained_var: 0.9942197799682617\n",
      "      vf_loss: 18.75013542175293\n",
      "    sample_time_ms: 58064.375\n",
      "    update_time_ms: 19.855\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.75422983545081\n",
      "    rl_1: 189.45793641524781\n",
      "  time_since_restore: 4406.5125913619995\n",
      "  time_this_iter_s: 51.796390771865845\n",
      "  time_total_s: 4406.5125913619995\n",
      "  timestamp: 1552402158\n",
      "  timesteps_since_restore: 1230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1230000\n",
      "  training_iteration: 123\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4406 s, 123 iter, 1230000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-50-04\n",
      "  done: false\n",
      "  episode_len_mean: 179.53\n",
      "  episode_reward_max: 368.7319662248836\n",
      "  episode_reward_mean: 322.2672114269592\n",
      "  episode_reward_min: 127.29341238053661\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 6588\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 10311.844\n",
      "    load_time_ms: 7.142\n",
      "    num_steps_sampled: 1240000\n",
      "    num_steps_trained: 1240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9721522924398883e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.793269157409668\n",
      "      kl: 0.004498952068388462\n",
      "      policy_loss: -0.001274845446459949\n",
      "      total_loss: 33.21784973144531\n",
      "      vf_explained_var: 0.9756253361701965\n",
      "      vf_loss: 33.2191162109375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2164818048477173\n",
      "      kl: 0.019430071115493774\n",
      "      policy_loss: 0.009668652899563313\n",
      "      total_loss: 32.45328903198242\n",
      "      vf_explained_var: 0.989859938621521\n",
      "      vf_loss: 32.44361877441406\n",
      "    sample_time_ms: 57307.74\n",
      "    update_time_ms: 19.467\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.65438300212344\n",
      "    rl_1: 187.61282842483578\n",
      "  time_since_restore: 4452.530978918076\n",
      "  time_this_iter_s: 46.01838755607605\n",
      "  time_total_s: 4452.530978918076\n",
      "  timestamp: 1552402204\n",
      "  timesteps_since_restore: 1240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1240000\n",
      "  training_iteration: 124\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4452 s, 124 iter, 1240000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-51-10\n",
      "  done: false\n",
      "  episode_len_mean: 181.45\n",
      "  episode_reward_max: 381.72018196588306\n",
      "  episode_reward_mean: 317.3255817276073\n",
      "  episode_reward_min: 127.29341238053661\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6644\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 10712.829\n",
      "    load_time_ms: 7.469\n",
      "    num_steps_sampled: 1250000\n",
      "    num_steps_trained: 1250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.860761462199441e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.80447918176651\n",
      "      kl: 0.004182243254035711\n",
      "      policy_loss: -0.0009761251858435571\n",
      "      total_loss: 44.8179817199707\n",
      "      vf_explained_var: 0.9724885821342468\n",
      "      vf_loss: 44.8189582824707\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.233161927522419e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2365713119506836\n",
      "      kl: 0.009137988090515137\n",
      "      policy_loss: 0.001421577064320445\n",
      "      total_loss: 42.87950897216797\n",
      "      vf_explained_var: 0.9874410629272461\n",
      "      vf_loss: 42.87808609008789\n",
      "    sample_time_ms: 61055.948\n",
      "    update_time_ms: 20.147\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.7115810044747\n",
      "    rl_1: 185.61400072313262\n",
      "  time_since_restore: 4518.337267637253\n",
      "  time_this_iter_s: 65.80628871917725\n",
      "  time_total_s: 4518.337267637253\n",
      "  timestamp: 1552402270\n",
      "  timesteps_since_restore: 1250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1250000\n",
      "  training_iteration: 125\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4518 s, 125 iter, 1250000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-52-15\n",
      "  done: false\n",
      "  episode_len_mean: 180.93\n",
      "  episode_reward_max: 381.72018196588306\n",
      "  episode_reward_mean: 319.8353151941337\n",
      "  episode_reward_min: 145.96295392176444\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6700\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 10941.964\n",
      "    load_time_ms: 7.844\n",
      "    num_steps_sampled: 1260000\n",
      "    num_steps_trained: 1260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.930380731099721e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7213571667671204\n",
      "      kl: 0.006054671481251717\n",
      "      policy_loss: -0.0012339247623458505\n",
      "      total_loss: 28.04232406616211\n",
      "      vf_explained_var: 0.9793389439582825\n",
      "      vf_loss: 28.043554306030273\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.116580963761209e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1648610830307007\n",
      "      kl: 0.028113478794693947\n",
      "      policy_loss: 0.007199252489954233\n",
      "      total_loss: 27.56501007080078\n",
      "      vf_explained_var: 0.9910633563995361\n",
      "      vf_loss: 27.557817459106445\n",
      "    sample_time_ms: 63290.173\n",
      "    update_time_ms: 21.109\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.19116057442497\n",
      "    rl_1: 187.64415461970873\n",
      "  time_since_restore: 4582.716235637665\n",
      "  time_this_iter_s: 64.37896800041199\n",
      "  time_total_s: 4582.716235637665\n",
      "  timestamp: 1552402335\n",
      "  timesteps_since_restore: 1260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1260000\n",
      "  training_iteration: 126\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4582 s, 126 iter, 1260000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-53-16\n",
      "  done: false\n",
      "  episode_len_mean: 179.5\n",
      "  episode_reward_max: 373.99265626696825\n",
      "  episode_reward_mean: 324.04825544590824\n",
      "  episode_reward_min: 131.257525706884\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 6754\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 11146.168\n",
      "    load_time_ms: 8.038\n",
      "    num_steps_sampled: 1270000\n",
      "    num_steps_trained: 1270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4651903655498604e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7073994874954224\n",
      "      kl: 0.008738361299037933\n",
      "      policy_loss: -0.0024560033343732357\n",
      "      total_loss: 36.22959518432617\n",
      "      vf_explained_var: 0.9750694036483765\n",
      "      vf_loss: 36.2320556640625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.116580963761209e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1734017133712769\n",
      "      kl: 0.013788399286568165\n",
      "      policy_loss: 0.0033168084919452667\n",
      "      total_loss: 35.14815902709961\n",
      "      vf_explained_var: 0.9890908598899841\n",
      "      vf_loss: 35.14484405517578\n",
      "    sample_time_ms: 63838.275\n",
      "    update_time_ms: 19.372\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.9008389602642\n",
      "    rl_1: 189.14741648564407\n",
      "  time_since_restore: 4643.658704042435\n",
      "  time_this_iter_s: 60.9424684047699\n",
      "  time_total_s: 4643.658704042435\n",
      "  timestamp: 1552402396\n",
      "  timesteps_since_restore: 1270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1270000\n",
      "  training_iteration: 127\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4643 s, 127 iter, 1270000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-54-15\n",
      "  done: false\n",
      "  episode_len_mean: 180.96\n",
      "  episode_reward_max: 381.73261198594764\n",
      "  episode_reward_mean: 321.0375353961733\n",
      "  episode_reward_min: 131.257525706884\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 6810\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 11094.715\n",
      "    load_time_ms: 7.711\n",
      "    num_steps_sampled: 1280000\n",
      "    num_steps_trained: 1280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2325951827749302e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.703842282295227\n",
      "      kl: 0.004965796135365963\n",
      "      policy_loss: -0.0017727360827848315\n",
      "      total_loss: 25.815162658691406\n",
      "      vf_explained_var: 0.9824976921081543\n",
      "      vf_loss: 25.816930770874023\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.116580963761209e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1736570596694946\n",
      "      kl: 0.02098962664604187\n",
      "      policy_loss: 0.003773879259824753\n",
      "      total_loss: 29.000093460083008\n",
      "      vf_explained_var: 0.9910815358161926\n",
      "      vf_loss: 28.996318817138672\n",
      "    sample_time_ms: 62432.337\n",
      "    update_time_ms: 19.213\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 133.8701950792334\n",
      "    rl_1: 187.16734031693989\n",
      "  time_since_restore: 4702.952587127686\n",
      "  time_this_iter_s: 59.293883085250854\n",
      "  time_total_s: 4702.952587127686\n",
      "  timestamp: 1552402455\n",
      "  timesteps_since_restore: 1280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1280000\n",
      "  training_iteration: 128\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4702 s, 128 iter, 1280000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-55-07\n",
      "  done: false\n",
      "  episode_len_mean: 181.8\n",
      "  episode_reward_max: 381.73261198594764\n",
      "  episode_reward_mean: 325.1111757514954\n",
      "  episode_reward_min: 148.39378121639487\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 6865\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 9988.591\n",
      "    load_time_ms: 7.383\n",
      "    num_steps_sampled: 1290000\n",
      "    num_steps_trained: 1290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.162975913874651e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7888201475143433\n",
      "      kl: 0.00781001802533865\n",
      "      policy_loss: -0.0015341839753091335\n",
      "      total_loss: 21.143604278564453\n",
      "      vf_explained_var: 0.9857996702194214\n",
      "      vf_loss: 21.145139694213867\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.116580963761209e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2854241132736206\n",
      "      kl: 0.025905419141054153\n",
      "      policy_loss: 0.005518538877367973\n",
      "      total_loss: 21.20920181274414\n",
      "      vf_explained_var: 0.993707001209259\n",
      "      vf_loss: 21.203683853149414\n",
      "    sample_time_ms: 59199.226\n",
      "    update_time_ms: 20.713\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.82563627802872\n",
      "    rl_1: 189.28553947346666\n",
      "  time_since_restore: 4754.887488603592\n",
      "  time_this_iter_s: 51.93490147590637\n",
      "  time_total_s: 4754.887488603592\n",
      "  timestamp: 1552402507\n",
      "  timesteps_since_restore: 1290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1290000\n",
      "  training_iteration: 129\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4754 s, 129 iter, 1290000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-56-00\n",
      "  done: false\n",
      "  episode_len_mean: 184.16\n",
      "  episode_reward_max: 373.2155568921851\n",
      "  episode_reward_mean: 324.24000283120046\n",
      "  episode_reward_min: 138.81407808727715\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 6918\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 9177.851\n",
      "    load_time_ms: 7.162\n",
      "    num_steps_sampled: 1300000\n",
      "    num_steps_trained: 1300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.0814879569373254e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8095552921295166\n",
      "      kl: 0.005292286165058613\n",
      "      policy_loss: -0.0016104242531582713\n",
      "      total_loss: 49.48099899291992\n",
      "      vf_explained_var: 0.9706507921218872\n",
      "      vf_loss: 49.48261642456055\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.116580963761209e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3677644729614258\n",
      "      kl: 0.008883417584002018\n",
      "      policy_loss: 0.0010073091834783554\n",
      "      total_loss: 54.53940200805664\n",
      "      vf_explained_var: 0.9841189384460449\n",
      "      vf_loss: 54.53840637207031\n",
      "    sample_time_ms: 55044.323\n",
      "    update_time_ms: 19.156\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 136.69376314957978\n",
      "    rl_1: 187.54623968162062\n",
      "  time_since_restore: 4807.445132255554\n",
      "  time_this_iter_s: 52.55764365196228\n",
      "  time_total_s: 4807.445132255554\n",
      "  timestamp: 1552402560\n",
      "  timesteps_since_restore: 1300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1300000\n",
      "  training_iteration: 130\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4807 s, 130 iter, 1300000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-56-50\n",
      "  done: false\n",
      "  episode_len_mean: 181.6\n",
      "  episode_reward_max: 374.60756649778585\n",
      "  episode_reward_mean: 316.15772091921986\n",
      "  episode_reward_min: -139.2881469368766\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 6975\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8255.899\n",
      "    load_time_ms: 7.099\n",
      "    num_steps_sampled: 1310000\n",
      "    num_steps_trained: 1310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5407439784686627e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7186652421951294\n",
      "      kl: 0.004738736432045698\n",
      "      policy_loss: -0.00015660852659493685\n",
      "      total_loss: 100.62396240234375\n",
      "      vf_explained_var: 0.935300886631012\n",
      "      vf_loss: 100.62410736083984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.0582904818806046e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.188632845878601\n",
      "      kl: 36.21879577636719\n",
      "      policy_loss: 0.057832278311252594\n",
      "      total_loss: 111.72211456298828\n",
      "      vf_explained_var: 0.9645645022392273\n",
      "      vf_loss: 111.66431427001953\n",
      "    sample_time_ms: 51864.061\n",
      "    update_time_ms: 17.692\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.7691046463421\n",
      "    rl_1: 183.38861627287773\n",
      "  time_since_restore: 4857.0523319244385\n",
      "  time_this_iter_s: 49.60719966888428\n",
      "  time_total_s: 4857.0523319244385\n",
      "  timestamp: 1552402610\n",
      "  timesteps_since_restore: 1310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1310000\n",
      "  training_iteration: 131\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4857 s, 131 iter, 1310000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-58-06\n",
      "  done: false\n",
      "  episode_len_mean: 177.97\n",
      "  episode_reward_max: 374.60756649778585\n",
      "  episode_reward_mean: 320.3218662730966\n",
      "  episode_reward_min: -139.2881469368766\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 7032\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7943.186\n",
      "    load_time_ms: 4.422\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.703719892343314e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6354014277458191\n",
      "      kl: 0.007672782056033611\n",
      "      policy_loss: -0.0013026295928284526\n",
      "      total_loss: 32.05070114135742\n",
      "      vf_explained_var: 0.9776710271835327\n",
      "      vf_loss: 32.051998138427734\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0874348342479833e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1969789266586304\n",
      "      kl: 0.020423900336027145\n",
      "      policy_loss: 0.00211896700784564\n",
      "      total_loss: 29.731538772583008\n",
      "      vf_explained_var: 0.9901765584945679\n",
      "      vf_loss: 29.729427337646484\n",
      "    sample_time_ms: 49880.801\n",
      "    update_time_ms: 14.553\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 133.88442523432025\n",
      "    rl_1: 186.43744103877637\n",
      "  time_since_restore: 4933.47766828537\n",
      "  time_this_iter_s: 76.4253363609314\n",
      "  time_total_s: 4933.47766828537\n",
      "  timestamp: 1552402686\n",
      "  timesteps_since_restore: 1320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 132\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 4933 s, 132 iter, 1320000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_15-59-47\n",
      "  done: false\n",
      "  episode_len_mean: 176.56\n",
      "  episode_reward_max: 368.71839459521794\n",
      "  episode_reward_mean: 323.0722166487924\n",
      "  episode_reward_min: 171.40489160146876\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 7088\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8940.42\n",
      "    load_time_ms: 7.657\n",
      "    num_steps_sampled: 1330000\n",
      "    num_steps_trained: 1330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.851859946171657e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6547408699989319\n",
      "      kl: 0.0037072880659252405\n",
      "      policy_loss: -0.0004204113211017102\n",
      "      total_loss: 25.240081787109375\n",
      "      vf_explained_var: 0.9825366139411926\n",
      "      vf_loss: 25.240501403808594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0874348342479833e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1633461713790894\n",
      "      kl: 0.020478380843997\n",
      "      policy_loss: 0.002828905126079917\n",
      "      total_loss: 25.623313903808594\n",
      "      vf_explained_var: 0.9919590950012207\n",
      "      vf_loss: 25.6204833984375\n",
      "    sample_time_ms: 53714.912\n",
      "    update_time_ms: 20.431\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 133.51029971507455\n",
      "    rl_1: 189.56191693371787\n",
      "  time_since_restore: 5033.712181329727\n",
      "  time_this_iter_s: 100.2345130443573\n",
      "  time_total_s: 5033.712181329727\n",
      "  timestamp: 1552402787\n",
      "  timesteps_since_restore: 1330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1330000\n",
      "  training_iteration: 133\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5033 s, 133 iter, 1330000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-00-49\n",
      "  done: false\n",
      "  episode_len_mean: 177.7\n",
      "  episode_reward_max: 369.0841417433668\n",
      "  episode_reward_mean: 317.41123958778314\n",
      "  episode_reward_min: 137.56260184409746\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 7144\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8835.875\n",
      "    load_time_ms: 7.801\n",
      "    num_steps_sampled: 1340000\n",
      "    num_steps_trained: 1340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9259299730858284e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6525645852088928\n",
      "      kl: 0.0052796052768826485\n",
      "      policy_loss: -0.0004074886965099722\n",
      "      total_loss: 40.79166793823242\n",
      "      vf_explained_var: 0.9754132032394409\n",
      "      vf_loss: 40.792076110839844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0874348342479833e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1881327629089355\n",
      "      kl: 0.03008962981402874\n",
      "      policy_loss: 0.007549668662250042\n",
      "      total_loss: 43.94453811645508\n",
      "      vf_explained_var: 0.9877833127975464\n",
      "      vf_loss: 43.936988830566406\n",
      "    sample_time_ms: 55390.469\n",
      "    update_time_ms: 21.45\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.75212947908744\n",
      "    rl_1: 184.65911010869556\n",
      "  time_since_restore: 5095.457794904709\n",
      "  time_this_iter_s: 61.74561357498169\n",
      "  time_total_s: 5095.457794904709\n",
      "  timestamp: 1552402849\n",
      "  timesteps_since_restore: 1340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1340000\n",
      "  training_iteration: 134\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5095 s, 134 iter, 1340000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-01-27\n",
      "  done: false\n",
      "  episode_len_mean: 177.1\n",
      "  episode_reward_max: 375.53331624019603\n",
      "  episode_reward_mean: 320.195752597088\n",
      "  episode_reward_min: 137.56260184409746\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 7200\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8616.096\n",
      "    load_time_ms: 7.723\n",
      "    num_steps_sampled: 1350000\n",
      "    num_steps_trained: 1350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.629649865429142e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5953810214996338\n",
      "      kl: 0.006822591181844473\n",
      "      policy_loss: -0.0005253117997199297\n",
      "      total_loss: 18.67649269104004\n",
      "      vf_explained_var: 0.987512469291687\n",
      "      vf_loss: 18.677017211914062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0874348342479833e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1291447877883911\n",
      "      kl: 0.03406831994652748\n",
      "      policy_loss: 0.007345983758568764\n",
      "      total_loss: 21.70780372619629\n",
      "      vf_explained_var: 0.9932219386100769\n",
      "      vf_loss: 21.700454711914062\n",
      "    sample_time_ms: 52887.845\n",
      "    update_time_ms: 20.804\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.64577504288917\n",
      "    rl_1: 185.54997755419885\n",
      "  time_since_restore: 5134.024379968643\n",
      "  time_this_iter_s: 38.566585063934326\n",
      "  time_total_s: 5134.024379968643\n",
      "  timestamp: 1552402887\n",
      "  timesteps_since_restore: 1350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1350000\n",
      "  training_iteration: 135\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5134 s, 135 iter, 1350000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-02-07\n",
      "  done: false\n",
      "  episode_len_mean: 177.93\n",
      "  episode_reward_max: 375.53331624019603\n",
      "  episode_reward_mean: 326.14704731796735\n",
      "  episode_reward_min: 151.69678576539155\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 7256\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8370.887\n",
      "    load_time_ms: 7.556\n",
      "    num_steps_sampled: 1360000\n",
      "    num_steps_trained: 1360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.814824932714571e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5778234004974365\n",
      "      kl: 0.006632815580815077\n",
      "      policy_loss: -0.0012283471878618002\n",
      "      total_loss: 20.17453384399414\n",
      "      vf_explained_var: 0.9864391684532166\n",
      "      vf_loss: 20.17576026916504\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0874348342479833e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1354894638061523\n",
      "      kl: 0.022847414016723633\n",
      "      policy_loss: 0.0030025250744074583\n",
      "      total_loss: 22.353071212768555\n",
      "      vf_explained_var: 0.9931145906448364\n",
      "      vf_loss: 22.350067138671875\n",
      "    sample_time_ms: 50712.867\n",
      "    update_time_ms: 20.205\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 136.63210184048978\n",
      "    rl_1: 189.51494547747757\n",
      "  time_since_restore: 5174.178748130798\n",
      "  time_this_iter_s: 40.15436816215515\n",
      "  time_total_s: 5174.178748130798\n",
      "  timestamp: 1552402927\n",
      "  timesteps_since_restore: 1360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1360000\n",
      "  training_iteration: 136\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5174 s, 136 iter, 1360000 ts, 326 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-02-47\n",
      "  done: false\n",
      "  episode_len_mean: 177.54\n",
      "  episode_reward_max: 374.1056293192243\n",
      "  episode_reward_mean: 326.34522337576635\n",
      "  episode_reward_min: 289.99806112069905\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 7313\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7765.096\n",
      "    load_time_ms: 7.354\n",
      "    num_steps_sampled: 1370000\n",
      "    num_steps_trained: 1370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4074124663572855e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.601728081703186\n",
      "      kl: 0.006405684631317854\n",
      "      policy_loss: -0.0012001811992377043\n",
      "      total_loss: 15.332521438598633\n",
      "      vf_explained_var: 0.9886305332183838\n",
      "      vf_loss: 15.333724975585938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0874348342479833e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1457659006118774\n",
      "      kl: 0.019374027848243713\n",
      "      policy_loss: 0.0043494608253240585\n",
      "      total_loss: 12.209120750427246\n",
      "      vf_explained_var: 0.9959180951118469\n",
      "      vf_loss: 12.20477294921875\n",
      "    sample_time_ms: 49204.076\n",
      "    update_time_ms: 20.535\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.6368930646141\n",
      "    rl_1: 190.70833031115217\n",
      "  time_since_restore: 5213.962122440338\n",
      "  time_this_iter_s: 39.783374309539795\n",
      "  time_total_s: 5213.962122440338\n",
      "  timestamp: 1552402967\n",
      "  timesteps_since_restore: 1370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1370000\n",
      "  training_iteration: 137\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5213 s, 137 iter, 1370000 ts, 326 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-03-32\n",
      "  done: false\n",
      "  episode_len_mean: 177.17\n",
      "  episode_reward_max: 383.2692477406153\n",
      "  episode_reward_mean: 325.05386097804285\n",
      "  episode_reward_min: 114.73581802517657\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 7369\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7896.618\n",
      "    load_time_ms: 7.443\n",
      "    num_steps_sampled: 1380000\n",
      "    num_steps_trained: 1380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037062331786428e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5588229298591614\n",
      "      kl: 0.005485354922711849\n",
      "      policy_loss: -0.0010126703418791294\n",
      "      total_loss: 22.538389205932617\n",
      "      vf_explained_var: 0.984530508518219\n",
      "      vf_loss: 22.539400100708008\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0874348342479833e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.124521255493164\n",
      "      kl: 0.02743113972246647\n",
      "      policy_loss: 0.003788480767980218\n",
      "      total_loss: 18.407800674438477\n",
      "      vf_explained_var: 0.994350254535675\n",
      "      vf_loss: 18.404008865356445\n",
      "    sample_time_ms: 47618.035\n",
      "    update_time_ms: 20.1\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.98749266930298\n",
      "    rl_1: 189.06636830873992\n",
      "  time_since_restore: 5258.692841053009\n",
      "  time_this_iter_s: 44.7307186126709\n",
      "  time_total_s: 5258.692841053009\n",
      "  timestamp: 1552403012\n",
      "  timesteps_since_restore: 1380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1380000\n",
      "  training_iteration: 138\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5258 s, 138 iter, 1380000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-04-14\n",
      "  done: false\n",
      "  episode_len_mean: 177.95\n",
      "  episode_reward_max: 383.2692477406153\n",
      "  episode_reward_mean: 325.3553771716631\n",
      "  episode_reward_min: 60.28502777642001\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 7426\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7595.207\n",
      "    load_time_ms: 7.202\n",
      "    num_steps_sampled: 1390000\n",
      "    num_steps_trained: 1390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531165893214e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5805261135101318\n",
      "      kl: 0.0033608709927648306\n",
      "      policy_loss: 0.0001435247395420447\n",
      "      total_loss: 33.94844436645508\n",
      "      vf_explained_var: 0.9765819907188416\n",
      "      vf_loss: 33.94829559326172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0874348342479833e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1499961614608765\n",
      "      kl: 0.016934718936681747\n",
      "      policy_loss: 0.0017471670871600509\n",
      "      total_loss: 29.667041778564453\n",
      "      vf_explained_var: 0.9905328750610352\n",
      "      vf_loss: 29.66529655456543\n",
      "    sample_time_ms: 46961.339\n",
      "    update_time_ms: 18.446\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 136.98735172350698\n",
      "    rl_1: 188.36802544815615\n",
      "  time_since_restore: 5301.017169952393\n",
      "  time_this_iter_s: 42.324328899383545\n",
      "  time_total_s: 5301.017169952393\n",
      "  timestamp: 1552403054\n",
      "  timesteps_since_restore: 1390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1390000\n",
      "  training_iteration: 139\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5301 s, 139 iter, 1390000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-04-49\n",
      "  done: false\n",
      "  episode_len_mean: 179.01\n",
      "  episode_reward_max: 379.2135773569388\n",
      "  episode_reward_mean: 330.55013219940815\n",
      "  episode_reward_min: 275.80316261423036\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 7481\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7560.973\n",
      "    load_time_ms: 7.108\n",
      "    num_steps_sampled: 1400000\n",
      "    num_steps_trained: 1400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.009265582946607e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.629163384437561\n",
      "      kl: 0.005509770009666681\n",
      "      policy_loss: -0.0017279001185670495\n",
      "      total_loss: 18.649749755859375\n",
      "      vf_explained_var: 0.9859158396720886\n",
      "      vf_loss: 18.651477813720703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0874348342479833e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1968351602554321\n",
      "      kl: 34.367923736572266\n",
      "      policy_loss: 0.019178040325641632\n",
      "      total_loss: 13.927671432495117\n",
      "      vf_explained_var: 0.9953556656837463\n",
      "      vf_loss: 13.908489227294922\n",
      "    sample_time_ms: 45217.58\n",
      "    update_time_ms: 17.832\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 139.01491718555653\n",
      "    rl_1: 191.5352150138517\n",
      "  time_since_restore: 5335.7906935215\n",
      "  time_this_iter_s: 34.773523569107056\n",
      "  time_total_s: 5335.7906935215\n",
      "  timestamp: 1552403089\n",
      "  timesteps_since_restore: 1400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1400000\n",
      "  training_iteration: 140\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5335 s, 140 iter, 1400000 ts, 331 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-05-29\n",
      "  done: false\n",
      "  episode_len_mean: 178.49\n",
      "  episode_reward_max: 371.32421516394754\n",
      "  episode_reward_mean: 324.91879760507686\n",
      "  episode_reward_min: 148.9216299723355\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 7538\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7164.266\n",
      "    load_time_ms: 6.952\n",
      "    num_steps_sampled: 1410000\n",
      "    num_steps_trained: 1410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5046327914733034e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6056732535362244\n",
      "      kl: 0.006173666566610336\n",
      "      policy_loss: -0.001454327953979373\n",
      "      total_loss: 21.839258193969727\n",
      "      vf_explained_var: 0.9839279055595398\n",
      "      vf_loss: 21.840715408325195\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.6311516051371214e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1737443208694458\n",
      "      kl: 0.0456705242395401\n",
      "      policy_loss: 0.006644733250141144\n",
      "      total_loss: 14.822854042053223\n",
      "      vf_explained_var: 0.9952107071876526\n",
      "      vf_loss: 14.816208839416504\n",
      "    sample_time_ms: 44625.371\n",
      "    update_time_ms: 17.891\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.44863284898005\n",
      "    rl_1: 189.4701647560968\n",
      "  time_since_restore: 5375.506245136261\n",
      "  time_this_iter_s: 39.71555161476135\n",
      "  time_total_s: 5375.506245136261\n",
      "  timestamp: 1552403129\n",
      "  timesteps_since_restore: 1410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1410000\n",
      "  training_iteration: 141\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5375 s, 141 iter, 1410000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-06-07\n",
      "  done: false\n",
      "  episode_len_mean: 178.25\n",
      "  episode_reward_max: 377.93712952684047\n",
      "  episode_reward_mean: 324.29561553907416\n",
      "  episode_reward_min: 148.9216299723355\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 7594\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6848.963\n",
      "    load_time_ms: 6.627\n",
      "    num_steps_sampled: 1420000\n",
      "    num_steps_trained: 1420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.523163957366517e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5635268092155457\n",
      "      kl: 0.0057598501443862915\n",
      "      policy_loss: -0.001438988372683525\n",
      "      total_loss: 27.021503448486328\n",
      "      vf_explained_var: 0.9812822341918945\n",
      "      vf_loss: 27.0229434967041\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.94673063887995e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1451606750488281\n",
      "      kl: 0.013819790445268154\n",
      "      policy_loss: 0.0015615408774465322\n",
      "      total_loss: 19.69355010986328\n",
      "      vf_explained_var: 0.9939681887626648\n",
      "      vf_loss: 19.691991806030273\n",
      "    sample_time_ms: 41060.787\n",
      "    update_time_ms: 17.624\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.35845666639537\n",
      "    rl_1: 188.93715887267882\n",
      "  time_since_restore: 5413.091288805008\n",
      "  time_this_iter_s: 37.58504366874695\n",
      "  time_total_s: 5413.091288805008\n",
      "  timestamp: 1552403167\n",
      "  timesteps_since_restore: 1420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1420000\n",
      "  training_iteration: 142\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5413 s, 142 iter, 1420000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-07-05\n",
      "  done: false\n",
      "  episode_len_mean: 175.22\n",
      "  episode_reward_max: 377.93712952684047\n",
      "  episode_reward_mean: 324.47954365881253\n",
      "  episode_reward_min: 276.8736803545717\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 7651\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6278.876\n",
      "    load_time_ms: 3.56\n",
      "    num_steps_sampled: 1430000\n",
      "    num_steps_trained: 1430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7615819786832586e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.50401371717453\n",
      "      kl: 0.004397011362016201\n",
      "      policy_loss: -0.0009888159111142159\n",
      "      total_loss: 19.46086883544922\n",
      "      vf_explained_var: 0.9849367141723633\n",
      "      vf_loss: 19.46185874938965\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.94673063887995e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.056758999824524\n",
      "      kl: 0.013051779940724373\n",
      "      policy_loss: 0.0018665962852537632\n",
      "      total_loss: 11.300020217895508\n",
      "      vf_explained_var: 0.9961637854576111\n",
      "      vf_loss: 11.298154830932617\n",
      "    sample_time_ms: 37426.218\n",
      "    update_time_ms: 11.707\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.54222152910464\n",
      "    rl_1: 189.9373221297079\n",
      "  time_since_restore: 5471.16999912262\n",
      "  time_this_iter_s: 58.078710317611694\n",
      "  time_total_s: 5471.16999912262\n",
      "  timestamp: 1552403225\n",
      "  timesteps_since_restore: 1430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1430000\n",
      "  training_iteration: 143\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5471 s, 143 iter, 1430000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-08-08\n",
      "  done: false\n",
      "  episode_len_mean: 175.0\n",
      "  episode_reward_max: 373.0252511289567\n",
      "  episode_reward_mean: 317.8449024921788\n",
      "  episode_reward_min: 135.89392800649358\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 7707\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6662.721\n",
      "    load_time_ms: 3.628\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8807909893416293e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.551163375377655\n",
      "      kl: 0.005523678380995989\n",
      "      policy_loss: -0.0013874558499082923\n",
      "      total_loss: 28.002792358398438\n",
      "      vf_explained_var: 0.9805117845535278\n",
      "      vf_loss: 28.004179000854492\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.94673063887995e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1096668243408203\n",
      "      kl: 0.016652731224894524\n",
      "      policy_loss: 0.0006483567995019257\n",
      "      total_loss: 30.04642677307129\n",
      "      vf_explained_var: 0.9913979768753052\n",
      "      vf_loss: 30.045780181884766\n",
      "    sample_time_ms: 37157.713\n",
      "    update_time_ms: 11.621\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.2363676790683\n",
      "    rl_1: 186.60853481311054\n",
      "  time_since_restore: 5534.079012155533\n",
      "  time_this_iter_s: 62.90901303291321\n",
      "  time_total_s: 5534.079012155533\n",
      "  timestamp: 1552403288\n",
      "  timesteps_since_restore: 1440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 144\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5534 s, 144 iter, 1440000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-08-52\n",
      "  done: false\n",
      "  episode_len_mean: 175.73\n",
      "  episode_reward_max: 391.7967330726152\n",
      "  episode_reward_mean: 314.91081349566406\n",
      "  episode_reward_min: 103.87658181450439\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 7765\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6594.644\n",
      "    load_time_ms: 3.414\n",
      "    num_steps_sampled: 1450000\n",
      "    num_steps_trained: 1450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.403954246058914e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.45225638151168823\n",
      "      kl: 0.00646133441478014\n",
      "      policy_loss: -0.002023466397076845\n",
      "      total_loss: 39.7080192565918\n",
      "      vf_explained_var: 0.9752165675163269\n",
      "      vf_loss: 39.71004104614258\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.94673063887995e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.976385235786438\n",
      "      kl: 0.01300947368144989\n",
      "      policy_loss: 0.0014421016676351428\n",
      "      total_loss: 37.464759826660156\n",
      "      vf_explained_var: 0.9897522330284119\n",
      "      vf_loss: 37.463321685791016\n",
      "    sample_time_ms: 37771.249\n",
      "    update_time_ms: 12.708\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.06818800119504\n",
      "    rl_1: 183.84262549446908\n",
      "  time_since_restore: 5578.106295585632\n",
      "  time_this_iter_s: 44.02728343009949\n",
      "  time_total_s: 5578.106295585632\n",
      "  timestamp: 1552403332\n",
      "  timesteps_since_restore: 1450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1450000\n",
      "  training_iteration: 145\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5578 s, 145 iter, 1450000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-09-23\n",
      "  done: false\n",
      "  episode_len_mean: 175.06\n",
      "  episode_reward_max: 388.58096305548816\n",
      "  episode_reward_mean: 319.4204802832608\n",
      "  episode_reward_min: 103.87658181450439\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 7822\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6389.588\n",
      "    load_time_ms: 3.353\n",
      "    num_steps_sampled: 1460000\n",
      "    num_steps_trained: 1460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.701977123029457e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.47655564546585083\n",
      "      kl: 0.004937204997986555\n",
      "      policy_loss: -0.0006651957519352436\n",
      "      total_loss: 18.80277442932129\n",
      "      vf_explained_var: 0.9860013127326965\n",
      "      vf_loss: 18.803438186645508\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.94673063887995e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9918736219406128\n",
      "      kl: 0.032490842044353485\n",
      "      policy_loss: 0.004063652362674475\n",
      "      total_loss: 11.62671184539795\n",
      "      vf_explained_var: 0.9962803721427917\n",
      "      vf_loss: 11.622647285461426\n",
      "    sample_time_ms: 37019.996\n",
      "    update_time_ms: 12.506\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.03054039758143\n",
      "    rl_1: 187.38993988567944\n",
      "  time_since_restore: 5608.684410095215\n",
      "  time_this_iter_s: 30.57811450958252\n",
      "  time_total_s: 5608.684410095215\n",
      "  timestamp: 1552403363\n",
      "  timesteps_since_restore: 1460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1460000\n",
      "  training_iteration: 146\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5608 s, 146 iter, 1460000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-10-00\n",
      "  done: false\n",
      "  episode_len_mean: 176.19\n",
      "  episode_reward_max: 388.58096305548816\n",
      "  episode_reward_mean: 324.58562585680636\n",
      "  episode_reward_min: 282.5464007589661\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 7878\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6334.226\n",
      "    load_time_ms: 3.398\n",
      "    num_steps_sampled: 1470000\n",
      "    num_steps_trained: 1470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3509892621639607e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5500503182411194\n",
      "      kl: 0.007069242186844349\n",
      "      policy_loss: -0.0008967677131295204\n",
      "      total_loss: 18.382715225219727\n",
      "      vf_explained_var: 0.9859874248504639\n",
      "      vf_loss: 18.38361358642578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.94673063887995e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1068607568740845\n",
      "      kl: 79.62159729003906\n",
      "      policy_loss: 0.017682624980807304\n",
      "      total_loss: 12.099715232849121\n",
      "      vf_explained_var: 0.9961344599723816\n",
      "      vf_loss: 12.082033157348633\n",
      "    sample_time_ms: 36791.484\n",
      "    update_time_ms: 11.925\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.6170476930066\n",
      "    rl_1: 189.96857816379972\n",
      "  time_since_restore: 5645.628178596497\n",
      "  time_this_iter_s: 36.94376850128174\n",
      "  time_total_s: 5645.628178596497\n",
      "  timestamp: 1552403400\n",
      "  timesteps_since_restore: 1470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1470000\n",
      "  training_iteration: 147\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5645 s, 147 iter, 1470000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-10-34\n",
      "  done: false\n",
      "  episode_len_mean: 175.99\n",
      "  episode_reward_max: 380.18410315020157\n",
      "  episode_reward_mean: 326.3138843007452\n",
      "  episode_reward_min: 264.6440197295091\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 7935\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5687.261\n",
      "    load_time_ms: 3.221\n",
      "    num_steps_sampled: 1480000\n",
      "    num_steps_trained: 1480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1754946310819804e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5068965554237366\n",
      "      kl: 0.005439375527203083\n",
      "      policy_loss: -0.0016335340915247798\n",
      "      total_loss: 24.36268424987793\n",
      "      vf_explained_var: 0.9817718267440796\n",
      "      vf_loss: 24.364322662353516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0420091757793377e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0845935344696045\n",
      "      kl: 57.102264404296875\n",
      "      policy_loss: 0.00036290258867666125\n",
      "      total_loss: 12.70937728881836\n",
      "      vf_explained_var: 0.9959364533424377\n",
      "      vf_loss: 12.709013938903809\n",
      "    sample_time_ms: 36353.63\n",
      "    update_time_ms: 12.043\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.86940252995194\n",
      "    rl_1: 190.44448177079323\n",
      "  time_since_restore: 5679.49693107605\n",
      "  time_this_iter_s: 33.86875247955322\n",
      "  time_total_s: 5679.49693107605\n",
      "  timestamp: 1552403434\n",
      "  timesteps_since_restore: 1480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1480000\n",
      "  training_iteration: 148\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5679 s, 148 iter, 1480000 ts, 326 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-11-00\n",
      "  done: false\n",
      "  episode_len_mean: 174.2\n",
      "  episode_reward_max: 369.1237526405454\n",
      "  episode_reward_mean: 321.1956675845242\n",
      "  episode_reward_min: 124.72839877942272\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 7993\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5504.572\n",
      "    load_time_ms: 3.064\n",
      "    num_steps_sampled: 1490000\n",
      "    num_steps_trained: 1490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.87746614891758e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.48150572180747986\n",
      "      kl: 0.004782611504197121\n",
      "      policy_loss: -0.0005884559941478074\n",
      "      total_loss: 25.65084457397461\n",
      "      vf_explained_var: 0.981870710849762\n",
      "      vf_loss: 25.65143585205078\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5630139898512053e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0572134256362915\n",
      "      kl: 18.284013748168945\n",
      "      policy_loss: 0.018267851322889328\n",
      "      total_loss: 19.013545989990234\n",
      "      vf_explained_var: 0.9945579171180725\n",
      "      vf_loss: 18.995277404785156\n",
      "    sample_time_ms: 34906.853\n",
      "    update_time_ms: 11.274\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.6527189507048\n",
      "    rl_1: 188.54294863381944\n",
      "  time_since_restore: 5705.513067722321\n",
      "  time_this_iter_s: 26.016136646270752\n",
      "  time_total_s: 5705.513067722321\n",
      "  timestamp: 1552403460\n",
      "  timesteps_since_restore: 1490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1490000\n",
      "  training_iteration: 149\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5705 s, 149 iter, 1490000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-11-23\n",
      "  done: false\n",
      "  episode_len_mean: 174.47\n",
      "  episode_reward_max: 385.18977877407457\n",
      "  episode_reward_mean: 321.85482349996465\n",
      "  episode_reward_min: 124.72839877942272\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 8051\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5089.013\n",
      "    load_time_ms: 3.027\n",
      "    num_steps_sampled: 1500000\n",
      "    num_steps_trained: 1500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.93873307445879e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5126386284828186\n",
      "      kl: 0.007722851354628801\n",
      "      policy_loss: -0.001232656417414546\n",
      "      total_loss: 30.560394287109375\n",
      "      vf_explained_var: 0.9781195521354675\n",
      "      vf_loss: 30.56162452697754\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.344521695635147e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1313321590423584\n",
      "      kl: 17.348430633544922\n",
      "      policy_loss: 0.015574872493743896\n",
      "      total_loss: 21.873645782470703\n",
      "      vf_explained_var: 0.9932073950767517\n",
      "      vf_loss: 21.858070373535156\n",
      "    sample_time_ms: 34182.108\n",
      "    update_time_ms: 11.361\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 133.46319812113518\n",
      "    rl_1: 188.39162537882947\n",
      "  time_since_restore: 5728.880504846573\n",
      "  time_this_iter_s: 23.36743712425232\n",
      "  time_total_s: 5728.880504846573\n",
      "  timestamp: 1552403483\n",
      "  timesteps_since_restore: 1500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1500000\n",
      "  training_iteration: 150\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5728 s, 150 iter, 1500000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-11-50\n",
      "  done: false\n",
      "  episode_len_mean: 173.23\n",
      "  episode_reward_max: 385.18977877407457\n",
      "  episode_reward_mean: 327.6273380464045\n",
      "  episode_reward_min: 287.0493984033049\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 8108\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5065.123\n",
      "    load_time_ms: 2.938\n",
      "    num_steps_sampled: 1510000\n",
      "    num_steps_trained: 1510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4693735437217167e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3974776566028595\n",
      "      kl: 0.005938164424151182\n",
      "      policy_loss: -0.00042987920460291207\n",
      "      total_loss: 19.910259246826172\n",
      "      vf_explained_var: 0.9844526648521423\n",
      "      vf_loss: 19.910690307617188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.516780863242101e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.010094404220581\n",
      "      kl: 66.08729553222656\n",
      "      policy_loss: 0.04607981815934181\n",
      "      total_loss: 10.149599075317383\n",
      "      vf_explained_var: 0.9966990351676941\n",
      "      vf_loss: 10.103520393371582\n",
      "    sample_time_ms: 32884.55\n",
      "    update_time_ms: 11.069\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 136.2621029492259\n",
      "    rl_1: 191.36523509717864\n",
      "  time_since_restore: 5755.376011371613\n",
      "  time_this_iter_s: 26.495506525039673\n",
      "  time_total_s: 5755.376011371613\n",
      "  timestamp: 1552403510\n",
      "  timesteps_since_restore: 1510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1510000\n",
      "  training_iteration: 151\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5755 s, 151 iter, 1510000 ts, 328 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-12-20\n",
      "  done: false\n",
      "  episode_len_mean: 172.61\n",
      "  episode_reward_max: 373.38985120056634\n",
      "  episode_reward_mean: 324.44455627470853\n",
      "  episode_reward_min: 157.35758620269405\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 8166\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4589.841\n",
      "    load_time_ms: 3.039\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.346867718608583e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4307716488838196\n",
      "      kl: 0.007737329229712486\n",
      "      policy_loss: -0.0007302660960704088\n",
      "      total_loss: 25.737947463989258\n",
      "      vf_explained_var: 0.9804456830024719\n",
      "      vf_loss: 25.73868179321289\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.275171165616181e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.070661187171936\n",
      "      kl: 1.5266534090042114\n",
      "      policy_loss: 0.004984256345778704\n",
      "      total_loss: 13.256383895874023\n",
      "      vf_explained_var: 0.9958460927009583\n",
      "      vf_loss: 13.251397132873535\n",
      "    sample_time_ms: 32663.039\n",
      "    update_time_ms: 11.031\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.9365675948963\n",
      "    rl_1: 189.50798867981229\n",
      "  time_since_restore: 5785.981120824814\n",
      "  time_this_iter_s: 30.605109453201294\n",
      "  time_total_s: 5785.981120824814\n",
      "  timestamp: 1552403540\n",
      "  timesteps_since_restore: 1520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 152\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5785 s, 152 iter, 1520000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-12-55\n",
      "  done: false\n",
      "  episode_len_mean: 173.76\n",
      "  episode_reward_max: 373.38985120056634\n",
      "  episode_reward_mean: 325.1857625494858\n",
      "  episode_reward_min: 157.35758620269405\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 8224\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4174.1\n",
      "    load_time_ms: 2.883\n",
      "    num_steps_sampled: 1530000\n",
      "    num_steps_trained: 1530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6733637943810755e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3670184910297394\n",
      "      kl: 0.00924141239374876\n",
      "      policy_loss: -0.0017372185830026865\n",
      "      total_loss: 28.45350456237793\n",
      "      vf_explained_var: 0.9792194962501526\n",
      "      vf_loss: 28.45524024963379\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.912758040893978e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0770292282104492\n",
      "      kl: 0.028524896129965782\n",
      "      policy_loss: 0.0033975900150835514\n",
      "      total_loss: 20.22004508972168\n",
      "      vf_explained_var: 0.993696928024292\n",
      "      vf_loss: 20.21664810180664\n",
      "    sample_time_ms: 30762.79\n",
      "    update_time_ms: 10.269\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.78585834627546\n",
      "    rl_1: 189.39990420321033\n",
      "  time_since_restore: 5820.875591039658\n",
      "  time_this_iter_s: 34.89447021484375\n",
      "  time_total_s: 5820.875591039658\n",
      "  timestamp: 1552403575\n",
      "  timesteps_since_restore: 1530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1530000\n",
      "  training_iteration: 153\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5820 s, 153 iter, 1530000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-13-38\n",
      "  done: false\n",
      "  episode_len_mean: 173.1\n",
      "  episode_reward_max: 376.05981201177286\n",
      "  episode_reward_mean: 321.1193432702693\n",
      "  episode_reward_min: 127.49306722283643\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 8282\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3896.428\n",
      "    load_time_ms: 3.066\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8366818971905377e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3807278573513031\n",
      "      kl: 0.006580987013876438\n",
      "      policy_loss: -0.0010447031818330288\n",
      "      total_loss: 22.37800407409668\n",
      "      vf_explained_var: 0.9839985370635986\n",
      "      vf_loss: 22.37904930114746\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.912758040893978e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0475267171859741\n",
      "      kl: 0.020760787650942802\n",
      "      policy_loss: 0.0012713259784504771\n",
      "      total_loss: 16.57499122619629\n",
      "      vf_explained_var: 0.9950165152549744\n",
      "      vf_loss: 16.57372283935547\n",
      "    sample_time_ms: 29005.941\n",
      "    update_time_ms: 9.291\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 133.54377323581204\n",
      "    rl_1: 187.57557003445726\n",
      "  time_since_restore: 5863.4255130290985\n",
      "  time_this_iter_s: 42.54992198944092\n",
      "  time_total_s: 5863.4255130290985\n",
      "  timestamp: 1552403618\n",
      "  timesteps_since_restore: 1540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 154\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5863 s, 154 iter, 1540000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-14-09\n",
      "  done: false\n",
      "  episode_len_mean: 172.03\n",
      "  episode_reward_max: 365.905491360634\n",
      "  episode_reward_mean: 323.7616399773284\n",
      "  episode_reward_min: 273.7585674853693\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 8340\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3859.948\n",
      "    load_time_ms: 3.074\n",
      "    num_steps_sampled: 1550000\n",
      "    num_steps_trained: 1550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.184110135184851e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3558163046836853\n",
      "      kl: 0.008644387125968933\n",
      "      policy_loss: 0.00021070738148409873\n",
      "      total_loss: 18.36286735534668\n",
      "      vf_explained_var: 0.9862973093986511\n",
      "      vf_loss: 18.36265754699707\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.912758040893978e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0585083961486816\n",
      "      kl: 368.50970458984375\n",
      "      policy_loss: 0.04808174818754196\n",
      "      total_loss: 9.663275718688965\n",
      "      vf_explained_var: 0.9969295859336853\n",
      "      vf_loss: 9.615194320678711\n",
      "    sample_time_ms: 27753.554\n",
      "    update_time_ms: 8.865\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.12273850110864\n",
      "    rl_1: 189.63890147621973\n",
      "  time_since_restore: 5894.556253910065\n",
      "  time_this_iter_s: 31.130740880966187\n",
      "  time_total_s: 5894.556253910065\n",
      "  timestamp: 1552403649\n",
      "  timesteps_since_restore: 1550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1550000\n",
      "  training_iteration: 155\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5894 s, 155 iter, 1550000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-14-44\n",
      "  done: false\n",
      "  episode_len_mean: 173.02\n",
      "  episode_reward_max: 369.24270854893246\n",
      "  episode_reward_mean: 324.778912497823\n",
      "  episode_reward_min: 285.0844300656947\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 8398\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3862.252\n",
      "    load_time_ms: 2.92\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.5920550675924255e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3179677426815033\n",
      "      kl: 0.006321902386844158\n",
      "      policy_loss: -0.0013171049067750573\n",
      "      total_loss: 16.090892791748047\n",
      "      vf_explained_var: 0.9879276752471924\n",
      "      vf_loss: 16.092206954956055\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1869137061340967e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9868674874305725\n",
      "      kl: 0.016351986676454544\n",
      "      policy_loss: 0.000330172770190984\n",
      "      total_loss: 8.342612266540527\n",
      "      vf_explained_var: 0.9972735643386841\n",
      "      vf_loss: 8.342280387878418\n",
      "    sample_time_ms: 28162.284\n",
      "    update_time_ms: 8.779\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.8132559470056\n",
      "    rl_1: 188.96565655081744\n",
      "  time_since_restore: 5929.247725248337\n",
      "  time_this_iter_s: 34.691471338272095\n",
      "  time_total_s: 5929.247725248337\n",
      "  timestamp: 1552403684\n",
      "  timesteps_since_restore: 1560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 156\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5929 s, 156 iter, 1560000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-15-17\n",
      "  done: false\n",
      "  episode_len_mean: 175.04\n",
      "  episode_reward_max: 385.8250969607831\n",
      "  episode_reward_mean: 319.35731216160775\n",
      "  episode_reward_min: 136.51151685387526\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 8454\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4016.865\n",
      "    load_time_ms: 3.183\n",
      "    num_steps_sampled: 1570000\n",
      "    num_steps_trained: 1570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2953268845640504e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3823421001434326\n",
      "      kl: 0.009938910603523254\n",
      "      policy_loss: -0.0018637627363204956\n",
      "      total_loss: 52.394752502441406\n",
      "      vf_explained_var: 0.9687191843986511\n",
      "      vf_loss: 52.39661407470703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1869137061340967e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0850679874420166\n",
      "      kl: 0.03458034619688988\n",
      "      policy_loss: -0.0005361523944884539\n",
      "      total_loss: 46.500301361083984\n",
      "      vf_explained_var: 0.9870065450668335\n",
      "      vf_loss: 46.50083923339844\n",
      "    sample_time_ms: 27672.31\n",
      "    update_time_ms: 9.003\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.70067941638948\n",
      "    rl_1: 184.65663274521822\n",
      "  time_since_restore: 5962.8362865448\n",
      "  time_this_iter_s: 33.58856129646301\n",
      "  time_total_s: 5962.8362865448\n",
      "  timestamp: 1552403717\n",
      "  timesteps_since_restore: 1570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1570000\n",
      "  training_iteration: 157\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5962 s, 157 iter, 1570000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-15-52\n",
      "  done: false\n",
      "  episode_len_mean: 176.73\n",
      "  episode_reward_max: 385.8250969607831\n",
      "  episode_reward_mean: 319.53737886547077\n",
      "  episode_reward_min: 126.64099719833797\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 8511\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4040.916\n",
      "    load_time_ms: 3.205\n",
      "    num_steps_sampled: 1580000\n",
      "    num_steps_trained: 1580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1476634422820252e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4347113072872162\n",
      "      kl: 0.009898331947624683\n",
      "      policy_loss: -0.002339692320674658\n",
      "      total_loss: 41.75284957885742\n",
      "      vf_explained_var: 0.971878707408905\n",
      "      vf_loss: 41.75518035888672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1869137061340967e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1646089553833008\n",
      "      kl: 22.51863670349121\n",
      "      policy_loss: -0.007283180020749569\n",
      "      total_loss: 35.29642868041992\n",
      "      vf_explained_var: 0.9892916679382324\n",
      "      vf_loss: 35.303714752197266\n",
      "    sample_time_ms: 27730.59\n",
      "    update_time_ms: 8.676\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.47693189785653\n",
      "    rl_1: 184.06044696761418\n",
      "  time_since_restore: 5997.525569677353\n",
      "  time_this_iter_s: 34.6892831325531\n",
      "  time_total_s: 5997.525569677353\n",
      "  timestamp: 1552403752\n",
      "  timesteps_since_restore: 1580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1580000\n",
      "  training_iteration: 158\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 5997 s, 158 iter, 1580000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-16-17\n",
      "  done: false\n",
      "  episode_len_mean: 178.21\n",
      "  episode_reward_max: 377.55178242553575\n",
      "  episode_reward_mean: 325.08919489835137\n",
      "  episode_reward_min: 126.64099719833797\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 8566\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4042.357\n",
      "    load_time_ms: 3.203\n",
      "    num_steps_sampled: 1590000\n",
      "    num_steps_trained: 1590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.74532370373175e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4142816960811615\n",
      "      kl: 0.006950859911739826\n",
      "      policy_loss: -0.0023155941162258387\n",
      "      total_loss: 33.12205123901367\n",
      "      vf_explained_var: 0.9771460294723511\n",
      "      vf_loss: 33.124366760253906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7803706108999334e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1749945878982544\n",
      "      kl: 37.28745651245117\n",
      "      policy_loss: -0.009466093964874744\n",
      "      total_loss: 27.563304901123047\n",
      "      vf_explained_var: 0.9916003346443176\n",
      "      vf_loss: 27.572763442993164\n",
      "    sample_time_ms: 27560.55\n",
      "    update_time_ms: 8.808\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 139.82257518260508\n",
      "    rl_1: 185.26661971574634\n",
      "  time_since_restore: 6021.85758805275\n",
      "  time_this_iter_s: 24.33201837539673\n",
      "  time_total_s: 6021.85758805275\n",
      "  timestamp: 1552403777\n",
      "  timesteps_since_restore: 1590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1590000\n",
      "  training_iteration: 159\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6021 s, 159 iter, 1590000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-16-40\n",
      "  done: false\n",
      "  episode_len_mean: 175.41\n",
      "  episode_reward_max: 375.05888101790123\n",
      "  episode_reward_mean: 317.6469491906683\n",
      "  episode_reward_min: 53.72954959431914\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 8624\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4035.592\n",
      "    load_time_ms: 3.188\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.872661851865875e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.34929677844047546\n",
      "      kl: 0.011387081816792488\n",
      "      policy_loss: -0.0013447960373014212\n",
      "      total_loss: 52.99760055541992\n",
      "      vf_explained_var: 0.9655728340148926\n",
      "      vf_loss: 52.99894714355469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.6705563299402063e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0346262454986572\n",
      "      kl: 0.01691233180463314\n",
      "      policy_loss: -0.00010669412586139515\n",
      "      total_loss: 50.45631790161133\n",
      "      vf_explained_var: 0.9841410517692566\n",
      "      vf_loss: 50.4564208984375\n",
      "    sample_time_ms: 27578.459\n",
      "    update_time_ms: 8.735\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.10870800653538\n",
      "    rl_1: 183.53824118413286\n",
      "  time_since_restore: 6045.336086034775\n",
      "  time_this_iter_s: 23.478497982025146\n",
      "  time_total_s: 6045.336086034775\n",
      "  timestamp: 1552403800\n",
      "  timesteps_since_restore: 1600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 160\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6045 s, 160 iter, 1600000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-17-06\n",
      "  done: false\n",
      "  episode_len_mean: 175.47\n",
      "  episode_reward_max: 373.900253830571\n",
      "  episode_reward_mean: 314.59532162647645\n",
      "  episode_reward_min: -140.59037367207472\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 8681\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4139.851\n",
      "    load_time_ms: 3.194\n",
      "    num_steps_sampled: 1610000\n",
      "    num_steps_trained: 1610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.872661851865875e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4434628486633301\n",
      "      kl: 0.024978183209896088\n",
      "      policy_loss: -0.005160908680409193\n",
      "      total_loss: 117.52310180664062\n",
      "      vf_explained_var: 0.9319552779197693\n",
      "      vf_loss: 117.52825927734375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.6705563299402063e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1260029077529907\n",
      "      kl: 166.4559326171875\n",
      "      policy_loss: 0.02355092205107212\n",
      "      total_loss: 127.91000366210938\n",
      "      vf_explained_var: 0.9646833539009094\n",
      "      vf_loss: 127.88648223876953\n",
      "    sample_time_ms: 27457.983\n",
      "    update_time_ms: 8.677\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.86485154313561\n",
      "    rl_1: 181.7304700833408\n",
      "  time_since_restore: 6071.675143003464\n",
      "  time_this_iter_s: 26.339056968688965\n",
      "  time_total_s: 6071.675143003464\n",
      "  timestamp: 1552403826\n",
      "  timesteps_since_restore: 1610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1610000\n",
      "  training_iteration: 161\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6071 s, 161 iter, 1610000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-17-37\n",
      "  done: false\n",
      "  episode_len_mean: 176.25\n",
      "  episode_reward_max: 382.93316790159815\n",
      "  episode_reward_mean: 313.37767781706\n",
      "  episode_reward_min: -140.59037367207472\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 8737\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4103.433\n",
      "    load_time_ms: 3.162\n",
      "    num_steps_sampled: 1620000\n",
      "    num_steps_trained: 1620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.872661851865875e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.37005460262298584\n",
      "      kl: 0.006012944504618645\n",
      "      policy_loss: -0.0023310210090130568\n",
      "      total_loss: 61.426483154296875\n",
      "      vf_explained_var: 0.9607619047164917\n",
      "      vf_loss: 61.42882537841797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.005833460934544e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0585213899612427\n",
      "      kl: 12.042745590209961\n",
      "      policy_loss: 0.009426495060324669\n",
      "      total_loss: 59.272735595703125\n",
      "      vf_explained_var: 0.9821950197219849\n",
      "      vf_loss: 59.26331329345703\n",
      "    sample_time_ms: 27527.35\n",
      "    update_time_ms: 9.099\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 133.3936228612091\n",
      "    rl_1: 179.98405495585084\n",
      "  time_since_restore: 6102.61389708519\n",
      "  time_this_iter_s: 30.938754081726074\n",
      "  time_total_s: 6102.61389708519\n",
      "  timestamp: 1552403857\n",
      "  timesteps_since_restore: 1620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1620000\n",
      "  training_iteration: 162\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6102 s, 162 iter, 1620000 ts, 313 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 178.58\n",
      "  episode_reward_max: 383.3082402563655\n",
      "  episode_reward_mean: 318.5791743420664\n",
      "  episode_reward_min: 93.15592360869067\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 8793\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4116.102\n",
      "    load_time_ms: 3.156\n",
      "    num_steps_sampled: 1630000\n",
      "    num_steps_trained: 1630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4293244336113134e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4231758713722229\n",
      "      kl: 0.003916938789188862\n",
      "      policy_loss: -0.00048348488053306937\n",
      "      total_loss: 55.55509567260742\n",
      "      vf_explained_var: 0.9667326807975769\n",
      "      vf_loss: 55.55556869506836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.0087512253775814e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0841846466064453\n",
      "      kl: 0.03478376194834709\n",
      "      policy_loss: 0.0009120375034399331\n",
      "      total_loss: 61.00172805786133\n",
      "      vf_explained_var: 0.9823623895645142\n",
      "      vf_loss: 61.00082015991211\n",
      "    sample_time_ms: 27323.484\n",
      "    update_time_ms: 9.253\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 136.8380571475706\n",
      "    rl_1: 181.74111719449567\n",
      "  time_since_restore: 6135.600619316101\n",
      "  time_this_iter_s: 32.986722230911255\n",
      "  time_total_s: 6135.600619316101\n",
      "  timestamp: 1552403890\n",
      "  timesteps_since_restore: 1630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1630000\n",
      "  training_iteration: 163\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6135 s, 163 iter, 1630000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-18-40\n",
      "  done: false\n",
      "  episode_len_mean: 177.48\n",
      "  episode_reward_max: 383.3082402563655\n",
      "  episode_reward_mean: 319.9395218442209\n",
      "  episode_reward_min: 94.11443797890013\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 8851\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3899.924\n",
      "    load_time_ms: 2.767\n",
      "    num_steps_sampled: 1640000\n",
      "    num_steps_trained: 1640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.146622168056567e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3666796386241913\n",
      "      kl: 0.006364847999066114\n",
      "      policy_loss: -0.0029129781760275364\n",
      "      total_loss: 34.27002716064453\n",
      "      vf_explained_var: 0.9770262241363525\n",
      "      vf_loss: 34.27293014526367\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.0087512253775814e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9950861930847168\n",
      "      kl: 23.93829345703125\n",
      "      policy_loss: 0.00748224463313818\n",
      "      total_loss: 37.0852165222168\n",
      "      vf_explained_var: 0.9879682660102844\n",
      "      vf_loss: 37.077735900878906\n",
      "    sample_time_ms: 26182.073\n",
      "    update_time_ms: 9.212\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.9448628979792\n",
      "    rl_1: 183.99465894624169\n",
      "  time_since_restore: 6164.556797504425\n",
      "  time_this_iter_s: 28.956178188323975\n",
      "  time_total_s: 6164.556797504425\n",
      "  timestamp: 1552403920\n",
      "  timesteps_since_restore: 1640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1640000\n",
      "  training_iteration: 164\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6164 s, 164 iter, 1640000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-19-13\n",
      "  done: false\n",
      "  episode_len_mean: 177.24\n",
      "  episode_reward_max: 385.3088563573551\n",
      "  episode_reward_mean: 317.4757177512671\n",
      "  episode_reward_min: -149.11089698826197\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 8906\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4078.338\n",
      "    load_time_ms: 2.901\n",
      "    num_steps_sampled: 1650000\n",
      "    num_steps_trained: 1650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6433760072445244e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.43775251507759094\n",
      "      kl: 0.007360916119068861\n",
      "      policy_loss: 0.0001086649062926881\n",
      "      total_loss: 106.56982421875\n",
      "      vf_explained_var: 0.9351693391799927\n",
      "      vf_loss: 106.56970977783203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.0131253905003e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.115645170211792\n",
      "      kl: 32.11226272583008\n",
      "      policy_loss: -0.00737321050837636\n",
      "      total_loss: 117.39158630371094\n",
      "      vf_explained_var: 0.9640827775001526\n",
      "      vf_loss: 117.39896392822266\n",
      "    sample_time_ms: 26208.977\n",
      "    update_time_ms: 8.469\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.62000495849222\n",
      "    rl_1: 182.85571279277485\n",
      "  time_since_restore: 6197.737368106842\n",
      "  time_this_iter_s: 33.18057060241699\n",
      "  time_total_s: 6197.737368106842\n",
      "  timestamp: 1552403953\n",
      "  timesteps_since_restore: 1650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1650000\n",
      "  training_iteration: 165\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6197 s, 165 iter, 1650000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-19-52\n",
      "  done: false\n",
      "  episode_len_mean: 175.49\n",
      "  episode_reward_max: 374.32771296151134\n",
      "  episode_reward_mean: 319.48817221197424\n",
      "  episode_reward_min: -149.11089698826197\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 8964\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4360.342\n",
      "    load_time_ms: 2.961\n",
      "    num_steps_sampled: 1660000\n",
      "    num_steps_trained: 1660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8216880036222622e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.29050877690315247\n",
      "      kl: 0.006914203520864248\n",
      "      policy_loss: -0.0008557123364880681\n",
      "      total_loss: 27.30289649963379\n",
      "      vf_explained_var: 0.9806870818138123\n",
      "      vf_loss: 27.303754806518555\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.35196913944729e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9120428562164307\n",
      "      kl: 377.111083984375\n",
      "      policy_loss: 0.06410938501358032\n",
      "      total_loss: 25.46830177307129\n",
      "      vf_explained_var: 0.9913754463195801\n",
      "      vf_loss: 25.404190063476562\n",
      "    sample_time_ms: 26335.664\n",
      "    update_time_ms: 8.252\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.6488399352089\n",
      "    rl_1: 184.8393322767654\n",
      "  time_since_restore: 6236.51061964035\n",
      "  time_this_iter_s: 38.7732515335083\n",
      "  time_total_s: 6236.51061964035\n",
      "  timestamp: 1552403992\n",
      "  timesteps_since_restore: 1660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1660000\n",
      "  training_iteration: 166\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6236 s, 166 iter, 1660000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-20-20\n",
      "  done: false\n",
      "  episode_len_mean: 176.33\n",
      "  episode_reward_max: 379.7514931326169\n",
      "  episode_reward_mean: 314.3564719737705\n",
      "  episode_reward_min: -143.69449559496329\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 9021\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4115.135\n",
      "    load_time_ms: 2.627\n",
      "    num_steps_sampled: 1670000\n",
      "    num_steps_trained: 1670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.407790785948902e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.41891786456108093\n",
      "      kl: 0.011215237900614738\n",
      "      policy_loss: -0.0008950306801125407\n",
      "      total_loss: 101.77701568603516\n",
      "      vf_explained_var: 0.9408218264579773\n",
      "      vf_loss: 101.77791595458984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.02795337829869e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0620185136795044\n",
      "      kl: 0.061393797397613525\n",
      "      policy_loss: 0.0016955110477283597\n",
      "      total_loss: 116.41655731201172\n",
      "      vf_explained_var: 0.9653314352035522\n",
      "      vf_loss: 116.41484832763672\n",
      "    sample_time_ms: 26057.752\n",
      "    update_time_ms: 7.984\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.67040399240076\n",
      "    rl_1: 181.68606798136977\n",
      "  time_since_restore: 6264.857170581818\n",
      "  time_this_iter_s: 28.346550941467285\n",
      "  time_total_s: 6264.857170581818\n",
      "  timestamp: 1552404020\n",
      "  timesteps_since_restore: 1670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1670000\n",
      "  training_iteration: 167\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6264 s, 167 iter, 1670000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-20-49\n",
      "  done: false\n",
      "  episode_len_mean: 180.19\n",
      "  episode_reward_max: 383.8362851274274\n",
      "  episode_reward_mean: 319.7889291531194\n",
      "  episode_reward_min: -143.69449559496329\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 9075\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4142.001\n",
      "    load_time_ms: 2.618\n",
      "    num_steps_sampled: 1680000\n",
      "    num_steps_trained: 1680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.407790785948902e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4248448312282562\n",
      "      kl: 0.01090735848993063\n",
      "      policy_loss: -0.001635333290323615\n",
      "      total_loss: 25.14419937133789\n",
      "      vf_explained_var: 0.9819900393486023\n",
      "      vf_loss: 25.145837783813477\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0419305637564026e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0929943323135376\n",
      "      kl: 11.623748779296875\n",
      "      policy_loss: -0.011635219678282738\n",
      "      total_loss: 26.333438873291016\n",
      "      vf_explained_var: 0.991104781627655\n",
      "      vf_loss: 26.345077514648438\n",
      "    sample_time_ms: 25490.829\n",
      "    update_time_ms: 7.994\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 136.34903248027533\n",
      "    rl_1: 183.43989667284401\n",
      "  time_since_restore: 6294.148770809174\n",
      "  time_this_iter_s: 29.291600227355957\n",
      "  time_total_s: 6294.148770809174\n",
      "  timestamp: 1552404049\n",
      "  timesteps_since_restore: 1680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1680000\n",
      "  training_iteration: 168\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6294 s, 168 iter, 1680000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-21-32\n",
      "  done: false\n",
      "  episode_len_mean: 180.17\n",
      "  episode_reward_max: 383.8362851274274\n",
      "  episode_reward_mean: 331.1944104915931\n",
      "  episode_reward_min: 117.90130085841457\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 9131\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4255.924\n",
      "    load_time_ms: 2.717\n",
      "    num_steps_sampled: 1690000\n",
      "    num_steps_trained: 1690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.407790785948902e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.324382483959198\n",
      "      kl: 0.004559807013720274\n",
      "      policy_loss: -1.0680693776521366e-05\n",
      "      total_loss: 37.55424118041992\n",
      "      vf_explained_var: 0.9749986529350281\n",
      "      vf_loss: 37.55425262451172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.5628960110707264e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0209882259368896\n",
      "      kl: 4.599900245666504\n",
      "      policy_loss: 0.0028455681167542934\n",
      "      total_loss: 35.610103607177734\n",
      "      vf_explained_var: 0.988693356513977\n",
      "      vf_loss: 35.60725784301758\n",
      "    sample_time_ms: 27233.738\n",
      "    update_time_ms: 7.973\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 142.4253450787974\n",
      "    rl_1: 188.76906541279567\n",
      "  time_since_restore: 6337.0559985637665\n",
      "  time_this_iter_s: 42.907227754592896\n",
      "  time_total_s: 6337.0559985637665\n",
      "  timestamp: 1552404092\n",
      "  timesteps_since_restore: 1690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1690000\n",
      "  training_iteration: 169\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6337 s, 169 iter, 1690000 ts, 331 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-22-12\n",
      "  done: false\n",
      "  episode_len_mean: 178.25\n",
      "  episode_reward_max: 390.966795659073\n",
      "  episode_reward_mean: 326.8470913615754\n",
      "  episode_reward_min: 117.90130085841457\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 9188\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4477.457\n",
      "    load_time_ms: 2.748\n",
      "    num_steps_sampled: 1700000\n",
      "    num_steps_trained: 1700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.203895392974451e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3097420334815979\n",
      "      kl: 0.012600046582520008\n",
      "      policy_loss: -7.830574759282172e-05\n",
      "      total_loss: 29.788448333740234\n",
      "      vf_explained_var: 0.9805995225906372\n",
      "      vf_loss: 29.788536071777344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.844341865936497e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9803402423858643\n",
      "      kl: 0.03410754352807999\n",
      "      policy_loss: 0.0008225496276281774\n",
      "      total_loss: 26.62064552307129\n",
      "      vf_explained_var: 0.9918815493583679\n",
      "      vf_loss: 26.619829177856445\n",
      "    sample_time_ms: 28678.569\n",
      "    update_time_ms: 8.399\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 139.98762129410545\n",
      "    rl_1: 186.8594700674699\n",
      "  time_since_restore: 6377.210136651993\n",
      "  time_this_iter_s: 40.15413808822632\n",
      "  time_total_s: 6377.210136651993\n",
      "  timestamp: 1552404132\n",
      "  timesteps_since_restore: 1700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1700000\n",
      "  training_iteration: 170\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6377 s, 170 iter, 1700000 ts, 327 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-22-54\n",
      "  done: false\n",
      "  episode_len_mean: 177.96\n",
      "  episode_reward_max: 390.966795659073\n",
      "  episode_reward_mean: 326.6431814453332\n",
      "  episode_reward_min: 126.54890910238055\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 9243\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4595.274\n",
      "    load_time_ms: 2.99\n",
      "    num_steps_sampled: 1710000\n",
      "    num_steps_trained: 1710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.203895392974451e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.35194697976112366\n",
      "      kl: 0.0077796922996640205\n",
      "      policy_loss: 0.0014141880674287677\n",
      "      total_loss: 36.14252853393555\n",
      "      vf_explained_var: 0.9776626229286194\n",
      "      vf_loss: 36.141109466552734\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.844341865936497e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0310728549957275\n",
      "      kl: 0.018172161653637886\n",
      "      policy_loss: 0.0013443920761346817\n",
      "      total_loss: 39.978546142578125\n",
      "      vf_explained_var: 0.9878911972045898\n",
      "      vf_loss: 39.97719955444336\n",
      "    sample_time_ms: 30029.143\n",
      "    update_time_ms: 8.953\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 139.93639098905828\n",
      "    rl_1: 186.70679045627492\n",
      "  time_since_restore: 6418.244751930237\n",
      "  time_this_iter_s: 41.03461527824402\n",
      "  time_total_s: 6418.244751930237\n",
      "  timestamp: 1552404174\n",
      "  timesteps_since_restore: 1710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1710000\n",
      "  training_iteration: 171\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6418 s, 171 iter, 1710000 ts, 327 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-23-43\n",
      "  done: false\n",
      "  episode_len_mean: 180.4\n",
      "  episode_reward_max: 379.2486368132101\n",
      "  episode_reward_mean: 322.08170770430775\n",
      "  episode_reward_min: 116.00314948860432\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 9299\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5094.58\n",
      "    load_time_ms: 3.434\n",
      "    num_steps_sampled: 1720000\n",
      "    num_steps_trained: 1720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.802596928649634e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.43764084577560425\n",
      "      kl: 0.0065090237185359\n",
      "      policy_loss: -0.0008521509589627385\n",
      "      total_loss: 61.770294189453125\n",
      "      vf_explained_var: 0.9643144607543945\n",
      "      vf_loss: 61.771141052246094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.844341865936497e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1796058416366577\n",
      "      kl: 0.02627846598625183\n",
      "      policy_loss: 0.0005221570609137416\n",
      "      total_loss: 72.9310302734375\n",
      "      vf_explained_var: 0.9784218072891235\n",
      "      vf_loss: 72.9305191040039\n",
      "    sample_time_ms: 31324.568\n",
      "    update_time_ms: 8.679\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 139.27740751047713\n",
      "    rl_1: 182.80430019383053\n",
      "  time_since_restore: 6467.147537946701\n",
      "  time_this_iter_s: 48.90278601646423\n",
      "  time_total_s: 6467.147537946701\n",
      "  timestamp: 1552404223\n",
      "  timesteps_since_restore: 1720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1720000\n",
      "  training_iteration: 172\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6467 s, 172 iter, 1720000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-24-27\n",
      "  done: false\n",
      "  episode_len_mean: 181.11\n",
      "  episode_reward_max: 387.1920748438971\n",
      "  episode_reward_mean: 324.5718089336489\n",
      "  episode_reward_min: 116.00314948860432\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 9353\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5298.357\n",
      "    load_time_ms: 3.701\n",
      "    num_steps_sampled: 1730000\n",
      "    num_steps_trained: 1730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.401298464324817e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39711740612983704\n",
      "      kl: 0.0064109633676707745\n",
      "      policy_loss: -0.0008194516994990408\n",
      "      total_loss: 36.746795654296875\n",
      "      vf_explained_var: 0.9777413606643677\n",
      "      vf_loss: 36.74761962890625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.844341865936497e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0878489017486572\n",
      "      kl: 34.468589782714844\n",
      "      policy_loss: -0.006207092199474573\n",
      "      total_loss: 41.29731369018555\n",
      "      vf_explained_var: 0.987109363079071\n",
      "      vf_loss: 41.30352783203125\n",
      "    sample_time_ms: 32255.692\n",
      "    update_time_ms: 8.905\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 140.41141005108145\n",
      "    rl_1: 184.16039888256742\n",
      "  time_since_restore: 6511.495715379715\n",
      "  time_this_iter_s: 44.348177433013916\n",
      "  time_total_s: 6511.495715379715\n",
      "  timestamp: 1552404267\n",
      "  timesteps_since_restore: 1730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1730000\n",
      "  training_iteration: 173\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6511 s, 173 iter, 1730000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-25-01\n",
      "  done: false\n",
      "  episode_len_mean: 179.38\n",
      "  episode_reward_max: 381.92774887391846\n",
      "  episode_reward_mean: 318.93354674187776\n",
      "  episode_reward_min: -141.56704299645043\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 9410\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5298.51\n",
      "    load_time_ms: 3.703\n",
      "    num_steps_sampled: 1740000\n",
      "    num_steps_trained: 1740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4188840687274933\n",
      "      kl: 0.022271307185292244\n",
      "      policy_loss: -0.0003764176508411765\n",
      "      total_loss: 173.79080200195312\n",
      "      vf_explained_var: 0.8956373333930969\n",
      "      vf_loss: 173.7911834716797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0266517100243931e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0987697839736938\n",
      "      kl: 227.14385986328125\n",
      "      policy_loss: 0.00641627935692668\n",
      "      total_loss: 192.5037841796875\n",
      "      vf_explained_var: 0.9404301047325134\n",
      "      vf_loss: 192.49734497070312\n",
      "    sample_time_ms: 32762.743\n",
      "    update_time_ms: 9.23\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 136.81566955052915\n",
      "    rl_1: 182.11787719134864\n",
      "  time_since_restore: 6545.5299680233\n",
      "  time_this_iter_s: 34.034252643585205\n",
      "  time_total_s: 6545.5299680233\n",
      "  timestamp: 1552404301\n",
      "  timesteps_since_restore: 1740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1740000\n",
      "  training_iteration: 174\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6545 s, 174 iter, 1740000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-25-37\n",
      "  done: false\n",
      "  episode_len_mean: 181.14\n",
      "  episode_reward_max: 378.1720375355267\n",
      "  episode_reward_mean: 319.7320236527369\n",
      "  episode_reward_min: -139.80873576002253\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 9465\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5079.831\n",
      "    load_time_ms: 3.55\n",
      "    num_steps_sampled: 1750000\n",
      "    num_steps_trained: 1750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4100096523761749\n",
      "      kl: 0.006329636555165052\n",
      "      policy_loss: -0.00032605582964606583\n",
      "      total_loss: 54.71625900268555\n",
      "      vf_explained_var: 0.9668893814086914\n",
      "      vf_loss: 54.716590881347656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5399767047687526e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1384336948394775\n",
      "      kl: 244.11170959472656\n",
      "      policy_loss: 0.033854223787784576\n",
      "      total_loss: 62.345890045166016\n",
      "      vf_explained_var: 0.9807845950126648\n",
      "      vf_loss: 62.31201934814453\n",
      "    sample_time_ms: 33249.029\n",
      "    update_time_ms: 9.949\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 137.47133671405024\n",
      "    rl_1: 182.26068693868672\n",
      "  time_since_restore: 6581.389540910721\n",
      "  time_this_iter_s: 35.859572887420654\n",
      "  time_total_s: 6581.389540910721\n",
      "  timestamp: 1552404337\n",
      "  timesteps_since_restore: 1750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1750000\n",
      "  training_iteration: 175\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6581 s, 175 iter, 1750000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-26-11\n",
      "  done: false\n",
      "  episode_len_mean: 180.87\n",
      "  episode_reward_max: 387.0640059562358\n",
      "  episode_reward_mean: 328.1560356937621\n",
      "  episode_reward_min: 85.08642299584915\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 9520\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4739.961\n",
      "    load_time_ms: 3.439\n",
      "    num_steps_sampled: 1760000\n",
      "    num_steps_trained: 1760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.28189605474472046\n",
      "      kl: 0.006586409639567137\n",
      "      policy_loss: -0.0017827787669375539\n",
      "      total_loss: 15.124948501586914\n",
      "      vf_explained_var: 0.9893315434455872\n",
      "      vf_loss: 15.12673282623291\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.309965255676476e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0391440391540527\n",
      "      kl: 8.498157501220703\n",
      "      policy_loss: 0.002761777024716139\n",
      "      total_loss: 11.671966552734375\n",
      "      vf_explained_var: 0.9960843324661255\n",
      "      vf_loss: 11.669203758239746\n",
      "    sample_time_ms: 33076.457\n",
      "    update_time_ms: 9.987\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 141.59030642824942\n",
      "    rl_1: 186.5657292655127\n",
      "  time_since_restore: 6615.036333084106\n",
      "  time_this_iter_s: 33.64679217338562\n",
      "  time_total_s: 6615.036333084106\n",
      "  timestamp: 1552404371\n",
      "  timesteps_since_restore: 1760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1760000\n",
      "  training_iteration: 176\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6615 s, 176 iter, 1760000 ts, 328 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-26-46\n",
      "  done: false\n",
      "  episode_len_mean: 180.66\n",
      "  episode_reward_max: 387.0640059562358\n",
      "  episode_reward_mean: 328.71256669670316\n",
      "  episode_reward_min: 123.73661434424184\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 9576\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4930.754\n",
      "    load_time_ms: 3.558\n",
      "    num_steps_sampled: 1770000\n",
      "    num_steps_trained: 1770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.37527191638946533\n",
      "      kl: 0.007043987512588501\n",
      "      policy_loss: -0.002965532708913088\n",
      "      total_loss: 56.68376159667969\n",
      "      vf_explained_var: 0.9658188223838806\n",
      "      vf_loss: 56.68672561645508\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.464948412910306e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1136271953582764\n",
      "      kl: 0.01667991653084755\n",
      "      policy_loss: 0.0015956099377945065\n",
      "      total_loss: 56.80319595336914\n",
      "      vf_explained_var: 0.9831991195678711\n",
      "      vf_loss: 56.80160903930664\n",
      "    sample_time_ms: 33546.56\n",
      "    update_time_ms: 9.817\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 142.35387145150023\n",
      "    rl_1: 186.3586952452029\n",
      "  time_since_restore: 6650.0021879673\n",
      "  time_this_iter_s: 34.96585488319397\n",
      "  time_total_s: 6650.0021879673\n",
      "  timestamp: 1552404406\n",
      "  timesteps_since_restore: 1770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1770000\n",
      "  training_iteration: 177\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6650 s, 177 iter, 1770000 ts, 329 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-27-27\n",
      "  done: false\n",
      "  episode_len_mean: 181.39\n",
      "  episode_reward_max: 386.08607223676825\n",
      "  episode_reward_mean: 326.9868062234344\n",
      "  episode_reward_min: 123.73661434424184\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 9631\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4965.377\n",
      "    load_time_ms: 4.355\n",
      "    num_steps_sampled: 1780000\n",
      "    num_steps_trained: 1780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2979009449481964\n",
      "      kl: 0.007689572870731354\n",
      "      policy_loss: -0.0010539216455072165\n",
      "      total_loss: 25.63047981262207\n",
      "      vf_explained_var: 0.9826529026031494\n",
      "      vf_loss: 25.631532669067383\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.464948412910306e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.075493335723877\n",
      "      kl: 0.029599705711007118\n",
      "      policy_loss: 0.0025398279540240765\n",
      "      total_loss: 25.611263275146484\n",
      "      vf_explained_var: 0.9916001558303833\n",
      "      vf_loss: 25.60872459411621\n",
      "    sample_time_ms: 34654.831\n",
      "    update_time_ms: 9.777\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 140.97624771759257\n",
      "    rl_1: 186.0105585058418\n",
      "  time_since_restore: 6690.730980396271\n",
      "  time_this_iter_s: 40.72879242897034\n",
      "  time_total_s: 6690.730980396271\n",
      "  timestamp: 1552404447\n",
      "  timesteps_since_restore: 1780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1780000\n",
      "  training_iteration: 178\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6690 s, 178 iter, 1780000 ts, 327 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-27-52\n",
      "  done: false\n",
      "  episode_len_mean: 180.79\n",
      "  episode_reward_max: 386.08607223676825\n",
      "  episode_reward_mean: 334.62528750274015\n",
      "  episode_reward_min: 163.97718007419604\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 9686\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4855.756\n",
      "    load_time_ms: 4.255\n",
      "    num_steps_sampled: 1790000\n",
      "    num_steps_trained: 1790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.36157819628715515\n",
      "      kl: 0.01180138997733593\n",
      "      policy_loss: -0.0005467043956741691\n",
      "      total_loss: 17.48826026916504\n",
      "      vf_explained_var: 0.9879614114761353\n",
      "      vf_loss: 17.48880958557129\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.464948412910306e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.106034517288208\n",
      "      kl: 145.49581909179688\n",
      "      policy_loss: 0.01663396693766117\n",
      "      total_loss: 15.912041664123535\n",
      "      vf_explained_var: 0.9947349429130554\n",
      "      vf_loss: 15.895410537719727\n",
      "    sample_time_ms: 33021.208\n",
      "    update_time_ms: 9.917\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 144.37376015251704\n",
      "    rl_1: 190.2515273502232\n",
      "  time_since_restore: 6716.201812982559\n",
      "  time_this_iter_s: 25.470832586288452\n",
      "  time_total_s: 6716.201812982559\n",
      "  timestamp: 1552404472\n",
      "  timesteps_since_restore: 1790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1790000\n",
      "  training_iteration: 179\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6716 s, 179 iter, 1790000 ts, 335 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-28-16\n",
      "  done: false\n",
      "  episode_len_mean: 182.97\n",
      "  episode_reward_max: 379.8861310713848\n",
      "  episode_reward_mean: 331.37088065275583\n",
      "  episode_reward_min: 140.8237216512103\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 9741\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4628.57\n",
      "    load_time_ms: 4.303\n",
      "    num_steps_sampled: 1800000\n",
      "    num_steps_trained: 1800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39249706268310547\n",
      "      kl: 0.005821987520903349\n",
      "      policy_loss: -0.001021399861201644\n",
      "      total_loss: 50.923431396484375\n",
      "      vf_explained_var: 0.9694333076477051\n",
      "      vf_loss: 50.92445755004883\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.197423810505541e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1361112594604492\n",
      "      kl: 0.04325738921761513\n",
      "      policy_loss: 0.000268953648628667\n",
      "      total_loss: 48.38066864013672\n",
      "      vf_explained_var: 0.9850929379463196\n",
      "      vf_loss: 48.38039779663086\n",
      "    sample_time_ms: 31602.154\n",
      "    update_time_ms: 9.71\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 143.86907696077125\n",
      "    rl_1: 187.50180369198452\n",
      "  time_since_restore: 6739.884174585342\n",
      "  time_this_iter_s: 23.682361602783203\n",
      "  time_total_s: 6739.884174585342\n",
      "  timestamp: 1552404496\n",
      "  timesteps_since_restore: 1800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1800000\n",
      "  training_iteration: 180\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6739 s, 180 iter, 1800000 ts, 331 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-28-39\n",
      "  done: false\n",
      "  episode_len_mean: 183.12\n",
      "  episode_reward_max: 379.8861310713848\n",
      "  episode_reward_mean: 317.31137915120735\n",
      "  episode_reward_min: -135.55334797197526\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 9795\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4371.919\n",
      "    load_time_ms: 4.05\n",
      "    num_steps_sampled: 1810000\n",
      "    num_steps_trained: 1810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4569200277328491\n",
      "      kl: 0.007948130369186401\n",
      "      policy_loss: -0.0012710698647424579\n",
      "      total_loss: 135.12249755859375\n",
      "      vf_explained_var: 0.9293336868286133\n",
      "      vf_loss: 135.12376403808594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.796135715758311e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.151307463645935\n",
      "      kl: 5.342641353607178\n",
      "      policy_loss: 0.047660548239946365\n",
      "      total_loss: 150.37570190429688\n",
      "      vf_explained_var: 0.9576411247253418\n",
      "      vf_loss: 150.3280792236328\n",
      "    sample_time_ms: 30119.792\n",
      "    update_time_ms: 9.134\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 137.62746410055365\n",
      "    rl_1: 179.68391505065367\n",
      "  time_since_restore: 6763.50922703743\n",
      "  time_this_iter_s: 23.625052452087402\n",
      "  time_total_s: 6763.50922703743\n",
      "  timestamp: 1552404519\n",
      "  timesteps_since_restore: 1810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1810000\n",
      "  training_iteration: 181\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6763 s, 181 iter, 1810000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-29-03\n",
      "  done: false\n",
      "  episode_len_mean: 180.27\n",
      "  episode_reward_max: 386.23683071245\n",
      "  episode_reward_mean: 308.18076279052997\n",
      "  episode_reward_min: -149.2088613776583\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 9851\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3816.848\n",
      "    load_time_ms: 3.531\n",
      "    num_steps_sampled: 1820000\n",
      "    num_steps_trained: 1820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2930097281932831\n",
      "      kl: 0.010286489501595497\n",
      "      policy_loss: -0.0010737399570643902\n",
      "      total_loss: 113.38304138183594\n",
      "      vf_explained_var: 0.935200035572052\n",
      "      vf_loss: 113.38410186767578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1694202514846283e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9624404907226562\n",
      "      kl: 147.79608154296875\n",
      "      policy_loss: 0.036739736795425415\n",
      "      total_loss: 122.37239837646484\n",
      "      vf_explained_var: 0.9631243944168091\n",
      "      vf_loss: 122.33568572998047\n",
      "    sample_time_ms: 28099.019\n",
      "    update_time_ms: 9.008\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.223867289452\n",
      "    rl_1: 175.95689550107804\n",
      "  time_since_restore: 6786.631959438324\n",
      "  time_this_iter_s: 23.122732400894165\n",
      "  time_total_s: 6786.631959438324\n",
      "  timestamp: 1552404543\n",
      "  timesteps_since_restore: 1820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1820000\n",
      "  training_iteration: 182\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6786 s, 182 iter, 1820000 ts, 308 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-29-26\n",
      "  done: false\n",
      "  episode_len_mean: 179.99\n",
      "  episode_reward_max: 386.23683071245\n",
      "  episode_reward_mean: 318.5066546844921\n",
      "  episode_reward_min: -149.2088613776583\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 9906\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3523.318\n",
      "    load_time_ms: 3.251\n",
      "    num_steps_sampled: 1830000\n",
      "    num_steps_trained: 1830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32277196645736694\n",
      "      kl: 0.008983044885098934\n",
      "      policy_loss: 0.000194070627912879\n",
      "      total_loss: 39.07334518432617\n",
      "      vf_explained_var: 0.9768356084823608\n",
      "      vf_loss: 39.07314682006836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7541304831060608e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0398143529891968\n",
      "      kl: 18.051998138427734\n",
      "      policy_loss: 0.01661183312535286\n",
      "      total_loss: 41.59811782836914\n",
      "      vf_explained_var: 0.9871487617492676\n",
      "      vf_loss: 41.581512451171875\n",
      "    sample_time_ms: 26273.439\n",
      "    update_time_ms: 8.506\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 136.68066877020675\n",
      "    rl_1: 181.82598591428538\n",
      "  time_since_restore: 6809.77210187912\n",
      "  time_this_iter_s: 23.1401424407959\n",
      "  time_total_s: 6809.77210187912\n",
      "  timestamp: 1552404566\n",
      "  timesteps_since_restore: 1830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1830000\n",
      "  training_iteration: 183\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6809 s, 183 iter, 1830000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-29-49\n",
      "  done: false\n",
      "  episode_len_mean: 182.21\n",
      "  episode_reward_max: 385.0612248048294\n",
      "  episode_reward_mean: 326.9212420542016\n",
      "  episode_reward_min: 97.60632562743433\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 9961\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3462.31\n",
      "    load_time_ms: 3.213\n",
      "    num_steps_sampled: 1840000\n",
      "    num_steps_trained: 1840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2713976800441742\n",
      "      kl: 0.01995990425348282\n",
      "      policy_loss: 0.0014809005660936236\n",
      "      total_loss: 31.301694869995117\n",
      "      vf_explained_var: 0.9813132882118225\n",
      "      vf_loss: 31.300214767456055\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.631195618779973e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9747450351715088\n",
      "      kl: 0.16943295300006866\n",
      "      policy_loss: 0.004093850497156382\n",
      "      total_loss: 30.15178108215332\n",
      "      vf_explained_var: 0.9906614422798157\n",
      "      vf_loss: 30.14769172668457\n",
      "    sample_time_ms: 25278.579\n",
      "    update_time_ms: 8.111\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 141.7702958825872\n",
      "    rl_1: 185.15094617161435\n",
      "  time_since_restore: 6833.2385013103485\n",
      "  time_this_iter_s: 23.466399431228638\n",
      "  time_total_s: 6833.2385013103485\n",
      "  timestamp: 1552404589\n",
      "  timesteps_since_restore: 1840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1840000\n",
      "  training_iteration: 184\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6833 s, 184 iter, 1840000 ts, 327 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-30-13\n",
      "  done: false\n",
      "  episode_len_mean: 181.01\n",
      "  episode_reward_max: 385.0612248048294\n",
      "  episode_reward_mean: 332.56016594657433\n",
      "  episode_reward_min: 97.89289471349866\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 10016\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3417.022\n",
      "    load_time_ms: 3.187\n",
      "    num_steps_sampled: 1850000\n",
      "    num_steps_trained: 1850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.19718101620674133\n",
      "      kl: 0.013234353624284267\n",
      "      policy_loss: 0.00021807171287946403\n",
      "      total_loss: 29.6445255279541\n",
      "      vf_explained_var: 0.9807952642440796\n",
      "      vf_loss: 29.644306182861328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.9467932164117224e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9307452440261841\n",
      "      kl: 171.1089630126953\n",
      "      policy_loss: 0.011308702640235424\n",
      "      total_loss: 25.65212059020996\n",
      "      vf_explained_var: 0.9914020895957947\n",
      "      vf_loss: 25.640811920166016\n",
      "    sample_time_ms: 24082.071\n",
      "    update_time_ms: 7.436\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 144.46181866128265\n",
      "    rl_1: 188.09834728529174\n",
      "  time_since_restore: 6856.670791149139\n",
      "  time_this_iter_s: 23.432289838790894\n",
      "  time_total_s: 6856.670791149139\n",
      "  timestamp: 1552404613\n",
      "  timesteps_since_restore: 1850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1850000\n",
      "  training_iteration: 185\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6856 s, 185 iter, 1850000 ts, 333 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-30-36\n",
      "  done: false\n",
      "  episode_len_mean: 179.48\n",
      "  episode_reward_max: 377.6438433421519\n",
      "  episode_reward_mean: 324.90856412437455\n",
      "  episode_reward_min: 77.2299980321418\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 10072\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3395.835\n",
      "    load_time_ms: 3.171\n",
      "    num_steps_sampled: 1860000\n",
      "    num_steps_trained: 1860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2616933286190033\n",
      "      kl: 0.008090687915682793\n",
      "      policy_loss: -0.0010014461586251855\n",
      "      total_loss: 26.004119873046875\n",
      "      vf_explained_var: 0.9834610223770142\n",
      "      vf_loss: 26.005123138427734\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.92018940110111e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9784579873085022\n",
      "      kl: 0.04109533876180649\n",
      "      policy_loss: 0.0031551227439194918\n",
      "      total_loss: 24.976428985595703\n",
      "      vf_explained_var: 0.9920712113380432\n",
      "      vf_loss: 24.973278045654297\n",
      "    sample_time_ms: 23093.329\n",
      "    update_time_ms: 7.433\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 139.17374850021616\n",
      "    rl_1: 185.73481562415824\n",
      "  time_since_restore: 6880.219674825668\n",
      "  time_this_iter_s: 23.54888367652893\n",
      "  time_total_s: 6880.219674825668\n",
      "  timestamp: 1552404636\n",
      "  timesteps_since_restore: 1860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1860000\n",
      "  training_iteration: 186\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6880 s, 186 iter, 1860000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-31-00\n",
      "  done: false\n",
      "  episode_len_mean: 180.94\n",
      "  episode_reward_max: 385.0274330971729\n",
      "  episode_reward_mean: 327.3669129546819\n",
      "  episode_reward_min: 110.53377098874233\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 10128\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3190.342\n",
      "    load_time_ms: 3.067\n",
      "    num_steps_sampled: 1870000\n",
      "    num_steps_trained: 1870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.29581913352012634\n",
      "      kl: 0.007983848452568054\n",
      "      policy_loss: -0.0022503079380840063\n",
      "      total_loss: 42.682037353515625\n",
      "      vf_explained_var: 0.9737644791603088\n",
      "      vf_loss: 42.684288024902344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.880283254618718e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9940139651298523\n",
      "      kl: 0.036672208458185196\n",
      "      policy_loss: 0.0016969587886705995\n",
      "      total_loss: 40.28254699707031\n",
      "      vf_explained_var: 0.9874876737594604\n",
      "      vf_loss: 40.28084945678711\n",
      "    sample_time_ms: 22166.557\n",
      "    update_time_ms: 7.648\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 140.27110658682585\n",
      "    rl_1: 187.09580636785606\n",
      "  time_since_restore: 6903.853840112686\n",
      "  time_this_iter_s: 23.634165287017822\n",
      "  time_total_s: 6903.853840112686\n",
      "  timestamp: 1552404660\n",
      "  timesteps_since_restore: 1870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1870000\n",
      "  training_iteration: 187\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6903 s, 187 iter, 1870000 ts, 327 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-31-23\n",
      "  done: false\n",
      "  episode_len_mean: 180.59\n",
      "  episode_reward_max: 385.0274330971729\n",
      "  episode_reward_mean: 328.7767096846716\n",
      "  episode_reward_min: 110.53377098874233\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 10184\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3093.311\n",
      "    load_time_ms: 2.314\n",
      "    num_steps_sampled: 1880000\n",
      "    num_steps_trained: 1880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.23763203620910645\n",
      "      kl: 0.004719849675893784\n",
      "      policy_loss: 0.0004840307810809463\n",
      "      total_loss: 23.36607551574707\n",
      "      vf_explained_var: 0.9843533635139465\n",
      "      vf_loss: 23.365591049194336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.880283254618718e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9683204293251038\n",
      "      kl: 0.04227736219763756\n",
      "      policy_loss: 0.0023478467483073473\n",
      "      total_loss: 23.08304786682129\n",
      "      vf_explained_var: 0.992446780204773\n",
      "      vf_loss: 23.08069610595703\n",
      "    sample_time_ms: 20500.637\n",
      "    update_time_ms: 7.486\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 140.66568024903196\n",
      "    rl_1: 188.1110294356397\n",
      "  time_since_restore: 6926.941305398941\n",
      "  time_this_iter_s: 23.087465286254883\n",
      "  time_total_s: 6926.941305398941\n",
      "  timestamp: 1552404683\n",
      "  timesteps_since_restore: 1880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1880000\n",
      "  training_iteration: 188\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6926 s, 188 iter, 1880000 ts, 329 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-31-47\n",
      "  done: false\n",
      "  episode_len_mean: 180.41\n",
      "  episode_reward_max: 374.9982928644692\n",
      "  episode_reward_mean: 330.6870438845145\n",
      "  episode_reward_min: 163.79655070379306\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 10238\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3079.283\n",
      "    load_time_ms: 2.426\n",
      "    num_steps_sampled: 1890000\n",
      "    num_steps_trained: 1890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3109622895717621\n",
      "      kl: 0.008343975059688091\n",
      "      policy_loss: -0.0011989535996690392\n",
      "      total_loss: 25.941143035888672\n",
      "      vf_explained_var: 0.982194185256958\n",
      "      vf_loss: 25.942340850830078\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.3320424881928077e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.027845859527588\n",
      "      kl: 28.741783142089844\n",
      "      policy_loss: 0.022887099534273148\n",
      "      total_loss: 23.937244415283203\n",
      "      vf_explained_var: 0.9921127557754517\n",
      "      vf_loss: 23.914358139038086\n",
      "    sample_time_ms: 20321.447\n",
      "    update_time_ms: 7.523\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 141.69123190104082\n",
      "    rl_1: 188.9958119834737\n",
      "  time_since_restore: 6950.481310844421\n",
      "  time_this_iter_s: 23.540005445480347\n",
      "  time_total_s: 6950.481310844421\n",
      "  timestamp: 1552404707\n",
      "  timesteps_since_restore: 1890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1890000\n",
      "  training_iteration: 189\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6950 s, 189 iter, 1890000 ts, 331 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-32-13\n",
      "  done: false\n",
      "  episode_len_mean: 182.54\n",
      "  episode_reward_max: 388.9056898604614\n",
      "  episode_reward_mean: 314.65823364860637\n",
      "  episode_reward_min: 97.87825530768293\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 10293\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.104\n",
      "    load_time_ms: 2.411\n",
      "    num_steps_sampled: 1900000\n",
      "    num_steps_trained: 1900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3611321449279785\n",
      "      kl: 0.017090488225221634\n",
      "      policy_loss: -0.004724711179733276\n",
      "      total_loss: 91.01818084716797\n",
      "      vf_explained_var: 0.9588273763656616\n",
      "      vf_loss: 91.02289581298828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.998064155805685e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0688905715942383\n",
      "      kl: 0.02790423482656479\n",
      "      policy_loss: 0.00146878429222852\n",
      "      total_loss: 99.63449096679688\n",
      "      vf_explained_var: 0.9753384590148926\n",
      "      vf_loss: 99.63301086425781\n",
      "    sample_time_ms: 20534.857\n",
      "    update_time_ms: 7.262\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.40920489083493\n",
      "    rl_1: 179.2490287577714\n",
      "  time_since_restore: 6976.742089033127\n",
      "  time_this_iter_s: 26.260778188705444\n",
      "  time_total_s: 6976.742089033127\n",
      "  timestamp: 1552404733\n",
      "  timesteps_since_restore: 1900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1900000\n",
      "  training_iteration: 190\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 6976 s, 190 iter, 1900000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-32-41\n",
      "  done: false\n",
      "  episode_len_mean: 178.66\n",
      "  episode_reward_max: 388.9056898604614\n",
      "  episode_reward_mean: 294.36673205775793\n",
      "  episode_reward_min: -143.70151531977208\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 10350\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3224.228\n",
      "    load_time_ms: 2.403\n",
      "    num_steps_sampled: 1910000\n",
      "    num_steps_trained: 1910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.33131903409957886\n",
      "      kl: 0.01759509928524494\n",
      "      policy_loss: -0.0016384745249524713\n",
      "      total_loss: 340.5730895996094\n",
      "      vf_explained_var: 0.811194896697998\n",
      "      vf_loss: 340.57470703125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.998064155805685e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9795849323272705\n",
      "      kl: 74.61956024169922\n",
      "      policy_loss: 0.01808040775358677\n",
      "      total_loss: 379.89251708984375\n",
      "      vf_explained_var: 0.8852853775024414\n",
      "      vf_loss: 379.8744812011719\n",
      "    sample_time_ms: 20898.052\n",
      "    update_time_ms: 7.352\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 126.25813951658573\n",
      "    rl_1: 168.1085925411721\n",
      "  time_since_restore: 7005.008188486099\n",
      "  time_this_iter_s: 28.266099452972412\n",
      "  time_total_s: 7005.008188486099\n",
      "  timestamp: 1552404761\n",
      "  timesteps_since_restore: 1910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1910000\n",
      "  training_iteration: 191\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7005 s, 191 iter, 1910000 ts, 294 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-33-17\n",
      "  done: false\n",
      "  episode_len_mean: 178.81\n",
      "  episode_reward_max: 383.94866513387103\n",
      "  episode_reward_mean: 299.3363652426274\n",
      "  episode_reward_min: -141.72039766341285\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 10405\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3534.89\n",
      "    load_time_ms: 2.502\n",
      "    num_steps_sampled: 1920000\n",
      "    num_steps_trained: 1920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3015969693660736\n",
      "      kl: 0.018910296261310577\n",
      "      policy_loss: -0.0005926869926042855\n",
      "      total_loss: 131.16749572753906\n",
      "      vf_explained_var: 0.9317579865455627\n",
      "      vf_loss: 131.1680908203125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9970963184118224e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.052179217338562\n",
      "      kl: 17.50021743774414\n",
      "      policy_loss: 0.036045175045728683\n",
      "      total_loss: 152.4471893310547\n",
      "      vf_explained_var: 0.9566618800163269\n",
      "      vf_loss: 152.41114807128906\n",
      "    sample_time_ms: 21781.731\n",
      "    update_time_ms: 7.886\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.68150341374195\n",
      "    rl_1: 170.65486182888534\n",
      "  time_since_restore: 7040.084272861481\n",
      "  time_this_iter_s: 35.07608437538147\n",
      "  time_total_s: 7040.084272861481\n",
      "  timestamp: 1552404797\n",
      "  timesteps_since_restore: 1920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1920000\n",
      "  training_iteration: 192\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7040 s, 192 iter, 1920000 ts, 299 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-33-49\n",
      "  done: false\n",
      "  episode_len_mean: 180.9\n",
      "  episode_reward_max: 383.94866513387103\n",
      "  episode_reward_mean: 315.50642162013946\n",
      "  episode_reward_min: -141.64094613847604\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 10461\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3616.235\n",
      "    load_time_ms: 2.53\n",
      "    num_steps_sampled: 1930000\n",
      "    num_steps_trained: 1930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3216330111026764\n",
      "      kl: 0.006770216394215822\n",
      "      policy_loss: -0.001472071511670947\n",
      "      total_loss: 27.281099319458008\n",
      "      vf_explained_var: 0.9828828573226929\n",
      "      vf_loss: 27.282573699951172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.495643969397965e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1046477556228638\n",
      "      kl: 4.040598392486572\n",
      "      policy_loss: 0.005873014219105244\n",
      "      total_loss: 33.444278717041016\n",
      "      vf_explained_var: 0.9892907738685608\n",
      "      vf_loss: 33.43840026855469\n",
      "    sample_time_ms: 22580.446\n",
      "    update_time_ms: 8.005\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.8005447202804\n",
      "    rl_1: 179.70587689985902\n",
      "  time_since_restore: 7072.031996011734\n",
      "  time_this_iter_s: 31.947723150253296\n",
      "  time_total_s: 7072.031996011734\n",
      "  timestamp: 1552404829\n",
      "  timesteps_since_restore: 1930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1930000\n",
      "  training_iteration: 193\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7072 s, 193 iter, 1930000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-34-24\n",
      "  done: false\n",
      "  episode_len_mean: 179.58\n",
      "  episode_reward_max: 382.7109555012621\n",
      "  episode_reward_mean: 329.0715776810659\n",
      "  episode_reward_min: 94.86265818043103\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 10516\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3637.479\n",
      "    load_time_ms: 2.557\n",
      "    num_steps_sampled: 1940000\n",
      "    num_steps_trained: 1940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1625146120786667\n",
      "      kl: 0.01111066434532404\n",
      "      policy_loss: -0.003420658176764846\n",
      "      total_loss: 37.58028793334961\n",
      "      vf_explained_var: 0.9776571989059448\n",
      "      vf_loss: 37.583702087402344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.743466462316716e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0133745670318604\n",
      "      kl: 0.014421073719859123\n",
      "      policy_loss: -0.00024808323360048234\n",
      "      total_loss: 40.88123321533203\n",
      "      vf_explained_var: 0.9870932102203369\n",
      "      vf_loss: 40.88147735595703\n",
      "    sample_time_ms: 23742.405\n",
      "    update_time_ms: 8.229\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 141.71669978727255\n",
      "    rl_1: 187.35487789379323\n",
      "  time_since_restore: 7107.337141752243\n",
      "  time_this_iter_s: 35.30514574050903\n",
      "  time_total_s: 7107.337141752243\n",
      "  timestamp: 1552404864\n",
      "  timesteps_since_restore: 1940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1940000\n",
      "  training_iteration: 194\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7107 s, 194 iter, 1940000 ts, 329 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-34-49\n",
      "  done: false\n",
      "  episode_len_mean: 177.96\n",
      "  episode_reward_max: 384.90398407410623\n",
      "  episode_reward_mean: 315.1619819787194\n",
      "  episode_reward_min: -145.47755859555454\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 10573\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3698.923\n",
      "    load_time_ms: 2.588\n",
      "    num_steps_sampled: 1950000\n",
      "    num_steps_trained: 1950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3514493703842163\n",
      "      kl: 0.010537711903452873\n",
      "      policy_loss: -0.0008661302854306996\n",
      "      total_loss: 235.88784790039062\n",
      "      vf_explained_var: 0.8718162178993225\n",
      "      vf_loss: 235.88870239257812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.743466462316716e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1177057027816772\n",
      "      kl: 57.87872314453125\n",
      "      policy_loss: 0.03235945850610733\n",
      "      total_loss: 269.523681640625\n",
      "      vf_explained_var: 0.9209982752799988\n",
      "      vf_loss: 269.4913330078125\n",
      "    sample_time_ms: 23816.225\n",
      "    update_time_ms: 8.014\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 136.24411332276486\n",
      "    rl_1: 178.91786865595452\n",
      "  time_since_restore: 7132.124527931213\n",
      "  time_this_iter_s: 24.787386178970337\n",
      "  time_total_s: 7132.124527931213\n",
      "  timestamp: 1552404889\n",
      "  timesteps_since_restore: 1950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1950000\n",
      "  training_iteration: 195\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7132 s, 195 iter, 1950000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-35-33\n",
      "  done: false\n",
      "  episode_len_mean: 176.62\n",
      "  episode_reward_max: 379.62180456219664\n",
      "  episode_reward_mean: 312.3007968890703\n",
      "  episode_reward_min: -145.47755859555454\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 10630\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4363.607\n",
      "    load_time_ms: 2.775\n",
      "    num_steps_sampled: 1960000\n",
      "    num_steps_trained: 1960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22079727053642273\n",
      "      kl: 0.00868901889771223\n",
      "      policy_loss: -0.0026551373302936554\n",
      "      total_loss: 37.20053482055664\n",
      "      vf_explained_var: 0.97597336769104\n",
      "      vf_loss: 37.20319366455078\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0115196982969643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.111349105834961\n",
      "      kl: 0.05317513644695282\n",
      "      policy_loss: 0.0035595898516476154\n",
      "      total_loss: 42.313873291015625\n",
      "      vf_explained_var: 0.9860487580299377\n",
      "      vf_loss: 42.31032180786133\n",
      "    sample_time_ms: 25192.348\n",
      "    update_time_ms: 8.62\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.26863846941092\n",
      "    rl_1: 178.03215841965937\n",
      "  time_since_restore: 7176.113575696945\n",
      "  time_this_iter_s: 43.98904776573181\n",
      "  time_total_s: 7176.113575696945\n",
      "  timestamp: 1552404933\n",
      "  timesteps_since_restore: 1960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1960000\n",
      "  training_iteration: 196\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7176 s, 196 iter, 1960000 ts, 312 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 179.27\n",
      "  episode_reward_max: 385.15737542898216\n",
      "  episode_reward_mean: 329.3177182083689\n",
      "  episode_reward_min: 127.68527887101453\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 10685\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4469.406\n",
      "    load_time_ms: 2.904\n",
      "    num_steps_sampled: 1970000\n",
      "    num_steps_trained: 1970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24099433422088623\n",
      "      kl: 0.010496588423848152\n",
      "      policy_loss: -0.00046374285011552274\n",
      "      total_loss: 36.55878448486328\n",
      "      vf_explained_var: 0.9765040874481201\n",
      "      vf_loss: 36.55924606323242\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.517279682970718e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1074120998382568\n",
      "      kl: 0.04776293411850929\n",
      "      policy_loss: 0.0025442636106163263\n",
      "      total_loss: 39.865684509277344\n",
      "      vf_explained_var: 0.9871740341186523\n",
      "      vf_loss: 39.8631477355957\n",
      "    sample_time_ms: 27521.552\n",
      "    update_time_ms: 11.835\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 142.52706659823747\n",
      "    rl_1: 186.79065161013148\n",
      "  time_since_restore: 7224.132560253143\n",
      "  time_this_iter_s: 48.01898455619812\n",
      "  time_total_s: 7224.132560253143\n",
      "  timestamp: 1552404981\n",
      "  timesteps_since_restore: 1970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1970000\n",
      "  training_iteration: 197\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7224 s, 197 iter, 1970000 ts, 329 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-36-49\n",
      "  done: false\n",
      "  episode_len_mean: 178.05\n",
      "  episode_reward_max: 385.15737542898216\n",
      "  episode_reward_mean: 326.8456580442713\n",
      "  episode_reward_min: -137.9349786483179\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 10741\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4514.254\n",
      "    load_time_ms: 2.876\n",
      "    num_steps_sampled: 1980000\n",
      "    num_steps_trained: 1980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.261533260345459\n",
      "      kl: 0.020253106951713562\n",
      "      policy_loss: -0.0025059545878320932\n",
      "      total_loss: 94.77330017089844\n",
      "      vf_explained_var: 0.939977765083313\n",
      "      vf_loss: 94.77580261230469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2759203376077064e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1075878143310547\n",
      "      kl: 11.316243171691895\n",
      "      policy_loss: 0.025585928931832314\n",
      "      total_loss: 105.69666290283203\n",
      "      vf_explained_var: 0.9653429985046387\n",
      "      vf_loss: 105.67107391357422\n",
      "    sample_time_ms: 27932.405\n",
      "    update_time_ms: 11.841\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 140.7815020088225\n",
      "    rl_1: 186.0641560354488\n",
      "  time_since_restore: 7251.77773809433\n",
      "  time_this_iter_s: 27.645177841186523\n",
      "  time_total_s: 7251.77773809433\n",
      "  timestamp: 1552405009\n",
      "  timesteps_since_restore: 1980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1980000\n",
      "  training_iteration: 198\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7251 s, 198 iter, 1980000 ts, 327 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-37-24\n",
      "  done: false\n",
      "  episode_len_mean: 176.21\n",
      "  episode_reward_max: 377.4375440254077\n",
      "  episode_reward_mean: 312.2143583628108\n",
      "  episode_reward_min: -143.15272997872242\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 10799\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4718.973\n",
      "    load_time_ms: 2.808\n",
      "    num_steps_sampled: 1990000\n",
      "    num_steps_trained: 1990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.278490275144577\n",
      "      kl: 0.019676748663187027\n",
      "      policy_loss: -0.0017703160410746932\n",
      "      total_loss: 175.5976104736328\n",
      "      vf_explained_var: 0.9000781178474426\n",
      "      vf_loss: 175.599365234375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.4138799643104734e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1222553253173828\n",
      "      kl: 4.047178745269775\n",
      "      policy_loss: 0.03722469508647919\n",
      "      total_loss: 203.8952178955078\n",
      "      vf_explained_var: 0.9367837309837341\n",
      "      vf_loss: 203.85800170898438\n",
      "    sample_time_ms: 28858.545\n",
      "    update_time_ms: 11.748\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.8658243794459\n",
      "    rl_1: 179.34853398336486\n",
      "  time_since_restore: 7286.624305963516\n",
      "  time_this_iter_s: 34.8465678691864\n",
      "  time_total_s: 7286.624305963516\n",
      "  timestamp: 1552405044\n",
      "  timesteps_since_restore: 1990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1990000\n",
      "  training_iteration: 199\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7286 s, 199 iter, 1990000 ts, 312 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-38-02\n",
      "  done: false\n",
      "  episode_len_mean: 176.66\n",
      "  episode_reward_max: 377.4375440254077\n",
      "  episode_reward_mean: 313.1563581483384\n",
      "  episode_reward_min: -141.15145584919276\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 10855\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4768.587\n",
      "    load_time_ms: 2.863\n",
      "    num_steps_sampled: 2000000\n",
      "    num_steps_trained: 2000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2123727798461914\n",
      "      kl: 0.012316057458519936\n",
      "      policy_loss: -0.0013589652953669429\n",
      "      total_loss: 38.97361755371094\n",
      "      vf_explained_var: 0.9746095538139343\n",
      "      vf_loss: 38.974971771240234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.120820217516253e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1109442710876465\n",
      "      kl: 0.01156968530267477\n",
      "      policy_loss: 0.00037217236240394413\n",
      "      total_loss: 47.93876647949219\n",
      "      vf_explained_var: 0.9838656783103943\n",
      "      vf_loss: 47.93838882446289\n",
      "    sample_time_ms: 29981.653\n",
      "    update_time_ms: 11.78\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.9489739532434\n",
      "    rl_1: 180.20738419509507\n",
      "  time_since_restore: 7324.615014076233\n",
      "  time_this_iter_s: 37.990708112716675\n",
      "  time_total_s: 7324.615014076233\n",
      "  timestamp: 1552405082\n",
      "  timesteps_since_restore: 2000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2000000\n",
      "  training_iteration: 200\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7324 s, 200 iter, 2000000 ts, 313 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-38-33\n",
      "  done: false\n",
      "  episode_len_mean: 173.51\n",
      "  episode_reward_max: 381.2546006051844\n",
      "  episode_reward_mean: 317.09219605845897\n",
      "  episode_reward_min: -142.31601348222554\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 10914\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4666.723\n",
      "    load_time_ms: 2.854\n",
      "    num_steps_sampled: 2010000\n",
      "    num_steps_trained: 2010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1283281147480011\n",
      "      kl: 0.017831649631261826\n",
      "      policy_loss: -0.0026628219056874514\n",
      "      total_loss: 140.66812133789062\n",
      "      vf_explained_var: 0.9174064993858337\n",
      "      vf_loss: 140.67079162597656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.120820217516253e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.001047968864441\n",
      "      kl: 18.66139030456543\n",
      "      policy_loss: 0.03199954330921173\n",
      "      total_loss: 156.02040100097656\n",
      "      vf_explained_var: 0.9502571225166321\n",
      "      vf_loss: 155.9884033203125\n",
      "    sample_time_ms: 30390.392\n",
      "    update_time_ms: 11.916\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.68393112665072\n",
      "    rl_1: 182.4082649318082\n",
      "  time_since_restore: 7355.945229768753\n",
      "  time_this_iter_s: 31.33021569252014\n",
      "  time_total_s: 7355.945229768753\n",
      "  timestamp: 1552405113\n",
      "  timesteps_since_restore: 2010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2010000\n",
      "  training_iteration: 201\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7355 s, 201 iter, 2010000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-38-57\n",
      "  done: false\n",
      "  episode_len_mean: 176.09\n",
      "  episode_reward_max: 381.2546006051844\n",
      "  episode_reward_mean: 321.5737127196332\n",
      "  episode_reward_min: -142.31601348222554\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 10969\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4358.624\n",
      "    load_time_ms: 2.778\n",
      "    num_steps_sampled: 2020000\n",
      "    num_steps_trained: 2020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24234630167484283\n",
      "      kl: 0.00985786784440279\n",
      "      policy_loss: -0.00042705697705969214\n",
      "      total_loss: 31.891178131103516\n",
      "      vf_explained_var: 0.979152500629425\n",
      "      vf_loss: 31.891597747802734\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.681228157870035e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.165995478630066\n",
      "      kl: 0.020248297601938248\n",
      "      policy_loss: 0.001405058428645134\n",
      "      total_loss: 37.74462127685547\n",
      "      vf_explained_var: 0.987738311290741\n",
      "      vf_loss: 37.74320983886719\n",
      "    sample_time_ms: 29545.892\n",
      "    update_time_ms: 11.271\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 137.9898826962185\n",
      "    rl_1: 183.5838300234147\n",
      "  time_since_restore: 7379.485135793686\n",
      "  time_this_iter_s: 23.53990602493286\n",
      "  time_total_s: 7379.485135793686\n",
      "  timestamp: 1552405137\n",
      "  timesteps_since_restore: 2020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2020000\n",
      "  training_iteration: 202\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7379 s, 202 iter, 2020000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-39-20\n",
      "  done: false\n",
      "  episode_len_mean: 175.93\n",
      "  episode_reward_max: 379.2309210571243\n",
      "  episode_reward_mean: 320.981882066837\n",
      "  episode_reward_min: -139.3092603198226\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 11027\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4278.383\n",
      "    load_time_ms: 2.801\n",
      "    num_steps_sampled: 2030000\n",
      "    num_steps_trained: 2030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.17777201533317566\n",
      "      kl: 0.023028256371617317\n",
      "      policy_loss: 0.0013631664915010333\n",
      "      total_loss: 94.57965850830078\n",
      "      vf_explained_var: 0.942368745803833\n",
      "      vf_loss: 94.57830047607422\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.681228157870035e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0367238521575928\n",
      "      kl: 3.583162784576416\n",
      "      policy_loss: 0.02704627625644207\n",
      "      total_loss: 109.94320678710938\n",
      "      vf_explained_var: 0.96446293592453\n",
      "      vf_loss: 109.91616821289062\n",
      "    sample_time_ms: 28772.939\n",
      "    update_time_ms: 11.249\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 137.58336451660784\n",
      "    rl_1: 183.39851755022914\n",
      "  time_since_restore: 7402.8952667713165\n",
      "  time_this_iter_s: 23.410130977630615\n",
      "  time_total_s: 7402.8952667713165\n",
      "  timestamp: 1552405160\n",
      "  timesteps_since_restore: 2030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2030000\n",
      "  training_iteration: 203\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7402 s, 203 iter, 2030000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-39-44\n",
      "  done: false\n",
      "  episode_len_mean: 173.86\n",
      "  episode_reward_max: 383.1788538710256\n",
      "  episode_reward_mean: 316.1885442525291\n",
      "  episode_reward_min: -139.96655634786742\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 11084\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4284.912\n",
      "    load_time_ms: 2.836\n",
      "    num_steps_sampled: 2040000\n",
      "    num_steps_trained: 2040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.16595503687858582\n",
      "      kl: 0.02259727381169796\n",
      "      policy_loss: 0.0003068545483984053\n",
      "      total_loss: 155.57632446289062\n",
      "      vf_explained_var: 0.9074447751045227\n",
      "      vf_loss: 155.57598876953125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1521842507855595e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0197831392288208\n",
      "      kl: 55.31909942626953\n",
      "      policy_loss: 0.026513060554862022\n",
      "      total_loss: 175.18154907226562\n",
      "      vf_explained_var: 0.9445268511772156\n",
      "      vf_loss: 175.155029296875\n",
      "    sample_time_ms: 27591.475\n",
      "    update_time_ms: 11.508\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.91361367998277\n",
      "    rl_1: 181.27493057254634\n",
      "  time_since_restore: 7426.452821016312\n",
      "  time_this_iter_s: 23.557554244995117\n",
      "  time_total_s: 7426.452821016312\n",
      "  timestamp: 1552405184\n",
      "  timesteps_since_restore: 2040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2040000\n",
      "  training_iteration: 204\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7426 s, 204 iter, 2040000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-40-10\n",
      "  done: false\n",
      "  episode_len_mean: 172.35\n",
      "  episode_reward_max: 383.1788538710256\n",
      "  episode_reward_mean: 312.82740402643304\n",
      "  episode_reward_min: -144.92908031942244\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 11142\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4279.38\n",
      "    load_time_ms: 2.891\n",
      "    num_steps_sampled: 2050000\n",
      "    num_steps_trained: 2050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.15495453774929047\n",
      "      kl: 0.014566653408110142\n",
      "      policy_loss: -0.0005287954118102789\n",
      "      total_loss: 130.52200317382812\n",
      "      vf_explained_var: 0.9132469296455383\n",
      "      vf_loss: 130.52252197265625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7282760509176875e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0394389629364014\n",
      "      kl: 12.378119468688965\n",
      "      policy_loss: 0.022563597187399864\n",
      "      total_loss: 149.98789978027344\n",
      "      vf_explained_var: 0.9503569006919861\n",
      "      vf_loss: 149.96533203125\n",
      "    sample_time_ms: 27778.504\n",
      "    update_time_ms: 11.888\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.96776905042606\n",
      "    rl_1: 179.85963497600696\n",
      "  time_since_restore: 7453.060397148132\n",
      "  time_this_iter_s: 26.60757613182068\n",
      "  time_total_s: 7453.060397148132\n",
      "  timestamp: 1552405210\n",
      "  timesteps_since_restore: 2050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2050000\n",
      "  training_iteration: 205\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7453 s, 205 iter, 2050000 ts, 313 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-40-46\n",
      "  done: false\n",
      "  episode_len_mean: 173.02\n",
      "  episode_reward_max: 380.94758995212277\n",
      "  episode_reward_mean: 317.5732195374115\n",
      "  episode_reward_min: -142.77122007086527\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 11201\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3859.322\n",
      "    load_time_ms: 2.979\n",
      "    num_steps_sampled: 2060000\n",
      "    num_steps_trained: 2060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.19689835608005524\n",
      "      kl: 0.010441118851304054\n",
      "      policy_loss: 0.0015318995574489236\n",
      "      total_loss: 90.45094299316406\n",
      "      vf_explained_var: 0.941418468952179\n",
      "      vf_loss: 90.44940185546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.5924143474270744e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1267346143722534\n",
      "      kl: 0.04216970130801201\n",
      "      policy_loss: 0.0037724142894148827\n",
      "      total_loss: 103.02013397216797\n",
      "      vf_explained_var: 0.9650520086288452\n",
      "      vf_loss: 103.016357421875\n",
      "    sample_time_ms: 27400.229\n",
      "    update_time_ms: 11.318\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.6128417477508\n",
      "    rl_1: 182.96037778966067\n",
      "  time_since_restore: 7489.041686296463\n",
      "  time_this_iter_s: 35.98128914833069\n",
      "  time_total_s: 7489.041686296463\n",
      "  timestamp: 1552405246\n",
      "  timesteps_since_restore: 2060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2060000\n",
      "  training_iteration: 206\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7489 s, 206 iter, 2060000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-41-20\n",
      "  done: false\n",
      "  episode_len_mean: 173.66\n",
      "  episode_reward_max: 379.2046918066386\n",
      "  episode_reward_mean: 312.15181099538376\n",
      "  episode_reward_min: -152.44653255930456\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 11258\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3933.188\n",
      "    load_time_ms: 3.144\n",
      "    num_steps_sampled: 2070000\n",
      "    num_steps_trained: 2070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.197196364402771\n",
      "      kl: 0.01010628417134285\n",
      "      policy_loss: -0.0013134439941495657\n",
      "      total_loss: 113.27117156982422\n",
      "      vf_explained_var: 0.9343703985214233\n",
      "      vf_loss: 113.27251434326172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.88862119587996e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0713214874267578\n",
      "      kl: 0.03905456140637398\n",
      "      policy_loss: 0.004213653039187193\n",
      "      total_loss: 128.8288116455078\n",
      "      vf_explained_var: 0.9610536098480225\n",
      "      vf_loss: 128.82460021972656\n",
      "    sample_time_ms: 25925.233\n",
      "    update_time_ms: 8.347\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 132.00170174989654\n",
      "    rl_1: 180.1501092454872\n",
      "  time_since_restore: 7523.028370380402\n",
      "  time_this_iter_s: 33.9866840839386\n",
      "  time_total_s: 7523.028370380402\n",
      "  timestamp: 1552405280\n",
      "  timesteps_since_restore: 2070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2070000\n",
      "  training_iteration: 207\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7523 s, 207 iter, 2070000 ts, 312 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-41-58\n",
      "  done: false\n",
      "  episode_len_mean: 173.37\n",
      "  episode_reward_max: 379.2046918066386\n",
      "  episode_reward_mean: 311.4774337998322\n",
      "  episode_reward_min: -152.44653255930456\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 11315\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4002.827\n",
      "    load_time_ms: 3.126\n",
      "    num_steps_sampled: 2080000\n",
      "    num_steps_trained: 2080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.17463979125022888\n",
      "      kl: 0.012890120036900043\n",
      "      policy_loss: 0.0007066504913382232\n",
      "      total_loss: 86.93537139892578\n",
      "      vf_explained_var: 0.9488815665245056\n",
      "      vf_loss: 86.93466186523438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.88862119587996e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.056672215461731\n",
      "      kl: 0.026417460292577744\n",
      "      policy_loss: -3.046255915251095e-05\n",
      "      total_loss: 100.52951049804688\n",
      "      vf_explained_var: 0.9694427251815796\n",
      "      vf_loss: 100.529541015625\n",
      "    sample_time_ms: 26849.305\n",
      "    update_time_ms: 8.891\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 131.87120639677886\n",
      "    rl_1: 179.60622740305337\n",
      "  time_since_restore: 7560.617758274078\n",
      "  time_this_iter_s: 37.58938789367676\n",
      "  time_total_s: 7560.617758274078\n",
      "  timestamp: 1552405318\n",
      "  timesteps_since_restore: 2080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2080000\n",
      "  training_iteration: 208\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7560 s, 208 iter, 2080000 ts, 311 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-42-53\n",
      "  done: false\n",
      "  episode_len_mean: 175.98\n",
      "  episode_reward_max: 376.73070090910034\n",
      "  episode_reward_mean: 324.56228050775087\n",
      "  episode_reward_min: 112.53006708334738\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 11372\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3998.744\n",
      "    load_time_ms: 3.224\n",
      "    num_steps_sampled: 2090000\n",
      "    num_steps_trained: 2090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1682332307100296\n",
      "      kl: 0.007898221723735332\n",
      "      policy_loss: -0.00037512282142415643\n",
      "      total_loss: 43.0640754699707\n",
      "      vf_explained_var: 0.9742802381515503\n",
      "      vf_loss: 43.06444549560547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.88862119587996e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0975310802459717\n",
      "      kl: 0.023005694150924683\n",
      "      policy_loss: 0.0003402157744858414\n",
      "      total_loss: 46.02913284301758\n",
      "      vf_explained_var: 0.9859062433242798\n",
      "      vf_loss: 46.0287971496582\n",
      "    sample_time_ms: 28854.776\n",
      "    update_time_ms: 9.059\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 138.36924586061016\n",
      "    rl_1: 186.19303464714068\n",
      "  time_since_restore: 7615.484107017517\n",
      "  time_this_iter_s: 54.86634874343872\n",
      "  time_total_s: 7615.484107017517\n",
      "  timestamp: 1552405373\n",
      "  timesteps_since_restore: 2090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2090000\n",
      "  training_iteration: 209\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7615 s, 209 iter, 2090000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-43-25\n",
      "  done: false\n",
      "  episode_len_mean: 175.51\n",
      "  episode_reward_max: 377.53498752304364\n",
      "  episode_reward_mean: 329.2551001834916\n",
      "  episode_reward_min: 112.53006708334738\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 11429\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3944.816\n",
      "    load_time_ms: 3.116\n",
      "    num_steps_sampled: 2100000\n",
      "    num_steps_trained: 2100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.18886147439479828\n",
      "      kl: 0.007809900213032961\n",
      "      policy_loss: -0.00039338826900348067\n",
      "      total_loss: 27.31007194519043\n",
      "      vf_explained_var: 0.9821728467941284\n",
      "      vf_loss: 27.310462951660156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.88862119587996e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.112597107887268\n",
      "      kl: 0.03307682275772095\n",
      "      policy_loss: 0.002283235779032111\n",
      "      total_loss: 27.37916374206543\n",
      "      vf_explained_var: 0.9910733103752136\n",
      "      vf_loss: 27.37688446044922\n",
      "    sample_time_ms: 28332.466\n",
      "    update_time_ms: 9.109\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 140.74674089925108\n",
      "    rl_1: 188.5083592842405\n",
      "  time_since_restore: 7647.709770679474\n",
      "  time_this_iter_s: 32.22566366195679\n",
      "  time_total_s: 7647.709770679474\n",
      "  timestamp: 1552405405\n",
      "  timesteps_since_restore: 2100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2100000\n",
      "  training_iteration: 210\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7647 s, 210 iter, 2100000 ts, 329 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 172.91\n",
      "  episode_reward_max: 377.53498752304364\n",
      "  episode_reward_mean: 314.71134256389774\n",
      "  episode_reward_min: -147.74251891756765\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 11488\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3987.837\n",
      "    load_time_ms: 3.147\n",
      "    num_steps_sampled: 2110000\n",
      "    num_steps_trained: 2110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.14878033101558685\n",
      "      kl: 0.018619151785969734\n",
      "      policy_loss: -0.00025597773492336273\n",
      "      total_loss: 163.30865478515625\n",
      "      vf_explained_var: 0.9082459807395935\n",
      "      vf_loss: 163.3088836669922\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.88862119587996e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0605329275131226\n",
      "      kl: 0.8310441374778748\n",
      "      policy_loss: 0.02120363526046276\n",
      "      total_loss: 186.60032653808594\n",
      "      vf_explained_var: 0.9425906538963318\n",
      "      vf_loss: 186.57911682128906\n",
      "    sample_time_ms: 27759.512\n",
      "    update_time_ms: 8.974\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.3581523409169\n",
      "    rl_1: 180.3531902229808\n",
      "  time_since_restore: 7673.740768432617\n",
      "  time_this_iter_s: 26.03099775314331\n",
      "  time_total_s: 7673.740768432617\n",
      "  timestamp: 1552405431\n",
      "  timesteps_since_restore: 2110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2110000\n",
      "  training_iteration: 211\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7673 s, 211 iter, 2110000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-44-15\n",
      "  done: false\n",
      "  episode_len_mean: 171.89\n",
      "  episode_reward_max: 372.1252618291639\n",
      "  episode_reward_mean: 301.62078038128567\n",
      "  episode_reward_min: -147.74251891756765\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 11545\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3987.614\n",
      "    load_time_ms: 3.124\n",
      "    num_steps_sampled: 2120000\n",
      "    num_steps_trained: 2120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24197837710380554\n",
      "      kl: 0.011559288017451763\n",
      "      policy_loss: -0.0018969213124364614\n",
      "      total_loss: 170.24256896972656\n",
      "      vf_explained_var: 0.9023403525352478\n",
      "      vf_loss: 170.24447631835938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.832934829586023e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1444134712219238\n",
      "      kl: 1.093799114227295\n",
      "      policy_loss: 0.0015627759275957942\n",
      "      total_loss: 195.80850219726562\n",
      "      vf_explained_var: 0.9409837126731873\n",
      "      vf_loss: 195.80691528320312\n",
      "    sample_time_ms: 27777.423\n",
      "    update_time_ms: 9.356\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 128.77883407366346\n",
      "    rl_1: 172.84194630762232\n",
      "  time_since_restore: 7697.460830450058\n",
      "  time_this_iter_s: 23.720062017440796\n",
      "  time_total_s: 7697.460830450058\n",
      "  timestamp: 1552405455\n",
      "  timesteps_since_restore: 2120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2120000\n",
      "  training_iteration: 212\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7697 s, 212 iter, 2120000 ts, 302 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-44-38\n",
      "  done: false\n",
      "  episode_len_mean: 174.21\n",
      "  episode_reward_max: 377.56228758609063\n",
      "  episode_reward_mean: 318.4363846424875\n",
      "  episode_reward_min: -143.32726225305987\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 11603\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3991.955\n",
      "    load_time_ms: 3.039\n",
      "    num_steps_sampled: 2130000\n",
      "    num_steps_trained: 2130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.17982009053230286\n",
      "      kl: 0.014288755133748055\n",
      "      policy_loss: 0.0005096696550026536\n",
      "      total_loss: 79.41937255859375\n",
      "      vf_explained_var: 0.9457458257675171\n",
      "      vf_loss: 79.41886138916016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.749398774932082e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1099773645401\n",
      "      kl: 3.2836320400238037\n",
      "      policy_loss: 0.008728227578103542\n",
      "      total_loss: 89.111083984375\n",
      "      vf_explained_var: 0.9705639481544495\n",
      "      vf_loss: 89.10235595703125\n",
      "    sample_time_ms: 27744.97\n",
      "    update_time_ms: 9.323\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 136.5886752639598\n",
      "    rl_1: 181.84770937852772\n",
      "  time_since_restore: 7720.589237213135\n",
      "  time_this_iter_s: 23.128406763076782\n",
      "  time_total_s: 7720.589237213135\n",
      "  timestamp: 1552405478\n",
      "  timesteps_since_restore: 2130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2130000\n",
      "  training_iteration: 213\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7720 s, 213 iter, 2130000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-45-02\n",
      "  done: false\n",
      "  episode_len_mean: 173.55\n",
      "  episode_reward_max: 388.1348420447826\n",
      "  episode_reward_mean: 313.0289481344846\n",
      "  episode_reward_min: -143.32726225305987\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 11660\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3967.456\n",
      "    load_time_ms: 2.991\n",
      "    num_steps_sampled: 2140000\n",
      "    num_steps_trained: 2140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1842074692249298\n",
      "      kl: 0.013612201437354088\n",
      "      policy_loss: -0.0010515650501474738\n",
      "      total_loss: 107.93507385253906\n",
      "      vf_explained_var: 0.9357563853263855\n",
      "      vf_loss: 107.93611907958984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.3124097295036385e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0954499244689941\n",
      "      kl: 0.015270436182618141\n",
      "      policy_loss: 0.001386043499223888\n",
      "      total_loss: 126.99034881591797\n",
      "      vf_explained_var: 0.9613828659057617\n",
      "      vf_loss: 126.98896026611328\n",
      "    sample_time_ms: 27793.322\n",
      "    update_time_ms: 8.764\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.28750408350294\n",
      "    rl_1: 178.74144405098158\n",
      "  time_since_restore: 7744.377534866333\n",
      "  time_this_iter_s: 23.788297653198242\n",
      "  time_total_s: 7744.377534866333\n",
      "  timestamp: 1552405502\n",
      "  timesteps_since_restore: 2140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2140000\n",
      "  training_iteration: 214\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7744 s, 214 iter, 2140000 ts, 313 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-45-27\n",
      "  done: false\n",
      "  episode_len_mean: 173.66\n",
      "  episode_reward_max: 388.1348420447826\n",
      "  episode_reward_mean: 314.8393066905221\n",
      "  episode_reward_min: -150.01076892416776\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 11718\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3922.045\n",
      "    load_time_ms: 2.993\n",
      "    num_steps_sampled: 2150000\n",
      "    num_steps_trained: 2150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.18521782755851746\n",
      "      kl: 0.011339694261550903\n",
      "      policy_loss: -3.682974784169346e-05\n",
      "      total_loss: 82.93790435791016\n",
      "      vf_explained_var: 0.9480726718902588\n",
      "      vf_loss: 82.93793487548828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.3124097295036385e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.115675687789917\n",
      "      kl: 0.02097178064286709\n",
      "      policy_loss: 0.0007065224344842136\n",
      "      total_loss: 93.04524230957031\n",
      "      vf_explained_var: 0.9704553484916687\n",
      "      vf_loss: 93.0445327758789\n",
      "    sample_time_ms: 27680.529\n",
      "    update_time_ms: 8.498\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.55962391283336\n",
      "    rl_1: 180.27968277768875\n",
      "  time_since_restore: 7769.396281242371\n",
      "  time_this_iter_s: 25.018746376037598\n",
      "  time_total_s: 7769.396281242371\n",
      "  timestamp: 1552405527\n",
      "  timesteps_since_restore: 2150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2150000\n",
      "  training_iteration: 215\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7769 s, 215 iter, 2150000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-45-54\n",
      "  done: false\n",
      "  episode_len_mean: 173.45\n",
      "  episode_reward_max: 376.48188367002155\n",
      "  episode_reward_mean: 314.4669364603025\n",
      "  episode_reward_min: -141.64327754976577\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 11776\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3700.079\n",
      "    load_time_ms: 2.742\n",
      "    num_steps_sampled: 2160000\n",
      "    num_steps_trained: 2160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.16065791249275208\n",
      "      kl: 0.028614282608032227\n",
      "      policy_loss: -0.00035247160121798515\n",
      "      total_loss: 203.49281311035156\n",
      "      vf_explained_var: 0.8787186741828918\n",
      "      vf_loss: 203.4931640625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.3124097295036385e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0981495380401611\n",
      "      kl: 14.168746948242188\n",
      "      policy_loss: 0.013410094194114208\n",
      "      total_loss: 233.04600524902344\n",
      "      vf_explained_var: 0.926083505153656\n",
      "      vf_loss: 233.0325469970703\n",
      "    sample_time_ms: 26943.799\n",
      "    update_time_ms: 8.317\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.0939164759887\n",
      "    rl_1: 180.37301998431383\n",
      "  time_since_restore: 7795.77756023407\n",
      "  time_this_iter_s: 26.38127899169922\n",
      "  time_total_s: 7795.77756023407\n",
      "  timestamp: 1552405554\n",
      "  timesteps_since_restore: 2160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2160000\n",
      "  training_iteration: 216\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7795 s, 216 iter, 2160000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-46-23\n",
      "  done: false\n",
      "  episode_len_mean: 173.29\n",
      "  episode_reward_max: 388.408319452024\n",
      "  episode_reward_mean: 315.2549427523792\n",
      "  episode_reward_min: -145.6998707764833\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 11833\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3587.617\n",
      "    load_time_ms: 2.471\n",
      "    num_steps_sampled: 2170000\n",
      "    num_steps_trained: 2170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22080852091312408\n",
      "      kl: 0.015131824649870396\n",
      "      policy_loss: 0.00023948706802912056\n",
      "      total_loss: 149.61801147460938\n",
      "      vf_explained_var: 0.9045683741569519\n",
      "      vf_loss: 149.61778259277344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9686152014086744e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1628427505493164\n",
      "      kl: 6.599475383758545\n",
      "      policy_loss: 0.003091029357165098\n",
      "      total_loss: 174.22023010253906\n",
      "      vf_explained_var: 0.9411115646362305\n",
      "      vf_loss: 174.21713256835938\n",
      "    sample_time_ms: 26599.0\n",
      "    update_time_ms: 8.012\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.93240395742902\n",
      "    rl_1: 180.32253879495008\n",
      "  time_since_restore: 7825.179017543793\n",
      "  time_this_iter_s: 29.4014573097229\n",
      "  time_total_s: 7825.179017543793\n",
      "  timestamp: 1552405583\n",
      "  timesteps_since_restore: 2170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2170000\n",
      "  training_iteration: 217\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7825 s, 217 iter, 2170000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-46-54\n",
      "  done: false\n",
      "  episode_len_mean: 177.04\n",
      "  episode_reward_max: 388.408319452024\n",
      "  episode_reward_mean: 324.5501434370746\n",
      "  episode_reward_min: -141.13468252212775\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 11890\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3777.448\n",
      "    load_time_ms: 2.518\n",
      "    num_steps_sampled: 2180000\n",
      "    num_steps_trained: 2180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.23696868121623993\n",
      "      kl: 0.010020428337156773\n",
      "      policy_loss: -0.0032406479585915804\n",
      "      total_loss: 91.2305679321289\n",
      "      vf_explained_var: 0.9427781701087952\n",
      "      vf_loss: 91.23380279541016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.952922975585359e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.145516037940979\n",
      "      kl: 0.021084722131490707\n",
      "      policy_loss: -0.0004956350894644856\n",
      "      total_loss: 104.322021484375\n",
      "      vf_explained_var: 0.9649379253387451\n",
      "      vf_loss: 104.32251739501953\n",
      "    sample_time_ms: 25770.58\n",
      "    update_time_ms: 7.463\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 140.86824219058062\n",
      "    rl_1: 183.681901246494\n",
      "  time_since_restore: 7856.388329744339\n",
      "  time_this_iter_s: 31.209312200546265\n",
      "  time_total_s: 7856.388329744339\n",
      "  timestamp: 1552405614\n",
      "  timesteps_since_restore: 2180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2180000\n",
      "  training_iteration: 218\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7856 s, 218 iter, 2180000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-47-29\n",
      "  done: false\n",
      "  episode_len_mean: 177.37\n",
      "  episode_reward_max: 370.76140059769847\n",
      "  episode_reward_mean: 331.69592915086747\n",
      "  episode_reward_min: 171.04173907230017\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 11946\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3665.001\n",
      "    load_time_ms: 2.409\n",
      "    num_steps_sampled: 2190000\n",
      "    num_steps_trained: 2190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1841590255498886\n",
      "      kl: 0.0065943109802901745\n",
      "      policy_loss: -0.0008679277962073684\n",
      "      total_loss: 13.885405540466309\n",
      "      vf_explained_var: 0.9896734952926636\n",
      "      vf_loss: 13.886272430419922\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.952922975585359e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1009899377822876\n",
      "      kl: 0.011346661485731602\n",
      "      policy_loss: 0.0014393493765965104\n",
      "      total_loss: 13.026981353759766\n",
      "      vf_explained_var: 0.9950904846191406\n",
      "      vf_loss: 13.025541305541992\n",
      "    sample_time_ms: 23889.838\n",
      "    update_time_ms: 8.043\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 142.76470612831258\n",
      "    rl_1: 188.93122302255478\n",
      "  time_since_restore: 7891.329993009567\n",
      "  time_this_iter_s: 34.94166326522827\n",
      "  time_total_s: 7891.329993009567\n",
      "  timestamp: 1552405649\n",
      "  timesteps_since_restore: 2190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2190000\n",
      "  training_iteration: 219\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7891 s, 219 iter, 2190000 ts, 332 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-47-59\n",
      "  done: false\n",
      "  episode_len_mean: 176.24\n",
      "  episode_reward_max: 375.9326115120743\n",
      "  episode_reward_mean: 328.6385660795962\n",
      "  episode_reward_min: -150.42253352297146\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 12002\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3634.437\n",
      "    load_time_ms: 2.428\n",
      "    num_steps_sampled: 2200000\n",
      "    num_steps_trained: 2200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1834312379360199\n",
      "      kl: 0.030773339793086052\n",
      "      policy_loss: -0.0023920638486742973\n",
      "      total_loss: 67.47557830810547\n",
      "      vf_explained_var: 0.9599325656890869\n",
      "      vf_loss: 67.47797393798828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.952922975585359e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.154832124710083\n",
      "      kl: 0.017117708921432495\n",
      "      policy_loss: -0.0015555439749732614\n",
      "      total_loss: 78.9103775024414\n",
      "      vf_explained_var: 0.9748168587684631\n",
      "      vf_loss: 78.91192626953125\n",
      "    sample_time_ms: 23689.642\n",
      "    update_time_ms: 8.681\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 141.85296367385502\n",
      "    rl_1: 186.7856024057412\n",
      "  time_since_restore: 7921.2585163116455\n",
      "  time_this_iter_s: 29.928523302078247\n",
      "  time_total_s: 7921.2585163116455\n",
      "  timestamp: 1552405679\n",
      "  timesteps_since_restore: 2200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2200000\n",
      "  training_iteration: 220\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7921 s, 220 iter, 2200000 ts, 329 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-48-26\n",
      "  done: false\n",
      "  episode_len_mean: 176.69\n",
      "  episode_reward_max: 379.2826345279848\n",
      "  episode_reward_mean: 326.10357985413316\n",
      "  episode_reward_min: -150.42253352297146\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 12059\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3619.546\n",
      "    load_time_ms: 2.442\n",
      "    num_steps_sampled: 2210000\n",
      "    num_steps_trained: 2210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1813015192747116\n",
      "      kl: 0.00813085027039051\n",
      "      policy_loss: 0.00026525213615968823\n",
      "      total_loss: 22.86769676208496\n",
      "      vf_explained_var: 0.9849862456321716\n",
      "      vf_loss: 22.867429733276367\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.952922975585359e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1182477474212646\n",
      "      kl: 0.012198599986732006\n",
      "      policy_loss: -0.0012228789273649454\n",
      "      total_loss: 25.907440185546875\n",
      "      vf_explained_var: 0.9911451935768127\n",
      "      vf_loss: 25.908662796020508\n",
      "    sample_time_ms: 23710.61\n",
      "    update_time_ms: 8.706\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 140.65904841307636\n",
      "    rl_1: 185.4445314410568\n",
      "  time_since_restore: 7947.349583864212\n",
      "  time_this_iter_s: 26.09106755256653\n",
      "  time_total_s: 7947.349583864212\n",
      "  timestamp: 1552405706\n",
      "  timesteps_since_restore: 2210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2210000\n",
      "  training_iteration: 221\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7947 s, 221 iter, 2210000 ts, 326 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-48-49\n",
      "  done: false\n",
      "  episode_len_mean: 176.8\n",
      "  episode_reward_max: 383.3093080663394\n",
      "  episode_reward_mean: 323.3174955097987\n",
      "  episode_reward_min: -137.26055073336835\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 12116\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3616.726\n",
      "    load_time_ms: 2.531\n",
      "    num_steps_sampled: 2220000\n",
      "    num_steps_trained: 2220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.16747763752937317\n",
      "      kl: 0.015479049645364285\n",
      "      policy_loss: -0.0024337179493159056\n",
      "      total_loss: 110.81189727783203\n",
      "      vf_explained_var: 0.9372653365135193\n",
      "      vf_loss: 110.81431579589844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.952922975585359e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1269606351852417\n",
      "      kl: 0.12923353910446167\n",
      "      policy_loss: 0.014261881820857525\n",
      "      total_loss: 118.23279571533203\n",
      "      vf_explained_var: 0.9636431932449341\n",
      "      vf_loss: 118.21855163574219\n",
      "    sample_time_ms: 23704.953\n",
      "    update_time_ms: 8.415\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 138.7940607396415\n",
      "    rl_1: 184.52343477015714\n",
      "  time_since_restore: 7970.982531070709\n",
      "  time_this_iter_s: 23.632947206497192\n",
      "  time_total_s: 7970.982531070709\n",
      "  timestamp: 1552405729\n",
      "  timesteps_since_restore: 2220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2220000\n",
      "  training_iteration: 222\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7970 s, 222 iter, 2220000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-49-13\n",
      "  done: false\n",
      "  episode_len_mean: 177.05\n",
      "  episode_reward_max: 383.3093080663394\n",
      "  episode_reward_mean: 312.8291932366846\n",
      "  episode_reward_min: -137.26055073336835\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 12173\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3679.256\n",
      "    load_time_ms: 2.531\n",
      "    num_steps_sampled: 2230000\n",
      "    num_steps_trained: 2230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1775069683790207\n",
      "      kl: 0.013948967680335045\n",
      "      policy_loss: -0.0017680692253634334\n",
      "      total_loss: 80.32174682617188\n",
      "      vf_explained_var: 0.9573835730552673\n",
      "      vf_loss: 80.32351684570312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.4293846368503864e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1526379585266113\n",
      "      kl: 0.007262053433805704\n",
      "      policy_loss: -0.0005395094631239772\n",
      "      total_loss: 86.3748779296875\n",
      "      vf_explained_var: 0.9749130010604858\n",
      "      vf_loss: 86.37541198730469\n",
      "    sample_time_ms: 23716.655\n",
      "    update_time_ms: 8.595\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.998662280224\n",
      "    rl_1: 177.83053095646062\n",
      "  time_since_restore: 7994.855060577393\n",
      "  time_this_iter_s: 23.87252950668335\n",
      "  time_total_s: 7994.855060577393\n",
      "  timestamp: 1552405753\n",
      "  timesteps_since_restore: 2230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2230000\n",
      "  training_iteration: 223\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 7994 s, 223 iter, 2230000 ts, 313 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-49-39\n",
      "  done: false\n",
      "  episode_len_mean: 175.53\n",
      "  episode_reward_max: 380.6744369103372\n",
      "  episode_reward_mean: 314.71329767218384\n",
      "  episode_reward_min: -137.10480657699733\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 12229\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3675.613\n",
      "    load_time_ms: 2.519\n",
      "    num_steps_sampled: 2240000\n",
      "    num_steps_trained: 2240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2010776549577713\n",
      "      kl: 0.10415330529212952\n",
      "      policy_loss: 0.008233779110014439\n",
      "      total_loss: 103.84405517578125\n",
      "      vf_explained_var: 0.9414947628974915\n",
      "      vf_loss: 103.83582305908203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2146923184251932e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1678208112716675\n",
      "      kl: 0.021779581904411316\n",
      "      policy_loss: -0.0011910020839422941\n",
      "      total_loss: 119.62818145751953\n",
      "      vf_explained_var: 0.9647946357727051\n",
      "      vf_loss: 119.62937927246094\n",
      "    sample_time_ms: 23878.942\n",
      "    update_time_ms: 9.71\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 136.04746224608283\n",
      "    rl_1: 178.66583542610095\n",
      "  time_since_restore: 8020.2414700984955\n",
      "  time_this_iter_s: 25.386409521102905\n",
      "  time_total_s: 8020.2414700984955\n",
      "  timestamp: 1552405779\n",
      "  timesteps_since_restore: 2240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2240000\n",
      "  training_iteration: 224\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 8020 s, 224 iter, 2240000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-50-02\n",
      "  done: false\n",
      "  episode_len_mean: 173.79\n",
      "  episode_reward_max: 381.5565466393143\n",
      "  episode_reward_mean: 310.1378297934546\n",
      "  episode_reward_min: -143.49300245252738\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 12287\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3667.022\n",
      "    load_time_ms: 2.527\n",
      "    num_steps_sampled: 2250000\n",
      "    num_steps_trained: 2250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.12315817177295685\n",
      "      kl: 0.020366599783301353\n",
      "      policy_loss: -0.0004213926149532199\n",
      "      total_loss: 188.96522521972656\n",
      "      vf_explained_var: 0.8943516612052917\n",
      "      vf_loss: 188.96563720703125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2146923184251932e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0847277641296387\n",
      "      kl: 9.732278823852539\n",
      "      policy_loss: 0.016063140705227852\n",
      "      total_loss: 217.77703857421875\n",
      "      vf_explained_var: 0.9358820915222168\n",
      "      vf_loss: 217.760986328125\n",
      "    sample_time_ms: 23758.196\n",
      "    update_time_ms: 9.744\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.2583843434752\n",
      "    rl_1: 175.87944544997944\n",
      "  time_since_restore: 8043.969957113266\n",
      "  time_this_iter_s: 23.728487014770508\n",
      "  time_total_s: 8043.969957113266\n",
      "  timestamp: 1552405802\n",
      "  timesteps_since_restore: 2250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2250000\n",
      "  training_iteration: 225\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 8043 s, 225 iter, 2250000 ts, 310 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-12_16-50-26\n",
      "  done: false\n",
      "  episode_len_mean: 175.96\n",
      "  episode_reward_max: 388.054843170877\n",
      "  episode_reward_mean: 313.06765439447446\n",
      "  episode_reward_min: -141.18960400263305\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 12343\n",
      "  experiment_id: abc197f3c9fd4810a90b9d1fdf241416\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3633.313\n",
      "    load_time_ms: 2.504\n",
      "    num_steps_sampled: 2260000\n",
      "    num_steps_trained: 2260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.17432084679603577\n",
      "      kl: 0.008700086735188961\n",
      "      policy_loss: 0.00034407430212013423\n",
      "      total_loss: 141.951416015625\n",
      "      vf_explained_var: 0.9234037399291992\n",
      "      vf_loss: 141.95111083984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.322037089859009e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1281331777572632\n",
      "      kl: 0.07256711274385452\n",
      "      policy_loss: 0.007376006804406643\n",
      "      total_loss: 162.23753356933594\n",
      "      vf_explained_var: 0.9527980089187622\n",
      "      vf_loss: 162.2301483154297\n",
      "    sample_time_ms: 23488.513\n",
      "    update_time_ms: 10.0\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18174\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.68964611620325\n",
      "    rl_1: 177.37800827827124\n",
      "  time_since_restore: 8067.320027351379\n",
      "  time_this_iter_s: 23.350070238113403\n",
      "  time_total_s: 8067.320027351379\n",
      "  timestamp: 1552405826\n",
      "  timesteps_since_restore: 2260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2260000\n",
      "  training_iteration: 226\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18174], 8067 s, 226 iter, 2260000 ts, 313 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,  # RL algorithm to run\n",
    "        \"env\": gym_name,  # environment name generated earlier\n",
    "        \"config\": {  # configuration params (must match \"run\" value)\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1000,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow_2)",
   "language": "python",
   "name": "flow_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
