{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING I3W\n",
    "\n",
    "\n",
    "# A) Create Envorinment, Vehicles etc\n",
    "\n",
    "### General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scenarios:\n",
      "['Scenario', 'BayBridgeScenario', 'BayBridgeTollScenario', 'BottleneckScenario', 'Figure8Scenario', 'SimpleGridScenario', 'HighwayScenario', 'LoopScenario', 'MergeScenario', 'TwoLoopsOneMergingScenario', 'MultiLoopScenario', 'IntersectionScenario', 'IntersectionScenarioTW']\n",
      "\n",
      "Available environments:\n",
      "['MultiEnv', 'MultiAgentAccelEnv', 'MultiWaveAttenuationPOEnv', 'MultiAgentIntersectionEnv']\n"
     ]
    }
   ],
   "source": [
    "# Define horizon as a variable to ensure consistent use across notebook (length of one rollout)\n",
    "HORIZON=500\n",
    "\n",
    "# name of the experiment\n",
    "experiment_name = \"IntersectionExample\"\n",
    "\n",
    "# scenario class\n",
    "import flow.scenarios as scenarios\n",
    "print(\"Available scenarios:\")\n",
    "print(scenarios.__all__)\n",
    "scenario_name = \"IntersectionTWScenario\"\n",
    "\n",
    "# environment class\n",
    "import flow.multiagent_envs as flowenvs\n",
    "print(\"\\nAvailable environments:\")\n",
    "print(flowenvs.__all__)\n",
    "env_name = \"MultiAgentIntersectionEnv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "from flow.scenarios.intersection import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "additionalNetParams = {\n",
    "            \"edge_length\": 40,\n",
    "            \"lanes\": 1,\n",
    "            \"speed_limit\": 30\n",
    "        }\n",
    "\n",
    "net_params = NetParams( no_internal_links=False,                  #default: True   !! damit Kreuzungen nicht Ã¼berspr. werden\n",
    "                        inflows=None,                             #default: None\n",
    "                        osm_path=None,                            #default: None\n",
    "                        netfile=None,                             #default: None\n",
    "                        additional_params=additionalNetParams     #default: None   !!\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InitialConfig Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "\n",
    "initial_config = InitialConfig( shuffle=True,                            #default: False         !!\n",
    "                                spacing=\"custom\",                        #default: \"uniform\"     !!\n",
    "                                min_gap=10,                              #default: 0\n",
    "                                perturbation=39.99,                      #default: 0.0            !!        \n",
    "                                x0=0,                                    #default: 0\n",
    "                                bunching=0,                              #default: 0\n",
    "                                lanes_distribution=float(\"inf\"),         #default: float(\"inf\")\n",
    "                                edges_distribution=\"all\",                #default: \"all\"\n",
    "                                additional_params=None )                 #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams( port = None,                  #default: None\n",
    "                          sim_step=0.1,                 #default: 0.1\n",
    "                          emission_path=None,           #default: None\n",
    "                          lateral_resolution=None,      #default: None\n",
    "                          no_step_log=True,             #default: True\n",
    "                          render=False,                 #default: False\n",
    "                          save_render=False,            #default: False\n",
    "                          sight_radius=25,              #default: 25\n",
    "                          show_radius=False,            #default: False\n",
    "                          pxpm=2,                       #default: 2\n",
    "                          overtake_right=False,         #default: False    \n",
    "                          seed=None,                    #default: None\n",
    "                          restart_instance=False,       #default: False\n",
    "                          print_warnings=True,          #default: True\n",
    "                          teleport_time=-1,             #default: -1\n",
    "                          num_clients=1,                #default: 1\n",
    "                          sumo_binary=None )            #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "additionalEnvParams = {\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 3,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 3,\n",
    "        \"target_velocity\": 30\n",
    "    }\n",
    "\n",
    "env_params = EnvParams( additional_params=additionalEnvParams, #default: None    !!\n",
    "                        horizon=HORIZON,                       #default: 500     !!\n",
    "                        warmup_steps=0,                        #default: 0       \n",
    "                        sims_per_step=1,                       #default: 1\n",
    "                        evaluate=False )                       #default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# import vehicles dynamics models\n",
    "#from flow.controllers import SumoCarFollowingController\n",
    "from flow.controllers import ContinuousRouter\n",
    "#from flow.controllers.lane_change_controllers import SumoLaneChangeController\n",
    "from flow.controllers.lane_change_controllers import StaticLaneChanger\n",
    "from flow.controllers import RLController\n",
    "from flow.core.params import SumoLaneChangeParams\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "from random import *\n",
    "\n",
    "vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RL-Agent controlled vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car following parameters, default: None\n",
    "cf_parameter = SumoCarFollowingParams(\n",
    "                speed_mode=\"aggressive\")\n",
    "# lane change parameters, default: None\n",
    "lc_parameter =  None\n",
    "\n",
    "vehicles.add( # name of the vehicle\n",
    "                veh_id = \"rl\",\n",
    "              # acceleration controller, default: (SumoCarFollowingController, {})\n",
    "                acceleration_controller=(RLController, {}),\n",
    "              # lane_change_controller, default: (SumoLaneChangeController, {})\n",
    "                lane_change_controller=(StaticLaneChanger,{}),\n",
    "              # routing controller, default: None\n",
    "                routing_controller=(ContinuousRouter, {}),\n",
    "              # initial speed, default: 0\n",
    "                initial_speed=0,\n",
    "              # number of vehicles, default: 1 \n",
    "                num_vehicles=2,\n",
    "                \n",
    "                car_following_params=cf_parameter\n",
    "              # speed mode, default: \"right_of_way\"\n",
    "                #speed_mode=\"aggressive\",\n",
    "              # lane change mode, default: \"no_lat_collide\"\n",
    "                #lane_change_mode=\"aggressive\", \n",
    "              # car following parameter, default: None\n",
    "                #sumo_car_following_params=cf_parameter,\n",
    "              # lane change parameter, default: None\n",
    "                #sumo_lc_params=lc_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict( # name of the experiment\n",
    "                      exp_tag=experiment_name,\n",
    "                    # name of the flow environment the experiment is running on\n",
    "                      env_name=env_name,\n",
    "                    # name of the scenario class the experiment uses\n",
    "                      scenario=scenario_name,\n",
    "                    # simulator that is used by the experiment\n",
    "                      simulator='traci',\n",
    "                    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "                      sim=sumo_params,\n",
    "                    # environment related parameters (see flow.core.params.EnvParams)\n",
    "                      env=env_params,\n",
    "                    # network-related parameters (see flow.core.params.NetParams and\n",
    "                    # the scenario's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "                      net=net_params,\n",
    "                    # vehicles to be placed in the network at the start of a rollout \n",
    "                    # (see flow.core.vehicles.Vehicles)\n",
    "                      veh=vehicles,\n",
    "                   # (optional) parameters affecting the positioning of vehicles upon \n",
    "                   # initialization/reset (see flow.core.params.InitialConfig)\n",
    "                      initial=initial_config\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-02-22_00-55-48_9744/logs.\n",
      "Waiting for redis server at 127.0.0.1:34548 to respond...\n",
      "Waiting for redis server at 127.0.0.1:26306 to respond...\n",
      "Starting the Plasma object store with 6.554658406 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=29bcce1bdc62dd4c2b2de9bde51d3a1d039d9d2877165009\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.2.102',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-02-22_00-55-48_9744/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-02-22_00-55-48_9744/sockets/raylet'],\n",
       " 'redis_address': '192.168.2.102:34548',\n",
       " 'webui_url': 'http://localhost:8889/notebooks/ray_ui.ipynb?token=29bcce1bdc62dd4c2b2de9bde51d3a1d039d9d2877165009'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "\n",
    "ray.init(redirect_output=True, num_cpus=N_CPUS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [64, 32]})  # size of hidden layers in network\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "#config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "#config[\"sample_batch_size\"] = config[\"train_batch_size\"]/config[\"num_workers\"] # 200 default, trotzdem zu hoch?\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 41527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.60796873449424\n",
      "1.6004638567410308\n"
     ]
    }
   ],
   "source": [
    "# multi agent policy mapping\n",
    "test_env = create_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "\n",
    "def gen_policy():\n",
    "    return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "# Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "policy_graphs = {'rl_0': gen_policy(), 'rl_1': gen_policy()}\n",
    "    \n",
    "def policy_mapping_fn(agent_id):\n",
    "    return agent_id\n",
    "\n",
    "config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn)\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "\n",
      "Created LogSyncer for /home/thorsten/ray_results/IntersectionExample/PPO_MultiAgentIntersectionEnv-v0_0_2019-02-22_00-55-50s2hylffo -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_00-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 452.3333333333333\n",
      "  episode_reward_max: 150.5330812030182\n",
      "  episode_reward_mean: 58.40426735411326\n",
      "  episode_reward_min: -133.99335420565546\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 21\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6746.774\n",
      "    load_time_ms: 195.097\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4168485403060913\n",
      "      kl: 0.0011385699035599828\n",
      "      policy_loss: -0.0016029876423999667\n",
      "      total_loss: 73.2187271118164\n",
      "      vf_explained_var: 0.009847481735050678\n",
      "      vf_loss: 73.2200927734375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4202088117599487\n",
      "      kl: 0.0007787386421114206\n",
      "      policy_loss: -0.0014160407008603215\n",
      "      total_loss: 68.10469818115234\n",
      "      vf_explained_var: 0.008265441283583641\n",
      "      vf_loss: 68.10594940185547\n",
      "    sample_time_ms: 34728.616\n",
      "    update_time_ms: 2419.856\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.234337243949916\n",
      "    rl_1: 26.16993011016337\n",
      "  time_since_restore: 44.26165199279785\n",
      "  time_this_iter_s: 44.26165199279785\n",
      "  time_total_s: 44.26165199279785\n",
      "  timestamp: 1550793446\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 44 s, 1 iter, 10000 ts, 58.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_00-58-03\n",
      "  done: false\n",
      "  episode_len_mean: 424.9347826086956\n",
      "  episode_reward_max: 189.29475415116798\n",
      "  episode_reward_mean: 29.08726909151368\n",
      "  episode_reward_min: -169.91358380346287\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 46\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6459.795\n",
      "    load_time_ms: 99.259\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4179725646972656\n",
      "      kl: 0.0012742679100483656\n",
      "      policy_loss: -0.0012588774552568793\n",
      "      total_loss: 160.71792602539062\n",
      "      vf_explained_var: 0.0478668250143528\n",
      "      vf_loss: 160.7190704345703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4196295738220215\n",
      "      kl: 0.00030120176961645484\n",
      "      policy_loss: -0.00011515302321640775\n",
      "      total_loss: 158.70773315429688\n",
      "      vf_explained_var: 0.05796191096305847\n",
      "      vf_loss: 158.70779418945312\n",
      "    sample_time_ms: 33062.761\n",
      "    update_time_ms: 1216.051\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 17.766820315713694\n",
      "    rl_1: 11.320448775799997\n",
      "  time_since_restore: 81.87131476402283\n",
      "  time_this_iter_s: 37.609662771224976\n",
      "  time_total_s: 81.87131476402283\n",
      "  timestamp: 1550793483\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 81 s, 2 iter, 20000 ts, 29.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_00-58-36\n",
      "  done: false\n",
      "  episode_len_mean: 427.7391304347826\n",
      "  episode_reward_max: 199.93007203177476\n",
      "  episode_reward_mean: 39.51289974763859\n",
      "  episode_reward_min: -169.91358380346287\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 69\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5868.241\n",
      "    load_time_ms: 66.896\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.415455937385559\n",
      "      kl: 0.0030523966997861862\n",
      "      policy_loss: -0.0024017025716602802\n",
      "      total_loss: 81.98223114013672\n",
      "      vf_explained_var: 0.05782698467373848\n",
      "      vf_loss: 81.9844970703125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4215668439865112\n",
      "      kl: 0.004005072638392448\n",
      "      policy_loss: -0.0034191554877907038\n",
      "      total_loss: 78.90750885009766\n",
      "      vf_explained_var: 0.07557586580514908\n",
      "      vf_loss: 78.91073608398438\n",
      "    sample_time_ms: 31454.863\n",
      "    update_time_ms: 816.675\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.42870734488502\n",
      "    rl_1: 17.08419240275358\n",
      "  time_since_restore: 114.83280420303345\n",
      "  time_this_iter_s: 32.96148943901062\n",
      "  time_total_s: 114.83280420303345\n",
      "  timestamp: 1550793516\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 114 s, 3 iter, 30000 ts, 39.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_00-59-09\n",
      "  done: false\n",
      "  episode_len_mean: 426.2391304347826\n",
      "  episode_reward_max: 199.93007203177476\n",
      "  episode_reward_mean: 39.2915624543512\n",
      "  episode_reward_min: -181.00289670748407\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 92\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5573.547\n",
      "    load_time_ms: 50.94\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4296268224716187\n",
      "      kl: 0.004237303975969553\n",
      "      policy_loss: -0.0029525761492550373\n",
      "      total_loss: 97.85600280761719\n",
      "      vf_explained_var: 0.083623006939888\n",
      "      vf_loss: 97.8588638305664\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4214067459106445\n",
      "      kl: 0.0009118381422013044\n",
      "      policy_loss: -0.0006676301709376276\n",
      "      total_loss: 92.7198715209961\n",
      "      vf_explained_var: 0.03976615145802498\n",
      "      vf_loss: 92.72051239013672\n",
      "    sample_time_ms: 30494.015\n",
      "    update_time_ms: 614.724\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.667654920734325\n",
      "    rl_1: 16.62390753361688\n",
      "  time_since_restore: 147.16344833374023\n",
      "  time_this_iter_s: 32.33064413070679\n",
      "  time_total_s: 147.16344833374023\n",
      "  timestamp: 1550793549\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 147 s, 4 iter, 40000 ts, 39.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_00-59-42\n",
      "  done: false\n",
      "  episode_len_mean: 410.83\n",
      "  episode_reward_max: 231.32417470785919\n",
      "  episode_reward_mean: 43.186451504935505\n",
      "  episode_reward_min: -181.00289670748407\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 118\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5356.996\n",
      "    load_time_ms: 41.405\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4217296838760376\n",
      "      kl: 0.0008324080263264477\n",
      "      policy_loss: -0.001443819492124021\n",
      "      total_loss: 90.03878784179688\n",
      "      vf_explained_var: 0.1024768203496933\n",
      "      vf_loss: 90.04022979736328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4223064184188843\n",
      "      kl: 0.008418683893978596\n",
      "      policy_loss: -0.005134430713951588\n",
      "      total_loss: 85.05848693847656\n",
      "      vf_explained_var: 0.054088182747364044\n",
      "      vf_loss: 85.06351470947266\n",
      "    sample_time_ms: 30040.59\n",
      "    update_time_ms: 493.386\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.415675031258296\n",
      "    rl_1: 16.77077647367722\n",
      "  time_since_restore: 179.91173005104065\n",
      "  time_this_iter_s: 32.748281717300415\n",
      "  time_total_s: 179.91173005104065\n",
      "  timestamp: 1550793582\n",
      "  timesteps_since_restore: 50000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 5\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 179 s, 5 iter, 50000 ts, 43.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-00-23\n",
      "  done: false\n",
      "  episode_len_mean: 396.08\n",
      "  episode_reward_max: 231.32417470785919\n",
      "  episode_reward_mean: 54.001281843668195\n",
      "  episode_reward_min: -181.00289670748407\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 147\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5738.233\n",
      "    load_time_ms: 35.091\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4260272979736328\n",
      "      kl: 0.003933774307370186\n",
      "      policy_loss: -0.003032002365216613\n",
      "      total_loss: 168.8543243408203\n",
      "      vf_explained_var: 0.09368833899497986\n",
      "      vf_loss: 168.8573455810547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4162856340408325\n",
      "      kl: 0.0062344977632164955\n",
      "      policy_loss: -0.0023058366496115923\n",
      "      total_loss: 173.1018524169922\n",
      "      vf_explained_var: 0.04708154872059822\n",
      "      vf_loss: 173.10414123535156\n",
      "    sample_time_ms: 30657.799\n",
      "    update_time_ms: 412.455\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.11259105003443\n",
      "    rl_1: 22.88869079363378\n",
      "  time_since_restore: 221.33490896224976\n",
      "  time_this_iter_s: 41.423178911209106\n",
      "  time_total_s: 221.33490896224976\n",
      "  timestamp: 1550793623\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 221 s, 6 iter, 60000 ts, 54 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-01-04\n",
      "  done: false\n",
      "  episode_len_mean: 369.89\n",
      "  episode_reward_max: 231.32417470785919\n",
      "  episode_reward_mean: 43.873145184335016\n",
      "  episode_reward_min: -181.00289670748407\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 175\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5651.471\n",
      "    load_time_ms: 30.584\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4130322933197021\n",
      "      kl: 0.001713162288069725\n",
      "      policy_loss: -0.002215361688286066\n",
      "      total_loss: 128.6913299560547\n",
      "      vf_explained_var: 0.12131721526384354\n",
      "      vf_loss: 128.6935272216797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4279084205627441\n",
      "      kl: 0.0034322701394557953\n",
      "      policy_loss: -0.001248156069777906\n",
      "      total_loss: 127.98997497558594\n",
      "      vf_explained_var: 0.08127842098474503\n",
      "      vf_loss: 127.99119567871094\n",
      "    sample_time_ms: 31443.101\n",
      "    update_time_ms: 355.349\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.559135937787413\n",
      "    rl_1: 19.3140092465476\n",
      "  time_since_restore: 262.65646409988403\n",
      "  time_this_iter_s: 41.32155513763428\n",
      "  time_total_s: 262.65646409988403\n",
      "  timestamp: 1550793664\n",
      "  timesteps_since_restore: 70000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 262 s, 7 iter, 70000 ts, 43.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-01-36\n",
      "  done: false\n",
      "  episode_len_mean: 336.16\n",
      "  episode_reward_max: 231.32417470785919\n",
      "  episode_reward_mean: 47.70685289316109\n",
      "  episode_reward_min: -171.31230152432153\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 208\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5353.054\n",
      "    load_time_ms: 27.07\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4054354429244995\n",
      "      kl: 0.0036845277063548565\n",
      "      policy_loss: -0.0029182080179452896\n",
      "      total_loss: 152.62196350097656\n",
      "      vf_explained_var: 0.14162476360797882\n",
      "      vf_loss: 152.62489318847656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4196555614471436\n",
      "      kl: 0.0008096161764115095\n",
      "      policy_loss: -0.0010015573352575302\n",
      "      total_loss: 142.29664611816406\n",
      "      vf_explained_var: 0.04941597208380699\n",
      "      vf_loss: 142.29766845703125\n",
      "    sample_time_ms: 31098.264\n",
      "    update_time_ms: 312.159\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.00263268650066\n",
      "    rl_1: 21.70422020666043\n",
      "  time_since_restore: 294.633585691452\n",
      "  time_this_iter_s: 31.977121591567993\n",
      "  time_total_s: 294.633585691452\n",
      "  timestamp: 1550793696\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 294 s, 8 iter, 80000 ts, 47.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-02-00\n",
      "  done: false\n",
      "  episode_len_mean: 301.62\n",
      "  episode_reward_max: 219.9538807136981\n",
      "  episode_reward_mean: 57.59262638546382\n",
      "  episode_reward_min: -178.0155407905042\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 247\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5120.068\n",
      "    load_time_ms: 24.354\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4060819149017334\n",
      "      kl: 0.0008307425887323916\n",
      "      policy_loss: -0.0002370354486629367\n",
      "      total_loss: 228.31781005859375\n",
      "      vf_explained_var: 0.15499037504196167\n",
      "      vf_loss: 228.31805419921875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4116528034210205\n",
      "      kl: 0.0017166989855468273\n",
      "      policy_loss: -0.0013404317433014512\n",
      "      total_loss: 200.22271728515625\n",
      "      vf_explained_var: 0.12019186466932297\n",
      "      vf_loss: 200.22406005859375\n",
      "    sample_time_ms: 29871.784\n",
      "    update_time_ms: 278.117\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.29154306698837\n",
      "    rl_1: 24.30108331847547\n",
      "  time_since_restore: 317.9751889705658\n",
      "  time_this_iter_s: 23.34160327911377\n",
      "  time_total_s: 317.9751889705658\n",
      "  timestamp: 1550793720\n",
      "  timesteps_since_restore: 90000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 9\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 317 s, 9 iter, 90000 ts, 57.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-02-23\n",
      "  done: false\n",
      "  episode_len_mean: 278.2\n",
      "  episode_reward_max: 228.1251415289585\n",
      "  episode_reward_mean: 59.67285583145381\n",
      "  episode_reward_min: -178.0155407905042\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 282\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4935.097\n",
      "    load_time_ms: 22.135\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4096665382385254\n",
      "      kl: 0.0052779545076191425\n",
      "      policy_loss: -0.0030526025220751762\n",
      "      total_loss: 217.42823791503906\n",
      "      vf_explained_var: 0.16093553602695465\n",
      "      vf_loss: 217.4313201904297\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.40286123752594\n",
      "      kl: 0.004117416217923164\n",
      "      policy_loss: -0.0018678322667255998\n",
      "      total_loss: 212.96206665039062\n",
      "      vf_explained_var: 0.13692748546600342\n",
      "      vf_loss: 212.9639129638672\n",
      "    sample_time_ms: 28893.847\n",
      "    update_time_ms: 251.257\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.20288345067868\n",
      "    rl_1: 24.469972380775122\n",
      "  time_since_restore: 341.36557030677795\n",
      "  time_this_iter_s: 23.390381336212158\n",
      "  time_total_s: 341.36557030677795\n",
      "  timestamp: 1550793743\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 341 s, 10 iter, 100000 ts, 59.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-02-47\n",
      "  done: false\n",
      "  episode_len_mean: 268.45\n",
      "  episode_reward_max: 228.1251415289585\n",
      "  episode_reward_mean: 34.810682972245075\n",
      "  episode_reward_min: -184.39664256481893\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 321\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4587.955\n",
      "    load_time_ms: 3.022\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4205504655838013\n",
      "      kl: 0.001482954598031938\n",
      "      policy_loss: -0.0013094773748889565\n",
      "      total_loss: 266.73162841796875\n",
      "      vf_explained_var: 0.20055542886257172\n",
      "      vf_loss: 266.73291015625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.416891098022461\n",
      "      kl: 0.0016574969049543142\n",
      "      policy_loss: -0.0008563926676288247\n",
      "      total_loss: 274.91351318359375\n",
      "      vf_explained_var: 0.1542540043592453\n",
      "      vf_loss: 274.9143981933594\n",
      "    sample_time_ms: 27486.519\n",
      "    update_time_ms: 10.153\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 20.00468717079743\n",
      "    rl_1: 14.805995801447636\n",
      "  time_since_restore: 365.3247277736664\n",
      "  time_this_iter_s: 23.959157466888428\n",
      "  time_total_s: 365.3247277736664\n",
      "  timestamp: 1550793767\n",
      "  timesteps_since_restore: 110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 11\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 365 s, 11 iter, 110000 ts, 34.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-03-11\n",
      "  done: false\n",
      "  episode_len_mean: 269.88\n",
      "  episode_reward_max: 228.1251415289585\n",
      "  episode_reward_mean: 31.91733158568541\n",
      "  episode_reward_min: -184.39664256481893\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 359\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4297.651\n",
      "    load_time_ms: 2.935\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.412753939628601\n",
      "      kl: 0.005149408243596554\n",
      "      policy_loss: -0.002080129226669669\n",
      "      total_loss: 258.5086975097656\n",
      "      vf_explained_var: 0.27719011902809143\n",
      "      vf_loss: 258.5107727050781\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.405474305152893\n",
      "      kl: 0.007528925780206919\n",
      "      policy_loss: -0.004171780310571194\n",
      "      total_loss: 242.56723022460938\n",
      "      vf_explained_var: 0.2222452610731125\n",
      "      vf_loss: 242.57144165039062\n",
      "    sample_time_ms: 26347.293\n",
      "    update_time_ms: 9.547\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 17.911697397689455\n",
      "    rl_1: 14.005634187995947\n",
      "  time_since_restore: 388.6253995895386\n",
      "  time_this_iter_s: 23.300671815872192\n",
      "  time_total_s: 388.6253995895386\n",
      "  timestamp: 1550793791\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 388 s, 12 iter, 120000 ts, 31.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-03-37\n",
      "  done: false\n",
      "  episode_len_mean: 268.35\n",
      "  episode_reward_max: 219.6967663431272\n",
      "  episode_reward_mean: 36.2862184260073\n",
      "  episode_reward_min: -184.39664256481893\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 396\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4306.325\n",
      "    load_time_ms: 3.069\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4163882732391357\n",
      "      kl: 0.003945964854210615\n",
      "      policy_loss: -0.0025166207924485207\n",
      "      total_loss: 212.69293212890625\n",
      "      vf_explained_var: 0.254625529050827\n",
      "      vf_loss: 212.69546508789062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4147900342941284\n",
      "      kl: 0.003348921425640583\n",
      "      policy_loss: -0.0004368099616840482\n",
      "      total_loss: 207.55450439453125\n",
      "      vf_explained_var: 0.25273942947387695\n",
      "      vf_loss: 207.55494689941406\n",
      "    sample_time_ms: 25693.842\n",
      "    update_time_ms: 8.401\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 20.19767488192382\n",
      "    rl_1: 16.08854354408348\n",
      "  time_since_restore: 415.1341211795807\n",
      "  time_this_iter_s: 26.508721590042114\n",
      "  time_total_s: 415.1341211795807\n",
      "  timestamp: 1550793817\n",
      "  timesteps_since_restore: 130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 13\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 415 s, 13 iter, 130000 ts, 36.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-04-04\n",
      "  done: false\n",
      "  episode_len_mean: 248.98\n",
      "  episode_reward_max: 219.6967663431272\n",
      "  episode_reward_mean: 34.521218041187105\n",
      "  episode_reward_min: -176.2290956596838\n",
      "  episodes_this_iter: 45\n",
      "  episodes_total: 441\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4178.759\n",
      "    load_time_ms: 2.975\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4102380275726318\n",
      "      kl: 0.001639257650822401\n",
      "      policy_loss: -0.001041919575072825\n",
      "      total_loss: 290.7196044921875\n",
      "      vf_explained_var: 0.2995088994503021\n",
      "      vf_loss: 290.72064208984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3995229005813599\n",
      "      kl: 0.0016907292883843184\n",
      "      policy_loss: -0.0021091161761432886\n",
      "      total_loss: 309.4118347167969\n",
      "      vf_explained_var: 0.23845714330673218\n",
      "      vf_loss: 309.4139404296875\n",
      "    sample_time_ms: 25290.485\n",
      "    update_time_ms: 8.213\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.41281034435453\n",
      "    rl_1: 18.10840769683258\n",
      "  time_since_restore: 442.15041160583496\n",
      "  time_this_iter_s: 27.016290426254272\n",
      "  time_total_s: 442.15041160583496\n",
      "  timestamp: 1550793844\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 14\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 442 s, 14 iter, 140000 ts, 34.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-04-34\n",
      "  done: false\n",
      "  episode_len_mean: 230.33\n",
      "  episode_reward_max: 210.0741878541329\n",
      "  episode_reward_mean: 26.773882147799007\n",
      "  episode_reward_min: -181.22632574289983\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 482\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4156.522\n",
      "    load_time_ms: 3.024\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4177895784378052\n",
      "      kl: 0.001920485869050026\n",
      "      policy_loss: -0.0008132904767990112\n",
      "      total_loss: 224.077392578125\n",
      "      vf_explained_var: 0.35439568758010864\n",
      "      vf_loss: 224.07823181152344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4161707162857056\n",
      "      kl: 0.003102703485637903\n",
      "      policy_loss: -0.0011978504480794072\n",
      "      total_loss: 253.4393310546875\n",
      "      vf_explained_var: 0.3154488205909729\n",
      "      vf_loss: 253.44053649902344\n",
      "    sample_time_ms: 24981.64\n",
      "    update_time_ms: 8.103\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 11.396872334775495\n",
      "    rl_1: 15.3770098130235\n",
      "  time_since_restore: 471.5862145423889\n",
      "  time_this_iter_s: 29.435802936553955\n",
      "  time_total_s: 471.5862145423889\n",
      "  timestamp: 1550793874\n",
      "  timesteps_since_restore: 150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 15\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 471 s, 15 iter, 150000 ts, 26.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-05-10\n",
      "  done: false\n",
      "  episode_len_mean: 235.36\n",
      "  episode_reward_max: 210.0741878541329\n",
      "  episode_reward_mean: 58.887211106305976\n",
      "  episode_reward_min: -181.22632574289983\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 524\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3832.453\n",
      "    load_time_ms: 3.041\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.402223825454712\n",
      "      kl: 0.0038229739293456078\n",
      "      policy_loss: -0.0014149517519399524\n",
      "      total_loss: 155.85198974609375\n",
      "      vf_explained_var: 0.4538276791572571\n",
      "      vf_loss: 155.85342407226562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3922357559204102\n",
      "      kl: 0.004908544011414051\n",
      "      policy_loss: -0.0015434760134667158\n",
      "      total_loss: 204.57022094726562\n",
      "      vf_explained_var: 0.4132053256034851\n",
      "      vf_loss: 204.57177734375\n",
      "    sample_time_ms: 24760.255\n",
      "    update_time_ms: 8.521\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.812039560609882\n",
      "    rl_1: 33.0751715456961\n",
      "  time_since_restore: 507.55558919906616\n",
      "  time_this_iter_s: 35.969374656677246\n",
      "  time_total_s: 507.55558919906616\n",
      "  timestamp: 1550793910\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 16\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 507 s, 16 iter, 160000 ts, 58.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-05-48\n",
      "  done: false\n",
      "  episode_len_mean: 226.17\n",
      "  episode_reward_max: 217.49812010459658\n",
      "  episode_reward_mean: 72.58178667152525\n",
      "  episode_reward_min: -184.13581415202782\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 570\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3803.391\n",
      "    load_time_ms: 2.938\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4006986618041992\n",
      "      kl: 0.0019083437509834766\n",
      "      policy_loss: -0.000536898267455399\n",
      "      total_loss: 222.04782104492188\n",
      "      vf_explained_var: 0.31642118096351624\n",
      "      vf_loss: 222.04835510253906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.400319218635559\n",
      "      kl: 0.0034322435967624187\n",
      "      policy_loss: -0.0019162235548719764\n",
      "      total_loss: 262.9438781738281\n",
      "      vf_explained_var: 0.32131242752075195\n",
      "      vf_loss: 262.94580078125\n",
      "    sample_time_ms: 24470.554\n",
      "    update_time_ms: 8.219\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.282827050706338\n",
      "    rl_1: 42.29895962081891\n",
      "  time_since_restore: 545.6843812465668\n",
      "  time_this_iter_s: 38.12879204750061\n",
      "  time_total_s: 545.6843812465668\n",
      "  timestamp: 1550793948\n",
      "  timesteps_since_restore: 170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 17\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 545 s, 17 iter, 170000 ts, 72.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-06-14\n",
      "  done: false\n",
      "  episode_len_mean: 225.07\n",
      "  episode_reward_max: 220.63113239757294\n",
      "  episode_reward_mean: 71.37915841444511\n",
      "  episode_reward_min: -184.13581415202782\n",
      "  episodes_this_iter: 47\n",
      "  episodes_total: 617\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3810.464\n",
      "    load_time_ms: 2.97\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4031394720077515\n",
      "      kl: 0.0012349667958915234\n",
      "      policy_loss: -0.0004399031458888203\n",
      "      total_loss: 193.69618225097656\n",
      "      vf_explained_var: 0.43399056792259216\n",
      "      vf_loss: 193.6966094970703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3945691585540771\n",
      "      kl: 0.0011826507980003953\n",
      "      policy_loss: -0.0004238239780534059\n",
      "      total_loss: 276.96820068359375\n",
      "      vf_explained_var: 0.3187512755393982\n",
      "      vf_loss: 276.9686584472656\n",
      "    sample_time_ms: 23905.257\n",
      "    update_time_ms: 8.099\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.315940622928323\n",
      "    rl_1: 46.063217791516784\n",
      "  time_since_restore: 572.0805444717407\n",
      "  time_this_iter_s: 26.39616322517395\n",
      "  time_total_s: 572.0805444717407\n",
      "  timestamp: 1550793974\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 18\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 572 s, 18 iter, 180000 ts, 71.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-06-39\n",
      "  done: false\n",
      "  episode_len_mean: 209.99\n",
      "  episode_reward_max: 220.63113239757294\n",
      "  episode_reward_mean: 82.96281922498123\n",
      "  episode_reward_min: -172.00856654670088\n",
      "  episodes_this_iter: 48\n",
      "  episodes_total: 665\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3831.923\n",
      "    load_time_ms: 2.974\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4032928943634033\n",
      "      kl: 0.0024127585347741842\n",
      "      policy_loss: -0.00188501738011837\n",
      "      total_loss: 186.85653686523438\n",
      "      vf_explained_var: 0.41528379917144775\n",
      "      vf_loss: 186.85842895507812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3859598636627197\n",
      "      kl: 0.007328852079808712\n",
      "      policy_loss: -0.004154646303504705\n",
      "      total_loss: 267.99920654296875\n",
      "      vf_explained_var: 0.2966386377811432\n",
      "      vf_loss: 268.0033874511719\n",
      "    sample_time_ms: 23968.996\n",
      "    update_time_ms: 8.154\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.033395484321076\n",
      "    rl_1: 56.92942374066015\n",
      "  time_since_restore: 596.2769079208374\n",
      "  time_this_iter_s: 24.19636344909668\n",
      "  time_total_s: 596.2769079208374\n",
      "  timestamp: 1550793999\n",
      "  timesteps_since_restore: 190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 19\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 596 s, 19 iter, 190000 ts, 83 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-07-02\n",
      "  done: false\n",
      "  episode_len_mean: 192.6\n",
      "  episode_reward_max: 217.9487933486108\n",
      "  episode_reward_mean: 71.53229679893875\n",
      "  episode_reward_min: -182.99226451535193\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 720\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3829.584\n",
      "    load_time_ms: 2.998\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4036473035812378\n",
      "      kl: 0.002852262929081917\n",
      "      policy_loss: -0.0009723320254124701\n",
      "      total_loss: 245.73390197753906\n",
      "      vf_explained_var: 0.4246518015861511\n",
      "      vf_loss: 245.73487854003906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3735417127609253\n",
      "      kl: 0.006354155950248241\n",
      "      policy_loss: -0.0031226377468556166\n",
      "      total_loss: 359.6773376464844\n",
      "      vf_explained_var: 0.325657457113266\n",
      "      vf_loss: 359.6804504394531\n",
      "    sample_time_ms: 23953.962\n",
      "    update_time_ms: 8.202\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 18.170878763158026\n",
      "    rl_1: 53.36141803578072\n",
      "  time_since_restore: 619.4970598220825\n",
      "  time_this_iter_s: 23.220151901245117\n",
      "  time_total_s: 619.4970598220825\n",
      "  timestamp: 1550794022\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 20\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 619 s, 20 iter, 200000 ts, 71.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-07-26\n",
      "  done: false\n",
      "  episode_len_mean: 175.75\n",
      "  episode_reward_max: 208.0423331991694\n",
      "  episode_reward_mean: 69.27523930385047\n",
      "  episode_reward_min: -182.99226451535193\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 779\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3826.36\n",
      "    load_time_ms: 2.854\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3936971426010132\n",
      "      kl: 0.009388383477926254\n",
      "      policy_loss: -0.005258150864392519\n",
      "      total_loss: 261.6445617675781\n",
      "      vf_explained_var: 0.45188215374946594\n",
      "      vf_loss: 261.6498107910156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3849554061889648\n",
      "      kl: 0.002413362031802535\n",
      "      policy_loss: -0.001309854444116354\n",
      "      total_loss: 368.2991943359375\n",
      "      vf_explained_var: 0.34852275252342224\n",
      "      vf_loss: 368.30047607421875\n",
      "    sample_time_ms: 23930.6\n",
      "    update_time_ms: 8.383\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.72305535106111\n",
      "    rl_1: 52.55218395278936\n",
      "  time_since_restore: 643.192174911499\n",
      "  time_this_iter_s: 23.695115089416504\n",
      "  time_total_s: 643.192174911499\n",
      "  timestamp: 1550794046\n",
      "  timesteps_since_restore: 210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 21\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 643 s, 21 iter, 210000 ts, 69.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-07-49\n",
      "  done: false\n",
      "  episode_len_mean: 159.79\n",
      "  episode_reward_max: 209.53657625667498\n",
      "  episode_reward_mean: 69.94653416769786\n",
      "  episode_reward_min: -178.52161321381521\n",
      "  episodes_this_iter: 63\n",
      "  episodes_total: 842\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3823.901\n",
      "    load_time_ms: 2.82\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3739087581634521\n",
      "      kl: 0.0029555857181549072\n",
      "      policy_loss: -0.0013049324043095112\n",
      "      total_loss: 313.15966796875\n",
      "      vf_explained_var: 0.4168943762779236\n",
      "      vf_loss: 313.1610107421875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3620940446853638\n",
      "      kl: 0.006194975692778826\n",
      "      policy_loss: -0.0025205512065440416\n",
      "      total_loss: 413.4086608886719\n",
      "      vf_explained_var: 0.3248295485973358\n",
      "      vf_loss: 413.41119384765625\n",
      "    sample_time_ms: 23971.311\n",
      "    update_time_ms: 8.591\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 20.88264911739368\n",
      "    rl_1: 49.06388505030416\n",
      "  time_since_restore: 666.8759875297546\n",
      "  time_this_iter_s: 23.683812618255615\n",
      "  time_total_s: 666.8759875297546\n",
      "  timestamp: 1550794069\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 22\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 666 s, 22 iter, 220000 ts, 69.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-08-13\n",
      "  done: false\n",
      "  episode_len_mean: 161.32\n",
      "  episode_reward_max: 209.53657625667498\n",
      "  episode_reward_mean: 58.12679839671625\n",
      "  episode_reward_min: -170.22020709939184\n",
      "  episodes_this_iter: 62\n",
      "  episodes_total: 904\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3669.611\n",
      "    load_time_ms: 2.679\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3833328485488892\n",
      "      kl: 0.0067842318676412106\n",
      "      policy_loss: -0.0026393570005893707\n",
      "      total_loss: 321.1412658691406\n",
      "      vf_explained_var: 0.40546292066574097\n",
      "      vf_loss: 321.1439208984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3692889213562012\n",
      "      kl: 0.00441921828314662\n",
      "      policy_loss: -0.0018857044633477926\n",
      "      total_loss: 435.38751220703125\n",
      "      vf_explained_var: 0.3509141206741333\n",
      "      vf_loss: 435.3894348144531\n",
      "    sample_time_ms: 23842.242\n",
      "    update_time_ms: 9.058\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 17.122545065884832\n",
      "    rl_1: 41.00425333083138\n",
      "  time_since_restore: 690.5494706630707\n",
      "  time_this_iter_s: 23.67348313331604\n",
      "  time_total_s: 690.5494706630707\n",
      "  timestamp: 1550794093\n",
      "  timesteps_since_restore: 230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 23\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 690 s, 23 iter, 230000 ts, 58.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-08-37\n",
      "  done: false\n",
      "  episode_len_mean: 162.01\n",
      "  episode_reward_max: 204.29304536709324\n",
      "  episode_reward_mean: 57.439311291596226\n",
      "  episode_reward_min: -170.22020709939184\n",
      "  episodes_this_iter: 64\n",
      "  episodes_total: 968\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3650.614\n",
      "    load_time_ms: 2.762\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.378320336341858\n",
      "      kl: 0.0038495485205203295\n",
      "      policy_loss: -0.0020201411098241806\n",
      "      total_loss: 295.5143127441406\n",
      "      vf_explained_var: 0.5304902195930481\n",
      "      vf_loss: 295.5163879394531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3558984994888306\n",
      "      kl: 0.005367820616811514\n",
      "      policy_loss: -0.0023130818735808134\n",
      "      total_loss: 402.1136169433594\n",
      "      vf_explained_var: 0.38884496688842773\n",
      "      vf_loss: 402.11590576171875\n",
      "    sample_time_ms: 23571.459\n",
      "    update_time_ms: 8.946\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 15.147076299003267\n",
      "    rl_1: 42.29223499259297\n",
      "  time_since_restore: 714.6699976921082\n",
      "  time_this_iter_s: 24.120527029037476\n",
      "  time_total_s: 714.6699976921082\n",
      "  timestamp: 1550794117\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 24\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 714 s, 24 iter, 240000 ts, 57.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-09-01\n",
      "  done: false\n",
      "  episode_len_mean: 142.62\n",
      "  episode_reward_max: 191.06416004405324\n",
      "  episode_reward_mean: 45.65777919020197\n",
      "  episode_reward_min: -177.4451894578212\n",
      "  episodes_this_iter: 70\n",
      "  episodes_total: 1038\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3546.303\n",
      "    load_time_ms: 2.607\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.352170705795288\n",
      "      kl: 0.00601216359063983\n",
      "      policy_loss: -0.002343680476769805\n",
      "      total_loss: 339.66192626953125\n",
      "      vf_explained_var: 0.49769774079322815\n",
      "      vf_loss: 339.66424560546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3486379384994507\n",
      "      kl: 0.005852893926203251\n",
      "      policy_loss: -0.002037725178524852\n",
      "      total_loss: 446.468505859375\n",
      "      vf_explained_var: 0.3825976550579071\n",
      "      vf_loss: 446.4704895019531\n",
      "    sample_time_ms: 23110.329\n",
      "    update_time_ms: 9.09\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 5.848214040182532\n",
      "    rl_1: 39.809565150019445\n",
      "  time_since_restore: 738.4506556987762\n",
      "  time_this_iter_s: 23.78065800666809\n",
      "  time_total_s: 738.4506556987762\n",
      "  timestamp: 1550794141\n",
      "  timesteps_since_restore: 250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 25\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 738 s, 25 iter, 250000 ts, 45.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-09-24\n",
      "  done: false\n",
      "  episode_len_mean: 144.26\n",
      "  episode_reward_max: 183.6642566723537\n",
      "  episode_reward_mean: 57.37501984632955\n",
      "  episode_reward_min: -181.22663192635196\n",
      "  episodes_this_iter: 70\n",
      "  episodes_total: 1108\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3428.916\n",
      "    load_time_ms: 2.507\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3532254695892334\n",
      "      kl: 0.00483339000493288\n",
      "      policy_loss: -0.0019648922607302666\n",
      "      total_loss: 245.6828155517578\n",
      "      vf_explained_var: 0.5570233464241028\n",
      "      vf_loss: 245.6847686767578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3556658029556274\n",
      "      kl: 0.00545327877625823\n",
      "      policy_loss: -0.0021449232008308172\n",
      "      total_loss: 398.0074768066406\n",
      "      vf_explained_var: 0.5371577739715576\n",
      "      vf_loss: 398.0096130371094\n",
      "    sample_time_ms: 21955.162\n",
      "    update_time_ms: 8.526\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 6.880109910111856\n",
      "    rl_1: 50.4949099362177\n",
      "  time_since_restore: 761.6878113746643\n",
      "  time_this_iter_s: 23.23715567588806\n",
      "  time_total_s: 761.6878113746643\n",
      "  timestamp: 1550794164\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 26\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 761 s, 26 iter, 260000 ts, 57.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-09-48\n",
      "  done: false\n",
      "  episode_len_mean: 144.73\n",
      "  episode_reward_max: 192.35418557638408\n",
      "  episode_reward_mean: 88.94849114241099\n",
      "  episode_reward_min: -180.4168643007975\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 1176\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3269.469\n",
      "    load_time_ms: 2.476\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9802322831784522e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3305867910385132\n",
      "      kl: 0.004884316120296717\n",
      "      policy_loss: -0.0020295109134167433\n",
      "      total_loss: 200.44293212890625\n",
      "      vf_explained_var: 0.615527868270874\n",
      "      vf_loss: 200.4449462890625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9802322831784522e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3347738981246948\n",
      "      kl: 0.007783517707139254\n",
      "      policy_loss: -0.004323374480009079\n",
      "      total_loss: 354.7154846191406\n",
      "      vf_explained_var: 0.4251863956451416\n",
      "      vf_loss: 354.7197570800781\n",
      "    sample_time_ms: 20641.468\n",
      "    update_time_ms: 8.23\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.058686915299482\n",
      "    rl_1: 64.8898042271115\n",
      "  time_since_restore: 785.0813159942627\n",
      "  time_this_iter_s: 23.39350461959839\n",
      "  time_total_s: 785.0813159942627\n",
      "  timestamp: 1550794188\n",
      "  timesteps_since_restore: 270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 27\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 785 s, 27 iter, 270000 ts, 88.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-10-12\n",
      "  done: false\n",
      "  episode_len_mean: 155.51\n",
      "  episode_reward_max: 193.28701664284657\n",
      "  episode_reward_mean: 79.75172771972171\n",
      "  episode_reward_min: -166.44195493595137\n",
      "  episodes_this_iter: 63\n",
      "  episodes_total: 1239\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3260.166\n",
      "    load_time_ms: 2.425\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3668005466461182\n",
      "      kl: 0.0015710355946794152\n",
      "      policy_loss: -0.0015704595716670156\n",
      "      total_loss: 258.66400146484375\n",
      "      vf_explained_var: 0.5493372678756714\n",
      "      vf_loss: 258.66552734375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3715265989303589\n",
      "      kl: 0.003529368666931987\n",
      "      policy_loss: -0.0018327004509046674\n",
      "      total_loss: 389.90606689453125\n",
      "      vf_explained_var: 0.6131587624549866\n",
      "      vf_loss: 389.9079284667969\n",
      "    sample_time_ms: 20390.385\n",
      "    update_time_ms: 8.021\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.199073621835847\n",
      "    rl_1: 56.55265409788586\n",
      "  time_since_restore: 808.8683013916016\n",
      "  time_this_iter_s: 23.786985397338867\n",
      "  time_total_s: 808.8683013916016\n",
      "  timestamp: 1550794212\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 28\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 808 s, 28 iter, 280000 ts, 79.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-10-35\n",
      "  done: false\n",
      "  episode_len_mean: 144.04\n",
      "  episode_reward_max: 190.24809128668937\n",
      "  episode_reward_mean: 83.51200966961058\n",
      "  episode_reward_min: -171.46730875455376\n",
      "  episodes_this_iter: 71\n",
      "  episodes_total: 1310\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3236.414\n",
      "    load_time_ms: 2.463\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3511013984680176\n",
      "      kl: 0.0053264182060956955\n",
      "      policy_loss: -0.002770098624750972\n",
      "      total_loss: 221.3379669189453\n",
      "      vf_explained_var: 0.6028544306755066\n",
      "      vf_loss: 221.3407440185547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3157110214233398\n",
      "      kl: 0.0026855638716369867\n",
      "      policy_loss: -0.001565483515150845\n",
      "      total_loss: 397.82244873046875\n",
      "      vf_explained_var: 0.4761660695075989\n",
      "      vf_loss: 397.8240051269531\n",
      "    sample_time_ms: 20368.253\n",
      "    update_time_ms: 8.042\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 21.69875566428543\n",
      "    rl_1: 61.81325400532517\n",
      "  time_since_restore: 832.6062688827515\n",
      "  time_this_iter_s: 23.737967491149902\n",
      "  time_total_s: 832.6062688827515\n",
      "  timestamp: 1550794235\n",
      "  timesteps_since_restore: 290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 29\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 832 s, 29 iter, 290000 ts, 83.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-11-00\n",
      "  done: false\n",
      "  episode_len_mean: 126.46\n",
      "  episode_reward_max: 196.00777381878666\n",
      "  episode_reward_mean: 88.30811412007628\n",
      "  episode_reward_min: -171.46730875455376\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 1390\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3244.299\n",
      "    load_time_ms: 2.498\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.309882402420044\n",
      "      kl: 0.006305908318608999\n",
      "      policy_loss: -0.0023808313999325037\n",
      "      total_loss: 269.25146484375\n",
      "      vf_explained_var: 0.5385839343070984\n",
      "      vf_loss: 269.25384521484375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2946950197219849\n",
      "      kl: 0.002150139771401882\n",
      "      policy_loss: -0.0014516330556944013\n",
      "      total_loss: 437.0401611328125\n",
      "      vf_explained_var: 0.4803895652294159\n",
      "      vf_loss: 437.0416259765625\n",
      "    sample_time_ms: 20461.642\n",
      "    update_time_ms: 7.695\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 20.9716235442838\n",
      "    rl_1: 67.33649057579245\n",
      "  time_since_restore: 856.8369853496552\n",
      "  time_this_iter_s: 24.230716466903687\n",
      "  time_total_s: 856.8369853496552\n",
      "  timestamp: 1550794260\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 30\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 856 s, 30 iter, 300000 ts, 88.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-11-24\n",
      "  done: false\n",
      "  episode_len_mean: 132.01\n",
      "  episode_reward_max: 199.33888191293758\n",
      "  episode_reward_mean: 71.63164500495\n",
      "  episode_reward_min: -170.39950448278213\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 1463\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3244.229\n",
      "    load_time_ms: 2.469\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3327035903930664\n",
      "      kl: 0.007682043127715588\n",
      "      policy_loss: -0.002656240016222\n",
      "      total_loss: 273.9869384765625\n",
      "      vf_explained_var: 0.48859772086143494\n",
      "      vf_loss: 273.9895935058594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3194435834884644\n",
      "      kl: 0.00719975121319294\n",
      "      policy_loss: -0.003288170089945197\n",
      "      total_loss: 449.15673828125\n",
      "      vf_explained_var: 0.5211272239685059\n",
      "      vf_loss: 449.1600646972656\n",
      "    sample_time_ms: 20482.651\n",
      "    update_time_ms: 7.362\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.73495401289789\n",
      "    rl_1: 54.896690992052115\n",
      "  time_since_restore: 880.7359509468079\n",
      "  time_this_iter_s: 23.89896559715271\n",
      "  time_total_s: 880.7359509468079\n",
      "  timestamp: 1550794284\n",
      "  timesteps_since_restore: 310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 31\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 880 s, 31 iter, 310000 ts, 71.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-11-48\n",
      "  done: false\n",
      "  episode_len_mean: 140.71\n",
      "  episode_reward_max: 201.01357952960984\n",
      "  episode_reward_mean: 88.30701885144083\n",
      "  episode_reward_min: -175.1024468418546\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 1538\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3246.391\n",
      "    load_time_ms: 2.473\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.313225884932663e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3300870656967163\n",
      "      kl: 0.002614332363009453\n",
      "      policy_loss: -0.001151957898400724\n",
      "      total_loss: 229.0818634033203\n",
      "      vf_explained_var: 0.5483222603797913\n",
      "      vf_loss: 229.08297729492188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.313225884932663e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3166921138763428\n",
      "      kl: 0.0016886508092284203\n",
      "      policy_loss: -0.0004812668194063008\n",
      "      total_loss: 413.9159851074219\n",
      "      vf_explained_var: 0.5289540886878967\n",
      "      vf_loss: 413.9164733886719\n",
      "    sample_time_ms: 20538.079\n",
      "    update_time_ms: 7.228\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.964658369482255\n",
      "    rl_1: 65.34236048195858\n",
      "  time_since_restore: 904.9961783885956\n",
      "  time_this_iter_s: 24.26022744178772\n",
      "  time_total_s: 904.9961783885956\n",
      "  timestamp: 1550794308\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 32\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 904 s, 32 iter, 320000 ts, 88.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-12-11\n",
      "  done: false\n",
      "  episode_len_mean: 129.11\n",
      "  episode_reward_max: 199.93251661343714\n",
      "  episode_reward_mean: 95.44403620832905\n",
      "  episode_reward_min: -179.8151195761942\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 1617\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3245.485\n",
      "    load_time_ms: 2.489\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3224976062774658\n",
      "      kl: 0.0029424154199659824\n",
      "      policy_loss: -0.0013923260848969221\n",
      "      total_loss: 208.1827850341797\n",
      "      vf_explained_var: 0.5967510938644409\n",
      "      vf_loss: 208.18418884277344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2542426586151123\n",
      "      kl: 0.014890323393046856\n",
      "      policy_loss: -0.004463952034711838\n",
      "      total_loss: 402.54913330078125\n",
      "      vf_explained_var: 0.4503059387207031\n",
      "      vf_loss: 402.5535888671875\n",
      "    sample_time_ms: 20520.769\n",
      "    update_time_ms: 6.798\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.909747590929584\n",
      "    rl_1: 72.53428861739947\n",
      "  time_since_restore: 928.4853191375732\n",
      "  time_this_iter_s: 23.48914074897766\n",
      "  time_total_s: 928.4853191375732\n",
      "  timestamp: 1550794331\n",
      "  timesteps_since_restore: 330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 33\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 928 s, 33 iter, 330000 ts, 95.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-12-35\n",
      "  done: false\n",
      "  episode_len_mean: 124.38\n",
      "  episode_reward_max: 201.6443843154352\n",
      "  episode_reward_mean: 78.79692345935698\n",
      "  episode_reward_min: -169.96519577535992\n",
      "  episodes_this_iter: 82\n",
      "  episodes_total: 1699\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3248.81\n",
      "    load_time_ms: 2.455\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.315018653869629\n",
      "      kl: 0.006983825005590916\n",
      "      policy_loss: -0.003553252201527357\n",
      "      total_loss: 306.18365478515625\n",
      "      vf_explained_var: 0.5451451539993286\n",
      "      vf_loss: 306.18719482421875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.266806721687317\n",
      "      kl: 0.003327337559312582\n",
      "      policy_loss: -0.002648755442351103\n",
      "      total_loss: 502.2663269042969\n",
      "      vf_explained_var: 0.514347493648529\n",
      "      vf_loss: 502.26898193359375\n",
      "    sample_time_ms: 20497.006\n",
      "    update_time_ms: 6.867\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.02756771049498\n",
      "    rl_1: 62.76935574886199\n",
      "  time_since_restore: 952.4029059410095\n",
      "  time_this_iter_s: 23.91758680343628\n",
      "  time_total_s: 952.4029059410095\n",
      "  timestamp: 1550794355\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 34\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 952 s, 34 iter, 340000 ts, 78.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-13-00\n",
      "  done: false\n",
      "  episode_len_mean: 133.04\n",
      "  episode_reward_max: 191.34156187989777\n",
      "  episode_reward_mean: 92.7033461381825\n",
      "  episode_reward_min: -166.96971562038738\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 1773\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3255.744\n",
      "    load_time_ms: 2.492\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3345189094543457\n",
      "      kl: 0.0043305582366883755\n",
      "      policy_loss: -0.0024988208897411823\n",
      "      total_loss: 242.1055450439453\n",
      "      vf_explained_var: 0.55666583776474\n",
      "      vf_loss: 242.10804748535156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.307053565979004\n",
      "      kl: 0.0026887848507612944\n",
      "      policy_loss: -0.002029839903116226\n",
      "      total_loss: 399.52783203125\n",
      "      vf_explained_var: 0.602013349533081\n",
      "      vf_loss: 399.5298767089844\n",
      "    sample_time_ms: 20521.907\n",
      "    update_time_ms: 6.804\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.683655263257652\n",
      "    rl_1: 69.01969087492483\n",
      "  time_since_restore: 976.5040740966797\n",
      "  time_this_iter_s: 24.101168155670166\n",
      "  time_total_s: 976.5040740966797\n",
      "  timestamp: 1550794380\n",
      "  timesteps_since_restore: 350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 35\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 976 s, 35 iter, 350000 ts, 92.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-13-24\n",
      "  done: false\n",
      "  episode_len_mean: 128.49\n",
      "  episode_reward_max: 188.64289130577643\n",
      "  episode_reward_mean: 99.98099876389307\n",
      "  episode_reward_min: -178.53503048372232\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 1851\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3259.172\n",
      "    load_time_ms: 2.438\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2930551767349243\n",
      "      kl: 0.0038756781723350286\n",
      "      policy_loss: -0.001876682392321527\n",
      "      total_loss: 222.5367889404297\n",
      "      vf_explained_var: 0.5373083353042603\n",
      "      vf_loss: 222.53868103027344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.280051827430725\n",
      "      kl: 0.002556515159085393\n",
      "      policy_loss: -0.0016982509987428784\n",
      "      total_loss: 378.6443176269531\n",
      "      vf_explained_var: 0.615351140499115\n",
      "      vf_loss: 378.64599609375\n",
      "    sample_time_ms: 20591.151\n",
      "    update_time_ms: 6.841\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.907455264930046\n",
      "    rl_1: 76.07354349896306\n",
      "  time_since_restore: 1000.4670014381409\n",
      "  time_this_iter_s: 23.96292734146118\n",
      "  time_total_s: 1000.4670014381409\n",
      "  timestamp: 1550794404\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 36\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1000 s, 36 iter, 360000 ts, 100 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-13-48\n",
      "  done: false\n",
      "  episode_len_mean: 124.85\n",
      "  episode_reward_max: 190.8548498403785\n",
      "  episode_reward_mean: 60.5074454255019\n",
      "  episode_reward_min: -180.54029077768175\n",
      "  episodes_this_iter: 82\n",
      "  episodes_total: 1933\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3259.413\n",
      "    load_time_ms: 2.438\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9103830890414573e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.316054344177246\n",
      "      kl: 0.003210007445886731\n",
      "      policy_loss: -0.0013681163545697927\n",
      "      total_loss: 357.2758483886719\n",
      "      vf_explained_var: 0.5166199207305908\n",
      "      vf_loss: 357.2772521972656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.301650881767273\n",
      "      kl: 0.005058978218585253\n",
      "      policy_loss: -0.0018087218049913645\n",
      "      total_loss: 468.5852355957031\n",
      "      vf_explained_var: 0.6257069706916809\n",
      "      vf_loss: 468.5871276855469\n",
      "    sample_time_ms: 20670.52\n",
      "    update_time_ms: 6.812\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 9.89473198015748\n",
      "    rl_1: 50.612713445344426\n",
      "  time_since_restore: 1024.6563601493835\n",
      "  time_this_iter_s: 24.189358711242676\n",
      "  time_total_s: 1024.6563601493835\n",
      "  timestamp: 1550794428\n",
      "  timesteps_since_restore: 370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 37\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1024 s, 37 iter, 370000 ts, 60.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-14-12\n",
      "  done: false\n",
      "  episode_len_mean: 129.02\n",
      "  episode_reward_max: 190.8548498403785\n",
      "  episode_reward_mean: 73.14506144848966\n",
      "  episode_reward_min: -170.5189864812089\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 2010\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3260.998\n",
      "    load_time_ms: 2.419\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3043721914291382\n",
      "      kl: 0.010616632178425789\n",
      "      policy_loss: -0.003387033473700285\n",
      "      total_loss: 293.36236572265625\n",
      "      vf_explained_var: 0.47894537448883057\n",
      "      vf_loss: 293.36572265625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9103830890414573e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3011583089828491\n",
      "      kl: 0.006302437745034695\n",
      "      policy_loss: -0.0027648855466395617\n",
      "      total_loss: 437.8069763183594\n",
      "      vf_explained_var: 0.6324130892753601\n",
      "      vf_loss: 437.80975341796875\n",
      "    sample_time_ms: 20665.95\n",
      "    update_time_ms: 6.928\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 13.94760407305714\n",
      "    rl_1: 59.197457375432506\n",
      "  time_since_restore: 1048.4164102077484\n",
      "  time_this_iter_s: 23.760050058364868\n",
      "  time_total_s: 1048.4164102077484\n",
      "  timestamp: 1550794452\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 38\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1048 s, 38 iter, 380000 ts, 73.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-14-35\n",
      "  done: false\n",
      "  episode_len_mean: 134.43\n",
      "  episode_reward_max: 182.3014962481455\n",
      "  episode_reward_mean: 91.45667019025512\n",
      "  episode_reward_min: -170.5189864812089\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 2086\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3263.124\n",
      "    load_time_ms: 2.35\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2952351570129395\n",
      "      kl: 0.007173282094299793\n",
      "      policy_loss: -0.0028819444123655558\n",
      "      total_loss: 180.6719970703125\n",
      "      vf_explained_var: 0.5943877100944519\n",
      "      vf_loss: 180.6748809814453\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2882287502288818\n",
      "      kl: 0.01070148404687643\n",
      "      policy_loss: -0.003376130945980549\n",
      "      total_loss: 334.5222473144531\n",
      "      vf_explained_var: 0.7275614142417908\n",
      "      vf_loss: 334.5256042480469\n",
      "    sample_time_ms: 20657.248\n",
      "    update_time_ms: 6.878\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 17.987458242972597\n",
      "    rl_1: 73.4692119472825\n",
      "  time_since_restore: 1072.089477777481\n",
      "  time_this_iter_s: 23.673067569732666\n",
      "  time_total_s: 1072.089477777481\n",
      "  timestamp: 1550794475\n",
      "  timesteps_since_restore: 390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 39\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1072 s, 39 iter, 390000 ts, 91.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-14-59\n",
      "  done: false\n",
      "  episode_len_mean: 123.17\n",
      "  episode_reward_max: 183.76372176653925\n",
      "  episode_reward_mean: 107.89954889743233\n",
      "  episode_reward_min: -174.8045651300975\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 2167\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3254.583\n",
      "    load_time_ms: 2.324\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.300398588180542\n",
      "      kl: 0.002726372331380844\n",
      "      policy_loss: -0.0017250390956178308\n",
      "      total_loss: 146.97216796875\n",
      "      vf_explained_var: 0.6216155290603638\n",
      "      vf_loss: 146.973876953125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2510453462600708\n",
      "      kl: 0.013322628103196621\n",
      "      policy_loss: -0.0032119308598339558\n",
      "      total_loss: 323.11614990234375\n",
      "      vf_explained_var: 0.6995617747306824\n",
      "      vf_loss: 323.119384765625\n",
      "    sample_time_ms: 20621.743\n",
      "    update_time_ms: 7.047\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.899516327194853\n",
      "    rl_1: 81.0000325702375\n",
      "  time_since_restore: 1095.878441810608\n",
      "  time_this_iter_s: 23.78896403312683\n",
      "  time_total_s: 1095.878441810608\n",
      "  timestamp: 1550794499\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 40\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1095 s, 40 iter, 400000 ts, 108 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-15-23\n",
      "  done: false\n",
      "  episode_len_mean: 112.48\n",
      "  episode_reward_max: 176.87849700894418\n",
      "  episode_reward_mean: 63.6582864509018\n",
      "  episode_reward_min: -183.39806408487016\n",
      "  episodes_this_iter: 88\n",
      "  episodes_total: 2255\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3256.462\n",
      "    load_time_ms: 2.318\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.292773962020874\n",
      "      kl: 0.004665011540055275\n",
      "      policy_loss: -0.0021522159222513437\n",
      "      total_loss: 317.126220703125\n",
      "      vf_explained_var: 0.5678359866142273\n",
      "      vf_loss: 317.12841796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2350679636001587\n",
      "      kl: 0.01042136363685131\n",
      "      policy_loss: -0.0040612537413835526\n",
      "      total_loss: 463.0582580566406\n",
      "      vf_explained_var: 0.6920161843299866\n",
      "      vf_loss: 463.062255859375\n",
      "    sample_time_ms: 20640.588\n",
      "    update_time_ms: 6.951\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 6.745396688952311\n",
      "    rl_1: 56.91288976194947\n",
      "  time_since_restore: 1119.9860627651215\n",
      "  time_this_iter_s: 24.10762095451355\n",
      "  time_total_s: 1119.9860627651215\n",
      "  timestamp: 1550794523\n",
      "  timesteps_since_restore: 410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 41\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1119 s, 41 iter, 410000 ts, 63.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-15-47\n",
      "  done: false\n",
      "  episode_len_mean: 135.14\n",
      "  episode_reward_max: 170.07010384545765\n",
      "  episode_reward_mean: 72.75363718779826\n",
      "  episode_reward_min: -183.39806408487016\n",
      "  episodes_this_iter: 69\n",
      "  episodes_total: 2324\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3256.419\n",
      "    load_time_ms: 2.313\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8189894306509108e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.373632550239563\n",
      "      kl: 0.0038404117804020643\n",
      "      policy_loss: -0.0016241788398474455\n",
      "      total_loss: 204.0466766357422\n",
      "      vf_explained_var: 0.5748862624168396\n",
      "      vf_loss: 204.0482940673828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3306679725646973\n",
      "      kl: 0.001503791892901063\n",
      "      policy_loss: -0.0007421295740641654\n",
      "      total_loss: 301.0885925292969\n",
      "      vf_explained_var: 0.8255905508995056\n",
      "      vf_loss: 301.0893249511719\n",
      "    sample_time_ms: 20600.534\n",
      "    update_time_ms: 6.883\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 13.903397824520079\n",
      "    rl_1: 58.8502393632782\n",
      "  time_since_restore: 1143.8427846431732\n",
      "  time_this_iter_s: 23.856721878051758\n",
      "  time_total_s: 1143.8427846431732\n",
      "  timestamp: 1550794547\n",
      "  timesteps_since_restore: 420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 42\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1143 s, 42 iter, 420000 ts, 72.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-16-11\n",
      "  done: false\n",
      "  episode_len_mean: 114.85\n",
      "  episode_reward_max: 174.32008688996723\n",
      "  episode_reward_mean: 76.1282136675318\n",
      "  episode_reward_min: -179.78202244966894\n",
      "  episodes_this_iter: 89\n",
      "  episodes_total: 2413\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3258.953\n",
      "    load_time_ms: 2.344\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2798126935958862\n",
      "      kl: 0.006130674853920937\n",
      "      policy_loss: -0.0015133050037547946\n",
      "      total_loss: 273.9261474609375\n",
      "      vf_explained_var: 0.5312689542770386\n",
      "      vf_loss: 273.927734375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2349625825881958\n",
      "      kl: 0.002034469274803996\n",
      "      policy_loss: -0.0016836723079904914\n",
      "      total_loss: 382.4508972167969\n",
      "      vf_explained_var: 0.7533078789710999\n",
      "      vf_loss: 382.4525451660156\n",
      "    sample_time_ms: 20629.387\n",
      "    update_time_ms: 6.849\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 11.790895171753757\n",
      "    rl_1: 64.33731849577805\n",
      "  time_since_restore: 1167.6443812847137\n",
      "  time_this_iter_s: 23.801596641540527\n",
      "  time_total_s: 1167.6443812847137\n",
      "  timestamp: 1550794571\n",
      "  timesteps_since_restore: 430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 43\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1167 s, 43 iter, 430000 ts, 76.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-16-35\n",
      "  done: false\n",
      "  episode_len_mean: 111.32\n",
      "  episode_reward_max: 176.78301797826396\n",
      "  episode_reward_mean: 94.38344790267989\n",
      "  episode_reward_min: -175.4063008129952\n",
      "  episodes_this_iter: 89\n",
      "  episodes_total: 2502\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3257.637\n",
      "    load_time_ms: 2.358\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2763761281967163\n",
      "      kl: 0.008673736825585365\n",
      "      policy_loss: -0.0033689099363982677\n",
      "      total_loss: 172.1150360107422\n",
      "      vf_explained_var: 0.6629601716995239\n",
      "      vf_loss: 172.118408203125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.182277798652649\n",
      "      kl: 0.006886234041303396\n",
      "      policy_loss: -0.0021123739425092936\n",
      "      total_loss: 369.3019714355469\n",
      "      vf_explained_var: 0.7154837846755981\n",
      "      vf_loss: 369.3041076660156\n",
      "    sample_time_ms: 20630.081\n",
      "    update_time_ms: 6.826\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.525651470418055\n",
      "    rl_1: 77.85779643226184\n",
      "  time_since_restore: 1191.5564200878143\n",
      "  time_this_iter_s: 23.912038803100586\n",
      "  time_total_s: 1191.5564200878143\n",
      "  timestamp: 1550794595\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 44\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1191 s, 44 iter, 440000 ts, 94.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-16-59\n",
      "  done: false\n",
      "  episode_len_mean: 124.83\n",
      "  episode_reward_max: 177.61336910425362\n",
      "  episode_reward_mean: 97.97900039431347\n",
      "  episode_reward_min: -173.63214305032636\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 2580\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3252.311\n",
      "    load_time_ms: 2.324\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3546369075775146\n",
      "      kl: 0.0035981046967208385\n",
      "      policy_loss: -0.00208056322298944\n",
      "      total_loss: 140.55799865722656\n",
      "      vf_explained_var: 0.6825318932533264\n",
      "      vf_loss: 140.56008911132812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8189894306509108e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2512683868408203\n",
      "      kl: 0.007004276849329472\n",
      "      policy_loss: -0.0026075453497469425\n",
      "      total_loss: 312.4713439941406\n",
      "      vf_explained_var: 0.8347238302230835\n",
      "      vf_loss: 312.4739074707031\n",
      "    sample_time_ms: 20617.094\n",
      "    update_time_ms: 6.682\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 17.900764867144844\n",
      "    rl_1: 80.07823552716864\n",
      "  time_since_restore: 1215.4714813232422\n",
      "  time_this_iter_s: 23.915061235427856\n",
      "  time_total_s: 1215.4714813232422\n",
      "  timestamp: 1550794619\n",
      "  timesteps_since_restore: 450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 45\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1215 s, 45 iter, 450000 ts, 98 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-17-23\n",
      "  done: false\n",
      "  episode_len_mean: 115.65\n",
      "  episode_reward_max: 181.32321806864894\n",
      "  episode_reward_mean: 96.73008905680256\n",
      "  episode_reward_min: -182.19176683828422\n",
      "  episodes_this_iter: 85\n",
      "  episodes_total: 2665\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3249.725\n",
      "    load_time_ms: 2.339\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1368683941568192e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.324345588684082\n",
      "      kl: 0.007350697182118893\n",
      "      policy_loss: -0.0015707401325926185\n",
      "      total_loss: 166.7189178466797\n",
      "      vf_explained_var: 0.7046629190444946\n",
      "      vf_loss: 166.72047424316406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2043813467025757\n",
      "      kl: 0.010693131946027279\n",
      "      policy_loss: -0.0030964557081460953\n",
      "      total_loss: 358.5019226074219\n",
      "      vf_explained_var: 0.8166921138763428\n",
      "      vf_loss: 358.5050048828125\n",
      "    sample_time_ms: 20604.487\n",
      "    update_time_ms: 6.633\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 17.34285773408503\n",
      "    rl_1: 79.3872313227175\n",
      "  time_since_restore: 1239.2809031009674\n",
      "  time_this_iter_s: 23.80942177772522\n",
      "  time_total_s: 1239.2809031009674\n",
      "  timestamp: 1550794643\n",
      "  timesteps_since_restore: 460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 46\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1239 s, 46 iter, 460000 ts, 96.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-17-47\n",
      "  done: false\n",
      "  episode_len_mean: 128.15\n",
      "  episode_reward_max: 180.79110832531816\n",
      "  episode_reward_mean: 94.61424304495739\n",
      "  episode_reward_min: -182.57248531451592\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 2743\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3250.574\n",
      "    load_time_ms: 2.34\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.351758360862732\n",
      "      kl: 0.004885719623416662\n",
      "      policy_loss: -0.002582317218184471\n",
      "      total_loss: 168.5773162841797\n",
      "      vf_explained_var: 0.7010359168052673\n",
      "      vf_loss: 168.5799102783203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2440667152404785\n",
      "      kl: 0.009760869666934013\n",
      "      policy_loss: -0.0015903781168162823\n",
      "      total_loss: 297.47686767578125\n",
      "      vf_explained_var: 0.881200909614563\n",
      "      vf_loss: 297.4784851074219\n",
      "    sample_time_ms: 20562.047\n",
      "    update_time_ms: 6.691\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.862905728238452\n",
      "    rl_1: 77.75133731671893\n",
      "  time_since_restore: 1263.053223848343\n",
      "  time_this_iter_s: 23.77232074737549\n",
      "  time_total_s: 1263.053223848343\n",
      "  timestamp: 1550794667\n",
      "  timesteps_since_restore: 470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 47\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1263 s, 47 iter, 470000 ts, 94.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-18-11\n",
      "  done: false\n",
      "  episode_len_mean: 127.41\n",
      "  episode_reward_max: 176.3236303593103\n",
      "  episode_reward_mean: 99.96561494877432\n",
      "  episode_reward_min: -180.54483345947952\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 2824\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3249.22\n",
      "    load_time_ms: 2.485\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.842170985392048e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3301700353622437\n",
      "      kl: 0.005632657092064619\n",
      "      policy_loss: -0.002893804805353284\n",
      "      total_loss: 116.94427490234375\n",
      "      vf_explained_var: 0.7641496658325195\n",
      "      vf_loss: 116.9471664428711\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1996952295303345\n",
      "      kl: 0.0072205099277198315\n",
      "      policy_loss: -0.0028702272102236748\n",
      "      total_loss: 267.51068115234375\n",
      "      vf_explained_var: 0.8725702166557312\n",
      "      vf_loss: 267.5135498046875\n",
      "    sample_time_ms: 20574.575\n",
      "    update_time_ms: 6.554\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 18.828507129218213\n",
      "    rl_1: 81.1371078195561\n",
      "  time_since_restore: 1286.9256720542908\n",
      "  time_this_iter_s: 23.872448205947876\n",
      "  time_total_s: 1286.9256720542908\n",
      "  timestamp: 1550794691\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 48\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1286 s, 48 iter, 480000 ts, 100 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-18-34\n",
      "  done: false\n",
      "  episode_len_mean: 125.44\n",
      "  episode_reward_max: 176.68026429040617\n",
      "  episode_reward_mean: 119.15990053947779\n",
      "  episode_reward_min: -173.50020577167754\n",
      "  episodes_this_iter: 83\n",
      "  episodes_total: 2907\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3247.858\n",
      "    load_time_ms: 2.535\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.421085492696024e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.306909203529358\n",
      "      kl: 0.004676383920013905\n",
      "      policy_loss: -0.0023115333169698715\n",
      "      total_loss: 122.0804672241211\n",
      "      vf_explained_var: 0.7452672123908997\n",
      "      vf_loss: 122.08277893066406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1684553623199463\n",
      "      kl: 0.004939571488648653\n",
      "      policy_loss: -0.0017422988312318921\n",
      "      total_loss: 304.96722412109375\n",
      "      vf_explained_var: 0.8333272933959961\n",
      "      vf_loss: 304.9689636230469\n",
      "    sample_time_ms: 20582.99\n",
      "    update_time_ms: 6.657\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 27.21387410773572\n",
      "    rl_1: 91.94602643174204\n",
      "  time_since_restore: 1310.6705238819122\n",
      "  time_this_iter_s: 23.74485182762146\n",
      "  time_total_s: 1310.6705238819122\n",
      "  timestamp: 1550794714\n",
      "  timesteps_since_restore: 490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 49\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1310 s, 49 iter, 490000 ts, 119 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-18-58\n",
      "  done: false\n",
      "  episode_len_mean: 99.76\n",
      "  episode_reward_max: 183.20735750867684\n",
      "  episode_reward_mean: 95.3049512976476\n",
      "  episode_reward_min: -178.28803995755143\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 3007\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3250.333\n",
      "    load_time_ms: 2.558\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.218662977218628\n",
      "      kl: 0.008256793953478336\n",
      "      policy_loss: -0.0021999841555953026\n",
      "      total_loss: 183.9994354248047\n",
      "      vf_explained_var: 0.5761311650276184\n",
      "      vf_loss: 184.0016326904297\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1368683941568192e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0399620532989502\n",
      "      kl: 0.009561783634126186\n",
      "      policy_loss: -0.003058892907574773\n",
      "      total_loss: 324.4989929199219\n",
      "      vf_explained_var: 0.773187518119812\n",
      "      vf_loss: 324.5020751953125\n",
      "    sample_time_ms: 20555.47\n",
      "    update_time_ms: 6.564\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.37323580873867\n",
      "    rl_1: 78.93171548890894\n",
      "  time_since_restore: 1334.2105412483215\n",
      "  time_this_iter_s: 23.5400173664093\n",
      "  time_total_s: 1334.2105412483215\n",
      "  timestamp: 1550794738\n",
      "  timesteps_since_restore: 500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 50\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1334 s, 50 iter, 500000 ts, 95.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-19-22\n",
      "  done: false\n",
      "  episode_len_mean: 111.22\n",
      "  episode_reward_max: 177.34109361368138\n",
      "  episode_reward_mean: 92.66867997368718\n",
      "  episode_reward_min: -177.84677581737125\n",
      "  episodes_this_iter: 90\n",
      "  episodes_total: 3097\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3249.017\n",
      "    load_time_ms: 2.593\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.55271373174006e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2815325260162354\n",
      "      kl: 0.006526839453727007\n",
      "      policy_loss: -0.0034173603635281324\n",
      "      total_loss: 173.34640502929688\n",
      "      vf_explained_var: 0.6602018475532532\n",
      "      vf_loss: 173.34979248046875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1257715225219727\n",
      "      kl: 0.007360447663813829\n",
      "      policy_loss: -0.0017567924223840237\n",
      "      total_loss: 286.3587646484375\n",
      "      vf_explained_var: 0.8485202193260193\n",
      "      vf_loss: 286.36053466796875\n",
      "    sample_time_ms: 20522.173\n",
      "    update_time_ms: 6.674\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 15.015894297702644\n",
      "    rl_1: 77.65278567598453\n",
      "  time_since_restore: 1357.9744811058044\n",
      "  time_this_iter_s: 23.76393985748291\n",
      "  time_total_s: 1357.9744811058044\n",
      "  timestamp: 1550794762\n",
      "  timesteps_since_restore: 510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 51\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1357 s, 51 iter, 510000 ts, 92.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-19-46\n",
      "  done: false\n",
      "  episode_len_mean: 106.08\n",
      "  episode_reward_max: 169.460333501175\n",
      "  episode_reward_mean: 113.85811884847075\n",
      "  episode_reward_min: -183.44969786958052\n",
      "  episodes_this_iter: 94\n",
      "  episodes_total: 3191\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3245.635\n",
      "    load_time_ms: 2.579\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.77635686587003e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2869956493377686\n",
      "      kl: 0.0044053285382688046\n",
      "      policy_loss: -0.0022664309944957495\n",
      "      total_loss: 83.17903900146484\n",
      "      vf_explained_var: 0.7120335698127747\n",
      "      vf_loss: 83.18131256103516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.842170985392048e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.083943486213684\n",
      "      kl: 0.023231051862239838\n",
      "      policy_loss: -0.005939378868788481\n",
      "      total_loss: 262.5335388183594\n",
      "      vf_explained_var: 0.7830525636672974\n",
      "      vf_loss: 262.5394592285156\n",
      "    sample_time_ms: 20529.642\n",
      "    update_time_ms: 6.884\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.140946700083614\n",
      "    rl_1: 91.71717214838715\n",
      "  time_since_restore: 1381.8752987384796\n",
      "  time_this_iter_s: 23.90081763267517\n",
      "  time_total_s: 1381.8752987384796\n",
      "  timestamp: 1550794786\n",
      "  timesteps_since_restore: 520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 52\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1381 s, 52 iter, 520000 ts, 114 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-20-10\n",
      "  done: false\n",
      "  episode_len_mean: 125.26\n",
      "  episode_reward_max: 173.76850300174942\n",
      "  episode_reward_mean: 114.10439594342039\n",
      "  episode_reward_min: -153.1492753799456\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 3268\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3245.739\n",
      "    load_time_ms: 2.542\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.88178432935015e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3811558485031128\n",
      "      kl: 0.0024974136613309383\n",
      "      policy_loss: -0.0015633064322173595\n",
      "      total_loss: 93.54378509521484\n",
      "      vf_explained_var: 0.7853354811668396\n",
      "      vf_loss: 93.54535675048828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.842170985392048e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.257534384727478\n",
      "      kl: 0.004081148188561201\n",
      "      policy_loss: -0.0023077374789863825\n",
      "      total_loss: 233.7242889404297\n",
      "      vf_explained_var: 0.8983862400054932\n",
      "      vf_loss: 233.7266082763672\n",
      "    sample_time_ms: 20539.866\n",
      "    update_time_ms: 7.119\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.98747050050798\n",
      "    rl_1: 90.11692544291242\n",
      "  time_since_restore: 1405.7814886569977\n",
      "  time_this_iter_s: 23.906189918518066\n",
      "  time_total_s: 1405.7814886569977\n",
      "  timestamp: 1550794810\n",
      "  timesteps_since_restore: 530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 53\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1405 s, 53 iter, 530000 ts, 114 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-20-34\n",
      "  done: false\n",
      "  episode_len_mean: 119.1\n",
      "  episode_reward_max: 177.86428847203274\n",
      "  episode_reward_mean: 120.11573059826064\n",
      "  episode_reward_min: -161.45190367297084\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3349\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3245.759\n",
      "    load_time_ms: 2.589\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.34260892868042\n",
      "      kl: 0.004077567718923092\n",
      "      policy_loss: -0.0021273023448884487\n",
      "      total_loss: 77.12358856201172\n",
      "      vf_explained_var: 0.7801321148872375\n",
      "      vf_loss: 77.12570190429688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.421085492696024e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.195773720741272\n",
      "      kl: 0.00800020806491375\n",
      "      policy_loss: -0.002218283247202635\n",
      "      total_loss: 226.5760955810547\n",
      "      vf_explained_var: 0.8982058763504028\n",
      "      vf_loss: 226.57835388183594\n",
      "    sample_time_ms: 20519.784\n",
      "    update_time_ms: 7.114\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.23777628307782\n",
      "    rl_1: 93.87795431518279\n",
      "  time_since_restore: 1429.4929029941559\n",
      "  time_this_iter_s: 23.711414337158203\n",
      "  time_total_s: 1429.4929029941559\n",
      "  timestamp: 1550794834\n",
      "  timesteps_since_restore: 540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 54\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1429 s, 54 iter, 540000 ts, 120 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-20-58\n",
      "  done: false\n",
      "  episode_len_mean: 119.43\n",
      "  episode_reward_max: 166.82631410536604\n",
      "  episode_reward_mean: 111.75185505431448\n",
      "  episode_reward_min: -175.94042179534165\n",
      "  episodes_this_iter: 85\n",
      "  episodes_total: 3434\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3245.659\n",
      "    load_time_ms: 2.621\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2204460823375376e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3379099369049072\n",
      "      kl: 0.0026477437932044268\n",
      "      policy_loss: -0.00229283282533288\n",
      "      total_loss: 114.30541229248047\n",
      "      vf_explained_var: 0.7221103310585022\n",
      "      vf_loss: 114.3077163696289\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1808496713638306\n",
      "      kl: 0.004373329225927591\n",
      "      policy_loss: -0.0009420435526408255\n",
      "      total_loss: 257.34381103515625\n",
      "      vf_explained_var: 0.8711318969726562\n",
      "      vf_loss: 257.3447570800781\n",
      "    sample_time_ms: 20569.269\n",
      "    update_time_ms: 7.124\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.099135516406555\n",
      "    rl_1: 88.65271953790793\n",
      "  time_since_restore: 1453.9008121490479\n",
      "  time_this_iter_s: 24.407909154891968\n",
      "  time_total_s: 1453.9008121490479\n",
      "  timestamp: 1550794858\n",
      "  timesteps_since_restore: 550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 55\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1453 s, 55 iter, 550000 ts, 112 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-21-22\n",
      "  done: false\n",
      "  episode_len_mean: 108.73\n",
      "  episode_reward_max: 175.4093357498778\n",
      "  episode_reward_mean: 128.05141920715982\n",
      "  episode_reward_min: -163.78995408095446\n",
      "  episodes_this_iter: 93\n",
      "  episodes_total: 3527\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3255.576\n",
      "    load_time_ms: 2.603\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1102230411687688e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3100337982177734\n",
      "      kl: 0.006445996928960085\n",
      "      policy_loss: -0.0037208974827080965\n",
      "      total_loss: 68.90874481201172\n",
      "      vf_explained_var: 0.7375779747962952\n",
      "      vf_loss: 68.91246032714844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.55271373174006e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0905767679214478\n",
      "      kl: 0.003922079689800739\n",
      "      policy_loss: -0.0028073175344616175\n",
      "      total_loss: 199.47923278808594\n",
      "      vf_explained_var: 0.8706480264663696\n",
      "      vf_loss: 199.48204040527344\n",
      "    sample_time_ms: 20545.072\n",
      "    update_time_ms: 7.136\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.68659080910738\n",
      "    rl_1: 98.36482839805248\n",
      "  time_since_restore: 1477.5672228336334\n",
      "  time_this_iter_s: 23.66641068458557\n",
      "  time_total_s: 1477.5672228336334\n",
      "  timestamp: 1550794882\n",
      "  timesteps_since_restore: 560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 56\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1477 s, 56 iter, 560000 ts, 128 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-21-45\n",
      "  done: false\n",
      "  episode_len_mean: 118.06\n",
      "  episode_reward_max: 176.59052812618506\n",
      "  episode_reward_mean: 119.48073461477264\n",
      "  episode_reward_min: -170.93347945385193\n",
      "  episodes_this_iter: 86\n",
      "  episodes_total: 3613\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3254.315\n",
      "    load_time_ms: 2.625\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.551115205843844e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3462189435958862\n",
      "      kl: 0.0030854602809995413\n",
      "      policy_loss: -0.0017469502054154873\n",
      "      total_loss: 84.81077575683594\n",
      "      vf_explained_var: 0.6948049068450928\n",
      "      vf_loss: 84.81251525878906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.77635686587003e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.169350266456604\n",
      "      kl: 0.005587129853665829\n",
      "      policy_loss: -0.004507944919168949\n",
      "      total_loss: 193.37020874023438\n",
      "      vf_explained_var: 0.9006498456001282\n",
      "      vf_loss: 193.374755859375\n",
      "    sample_time_ms: 20543.638\n",
      "    update_time_ms: 7.331\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.319204839253217\n",
      "    rl_1: 93.16152977551943\n",
      "  time_since_restore: 1501.3161056041718\n",
      "  time_this_iter_s: 23.74888277053833\n",
      "  time_total_s: 1501.3161056041718\n",
      "  timestamp: 1550794905\n",
      "  timesteps_since_restore: 570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 57\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1501 s, 57 iter, 570000 ts, 119 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-22-10\n",
      "  done: false\n",
      "  episode_len_mean: 115.43\n",
      "  episode_reward_max: 171.40566858993094\n",
      "  episode_reward_mean: 110.6400225451086\n",
      "  episode_reward_min: -176.7495997335483\n",
      "  episodes_this_iter: 85\n",
      "  episodes_total: 3698\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3253.338\n",
      "    load_time_ms: 2.589\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.775557602921922e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3695416450500488\n",
      "      kl: 0.0035441636573523283\n",
      "      policy_loss: -0.0012319224188104272\n",
      "      total_loss: 87.60347747802734\n",
      "      vf_explained_var: 0.784863293170929\n",
      "      vf_loss: 87.60470581054688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.88178432935015e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1942020654678345\n",
      "      kl: 0.003558626165613532\n",
      "      policy_loss: -0.001964925555512309\n",
      "      total_loss: 189.30966186523438\n",
      "      vf_explained_var: 0.9232182502746582\n",
      "      vf_loss: 189.31163024902344\n",
      "    sample_time_ms: 20595.52\n",
      "    update_time_ms: 7.31\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.580562753831988\n",
      "    rl_1: 88.05945979127664\n",
      "  time_since_restore: 1525.696647644043\n",
      "  time_this_iter_s: 24.380542039871216\n",
      "  time_total_s: 1525.696647644043\n",
      "  timestamp: 1550794930\n",
      "  timesteps_since_restore: 580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 58\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1525 s, 58 iter, 580000 ts, 111 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-22-34\n",
      "  done: false\n",
      "  episode_len_mean: 105.5\n",
      "  episode_reward_max: 171.1116270689427\n",
      "  episode_reward_mean: 102.98721741144533\n",
      "  episode_reward_min: -170.24451377103216\n",
      "  episodes_this_iter: 92\n",
      "  episodes_total: 3790\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3252.62\n",
      "    load_time_ms: 2.504\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.387778801460961e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3185369968414307\n",
      "      kl: 0.007803467568010092\n",
      "      policy_loss: -0.0031547709368169308\n",
      "      total_loss: 160.8222198486328\n",
      "      vf_explained_var: 0.6307210922241211\n",
      "      vf_loss: 160.82537841796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.110372543334961\n",
      "      kl: 0.0071773529052734375\n",
      "      policy_loss: -0.0018488150089979172\n",
      "      total_loss: 287.6702880859375\n",
      "      vf_explained_var: 0.839163064956665\n",
      "      vf_loss: 287.6720886230469\n",
      "    sample_time_ms: 20597.781\n",
      "    update_time_ms: 7.593\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 18.659826675631983\n",
      "    rl_1: 84.32739073581337\n",
      "  time_since_restore: 1549.4581360816956\n",
      "  time_this_iter_s: 23.761488437652588\n",
      "  time_total_s: 1549.4581360816956\n",
      "  timestamp: 1550794954\n",
      "  timesteps_since_restore: 590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 59\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1549 s, 59 iter, 590000 ts, 103 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-22-58\n",
      "  done: false\n",
      "  episode_len_mean: 121.01\n",
      "  episode_reward_max: 177.6724142742527\n",
      "  episode_reward_mean: 119.51392545446436\n",
      "  episode_reward_min: -184.60027983653436\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 3869\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3249.082\n",
      "    load_time_ms: 2.467\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.938894007304805e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3544281721115112\n",
      "      kl: 0.0037880586460232735\n",
      "      policy_loss: -0.0017219346482306719\n",
      "      total_loss: 103.26123046875\n",
      "      vf_explained_var: 0.7455003261566162\n",
      "      vf_loss: 103.26295471191406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2204460823375376e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.204121470451355\n",
      "      kl: 0.010284480638802052\n",
      "      policy_loss: -0.003977613523602486\n",
      "      total_loss: 237.9939727783203\n",
      "      vf_explained_var: 0.8949704170227051\n",
      "      vf_loss: 237.99789428710938\n",
      "    sample_time_ms: 20637.687\n",
      "    update_time_ms: 7.505\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 27.71159577437306\n",
      "    rl_1: 91.80232968009133\n",
      "  time_since_restore: 1573.3591167926788\n",
      "  time_this_iter_s: 23.900980710983276\n",
      "  time_total_s: 1573.3591167926788\n",
      "  timestamp: 1550794978\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 60\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1573 s, 60 iter, 600000 ts, 120 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-23-22\n",
      "  done: false\n",
      "  episode_len_mean: 122.18\n",
      "  episode_reward_max: 179.30045330635863\n",
      "  episode_reward_mean: 124.87243726108218\n",
      "  episode_reward_min: -165.1761892358528\n",
      "  episodes_this_iter: 89\n",
      "  episodes_total: 3958\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3255.991\n",
      "    load_time_ms: 2.432\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.4694470036524025e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2971556186676025\n",
      "      kl: 0.01063390914350748\n",
      "      policy_loss: -0.003182049375027418\n",
      "      total_loss: 67.30972290039062\n",
      "      vf_explained_var: 0.808302640914917\n",
      "      vf_loss: 67.31290435791016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2204460823375376e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.094062328338623\n",
      "      kl: 0.010184017941355705\n",
      "      policy_loss: -0.004616152960807085\n",
      "      total_loss: 218.88917541503906\n",
      "      vf_explained_var: 0.8902173042297363\n",
      "      vf_loss: 218.8937530517578\n",
      "    sample_time_ms: 20676.616\n",
      "    update_time_ms: 7.429\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.90408600069165\n",
      "    rl_1: 94.96835126039053\n",
      "  time_since_restore: 1597.5799841880798\n",
      "  time_this_iter_s: 24.220867395401\n",
      "  time_total_s: 1597.5799841880798\n",
      "  timestamp: 1550795002\n",
      "  timesteps_since_restore: 610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 61\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1597 s, 61 iter, 610000 ts, 125 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-23-46\n",
      "  done: false\n",
      "  episode_len_mean: 115.05\n",
      "  episode_reward_max: 183.77235007281953\n",
      "  episode_reward_mean: 114.57348571626225\n",
      "  episode_reward_min: -178.57655199566398\n",
      "  episodes_this_iter: 86\n",
      "  episodes_total: 4044\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3254.679\n",
      "    load_time_ms: 2.458\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.4694470036524025e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.314827561378479\n",
      "      kl: 0.002075443509966135\n",
      "      policy_loss: -0.0019183483673259616\n",
      "      total_loss: 113.70283508300781\n",
      "      vf_explained_var: 0.6725086569786072\n",
      "      vf_loss: 113.70476531982422\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2204460823375376e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.164415955543518\n",
      "      kl: 0.0038239574059844017\n",
      "      policy_loss: -0.002572225406765938\n",
      "      total_loss: 210.77175903320312\n",
      "      vf_explained_var: 0.8988924026489258\n",
      "      vf_loss: 210.7743377685547\n",
      "    sample_time_ms: 20659.619\n",
      "    update_time_ms: 7.209\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.871615977690247\n",
      "    rl_1: 88.701869738572\n",
      "  time_since_restore: 1621.2943732738495\n",
      "  time_this_iter_s: 23.714389085769653\n",
      "  time_total_s: 1621.2943732738495\n",
      "  timestamp: 1550795026\n",
      "  timesteps_since_restore: 620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 62\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1621 s, 62 iter, 620000 ts, 115 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-24-10\n",
      "  done: false\n",
      "  episode_len_mean: 118.67\n",
      "  episode_reward_max: 186.6718810375657\n",
      "  episode_reward_mean: 121.54726931191715\n",
      "  episode_reward_min: -163.43815656401418\n",
      "  episodes_this_iter: 86\n",
      "  episodes_total: 4130\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3253.96\n",
      "    load_time_ms: 2.445\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3349840641021729\n",
      "      kl: 0.007342537399381399\n",
      "      policy_loss: -0.004253072664141655\n",
      "      total_loss: 116.81550598144531\n",
      "      vf_explained_var: 0.7130103707313538\n",
      "      vf_loss: 116.81974029541016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1102230411687688e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1596285104751587\n",
      "      kl: 0.007745629642158747\n",
      "      policy_loss: -0.0019495215965434909\n",
      "      total_loss: 229.5944061279297\n",
      "      vf_explained_var: 0.8874232769012451\n",
      "      vf_loss: 229.59640502929688\n",
      "    sample_time_ms: 20689.873\n",
      "    update_time_ms: 7.064\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.94094973969959\n",
      "    rl_1: 91.60631957221759\n",
      "  time_since_restore: 1645.4932897090912\n",
      "  time_this_iter_s: 24.1989164352417\n",
      "  time_total_s: 1645.4932897090912\n",
      "  timestamp: 1550795050\n",
      "  timesteps_since_restore: 630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 63\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1645 s, 63 iter, 630000 ts, 122 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-24-34\n",
      "  done: false\n",
      "  episode_len_mean: 116.02\n",
      "  episode_reward_max: 186.6718810375657\n",
      "  episode_reward_mean: 122.86361677174767\n",
      "  episode_reward_min: -163.43815656401418\n",
      "  episodes_this_iter: 84\n",
      "  episodes_total: 4214\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3254.518\n",
      "    load_time_ms: 2.335\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3411954641342163\n",
      "      kl: 0.004473385401070118\n",
      "      policy_loss: -0.0018209642730653286\n",
      "      total_loss: 97.51459503173828\n",
      "      vf_explained_var: 0.7498669028282166\n",
      "      vf_loss: 97.51641082763672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.551115205843844e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1817504167556763\n",
      "      kl: 0.006648732349276543\n",
      "      policy_loss: -0.002789387945085764\n",
      "      total_loss: 178.83279418945312\n",
      "      vf_explained_var: 0.9216444492340088\n",
      "      vf_loss: 178.83560180664062\n",
      "    sample_time_ms: 20714.551\n",
      "    update_time_ms: 7.1\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.355420432799686\n",
      "    rl_1: 91.50819633894798\n",
      "  time_since_restore: 1669.4543392658234\n",
      "  time_this_iter_s: 23.961049556732178\n",
      "  time_total_s: 1669.4543392658234\n",
      "  timestamp: 1550795074\n",
      "  timesteps_since_restore: 640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 64\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1669 s, 64 iter, 640000 ts, 123 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-24-58\n",
      "  done: false\n",
      "  episode_len_mean: 116.53\n",
      "  episode_reward_max: 185.61868709532342\n",
      "  episode_reward_mean: 117.51709928127646\n",
      "  episode_reward_min: -164.22868930936664\n",
      "  episodes_this_iter: 83\n",
      "  episodes_total: 4297\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3254.363\n",
      "    load_time_ms: 2.329\n",
      "    num_steps_sampled: 650000\n",
      "    num_steps_trained: 650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.336808754565503e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3345259428024292\n",
      "      kl: 0.002412100788205862\n",
      "      policy_loss: -0.0008797586197033525\n",
      "      total_loss: 80.4902114868164\n",
      "      vf_explained_var: 0.7873202562332153\n",
      "      vf_loss: 80.4910888671875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.775557602921922e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1966346502304077\n",
      "      kl: 0.00396902346983552\n",
      "      policy_loss: -0.002858471591025591\n",
      "      total_loss: 178.06800842285156\n",
      "      vf_explained_var: 0.9378423690795898\n",
      "      vf_loss: 178.07086181640625\n",
      "    sample_time_ms: 20664.958\n",
      "    update_time_ms: 7.079\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.429052476721054\n",
      "    rl_1: 89.08804680455542\n",
      "  time_since_restore: 1693.3656387329102\n",
      "  time_this_iter_s: 23.911299467086792\n",
      "  time_total_s: 1693.3656387329102\n",
      "  timestamp: 1550795098\n",
      "  timesteps_since_restore: 650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 650000\n",
      "  training_iteration: 65\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1693 s, 65 iter, 650000 ts, 118 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-25-22\n",
      "  done: false\n",
      "  episode_len_mean: 117.14\n",
      "  episode_reward_max: 187.2424923679131\n",
      "  episode_reward_mean: 112.90288201639558\n",
      "  episode_reward_min: -170.06978707640909\n",
      "  episodes_this_iter: 85\n",
      "  episodes_total: 4382\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3255.506\n",
      "    load_time_ms: 2.329\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.1684043772827515e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3337827920913696\n",
      "      kl: 0.003512693801894784\n",
      "      policy_loss: -0.001112407073378563\n",
      "      total_loss: 126.55445098876953\n",
      "      vf_explained_var: 0.7457501292228699\n",
      "      vf_loss: 126.5555648803711\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.387778801460961e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.184677004814148\n",
      "      kl: 0.007442066445946693\n",
      "      policy_loss: -0.003417262574657798\n",
      "      total_loss: 175.16612243652344\n",
      "      vf_explained_var: 0.9270222187042236\n",
      "      vf_loss: 175.16957092285156\n",
      "    sample_time_ms: 20671.914\n",
      "    update_time_ms: 7.034\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.264770353835992\n",
      "    rl_1: 83.63811166255958\n",
      "  time_since_restore: 1717.1111207008362\n",
      "  time_this_iter_s: 23.745481967926025\n",
      "  time_total_s: 1717.1111207008362\n",
      "  timestamp: 1550795122\n",
      "  timesteps_since_restore: 660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 66\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1717 s, 66 iter, 660000 ts, 113 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-25-46\n",
      "  done: false\n",
      "  episode_len_mean: 101.23762376237623\n",
      "  episode_reward_max: 186.72844847011942\n",
      "  episode_reward_mean: 103.63875012296444\n",
      "  episode_reward_min: -165.53898428565427\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 4483\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3252.812\n",
      "    load_time_ms: 2.306\n",
      "    num_steps_sampled: 670000\n",
      "    num_steps_trained: 670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842021886413758e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2587469816207886\n",
      "      kl: 0.005999736487865448\n",
      "      policy_loss: -0.002617759397253394\n",
      "      total_loss: 177.23377990722656\n",
      "      vf_explained_var: 0.7004461288452148\n",
      "      vf_loss: 177.23643493652344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.938894007304805e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0501749515533447\n",
      "      kl: 0.0069020590744912624\n",
      "      policy_loss: -0.0018085885094478726\n",
      "      total_loss: 192.28546142578125\n",
      "      vf_explained_var: 0.9019456505775452\n",
      "      vf_loss: 192.2872772216797\n",
      "    sample_time_ms: 20674.591\n",
      "    update_time_ms: 6.826\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.051007855088827\n",
      "    rl_1: 80.58774226787561\n",
      "  time_since_restore: 1740.8602149486542\n",
      "  time_this_iter_s: 23.749094247817993\n",
      "  time_total_s: 1740.8602149486542\n",
      "  timestamp: 1550795146\n",
      "  timesteps_since_restore: 670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 670000\n",
      "  training_iteration: 67\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1740 s, 67 iter, 670000 ts, 104 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-26-10\n",
      "  done: false\n",
      "  episode_len_mean: 112.43\n",
      "  episode_reward_max: 192.08953259294222\n",
      "  episode_reward_mean: 127.71333592276193\n",
      "  episode_reward_min: -158.189175043013\n",
      "  episodes_this_iter: 88\n",
      "  episodes_total: 4571\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3253.19\n",
      "    load_time_ms: 2.206\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.421010943206879e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3297693729400635\n",
      "      kl: 0.003301680786535144\n",
      "      policy_loss: -0.0017514752689749002\n",
      "      total_loss: 94.4673843383789\n",
      "      vf_explained_var: 0.799274742603302\n",
      "      vf_loss: 94.46914672851562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.4694470036524025e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1546857357025146\n",
      "      kl: 0.00925852358341217\n",
      "      policy_loss: -0.003115335712209344\n",
      "      total_loss: 139.91494750976562\n",
      "      vf_explained_var: 0.9400824308395386\n",
      "      vf_loss: 139.91807556152344\n",
      "    sample_time_ms: 20635.714\n",
      "    update_time_ms: 6.829\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.76240755019447\n",
      "    rl_1: 91.95092837256747\n",
      "  time_since_restore: 1764.85400223732\n",
      "  time_this_iter_s: 23.99378728866577\n",
      "  time_total_s: 1764.85400223732\n",
      "  timestamp: 1550795170\n",
      "  timesteps_since_restore: 680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 68\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1764 s, 68 iter, 680000 ts, 128 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-26-33\n",
      "  done: false\n",
      "  episode_len_mean: 105.98\n",
      "  episode_reward_max: 191.83140059931267\n",
      "  episode_reward_mean: 130.39035763427503\n",
      "  episode_reward_min: -161.26935681718885\n",
      "  episodes_this_iter: 93\n",
      "  episodes_total: 4664\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3252.939\n",
      "    load_time_ms: 2.219\n",
      "    num_steps_sampled: 690000\n",
      "    num_steps_trained: 690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.7105054716034394e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2837905883789062\n",
      "      kl: 0.009237204678356647\n",
      "      policy_loss: -0.002876357641071081\n",
      "      total_loss: 86.90476989746094\n",
      "      vf_explained_var: 0.7713517546653748\n",
      "      vf_loss: 86.90764617919922\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0972100496292114\n",
      "      kl: 0.007035539485514164\n",
      "      policy_loss: -0.0014508049935102463\n",
      "      total_loss: 143.16690063476562\n",
      "      vf_explained_var: 0.9253970384597778\n",
      "      vf_loss: 143.16835021972656\n",
      "    sample_time_ms: 20646.253\n",
      "    update_time_ms: 6.676\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.71808950736492\n",
      "    rl_1: 93.6722681269101\n",
      "  time_since_restore: 1788.7160639762878\n",
      "  time_this_iter_s: 23.862061738967896\n",
      "  time_total_s: 1788.7160639762878\n",
      "  timestamp: 1550795193\n",
      "  timesteps_since_restore: 690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 690000\n",
      "  training_iteration: 69\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1788 s, 69 iter, 690000 ts, 130 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-26-57\n",
      "  done: false\n",
      "  episode_len_mean: 103.38\n",
      "  episode_reward_max: 192.28956044282694\n",
      "  episode_reward_mean: 106.02320458531928\n",
      "  episode_reward_min: -169.51618376640812\n",
      "  episodes_this_iter: 95\n",
      "  episodes_total: 4759\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3256.346\n",
      "    load_time_ms: 2.283\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3552527358017197e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3520702123641968\n",
      "      kl: 0.0028067873790860176\n",
      "      policy_loss: -0.0009513999102637172\n",
      "      total_loss: 179.3568115234375\n",
      "      vf_explained_var: 0.729738175868988\n",
      "      vf_loss: 179.35775756835938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1433948278427124\n",
      "      kl: 0.0048938631080091\n",
      "      policy_loss: -0.0034879962913691998\n",
      "      total_loss: 230.75531005859375\n",
      "      vf_explained_var: 0.8959527611732483\n",
      "      vf_loss: 230.7587432861328\n",
      "    sample_time_ms: 20643.657\n",
      "    update_time_ms: 6.814\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.04766597559646\n",
      "    rl_1: 77.97553860972282\n",
      "  time_since_restore: 1812.6288304328918\n",
      "  time_this_iter_s: 23.912766456604004\n",
      "  time_total_s: 1812.6288304328918\n",
      "  timestamp: 1550795217\n",
      "  timesteps_since_restore: 700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 70\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1812 s, 70 iter, 700000 ts, 106 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-27-22\n",
      "  done: false\n",
      "  episode_len_mean: 107.51\n",
      "  episode_reward_max: 190.02055617630137\n",
      "  episode_reward_mean: 105.64239605641573\n",
      "  episode_reward_min: -176.77781374154765\n",
      "  episodes_this_iter: 94\n",
      "  episodes_total: 4853\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3256.622\n",
      "    load_time_ms: 2.356\n",
      "    num_steps_sampled: 710000\n",
      "    num_steps_trained: 710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.776263679008599e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.316589593887329\n",
      "      kl: 0.0025800045114010572\n",
      "      policy_loss: -0.0011062180856242776\n",
      "      total_loss: 151.5454864501953\n",
      "      vf_explained_var: 0.7921720743179321\n",
      "      vf_loss: 151.54660034179688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.336808754565503e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1302478313446045\n",
      "      kl: 0.007101891096681356\n",
      "      policy_loss: -0.002259772503748536\n",
      "      total_loss: 190.0620880126953\n",
      "      vf_explained_var: 0.9302328824996948\n",
      "      vf_loss: 190.0643310546875\n",
      "    sample_time_ms: 20627.693\n",
      "    update_time_ms: 6.847\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.41317957347469\n",
      "    rl_1: 79.22921648294106\n",
      "  time_since_restore: 1836.695360660553\n",
      "  time_this_iter_s: 24.066530227661133\n",
      "  time_total_s: 1836.695360660553\n",
      "  timestamp: 1550795242\n",
      "  timesteps_since_restore: 710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 710000\n",
      "  training_iteration: 71\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1836 s, 71 iter, 710000 ts, 106 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-27-45\n",
      "  done: false\n",
      "  episode_len_mean: 118.21\n",
      "  episode_reward_max: 190.76074678424803\n",
      "  episode_reward_mean: 114.1709668596854\n",
      "  episode_reward_min: -165.43780778166632\n",
      "  episodes_this_iter: 85\n",
      "  episodes_total: 4938\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3260.15\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.3881318395042993e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.361517071723938\n",
      "      kl: 0.008655151352286339\n",
      "      policy_loss: -0.0037460613530129194\n",
      "      total_loss: 98.27693939208984\n",
      "      vf_explained_var: 0.7976440787315369\n",
      "      vf_loss: 98.28067016601562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.1684043772827515e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2085447311401367\n",
      "      kl: 0.004360762890428305\n",
      "      policy_loss: -0.003378122579306364\n",
      "      total_loss: 130.7518310546875\n",
      "      vf_explained_var: 0.9368818402290344\n",
      "      vf_loss: 130.75518798828125\n",
      "    sample_time_ms: 20610.041\n",
      "    update_time_ms: 6.987\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.01460830101686\n",
      "    rl_1: 85.15635855866853\n",
      "  time_since_restore: 1860.2704980373383\n",
      "  time_this_iter_s: 23.57513737678528\n",
      "  time_total_s: 1860.2704980373383\n",
      "  timestamp: 1550795265\n",
      "  timesteps_since_restore: 720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 72\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1860 s, 72 iter, 720000 ts, 114 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-28-09\n",
      "  done: false\n",
      "  episode_len_mean: 109.38\n",
      "  episode_reward_max: 191.94084792080795\n",
      "  episode_reward_mean: 135.94704643652318\n",
      "  episode_reward_min: -169.9590952013491\n",
      "  episodes_this_iter: 91\n",
      "  episodes_total: 5029\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3261.564\n",
      "    load_time_ms: 2.391\n",
      "    num_steps_sampled: 730000\n",
      "    num_steps_trained: 730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6940659197521496e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.303631067276001\n",
      "      kl: 0.008624080568552017\n",
      "      policy_loss: -0.0024596625007689\n",
      "      total_loss: 82.88935852050781\n",
      "      vf_explained_var: 0.801888108253479\n",
      "      vf_loss: 82.8918228149414\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0842021886413758e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1315032243728638\n",
      "      kl: 0.009538346901535988\n",
      "      policy_loss: -0.002507593482732773\n",
      "      total_loss: 132.4918670654297\n",
      "      vf_explained_var: 0.9269608855247498\n",
      "      vf_loss: 132.49435424804688\n",
      "    sample_time_ms: 20607.358\n",
      "    update_time_ms: 6.998\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.151731025387306\n",
      "    rl_1: 94.79531541113585\n",
      "  time_since_restore: 1884.4587149620056\n",
      "  time_this_iter_s: 24.18821692466736\n",
      "  time_total_s: 1884.4587149620056\n",
      "  timestamp: 1550795289\n",
      "  timesteps_since_restore: 730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 730000\n",
      "  training_iteration: 73\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1884 s, 73 iter, 730000 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-28-34\n",
      "  done: false\n",
      "  episode_len_mean: 106.62\n",
      "  episode_reward_max: 196.436177518453\n",
      "  episode_reward_mean: 123.57768981470112\n",
      "  episode_reward_min: -165.23988951114146\n",
      "  episodes_this_iter: 93\n",
      "  episodes_total: 5122\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3262.198\n",
      "    load_time_ms: 2.431\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.470329598760748e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2624785900115967\n",
      "      kl: 0.004749258514493704\n",
      "      policy_loss: -0.0038384960498660803\n",
      "      total_loss: 135.2014923095703\n",
      "      vf_explained_var: 0.7545996308326721\n",
      "      vf_loss: 135.20533752441406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.421010943206879e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1046228408813477\n",
      "      kl: 0.00806881207972765\n",
      "      policy_loss: -0.004139736760407686\n",
      "      total_loss: 211.70680236816406\n",
      "      vf_explained_var: 0.9063730239868164\n",
      "      vf_loss: 211.71096801757812\n",
      "    sample_time_ms: 20621.023\n",
      "    update_time_ms: 7.035\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.332153083819975\n",
      "    rl_1: 87.24553673088113\n",
      "  time_since_restore: 1908.5637304782867\n",
      "  time_this_iter_s: 24.105015516281128\n",
      "  time_total_s: 1908.5637304782867\n",
      "  timestamp: 1550795314\n",
      "  timesteps_since_restore: 740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 74\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1908 s, 74 iter, 740000 ts, 124 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-28-58\n",
      "  done: false\n",
      "  episode_len_mean: 93.89719626168224\n",
      "  episode_reward_max: 194.61435917495743\n",
      "  episode_reward_mean: 119.91462911940617\n",
      "  episode_reward_min: -164.0726252878084\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 5229\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3263.367\n",
      "    load_time_ms: 2.413\n",
      "    num_steps_sampled: 750000\n",
      "    num_steps_trained: 750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.235164799380374e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.221577525138855\n",
      "      kl: 0.002850819379091263\n",
      "      policy_loss: -0.002334281802177429\n",
      "      total_loss: 136.03639221191406\n",
      "      vf_explained_var: 0.7107134461402893\n",
      "      vf_loss: 136.03871154785156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.7105054716034394e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9880563020706177\n",
      "      kl: 0.00710069527849555\n",
      "      policy_loss: -0.0012572500854730606\n",
      "      total_loss: 163.6891326904297\n",
      "      vf_explained_var: 0.8851711750030518\n",
      "      vf_loss: 163.69039916992188\n",
      "    sample_time_ms: 20657.939\n",
      "    update_time_ms: 7.075\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.101941771667676\n",
      "    rl_1: 86.81268734773853\n",
      "  time_since_restore: 1932.8550131320953\n",
      "  time_this_iter_s: 24.291282653808594\n",
      "  time_total_s: 1932.8550131320953\n",
      "  timestamp: 1550795338\n",
      "  timesteps_since_restore: 750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 750000\n",
      "  training_iteration: 75\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1932 s, 75 iter, 750000 ts, 120 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-29-22\n",
      "  done: false\n",
      "  episode_len_mean: 97.29126213592232\n",
      "  episode_reward_max: 195.26437635877917\n",
      "  episode_reward_mean: 114.45471846889333\n",
      "  episode_reward_min: -156.89852533677896\n",
      "  episodes_this_iter: 103\n",
      "  episodes_total: 5332\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3270.475\n",
      "    load_time_ms: 2.414\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.117582399690187e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2633779048919678\n",
      "      kl: 0.005533247254788876\n",
      "      policy_loss: -0.0012162926141172647\n",
      "      total_loss: 146.1669158935547\n",
      "      vf_explained_var: 0.7514753341674805\n",
      "      vf_loss: 146.1681365966797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.3552527358017197e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0518449544906616\n",
      "      kl: 0.005102523602545261\n",
      "      policy_loss: -0.0017093407223001122\n",
      "      total_loss: 173.1197509765625\n",
      "      vf_explained_var: 0.8973435759544373\n",
      "      vf_loss: 173.12144470214844\n",
      "    sample_time_ms: 20659.445\n",
      "    update_time_ms: 7.175\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.66626687911213\n",
      "    rl_1: 82.7884515897812\n",
      "  time_since_restore: 1956.6890044212341\n",
      "  time_this_iter_s: 23.833991289138794\n",
      "  time_total_s: 1956.6890044212341\n",
      "  timestamp: 1550795362\n",
      "  timesteps_since_restore: 760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 76\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1956 s, 76 iter, 760000 ts, 114 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-29-46\n",
      "  done: false\n",
      "  episode_len_mean: 92.76635514018692\n",
      "  episode_reward_max: 200.08625767968735\n",
      "  episode_reward_mean: 124.43596598010998\n",
      "  episode_reward_min: -174.10413799130652\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 5439\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3272.217\n",
      "    load_time_ms: 2.446\n",
      "    num_steps_sampled: 770000\n",
      "    num_steps_trained: 770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2223188877105713\n",
      "      kl: 0.0025830345693975687\n",
      "      policy_loss: -0.0013720083516091108\n",
      "      total_loss: 120.29341888427734\n",
      "      vf_explained_var: 0.7678002715110779\n",
      "      vf_loss: 120.29476165771484\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.776263679008599e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9940620064735413\n",
      "      kl: 0.010081385262310505\n",
      "      policy_loss: -0.00475286552682519\n",
      "      total_loss: 145.2643585205078\n",
      "      vf_explained_var: 0.9034045338630676\n",
      "      vf_loss: 145.26913452148438\n",
      "    sample_time_ms: 20673.763\n",
      "    update_time_ms: 7.081\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.09681346617766\n",
      "    rl_1: 87.33915251393233\n",
      "  time_since_restore: 1980.5972485542297\n",
      "  time_this_iter_s: 23.908244132995605\n",
      "  time_total_s: 1980.5972485542297\n",
      "  timestamp: 1550795386\n",
      "  timesteps_since_restore: 770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 770000\n",
      "  training_iteration: 77\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 1980 s, 77 iter, 770000 ts, 124 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-30-10\n",
      "  done: false\n",
      "  episode_len_mean: 93.80188679245283\n",
      "  episode_reward_max: 195.45722795055565\n",
      "  episode_reward_mean: 132.64078211471875\n",
      "  episode_reward_min: -169.96454131130304\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 5545\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3274.066\n",
      "    load_time_ms: 2.44\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.171540379524231\n",
      "      kl: 0.0055174692533910275\n",
      "      policy_loss: -0.0026743835769593716\n",
      "      total_loss: 82.4187240600586\n",
      "      vf_explained_var: 0.784387469291687\n",
      "      vf_loss: 82.4214096069336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.776263679008599e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9630587697029114\n",
      "      kl: 0.004756357055157423\n",
      "      policy_loss: -0.0019996140617877245\n",
      "      total_loss: 98.05445861816406\n",
      "      vf_explained_var: 0.9302039742469788\n",
      "      vf_loss: 98.05646514892578\n",
      "    sample_time_ms: 20693.634\n",
      "    update_time_ms: 7.354\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.69389979373338\n",
      "    rl_1: 94.94688232098537\n",
      "  time_since_restore: 2004.8107261657715\n",
      "  time_this_iter_s: 24.213477611541748\n",
      "  time_total_s: 2004.8107261657715\n",
      "  timestamp: 1550795410\n",
      "  timesteps_since_restore: 780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 78\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2004 s, 78 iter, 780000 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-30-34\n",
      "  done: false\n",
      "  episode_len_mean: 90.36936936936937\n",
      "  episode_reward_max: 197.8773084978851\n",
      "  episode_reward_mean: 114.79273159261284\n",
      "  episode_reward_min: -172.50101842625213\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 5656\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3276.22\n",
      "    load_time_ms: 2.439\n",
      "    num_steps_sampled: 790000\n",
      "    num_steps_trained: 790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.646977999612734e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2335230112075806\n",
      "      kl: 0.007749979384243488\n",
      "      policy_loss: -0.0026471829041838646\n",
      "      total_loss: 199.0579071044922\n",
      "      vf_explained_var: 0.7034131288528442\n",
      "      vf_loss: 199.06056213378906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.3881318395042993e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0044341087341309\n",
      "      kl: 0.007515200413763523\n",
      "      policy_loss: -0.0030841880943626165\n",
      "      total_loss: 257.6041564941406\n",
      "      vf_explained_var: 0.8427732586860657\n",
      "      vf_loss: 257.6072082519531\n",
      "    sample_time_ms: 20724.474\n",
      "    update_time_ms: 7.106\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.66316376348684\n",
      "    rl_1: 82.12956782912595\n",
      "  time_since_restore: 2028.9998512268066\n",
      "  time_this_iter_s: 24.189125061035156\n",
      "  time_total_s: 2028.9998512268066\n",
      "  timestamp: 1550795434\n",
      "  timesteps_since_restore: 790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 790000\n",
      "  training_iteration: 79\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2028 s, 79 iter, 790000 ts, 115 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-30-59\n",
      "  done: false\n",
      "  episode_len_mean: 106.88\n",
      "  episode_reward_max: 197.80522678870406\n",
      "  episode_reward_mean: 137.96776481653148\n",
      "  episode_reward_min: -155.4265601379507\n",
      "  episodes_this_iter: 94\n",
      "  episodes_total: 5750\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3273.581\n",
      "    load_time_ms: 2.401\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.323488999806367e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3067679405212402\n",
      "      kl: 0.004810798447579145\n",
      "      policy_loss: -0.0020466588903218508\n",
      "      total_loss: 66.1396713256836\n",
      "      vf_explained_var: 0.85757976770401\n",
      "      vf_loss: 66.14170837402344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.6940659197521496e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.128906488418579\n",
      "      kl: 0.009668386541306973\n",
      "      policy_loss: -0.00286370818503201\n",
      "      total_loss: 81.45635223388672\n",
      "      vf_explained_var: 0.9486082196235657\n",
      "      vf_loss: 81.45921325683594\n",
      "    sample_time_ms: 20770.854\n",
      "    update_time_ms: 7.087\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.60999746923583\n",
      "    rl_1: 95.35776734729565\n",
      "  time_since_restore: 2053.350162744522\n",
      "  time_this_iter_s: 24.350311517715454\n",
      "  time_total_s: 2053.350162744522\n",
      "  timestamp: 1550795459\n",
      "  timesteps_since_restore: 800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 80\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2053 s, 80 iter, 800000 ts, 138 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-31-23\n",
      "  done: false\n",
      "  episode_len_mean: 95.07619047619048\n",
      "  episode_reward_max: 200.5840476563429\n",
      "  episode_reward_mean: 136.52228516761386\n",
      "  episode_reward_min: -173.8046117223853\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 5855\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3265.789\n",
      "    load_time_ms: 2.35\n",
      "    num_steps_sampled: 810000\n",
      "    num_steps_trained: 810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.617444999031835e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.175653100013733\n",
      "      kl: 0.00391713622957468\n",
      "      policy_loss: -0.0029029028955847025\n",
      "      total_loss: 100.86310577392578\n",
      "      vf_explained_var: 0.7232866287231445\n",
      "      vf_loss: 100.86602020263672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.470329598760748e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9743189811706543\n",
      "      kl: 0.012395997531712055\n",
      "      policy_loss: -0.003471587784588337\n",
      "      total_loss: 171.83106994628906\n",
      "      vf_explained_var: 0.8517321944236755\n",
      "      vf_loss: 171.8345489501953\n",
      "    sample_time_ms: 20768.573\n",
      "    update_time_ms: 7.017\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.03661345454566\n",
      "    rl_1: 95.48567171306819\n",
      "  time_since_restore: 2077.312003850937\n",
      "  time_this_iter_s: 23.961841106414795\n",
      "  time_total_s: 2077.312003850937\n",
      "  timestamp: 1550795483\n",
      "  timesteps_since_restore: 810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 810000\n",
      "  training_iteration: 81\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2077 s, 81 iter, 810000 ts, 137 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-31-47\n",
      "  done: false\n",
      "  episode_len_mean: 94.95238095238095\n",
      "  episode_reward_max: 201.98946280143088\n",
      "  episode_reward_mean: 130.1076338466618\n",
      "  episode_reward_min: -164.69141370780798\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 5960\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3265.737\n",
      "    load_time_ms: 2.306\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.3087224995159173e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1814920902252197\n",
      "      kl: 0.0043813688680529594\n",
      "      policy_loss: -0.0013309870846569538\n",
      "      total_loss: 77.66622924804688\n",
      "      vf_explained_var: 0.856463611125946\n",
      "      vf_loss: 77.66755676269531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.470329598760748e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9868150353431702\n",
      "      kl: 0.007460181601345539\n",
      "      policy_loss: -0.0028063880745321512\n",
      "      total_loss: 60.60488510131836\n",
      "      vf_explained_var: 0.9638940691947937\n",
      "      vf_loss: 60.60769271850586\n",
      "    sample_time_ms: 20808.604\n",
      "    update_time_ms: 7.137\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.9480714765019\n",
      "    rl_1: 92.1595623701599\n",
      "  time_since_restore: 2101.288988351822\n",
      "  time_this_iter_s: 23.97698450088501\n",
      "  time_total_s: 2101.288988351822\n",
      "  timestamp: 1550795507\n",
      "  timesteps_since_restore: 820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 82\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2101 s, 82 iter, 820000 ts, 130 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-32-11\n",
      "  done: false\n",
      "  episode_len_mean: 91.21818181818182\n",
      "  episode_reward_max: 204.64775700958322\n",
      "  episode_reward_mean: 122.71566876417933\n",
      "  episode_reward_min: -168.60277750371242\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 6070\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3267.626\n",
      "    load_time_ms: 2.31\n",
      "    num_steps_sampled: 830000\n",
      "    num_steps_trained: 830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6543612497579586e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1520847082138062\n",
      "      kl: 0.0050807856023311615\n",
      "      policy_loss: -0.0032432572916150093\n",
      "      total_loss: 150.89805603027344\n",
      "      vf_explained_var: 0.724803626537323\n",
      "      vf_loss: 150.90130615234375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.235164799380374e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9463725686073303\n",
      "      kl: 0.0066441986709833145\n",
      "      policy_loss: -0.0013509190175682306\n",
      "      total_loss: 191.156494140625\n",
      "      vf_explained_var: 0.8788542151451111\n",
      "      vf_loss: 191.15785217285156\n",
      "    sample_time_ms: 20799.353\n",
      "    update_time_ms: 7.035\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.54494895088891\n",
      "    rl_1: 88.1707198132904\n",
      "  time_since_restore: 2125.4034509658813\n",
      "  time_this_iter_s: 24.11446261405945\n",
      "  time_total_s: 2125.4034509658813\n",
      "  timestamp: 1550795531\n",
      "  timesteps_since_restore: 830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 830000\n",
      "  training_iteration: 83\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2125 s, 83 iter, 830000 ts, 123 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-32-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.06\n",
      "  episode_reward_max: 198.06856261482415\n",
      "  episode_reward_mean: 147.83907425343668\n",
      "  episode_reward_min: -159.8349731682063\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 6170\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3265.331\n",
      "    load_time_ms: 2.287\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.271806248789793e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1986404657363892\n",
      "      kl: 0.009201409295201302\n",
      "      policy_loss: -0.0034872067626565695\n",
      "      total_loss: 27.35628318786621\n",
      "      vf_explained_var: 0.909456193447113\n",
      "      vf_loss: 27.359771728515625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.117582399690187e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0196212530136108\n",
      "      kl: 0.00658034672960639\n",
      "      policy_loss: -0.0028504079673439264\n",
      "      total_loss: 55.12516403198242\n",
      "      vf_explained_var: 0.9542509913444519\n",
      "      vf_loss: 55.12801742553711\n",
      "    sample_time_ms: 20783.114\n",
      "    update_time_ms: 7.103\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.3923401278087\n",
      "    rl_1: 102.44673412562797\n",
      "  time_since_restore: 2149.32386469841\n",
      "  time_this_iter_s: 23.920413732528687\n",
      "  time_total_s: 2149.32386469841\n",
      "  timestamp: 1550795555\n",
      "  timesteps_since_restore: 840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 84\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2149 s, 84 iter, 840000 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-32-59\n",
      "  done: false\n",
      "  episode_len_mean: 95.73076923076923\n",
      "  episode_reward_max: 197.625680390365\n",
      "  episode_reward_mean: 136.3229222583298\n",
      "  episode_reward_min: -169.6557618109527\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 6274\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3265.646\n",
      "    load_time_ms: 2.385\n",
      "    num_steps_sampled: 850000\n",
      "    num_steps_trained: 850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.1359031243948966e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1536918878555298\n",
      "      kl: 0.006888661067932844\n",
      "      policy_loss: -0.003233571071177721\n",
      "      total_loss: 73.61267852783203\n",
      "      vf_explained_var: 0.8428335785865784\n",
      "      vf_loss: 73.61591339111328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9774201512336731\n",
      "      kl: 0.00606972211971879\n",
      "      policy_loss: -0.0012218812480568886\n",
      "      total_loss: 101.85419464111328\n",
      "      vf_explained_var: 0.9341934323310852\n",
      "      vf_loss: 101.85541534423828\n",
      "    sample_time_ms: 20765.093\n",
      "    update_time_ms: 7.127\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.73865265068635\n",
      "    rl_1: 96.58426960764346\n",
      "  time_since_restore: 2173.4421417713165\n",
      "  time_this_iter_s: 24.118277072906494\n",
      "  time_total_s: 2173.4421417713165\n",
      "  timestamp: 1550795579\n",
      "  timesteps_since_restore: 850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 850000\n",
      "  training_iteration: 85\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2173 s, 85 iter, 850000 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-33-23\n",
      "  done: false\n",
      "  episode_len_mean: 99.18811881188118\n",
      "  episode_reward_max: 202.08121880806405\n",
      "  episode_reward_mean: 151.72984314039365\n",
      "  episode_reward_min: -150.45556911487574\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 6375\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3247.331\n",
      "    load_time_ms: 2.412\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.0679515621974483e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1817958354949951\n",
      "      kl: 0.008518842048943043\n",
      "      policy_loss: -0.0022946568205952644\n",
      "      total_loss: 53.46144485473633\n",
      "      vf_explained_var: 0.8526107668876648\n",
      "      vf_loss: 53.463741302490234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9938066005706787\n",
      "      kl: 0.00705860136076808\n",
      "      policy_loss: -0.0032251805532723665\n",
      "      total_loss: 108.56700897216797\n",
      "      vf_explained_var: 0.914263129234314\n",
      "      vf_loss: 108.57025146484375\n",
      "    sample_time_ms: 20803.982\n",
      "    update_time_ms: 7.202\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.556259330416594\n",
      "    rl_1: 103.17358380997707\n",
      "  time_since_restore: 2197.4836690425873\n",
      "  time_this_iter_s: 24.041527271270752\n",
      "  time_total_s: 2197.4836690425873\n",
      "  timestamp: 1550795603\n",
      "  timesteps_since_restore: 860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 86\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2197 s, 86 iter, 860000 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-33-47\n",
      "  done: false\n",
      "  episode_len_mean: 95.66346153846153\n",
      "  episode_reward_max: 192.00714560664403\n",
      "  episode_reward_mean: 144.82075813198642\n",
      "  episode_reward_min: -158.57068884760787\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 6479\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3249.618\n",
      "    load_time_ms: 2.365\n",
      "    num_steps_sampled: 870000\n",
      "    num_steps_trained: 870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0339757810987241e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1037322282791138\n",
      "      kl: 0.011658399365842342\n",
      "      policy_loss: -0.0023663772735744715\n",
      "      total_loss: 29.166982650756836\n",
      "      vf_explained_var: 0.9093634486198425\n",
      "      vf_loss: 29.169342041015625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.646977999612734e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9461255073547363\n",
      "      kl: 0.008020356297492981\n",
      "      policy_loss: -0.002849191427230835\n",
      "      total_loss: 49.61269760131836\n",
      "      vf_explained_var: 0.9610624313354492\n",
      "      vf_loss: 49.615535736083984\n",
      "    sample_time_ms: 20830.072\n",
      "    update_time_ms: 7.399\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.556464175106186\n",
      "    rl_1: 101.26429395688022\n",
      "  time_since_restore: 2221.6777050495148\n",
      "  time_this_iter_s: 24.19403600692749\n",
      "  time_total_s: 2221.6777050495148\n",
      "  timestamp: 1550795627\n",
      "  timesteps_since_restore: 870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 870000\n",
      "  training_iteration: 87\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2221 s, 87 iter, 870000 ts, 145 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-34-11\n",
      "  done: false\n",
      "  episode_len_mean: 92.71559633027523\n",
      "  episode_reward_max: 202.17123603490424\n",
      "  episode_reward_mean: 133.60202049536517\n",
      "  episode_reward_min: -153.9960990395907\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 6588\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3250.532\n",
      "    load_time_ms: 2.434\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0339757810987241e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.124837875366211\n",
      "      kl: 0.005067326128482819\n",
      "      policy_loss: -0.003991350531578064\n",
      "      total_loss: 104.79329681396484\n",
      "      vf_explained_var: 0.7532781958580017\n",
      "      vf_loss: 104.79728698730469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.323488999806367e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9592978358268738\n",
      "      kl: 0.0049400790594518185\n",
      "      policy_loss: -0.0017333480063825846\n",
      "      total_loss: 147.37850952148438\n",
      "      vf_explained_var: 0.8794620037078857\n",
      "      vf_loss: 147.38023376464844\n",
      "    sample_time_ms: 20805.565\n",
      "    update_time_ms: 7.127\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.793412091898716\n",
      "    rl_1: 94.80860840346645\n",
      "  time_since_restore: 2245.6548159122467\n",
      "  time_this_iter_s: 23.977110862731934\n",
      "  time_total_s: 2245.6548159122467\n",
      "  timestamp: 1550795651\n",
      "  timesteps_since_restore: 880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 88\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2245 s, 88 iter, 880000 ts, 134 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-34-36\n",
      "  done: false\n",
      "  episode_len_mean: 92.3047619047619\n",
      "  episode_reward_max: 195.17073899305404\n",
      "  episode_reward_mean: 136.1734919158388\n",
      "  episode_reward_min: -150.4930376348261\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 6693\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3254.159\n",
      "    load_time_ms: 2.564\n",
      "    num_steps_sampled: 890000\n",
      "    num_steps_trained: 890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.169878905493621e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.150177240371704\n",
      "      kl: 0.0062283119186758995\n",
      "      policy_loss: -0.0024819010868668556\n",
      "      total_loss: 55.6984748840332\n",
      "      vf_explained_var: 0.8735887408256531\n",
      "      vf_loss: 55.70095443725586\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.617444999031835e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9557042121887207\n",
      "      kl: 0.007158887106925249\n",
      "      policy_loss: -0.002355454256758094\n",
      "      total_loss: 59.91703414916992\n",
      "      vf_explained_var: 0.9574121236801147\n",
      "      vf_loss: 59.91940689086914\n",
      "    sample_time_ms: 20822.276\n",
      "    update_time_ms: 7.259\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.42033272413321\n",
      "    rl_1: 96.7531591917056\n",
      "  time_since_restore: 2270.0502190589905\n",
      "  time_this_iter_s: 24.395403146743774\n",
      "  time_total_s: 2270.0502190589905\n",
      "  timestamp: 1550795676\n",
      "  timesteps_since_restore: 890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 890000\n",
      "  training_iteration: 89\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2270 s, 89 iter, 890000 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-35-00\n",
      "  done: false\n",
      "  episode_len_mean: 95.61682242990655\n",
      "  episode_reward_max: 200.5341628117068\n",
      "  episode_reward_mean: 136.1765847871039\n",
      "  episode_reward_min: -161.97623648490605\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 6800\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3257.665\n",
      "    load_time_ms: 2.644\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5849394527468104e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1423107385635376\n",
      "      kl: 0.008469866588711739\n",
      "      policy_loss: -0.002818414242938161\n",
      "      total_loss: 61.86900329589844\n",
      "      vf_explained_var: 0.8621532917022705\n",
      "      vf_loss: 61.871826171875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.3087224995159173e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9665327668190002\n",
      "      kl: 0.0051428875885903835\n",
      "      policy_loss: -0.0015824889997020364\n",
      "      total_loss: 62.522918701171875\n",
      "      vf_explained_var: 0.9540823698043823\n",
      "      vf_loss: 62.52450180053711\n",
      "    sample_time_ms: 20798.892\n",
      "    update_time_ms: 7.252\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.239540844804466\n",
      "    rl_1: 96.93704394229944\n",
      "  time_since_restore: 2294.203774213791\n",
      "  time_this_iter_s: 24.153555154800415\n",
      "  time_total_s: 2294.203774213791\n",
      "  timestamp: 1550795700\n",
      "  timesteps_since_restore: 900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 90\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2294 s, 90 iter, 900000 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-35-24\n",
      "  done: false\n",
      "  episode_len_mean: 89.85714285714286\n",
      "  episode_reward_max: 200.39044507920093\n",
      "  episode_reward_mean: 122.15965072779201\n",
      "  episode_reward_min: -168.321071067179\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 6912\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3258.342\n",
      "    load_time_ms: 2.699\n",
      "    num_steps_sampled: 910000\n",
      "    num_steps_trained: 910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2924697263734052e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1147903203964233\n",
      "      kl: 0.004592789337038994\n",
      "      policy_loss: -0.002086315071210265\n",
      "      total_loss: 109.97956848144531\n",
      "      vf_explained_var: 0.8180963397026062\n",
      "      vf_loss: 109.98165893554688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.6543612497579586e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9367655515670776\n",
      "      kl: 0.010652625001966953\n",
      "      policy_loss: -0.0038827573880553246\n",
      "      total_loss: 112.20724487304688\n",
      "      vf_explained_var: 0.9289820790290833\n",
      "      vf_loss: 112.21112823486328\n",
      "    sample_time_ms: 20812.975\n",
      "    update_time_ms: 7.269\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.345098363571516\n",
      "    rl_1: 87.81455236422049\n",
      "  time_since_restore: 2318.3142676353455\n",
      "  time_this_iter_s: 24.110493421554565\n",
      "  time_total_s: 2318.3142676353455\n",
      "  timestamp: 1550795724\n",
      "  timesteps_since_restore: 910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 910000\n",
      "  training_iteration: 91\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2318 s, 91 iter, 910000 ts, 122 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-35-49\n",
      "  done: false\n",
      "  episode_len_mean: 107.18\n",
      "  episode_reward_max: 195.31113679484076\n",
      "  episode_reward_mean: 132.6754884526561\n",
      "  episode_reward_min: -149.89964330467882\n",
      "  episodes_this_iter: 92\n",
      "  episodes_total: 7004\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3257.517\n",
      "    load_time_ms: 2.713\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.462348631867026e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2907685041427612\n",
      "      kl: 0.004733544308692217\n",
      "      policy_loss: -0.002028296934440732\n",
      "      total_loss: 62.25399398803711\n",
      "      vf_explained_var: 0.9006587266921997\n",
      "      vf_loss: 62.25602340698242\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.6543612497579586e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1365331411361694\n",
      "      kl: 0.004896215163171291\n",
      "      policy_loss: -0.000720228417776525\n",
      "      total_loss: 94.1982192993164\n",
      "      vf_explained_var: 0.9577990770339966\n",
      "      vf_loss: 94.19892883300781\n",
      "    sample_time_ms: 20879.844\n",
      "    update_time_ms: 7.302\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.11175657482181\n",
      "    rl_1: 94.56373187783429\n",
      "  time_since_restore: 2342.95800113678\n",
      "  time_this_iter_s: 24.643733501434326\n",
      "  time_total_s: 2342.95800113678\n",
      "  timestamp: 1550795749\n",
      "  timesteps_since_restore: 920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 92\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2342 s, 92 iter, 920000 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-36-13\n",
      "  done: false\n",
      "  episode_len_mean: 108.55\n",
      "  episode_reward_max: 191.2441481190381\n",
      "  episode_reward_mean: 144.01995018525952\n",
      "  episode_reward_min: -148.4971057362997\n",
      "  episodes_this_iter: 90\n",
      "  episodes_total: 7094\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3249.708\n",
      "    load_time_ms: 2.75\n",
      "    num_steps_sampled: 930000\n",
      "    num_steps_trained: 930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.231174315933513e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2625993490219116\n",
      "      kl: 0.0073403287678956985\n",
      "      policy_loss: -0.0038743014447391033\n",
      "      total_loss: 30.887109756469727\n",
      "      vf_explained_var: 0.9136428833007812\n",
      "      vf_loss: 30.890987396240234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.271806248789793e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1019670963287354\n",
      "      kl: 0.007580359000712633\n",
      "      policy_loss: -0.003302423981949687\n",
      "      total_loss: 69.4770278930664\n",
      "      vf_explained_var: 0.9534141421318054\n",
      "      vf_loss: 69.48033905029297\n",
      "    sample_time_ms: 20847.241\n",
      "    update_time_ms: 7.356\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.06512162450264\n",
      "    rl_1: 100.95482856075694\n",
      "  time_since_restore: 2366.6677646636963\n",
      "  time_this_iter_s: 23.709763526916504\n",
      "  time_total_s: 2366.6677646636963\n",
      "  timestamp: 1550795773\n",
      "  timesteps_since_restore: 930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 930000\n",
      "  training_iteration: 93\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2366 s, 93 iter, 930000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-36-36\n",
      "  done: false\n",
      "  episode_len_mean: 91.1743119266055\n",
      "  episode_reward_max: 199.36890464447242\n",
      "  episode_reward_mean: 141.04208254945323\n",
      "  episode_reward_min: -167.52930364765956\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 7203\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3246.378\n",
      "    load_time_ms: 2.735\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6155871579667565e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.105676531791687\n",
      "      kl: 0.0025705136358737946\n",
      "      policy_loss: -0.0018417558167129755\n",
      "      total_loss: 51.89979553222656\n",
      "      vf_explained_var: 0.892915666103363\n",
      "      vf_loss: 51.90163803100586\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.1359031243948966e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9069390892982483\n",
      "      kl: 0.005235534161329269\n",
      "      policy_loss: -0.0033695774618536234\n",
      "      total_loss: 51.71928024291992\n",
      "      vf_explained_var: 0.961281418800354\n",
      "      vf_loss: 51.72265625\n",
      "    sample_time_ms: 20827.38\n",
      "    update_time_ms: 7.162\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.86215765962284\n",
      "    rl_1: 98.1799248898304\n",
      "  time_since_restore: 2390.3532421588898\n",
      "  time_this_iter_s: 23.68547749519348\n",
      "  time_total_s: 2390.3532421588898\n",
      "  timestamp: 1550795796\n",
      "  timesteps_since_restore: 940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 94\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2390 s, 94 iter, 940000 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-37-00\n",
      "  done: false\n",
      "  episode_len_mean: 92.3302752293578\n",
      "  episode_reward_max: 202.27794971693302\n",
      "  episode_reward_mean: 135.2832726993241\n",
      "  episode_reward_min: -173.24682042904897\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 7312\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3243.943\n",
      "    load_time_ms: 2.624\n",
      "    num_steps_sampled: 950000\n",
      "    num_steps_trained: 950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.077935789833782e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0847843885421753\n",
      "      kl: 0.008772800676524639\n",
      "      policy_loss: -0.0015778776723891497\n",
      "      total_loss: 63.33479690551758\n",
      "      vf_explained_var: 0.870486319065094\n",
      "      vf_loss: 63.33637619018555\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.0679515621974483e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9220436215400696\n",
      "      kl: 0.0030210085678845644\n",
      "      policy_loss: -0.0022428978700190783\n",
      "      total_loss: 79.76710510253906\n",
      "      vf_explained_var: 0.938426673412323\n",
      "      vf_loss: 79.76935577392578\n",
      "    sample_time_ms: 20790.334\n",
      "    update_time_ms: 7.092\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.06573050878693\n",
      "    rl_1: 96.21754219053719\n",
      "  time_since_restore: 2414.073162317276\n",
      "  time_this_iter_s: 23.71992015838623\n",
      "  time_total_s: 2414.073162317276\n",
      "  timestamp: 1550795820\n",
      "  timesteps_since_restore: 950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 950000\n",
      "  training_iteration: 95\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2414 s, 95 iter, 950000 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-37-24\n",
      "  done: false\n",
      "  episode_len_mean: 89.49107142857143\n",
      "  episode_reward_max: 200.35377262499094\n",
      "  episode_reward_mean: 130.17239953441452\n",
      "  episode_reward_min: -148.66977406551152\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 7424\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3240.311\n",
      "    load_time_ms: 2.617\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.038967894916891e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0552215576171875\n",
      "      kl: 0.006721074227243662\n",
      "      policy_loss: -0.003683962859213352\n",
      "      total_loss: 61.29729461669922\n",
      "      vf_explained_var: 0.8852822780609131\n",
      "      vf_loss: 61.300994873046875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0339757810987241e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8989735245704651\n",
      "      kl: 0.006367349997162819\n",
      "      policy_loss: -0.0008441796526312828\n",
      "      total_loss: 69.44756317138672\n",
      "      vf_explained_var: 0.9534311294555664\n",
      "      vf_loss: 69.44840240478516\n",
      "    sample_time_ms: 20771.499\n",
      "    update_time_ms: 7.185\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.30956887469269\n",
      "    rl_1: 91.86283065972181\n",
      "  time_since_restore: 2437.8896498680115\n",
      "  time_this_iter_s: 23.816487550735474\n",
      "  time_total_s: 2437.8896498680115\n",
      "  timestamp: 1550795844\n",
      "  timesteps_since_restore: 960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 96\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2437 s, 96 iter, 960000 ts, 130 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-37-48\n",
      "  done: false\n",
      "  episode_len_mean: 98.56862745098039\n",
      "  episode_reward_max: 199.8189253172687\n",
      "  episode_reward_mean: 132.77466856174962\n",
      "  episode_reward_min: -152.054279697546\n",
      "  episodes_this_iter: 102\n",
      "  episodes_total: 7526\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3234.265\n",
      "    load_time_ms: 2.665\n",
      "    num_steps_sampled: 970000\n",
      "    num_steps_trained: 970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.0194839474584456e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1390224695205688\n",
      "      kl: 0.0033717930782586336\n",
      "      policy_loss: -0.0005395626649260521\n",
      "      total_loss: 104.80379486083984\n",
      "      vf_explained_var: 0.7726329565048218\n",
      "      vf_loss: 104.80433654785156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.169878905493621e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0078853368759155\n",
      "      kl: 0.004551474936306477\n",
      "      policy_loss: -0.0023024023976176977\n",
      "      total_loss: 163.2009735107422\n",
      "      vf_explained_var: 0.8835425972938538\n",
      "      vf_loss: 163.20326232910156\n",
      "    sample_time_ms: 20719.53\n",
      "    update_time_ms: 7.007\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.974259011857306\n",
      "    rl_1: 93.80040954989228\n",
      "  time_since_restore: 2461.500827550888\n",
      "  time_this_iter_s: 23.611177682876587\n",
      "  time_total_s: 2461.500827550888\n",
      "  timestamp: 1550795868\n",
      "  timesteps_since_restore: 970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 970000\n",
      "  training_iteration: 97\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2461 s, 97 iter, 970000 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-38-12\n",
      "  done: false\n",
      "  episode_len_mean: 99.61\n",
      "  episode_reward_max: 204.15122733052905\n",
      "  episode_reward_mean: 132.78794736531364\n",
      "  episode_reward_min: -163.83269020506788\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 7626\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3228.285\n",
      "    load_time_ms: 2.596\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0097419737292228e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1558592319488525\n",
      "      kl: 0.005969494581222534\n",
      "      policy_loss: -0.0023936766665428877\n",
      "      total_loss: 64.58219146728516\n",
      "      vf_explained_var: 0.885518491268158\n",
      "      vf_loss: 64.58456420898438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.5849394527468104e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0033133029937744\n",
      "      kl: 0.004868845455348492\n",
      "      policy_loss: -0.0013906577369198203\n",
      "      total_loss: 75.86865234375\n",
      "      vf_explained_var: 0.9490016102790833\n",
      "      vf_loss: 75.87004089355469\n",
      "    sample_time_ms: 20724.39\n",
      "    update_time_ms: 7.171\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.447904367623366\n",
      "    rl_1: 93.34004299769028\n",
      "  time_since_restore: 2485.4700667858124\n",
      "  time_this_iter_s: 23.969239234924316\n",
      "  time_total_s: 2485.4700667858124\n",
      "  timestamp: 1550795892\n",
      "  timesteps_since_restore: 980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 98\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2485 s, 98 iter, 980000 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-38-36\n",
      "  done: false\n",
      "  episode_len_mean: 91.06422018348624\n",
      "  episode_reward_max: 203.26718649168893\n",
      "  episode_reward_mean: 142.57819416351532\n",
      "  episode_reward_min: -160.14366618273874\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 7735\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3222.151\n",
      "    load_time_ms: 2.553\n",
      "    num_steps_sampled: 990000\n",
      "    num_steps_trained: 990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.048709868646114e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.984801173210144\n",
      "      kl: 0.0055767991580069065\n",
      "      policy_loss: -0.001922188326716423\n",
      "      total_loss: 104.2891616821289\n",
      "      vf_explained_var: 0.8028564453125\n",
      "      vf_loss: 104.29106903076172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2924697263734052e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8766613602638245\n",
      "      kl: 0.00928780622780323\n",
      "      policy_loss: -0.002020208165049553\n",
      "      total_loss: 167.238037109375\n",
      "      vf_explained_var: 0.874893307685852\n",
      "      vf_loss: 167.2400665283203\n",
      "    sample_time_ms: 20677.098\n",
      "    update_time_ms: 7.337\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.29707265465454\n",
      "    rl_1: 97.28112150886078\n",
      "  time_since_restore: 2509.331799507141\n",
      "  time_this_iter_s: 23.861732721328735\n",
      "  time_total_s: 2509.331799507141\n",
      "  timestamp: 1550795916\n",
      "  timesteps_since_restore: 990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 990000\n",
      "  training_iteration: 99\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2509 s, 99 iter, 990000 ts, 143 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-39-00\n",
      "  done: false\n",
      "  episode_len_mean: 95.35238095238095\n",
      "  episode_reward_max: 199.7523566971562\n",
      "  episode_reward_mean: 143.77085820215322\n",
      "  episode_reward_min: -157.4879959620276\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 7840\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3217.093\n",
      "    load_time_ms: 2.491\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.524354934323057e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0698421001434326\n",
      "      kl: 0.0029913298785686493\n",
      "      policy_loss: -0.0027880528941750526\n",
      "      total_loss: 43.55448532104492\n",
      "      vf_explained_var: 0.9046484231948853\n",
      "      vf_loss: 43.55727767944336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.462348631867026e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9371418952941895\n",
      "      kl: 0.0044981446117162704\n",
      "      policy_loss: -0.0024345479905605316\n",
      "      total_loss: 60.42748260498047\n",
      "      vf_explained_var: 0.9513218998908997\n",
      "      vf_loss: 60.4299201965332\n",
      "    sample_time_ms: 20678.567\n",
      "    update_time_ms: 7.262\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.7418177915981\n",
      "    rl_1: 99.02904041055513\n",
      "  time_since_restore: 2533.444199323654\n",
      "  time_this_iter_s: 24.11239981651306\n",
      "  time_total_s: 2533.444199323654\n",
      "  timestamp: 1550795940\n",
      "  timesteps_since_restore: 1000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 100\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2533 s, 100 iter, 1000000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-39-24\n",
      "  done: false\n",
      "  episode_len_mean: 97.37864077669903\n",
      "  episode_reward_max: 203.19209754022796\n",
      "  episode_reward_mean: 139.9802772139809\n",
      "  episode_reward_min: -158.1256671278815\n",
      "  episodes_this_iter: 103\n",
      "  episodes_total: 7943\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3213.942\n",
      "    load_time_ms: 2.419\n",
      "    num_steps_sampled: 1010000\n",
      "    num_steps_trained: 1010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2621774671615285e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0277373790740967\n",
      "      kl: 0.005806515458971262\n",
      "      policy_loss: -0.0033538455609232187\n",
      "      total_loss: 45.26683044433594\n",
      "      vf_explained_var: 0.9113024473190308\n",
      "      vf_loss: 45.27018356323242\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.231174315933513e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9272990822792053\n",
      "      kl: 0.00821128860116005\n",
      "      policy_loss: -0.0030721777584403753\n",
      "      total_loss: 50.119361877441406\n",
      "      vf_explained_var: 0.9636561870574951\n",
      "      vf_loss: 50.122440338134766\n",
      "    sample_time_ms: 20667.892\n",
      "    update_time_ms: 7.435\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.31267586717312\n",
      "    rl_1: 98.66760134680777\n",
      "  time_since_restore: 2557.417450428009\n",
      "  time_this_iter_s: 23.97325110435486\n",
      "  time_total_s: 2557.417450428009\n",
      "  timestamp: 1550795964\n",
      "  timesteps_since_restore: 1010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1010000\n",
      "  training_iteration: 101\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2557 s, 101 iter, 1010000 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-39-48\n",
      "  done: false\n",
      "  episode_len_mean: 102.23\n",
      "  episode_reward_max: 206.54900565725893\n",
      "  episode_reward_mean: 145.28771514833224\n",
      "  episode_reward_min: -152.74086057190203\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 8040\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3209.795\n",
      "    load_time_ms: 2.395\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.3108873358076425e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1066395044326782\n",
      "      kl: 0.007435662671923637\n",
      "      policy_loss: -0.0058265021070837975\n",
      "      total_loss: 48.10980224609375\n",
      "      vf_explained_var: 0.893119215965271\n",
      "      vf_loss: 48.115623474121094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.6155871579667565e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.985201895236969\n",
      "      kl: 0.008219625800848007\n",
      "      policy_loss: -0.0017492464976385236\n",
      "      total_loss: 96.49260711669922\n",
      "      vf_explained_var: 0.929675281047821\n",
      "      vf_loss: 96.49435424804688\n",
      "    sample_time_ms: 20597.677\n",
      "    update_time_ms: 7.936\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.242347213955014\n",
      "    rl_1: 101.04536793437721\n",
      "  time_since_restore: 2581.3142278194427\n",
      "  time_this_iter_s: 23.896777391433716\n",
      "  time_total_s: 2581.3142278194427\n",
      "  timestamp: 1550795988\n",
      "  timesteps_since_restore: 1020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 102\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2581 s, 102 iter, 1020000 ts, 145 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-40-12\n",
      "  done: false\n",
      "  episode_len_mean: 92.75229357798165\n",
      "  episode_reward_max: 206.67885195347947\n",
      "  episode_reward_mean: 140.6271440134884\n",
      "  episode_reward_min: -156.1674493102812\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 8149\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3214.091\n",
      "    load_time_ms: 2.358\n",
      "    num_steps_sampled: 1030000\n",
      "    num_steps_trained: 1030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.1554436679038213e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9992013573646545\n",
      "      kl: 0.00617413641884923\n",
      "      policy_loss: -0.003462486434727907\n",
      "      total_loss: 71.45750427246094\n",
      "      vf_explained_var: 0.8574853539466858\n",
      "      vf_loss: 71.4609603881836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.077935789833782e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8614987134933472\n",
      "      kl: 0.005059866700321436\n",
      "      policy_loss: -0.0016822210745885968\n",
      "      total_loss: 105.83177947998047\n",
      "      vf_explained_var: 0.9177705645561218\n",
      "      vf_loss: 105.83345794677734\n",
      "    sample_time_ms: 20635.761\n",
      "    update_time_ms: 7.843\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.32453317343527\n",
      "    rl_1: 98.30261084005309\n",
      "  time_since_restore: 2605.4485383033752\n",
      "  time_this_iter_s: 24.134310483932495\n",
      "  time_total_s: 2605.4485383033752\n",
      "  timestamp: 1550796012\n",
      "  timesteps_since_restore: 1030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1030000\n",
      "  training_iteration: 103\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2605 s, 103 iter, 1030000 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-40-36\n",
      "  done: false\n",
      "  episode_len_mean: 97.97029702970298\n",
      "  episode_reward_max: 196.57308679019152\n",
      "  episode_reward_mean: 139.9003267404929\n",
      "  episode_reward_min: -148.2144060334026\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 8250\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3217.932\n",
      "    load_time_ms: 2.404\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5777218339519106e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1192874908447266\n",
      "      kl: 0.006314156576991081\n",
      "      policy_loss: -0.0026639727875590324\n",
      "      total_loss: 27.292953491210938\n",
      "      vf_explained_var: 0.9281550645828247\n",
      "      vf_loss: 27.29561996459961\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.038967894916891e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9606369137763977\n",
      "      kl: 0.006876008585095406\n",
      "      policy_loss: -0.0031849259976297617\n",
      "      total_loss: 42.964630126953125\n",
      "      vf_explained_var: 0.960471510887146\n",
      "      vf_loss: 42.96781539916992\n",
      "    sample_time_ms: 20677.113\n",
      "    update_time_ms: 7.989\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.0956095301234\n",
      "    rl_1: 97.80471721036949\n",
      "  time_since_restore: 2629.5905458927155\n",
      "  time_this_iter_s: 24.14200758934021\n",
      "  time_total_s: 2629.5905458927155\n",
      "  timestamp: 1550796036\n",
      "  timesteps_since_restore: 1040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 104\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2629 s, 104 iter, 1040000 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-41-00\n",
      "  done: false\n",
      "  episode_len_mean: 91.23636363636363\n",
      "  episode_reward_max: 203.0704429355403\n",
      "  episode_reward_mean: 127.6976301554117\n",
      "  episode_reward_min: -157.79176197465821\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 8360\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3216.487\n",
      "    load_time_ms: 2.411\n",
      "    num_steps_sampled: 1050000\n",
      "    num_steps_trained: 1050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.888609169759553e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0163953304290771\n",
      "      kl: 0.004330936353653669\n",
      "      policy_loss: -0.002445393241941929\n",
      "      total_loss: 95.45530700683594\n",
      "      vf_explained_var: 0.8317531943321228\n",
      "      vf_loss: 95.45774841308594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.0194839474584456e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8425368070602417\n",
      "      kl: 0.008376666344702244\n",
      "      policy_loss: -0.0023867154959589243\n",
      "      total_loss: 135.96389770507812\n",
      "      vf_explained_var: 0.8970741629600525\n",
      "      vf_loss: 135.96627807617188\n",
      "    sample_time_ms: 20662.92\n",
      "    update_time_ms: 8.186\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.77537071162631\n",
      "    rl_1: 90.92225944378542\n",
      "  time_since_restore: 2653.1568553447723\n",
      "  time_this_iter_s: 23.566309452056885\n",
      "  time_total_s: 2653.1568553447723\n",
      "  timestamp: 1550796060\n",
      "  timesteps_since_restore: 1050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1050000\n",
      "  training_iteration: 105\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2653 s, 105 iter, 1050000 ts, 128 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-41-24\n",
      "  done: false\n",
      "  episode_len_mean: 96.58653846153847\n",
      "  episode_reward_max: 202.7548594697982\n",
      "  episode_reward_mean: 139.9364217172065\n",
      "  episode_reward_min: -158.43071940275854\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 8464\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3217.03\n",
      "    load_time_ms: 2.435\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.9443045848797766e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0127378702163696\n",
      "      kl: 0.005572502966970205\n",
      "      policy_loss: -0.0012555731227621436\n",
      "      total_loss: 77.97045135498047\n",
      "      vf_explained_var: 0.8337088227272034\n",
      "      vf_loss: 77.97171783447266\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0097419737292228e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8678378462791443\n",
      "      kl: 0.00616313936188817\n",
      "      policy_loss: -0.002436177572235465\n",
      "      total_loss: 131.31044006347656\n",
      "      vf_explained_var: 0.8833057880401611\n",
      "      vf_loss: 131.31288146972656\n",
      "    sample_time_ms: 20681.351\n",
      "    update_time_ms: 7.925\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.53054517141557\n",
      "    rl_1: 97.40587654579096\n",
      "  time_since_restore: 2677.1622693538666\n",
      "  time_this_iter_s: 24.00541400909424\n",
      "  time_total_s: 2677.1622693538666\n",
      "  timestamp: 1550796084\n",
      "  timesteps_since_restore: 1060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 106\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2677 s, 106 iter, 1060000 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-41-47\n",
      "  done: false\n",
      "  episode_len_mean: 95.86538461538461\n",
      "  episode_reward_max: 200.35836082019165\n",
      "  episode_reward_mean: 144.18089185883318\n",
      "  episode_reward_min: -151.4579022829979\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 8568\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3218.04\n",
      "    load_time_ms: 2.404\n",
      "    num_steps_sampled: 1070000\n",
      "    num_steps_trained: 1070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9721522924398883e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.028665542602539\n",
      "      kl: 0.008594994433224201\n",
      "      policy_loss: -0.0025191314052790403\n",
      "      total_loss: 37.63452911376953\n",
      "      vf_explained_var: 0.9035521149635315\n",
      "      vf_loss: 37.637046813964844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.048709868646114e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8769636154174805\n",
      "      kl: 0.008475245907902718\n",
      "      policy_loss: -0.0038892822340130806\n",
      "      total_loss: 61.750450134277344\n",
      "      vf_explained_var: 0.9433595538139343\n",
      "      vf_loss: 61.754337310791016\n",
      "    sample_time_ms: 20691.326\n",
      "    update_time_ms: 8.004\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.71401115760071\n",
      "    rl_1: 101.46688070123247\n",
      "  time_since_restore: 2700.883057832718\n",
      "  time_this_iter_s: 23.72078847885132\n",
      "  time_total_s: 2700.883057832718\n",
      "  timestamp: 1550796107\n",
      "  timesteps_since_restore: 1070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1070000\n",
      "  training_iteration: 107\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2700 s, 107 iter, 1070000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-42-12\n",
      "  done: false\n",
      "  episode_len_mean: 99.00990099009901\n",
      "  episode_reward_max: 202.7048212218671\n",
      "  episode_reward_mean: 147.88352879560412\n",
      "  episode_reward_min: -157.82660219727885\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 8669\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3217.551\n",
      "    load_time_ms: 2.488\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.860761462199441e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0916602611541748\n",
      "      kl: 0.005908446852117777\n",
      "      policy_loss: -0.0026674966793507338\n",
      "      total_loss: 19.47810173034668\n",
      "      vf_explained_var: 0.948204517364502\n",
      "      vf_loss: 19.48076820373535\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.524354934323057e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9374681115150452\n",
      "      kl: 0.002569080563262105\n",
      "      policy_loss: -0.0010711372597143054\n",
      "      total_loss: 36.40242385864258\n",
      "      vf_explained_var: 0.968843400478363\n",
      "      vf_loss: 36.40349578857422\n",
      "    sample_time_ms: 20718.698\n",
      "    update_time_ms: 7.907\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.004222338280734\n",
      "    rl_1: 102.87930645732337\n",
      "  time_since_restore: 2725.1174170970917\n",
      "  time_this_iter_s: 24.23435926437378\n",
      "  time_total_s: 2725.1174170970917\n",
      "  timestamp: 1550796132\n",
      "  timesteps_since_restore: 1080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 108\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2725 s, 108 iter, 1080000 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-42-36\n",
      "  done: false\n",
      "  episode_len_mean: 91.60550458715596\n",
      "  episode_reward_max: 208.876234348564\n",
      "  episode_reward_mean: 135.75096521370378\n",
      "  episode_reward_min: -165.75736953912556\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 8778\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3216.833\n",
      "    load_time_ms: 2.456\n",
      "    num_steps_sampled: 1090000\n",
      "    num_steps_trained: 1090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.930380731099721e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0112273693084717\n",
      "      kl: 0.007122019771486521\n",
      "      policy_loss: -0.005602166056632996\n",
      "      total_loss: 100.23220825195312\n",
      "      vf_explained_var: 0.8352523446083069\n",
      "      vf_loss: 100.2378158569336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2621774671615285e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8466231226921082\n",
      "      kl: 0.0071878256276249886\n",
      "      policy_loss: -0.002421811455860734\n",
      "      total_loss: 156.55995178222656\n",
      "      vf_explained_var: 0.8916780352592468\n",
      "      vf_loss: 156.5623779296875\n",
      "    sample_time_ms: 20707.371\n",
      "    update_time_ms: 7.721\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.35124083734461\n",
      "    rl_1: 93.39972437635912\n",
      "  time_since_restore: 2748.85440659523\n",
      "  time_this_iter_s: 23.736989498138428\n",
      "  time_total_s: 2748.85440659523\n",
      "  timestamp: 1550796156\n",
      "  timesteps_since_restore: 1090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1090000\n",
      "  training_iteration: 109\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2748 s, 109 iter, 1090000 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-43-00\n",
      "  done: false\n",
      "  episode_len_mean: 93.26666666666667\n",
      "  episode_reward_max: 210.88944635949395\n",
      "  episode_reward_mean: 147.91999434651288\n",
      "  episode_reward_min: -152.42524727820813\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 8883\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3217.594\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4651903655498604e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9759768843650818\n",
      "      kl: 0.007545963395386934\n",
      "      policy_loss: -0.0037575322203338146\n",
      "      total_loss: 30.01011085510254\n",
      "      vf_explained_var: 0.930526614189148\n",
      "      vf_loss: 30.01386833190918\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.3108873358076425e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8247373700141907\n",
      "      kl: 0.007448467891663313\n",
      "      policy_loss: -0.0030694527085870504\n",
      "      total_loss: 39.42610168457031\n",
      "      vf_explained_var: 0.9659234881401062\n",
      "      vf_loss: 39.429168701171875\n",
      "    sample_time_ms: 20699.437\n",
      "    update_time_ms: 7.686\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 46.7508975127655\n",
      "    rl_1: 101.16909683374739\n",
      "  time_since_restore: 2772.8962411880493\n",
      "  time_this_iter_s: 24.041834592819214\n",
      "  time_total_s: 2772.8962411880493\n",
      "  timestamp: 1550796180\n",
      "  timesteps_since_restore: 1100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 110\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2772 s, 110 iter, 1100000 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-43-24\n",
      "  done: false\n",
      "  episode_len_mean: 97.63809523809523\n",
      "  episode_reward_max: 206.95440727873273\n",
      "  episode_reward_mean: 144.3970415356713\n",
      "  episode_reward_min: -142.73337262501164\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 8988\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3217.892\n",
      "    load_time_ms: 2.455\n",
      "    num_steps_sampled: 1110000\n",
      "    num_steps_trained: 1110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2325951827749302e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0490608215332031\n",
      "      kl: 0.007032706402242184\n",
      "      policy_loss: -0.003085892181843519\n",
      "      total_loss: 62.362098693847656\n",
      "      vf_explained_var: 0.8630324006080627\n",
      "      vf_loss: 62.36519241333008\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.1554436679038213e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.878932535648346\n",
      "      kl: 0.006024423521012068\n",
      "      policy_loss: -0.004228588193655014\n",
      "      total_loss: 106.03468322753906\n",
      "      vf_explained_var: 0.9092872738838196\n",
      "      vf_loss: 106.03890991210938\n",
      "    sample_time_ms: 20685.188\n",
      "    update_time_ms: 7.596\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.25644880024107\n",
      "    rl_1: 99.14059273543022\n",
      "  time_since_restore: 2796.732351541519\n",
      "  time_this_iter_s: 23.83611035346985\n",
      "  time_total_s: 2796.732351541519\n",
      "  timestamp: 1550796204\n",
      "  timesteps_since_restore: 1110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1110000\n",
      "  training_iteration: 111\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2796 s, 111 iter, 1110000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-43-47\n",
      "  done: false\n",
      "  episode_len_mean: 106.96\n",
      "  episode_reward_max: 201.97987042363093\n",
      "  episode_reward_mean: 145.89018854194447\n",
      "  episode_reward_min: -153.39845099599805\n",
      "  episodes_this_iter: 92\n",
      "  episodes_total: 9080\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3218.51\n",
      "    load_time_ms: 2.487\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.162975913874651e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1622979640960693\n",
      "      kl: 0.005676773376762867\n",
      "      policy_loss: -0.00496362429112196\n",
      "      total_loss: 27.797378540039062\n",
      "      vf_explained_var: 0.9209029078483582\n",
      "      vf_loss: 27.802339553833008\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5777218339519106e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9888278245925903\n",
      "      kl: 0.010865123942494392\n",
      "      policy_loss: -0.004345612600445747\n",
      "      total_loss: 48.31789779663086\n",
      "      vf_explained_var: 0.9587185978889465\n",
      "      vf_loss: 48.32223892211914\n",
      "    sample_time_ms: 20633.489\n",
      "    update_time_ms: 6.848\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.946597902172705\n",
      "    rl_1: 101.94359063977176\n",
      "  time_since_restore: 2820.1130905151367\n",
      "  time_this_iter_s: 23.380738973617554\n",
      "  time_total_s: 2820.1130905151367\n",
      "  timestamp: 1550796227\n",
      "  timesteps_since_restore: 1120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 112\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2820 s, 112 iter, 1120000 ts, 146 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-44-11\n",
      "  done: false\n",
      "  episode_len_mean: 99.32673267326733\n",
      "  episode_reward_max: 208.33145238819677\n",
      "  episode_reward_mean: 136.33670409224823\n",
      "  episode_reward_min: -160.82824199964688\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 9181\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3216.613\n",
      "    load_time_ms: 2.547\n",
      "    num_steps_sampled: 1130000\n",
      "    num_steps_trained: 1130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.0814879569373254e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1073896884918213\n",
      "      kl: 0.005198979284614325\n",
      "      policy_loss: -0.003732233075425029\n",
      "      total_loss: 28.76004409790039\n",
      "      vf_explained_var: 0.9376025199890137\n",
      "      vf_loss: 28.763784408569336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5777218339519106e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9116668701171875\n",
      "      kl: 0.0026839275378733873\n",
      "      policy_loss: -0.0013937776675447822\n",
      "      total_loss: 41.42919158935547\n",
      "      vf_explained_var: 0.9663313627243042\n",
      "      vf_loss: 41.43058395385742\n",
      "    sample_time_ms: 20582.701\n",
      "    update_time_ms: 6.96\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.41874720341173\n",
      "    rl_1: 97.9179568888365\n",
      "  time_since_restore: 2843.72145986557\n",
      "  time_this_iter_s: 23.60836935043335\n",
      "  time_total_s: 2843.72145986557\n",
      "  timestamp: 1550796251\n",
      "  timesteps_since_restore: 1130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1130000\n",
      "  training_iteration: 113\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2843 s, 113 iter, 1130000 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-44-34\n",
      "  done: false\n",
      "  episode_len_mean: 102.96\n",
      "  episode_reward_max: 207.95266662462117\n",
      "  episode_reward_mean: 153.67792474968394\n",
      "  episode_reward_min: 101.03557143087613\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 9278\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3215.61\n",
      "    load_time_ms: 2.574\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5407439784686627e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0804972648620605\n",
      "      kl: 0.010846550576388836\n",
      "      policy_loss: -0.0038244707975536585\n",
      "      total_loss: 7.264969348907471\n",
      "      vf_explained_var: 0.9824501276016235\n",
      "      vf_loss: 7.268795490264893\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.888609169759553e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9131672978401184\n",
      "      kl: 0.004436218645423651\n",
      "      policy_loss: -0.003765617497265339\n",
      "      total_loss: 16.421897888183594\n",
      "      vf_explained_var: 0.9875050187110901\n",
      "      vf_loss: 16.425662994384766\n",
      "    sample_time_ms: 20545.209\n",
      "    update_time_ms: 7.186\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.631265532113524\n",
      "    rl_1: 105.04665921757045\n",
      "  time_since_restore: 2867.4805755615234\n",
      "  time_this_iter_s: 23.75911569595337\n",
      "  time_total_s: 2867.4805755615234\n",
      "  timestamp: 1550796274\n",
      "  timesteps_since_restore: 1140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 114\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2867 s, 114 iter, 1140000 ts, 154 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-44-59\n",
      "  done: false\n",
      "  episode_len_mean: 101.13\n",
      "  episode_reward_max: 208.4654511647646\n",
      "  episode_reward_mean: 146.4024077637333\n",
      "  episode_reward_min: -159.21002897069718\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 9376\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3215.031\n",
      "    load_time_ms: 2.561\n",
      "    num_steps_sampled: 1150000\n",
      "    num_steps_trained: 1150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5407439784686627e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.062874674797058\n",
      "      kl: 0.007326159160584211\n",
      "      policy_loss: -0.004514588974416256\n",
      "      total_loss: 15.582707405090332\n",
      "      vf_explained_var: 0.9587997198104858\n",
      "      vf_loss: 15.5872220993042\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.9443045848797766e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8806642293930054\n",
      "      kl: 0.006374019663780928\n",
      "      policy_loss: -0.003079975489526987\n",
      "      total_loss: 31.768348693847656\n",
      "      vf_explained_var: 0.9703975319862366\n",
      "      vf_loss: 31.771427154541016\n",
      "    sample_time_ms: 20610.081\n",
      "    update_time_ms: 7.215\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.9514158709198\n",
      "    rl_1: 101.45099189281348\n",
      "  time_since_restore: 2891.6884508132935\n",
      "  time_this_iter_s: 24.20787525177002\n",
      "  time_total_s: 2891.6884508132935\n",
      "  timestamp: 1550796299\n",
      "  timesteps_since_restore: 1150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1150000\n",
      "  training_iteration: 115\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2891 s, 115 iter, 1150000 ts, 146 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-45-23\n",
      "  done: false\n",
      "  episode_len_mean: 99.26732673267327\n",
      "  episode_reward_max: 204.80245800392137\n",
      "  episode_reward_mean: 149.30489151578482\n",
      "  episode_reward_min: -141.76270275824749\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 9477\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3217.145\n",
      "    load_time_ms: 2.517\n",
      "    num_steps_sampled: 1160000\n",
      "    num_steps_trained: 1160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.703719892343314e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0168695449829102\n",
      "      kl: 0.0035498356446623802\n",
      "      policy_loss: -0.004404046572744846\n",
      "      total_loss: 33.57024002075195\n",
      "      vf_explained_var: 0.9242837429046631\n",
      "      vf_loss: 33.57464599609375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9721522924398883e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8627862334251404\n",
      "      kl: 0.00495342630892992\n",
      "      policy_loss: -0.0005679709138348699\n",
      "      total_loss: 64.42340850830078\n",
      "      vf_explained_var: 0.9444764852523804\n",
      "      vf_loss: 64.42398834228516\n",
      "    sample_time_ms: 20595.758\n",
      "    update_time_ms: 7.349\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.283831776410395\n",
      "    rl_1: 101.02105973937441\n",
      "  time_since_restore: 2915.571889400482\n",
      "  time_this_iter_s: 23.88343858718872\n",
      "  time_total_s: 2915.571889400482\n",
      "  timestamp: 1550796323\n",
      "  timesteps_since_restore: 1160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1160000\n",
      "  training_iteration: 116\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2915 s, 116 iter, 1160000 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 94.95238095238095\n",
      "  episode_reward_max: 208.95710557463337\n",
      "  episode_reward_mean: 149.30490672396297\n",
      "  episode_reward_min: -156.18448499636597\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 9582\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3216.522\n",
      "    load_time_ms: 2.498\n",
      "    num_steps_sampled: 1170000\n",
      "    num_steps_trained: 1170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.851859946171657e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9636021256446838\n",
      "      kl: 0.003402914386242628\n",
      "      policy_loss: -0.001748054986819625\n",
      "      total_loss: 40.19501876831055\n",
      "      vf_explained_var: 0.923746645450592\n",
      "      vf_loss: 40.19676971435547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.860761462199441e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7773580551147461\n",
      "      kl: 0.005413036327809095\n",
      "      policy_loss: -0.0031806230545043945\n",
      "      total_loss: 63.75389862060547\n",
      "      vf_explained_var: 0.9460398554801941\n",
      "      vf_loss: 63.757080078125\n",
      "    sample_time_ms: 20600.894\n",
      "    update_time_ms: 7.468\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 46.79556477888414\n",
      "    rl_1: 102.50934194507882\n",
      "  time_since_restore: 2939.339889526367\n",
      "  time_this_iter_s: 23.76800012588501\n",
      "  time_total_s: 2939.339889526367\n",
      "  timestamp: 1550796346\n",
      "  timesteps_since_restore: 1170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1170000\n",
      "  training_iteration: 117\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2939 s, 117 iter, 1170000 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-46-10\n",
      "  done: false\n",
      "  episode_len_mean: 95.08490566037736\n",
      "  episode_reward_max: 212.08743387789548\n",
      "  episode_reward_mean: 151.88854640388914\n",
      "  episode_reward_min: 101.9957695006806\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 9688\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3210.35\n",
      "    load_time_ms: 2.506\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9259299730858284e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9588030576705933\n",
      "      kl: 0.006117055658251047\n",
      "      policy_loss: -0.003409516764804721\n",
      "      total_loss: 8.22309398651123\n",
      "      vf_explained_var: 0.9764364361763\n",
      "      vf_loss: 8.226503372192383\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.930380731099721e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7683289647102356\n",
      "      kl: 0.009978733025491238\n",
      "      policy_loss: -0.002825589617714286\n",
      "      total_loss: 17.723039627075195\n",
      "      vf_explained_var: 0.9823641777038574\n",
      "      vf_loss: 17.725866317749023\n",
      "    sample_time_ms: 20542.771\n",
      "    update_time_ms: 7.493\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 47.74690456315429\n",
      "    rl_1: 104.14164184073483\n",
      "  time_since_restore: 2962.9348340034485\n",
      "  time_this_iter_s: 23.5949444770813\n",
      "  time_total_s: 2962.9348340034485\n",
      "  timestamp: 1550796370\n",
      "  timesteps_since_restore: 1180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 118\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2962 s, 118 iter, 1180000 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-46-33\n",
      "  done: false\n",
      "  episode_len_mean: 95.60952380952381\n",
      "  episode_reward_max: 209.8220615488158\n",
      "  episode_reward_mean: 151.77264964321512\n",
      "  episode_reward_min: -134.09259152022642\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 9793\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3200.079\n",
      "    load_time_ms: 2.447\n",
      "    num_steps_sampled: 1190000\n",
      "    num_steps_trained: 1190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.629649865429142e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9212173819541931\n",
      "      kl: 0.008922259323298931\n",
      "      policy_loss: -0.002380961086601019\n",
      "      total_loss: 30.798500061035156\n",
      "      vf_explained_var: 0.9248841404914856\n",
      "      vf_loss: 30.80088233947754\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.4651903655498604e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7511589527130127\n",
      "      kl: 0.004989064298570156\n",
      "      policy_loss: -0.0014174991520121694\n",
      "      total_loss: 53.6955680847168\n",
      "      vf_explained_var: 0.9486938118934631\n",
      "      vf_loss: 53.696990966796875\n",
      "    sample_time_ms: 20486.629\n",
      "    update_time_ms: 7.444\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 47.98060185250276\n",
      "    rl_1: 103.79204779071237\n",
      "  time_since_restore: 2986.006176710129\n",
      "  time_this_iter_s: 23.071342706680298\n",
      "  time_total_s: 2986.006176710129\n",
      "  timestamp: 1550796393\n",
      "  timesteps_since_restore: 1190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1190000\n",
      "  training_iteration: 119\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 2986 s, 119 iter, 1190000 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-46-57\n",
      "  done: false\n",
      "  episode_len_mean: 94.12380952380953\n",
      "  episode_reward_max: 210.92661764470157\n",
      "  episode_reward_mean: 155.09882058904358\n",
      "  episode_reward_min: -151.4583109369103\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 9898\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3189.345\n",
      "    load_time_ms: 2.557\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.814824932714571e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9081196784973145\n",
      "      kl: 0.00774905551224947\n",
      "      policy_loss: -0.0025208829902112484\n",
      "      total_loss: 16.921247482299805\n",
      "      vf_explained_var: 0.9620886445045471\n",
      "      vf_loss: 16.923770904541016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2325951827749302e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7157872915267944\n",
      "      kl: 0.007796789053827524\n",
      "      policy_loss: -0.0026100201066583395\n",
      "      total_loss: 32.30813980102539\n",
      "      vf_explained_var: 0.9685370326042175\n",
      "      vf_loss: 32.31074523925781\n",
      "    sample_time_ms: 20489.172\n",
      "    update_time_ms: 7.382\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.130609379609886\n",
      "    rl_1: 104.9682112094337\n",
      "  time_since_restore: 3009.9658250808716\n",
      "  time_this_iter_s: 23.959648370742798\n",
      "  time_total_s: 3009.9658250808716\n",
      "  timestamp: 1550796417\n",
      "  timesteps_since_restore: 1200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 120\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3009 s, 120 iter, 1200000 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-47-21\n",
      "  done: false\n",
      "  episode_len_mean: 95.64423076923077\n",
      "  episode_reward_max: 209.71448948107889\n",
      "  episode_reward_mean: 148.6519995537471\n",
      "  episode_reward_min: -139.01937180242157\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 10002\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3195.497\n",
      "    load_time_ms: 2.484\n",
      "    num_steps_sampled: 1210000\n",
      "    num_steps_trained: 1210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4074124663572855e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9918311834335327\n",
      "      kl: 0.006231258623301983\n",
      "      policy_loss: -0.0033106927294284105\n",
      "      total_loss: 13.569863319396973\n",
      "      vf_explained_var: 0.961290717124939\n",
      "      vf_loss: 13.573174476623535\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.162975913874651e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8012427687644958\n",
      "      kl: 0.0033979336731135845\n",
      "      policy_loss: -0.0018947225762531161\n",
      "      total_loss: 25.398115158081055\n",
      "      vf_explained_var: 0.9755763411521912\n",
      "      vf_loss: 25.400007247924805\n",
      "    sample_time_ms: 20472.361\n",
      "    update_time_ms: 7.293\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 46.590146694000666\n",
      "    rl_1: 102.06185285974645\n",
      "  time_since_restore: 3033.6893904209137\n",
      "  time_this_iter_s: 23.723565340042114\n",
      "  time_total_s: 3033.6893904209137\n",
      "  timestamp: 1550796441\n",
      "  timesteps_since_restore: 1210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1210000\n",
      "  training_iteration: 121\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3033 s, 121 iter, 1210000 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-47-44\n",
      "  done: false\n",
      "  episode_len_mean: 102.02\n",
      "  episode_reward_max: 207.42248912204846\n",
      "  episode_reward_mean: 154.220266295679\n",
      "  episode_reward_min: -126.29546883470408\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 10101\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3188.863\n",
      "    load_time_ms: 2.49\n",
      "    num_steps_sampled: 1220000\n",
      "    num_steps_trained: 1220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037062331786428e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0745046138763428\n",
      "      kl: 0.003601052798330784\n",
      "      policy_loss: -0.0013488903641700745\n",
      "      total_loss: 29.733667373657227\n",
      "      vf_explained_var: 0.9300603866577148\n",
      "      vf_loss: 29.735013961791992\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0814879569373254e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8677218556404114\n",
      "      kl: 0.0062833973206579685\n",
      "      policy_loss: -0.0022842134349048138\n",
      "      total_loss: 46.62481689453125\n",
      "      vf_explained_var: 0.9597315788269043\n",
      "      vf_loss: 46.627105712890625\n",
      "    sample_time_ms: 20486.72\n",
      "    update_time_ms: 7.274\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.95139790126544\n",
      "    rl_1: 103.26886839441359\n",
      "  time_since_restore: 3057.1470715999603\n",
      "  time_this_iter_s: 23.45768117904663\n",
      "  time_total_s: 3057.1470715999603\n",
      "  timestamp: 1550796464\n",
      "  timesteps_since_restore: 1220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1220000\n",
      "  training_iteration: 122\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3057 s, 122 iter, 1220000 ts, 154 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-48-09\n",
      "  done: false\n",
      "  episode_len_mean: 93.71028037383178\n",
      "  episode_reward_max: 218.57750870017549\n",
      "  episode_reward_mean: 149.90582091711852\n",
      "  episode_reward_min: -157.0900297159626\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 10208\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3181.105\n",
      "    load_time_ms: 2.483\n",
      "    num_steps_sampled: 1230000\n",
      "    num_steps_trained: 1230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531165893214e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9086808562278748\n",
      "      kl: 0.010865435935556889\n",
      "      policy_loss: -0.0058358656242489815\n",
      "      total_loss: 61.071983337402344\n",
      "      vf_explained_var: 0.9064174294471741\n",
      "      vf_loss: 61.07781219482422\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5407439784686627e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7103741765022278\n",
      "      kl: 0.004385377746075392\n",
      "      policy_loss: -0.0009186500101350248\n",
      "      total_loss: 103.16674041748047\n",
      "      vf_explained_var: 0.9339102506637573\n",
      "      vf_loss: 103.16767120361328\n",
      "    sample_time_ms: 20542.09\n",
      "    update_time_ms: 7.298\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.64738045282706\n",
      "    rl_1: 100.25844046429152\n",
      "  time_since_restore: 3081.230103969574\n",
      "  time_this_iter_s: 24.083032369613647\n",
      "  time_total_s: 3081.230103969574\n",
      "  timestamp: 1550796489\n",
      "  timesteps_since_restore: 1230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1230000\n",
      "  training_iteration: 123\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3081 s, 123 iter, 1230000 ts, 150 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-48-33\n",
      "  done: false\n",
      "  episode_len_mean: 94.71844660194175\n",
      "  episode_reward_max: 217.8809882278847\n",
      "  episode_reward_mean: 163.23394454964173\n",
      "  episode_reward_min: 99.68561126310762\n",
      "  episodes_this_iter: 103\n",
      "  episodes_total: 10311\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3169.46\n",
      "    load_time_ms: 2.399\n",
      "    num_steps_sampled: 1240000\n",
      "    num_steps_trained: 1240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531165893214e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9346190094947815\n",
      "      kl: 0.007520340848714113\n",
      "      policy_loss: -0.003910624887794256\n",
      "      total_loss: 4.901879787445068\n",
      "      vf_explained_var: 0.9882354736328125\n",
      "      vf_loss: 4.905790328979492\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.703719892343314e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7366180419921875\n",
      "      kl: 0.006372976582497358\n",
      "      policy_loss: -0.003060067305341363\n",
      "      total_loss: 9.919266700744629\n",
      "      vf_explained_var: 0.9898315072059631\n",
      "      vf_loss: 9.922325134277344\n",
      "    sample_time_ms: 20576.962\n",
      "    update_time_ms: 6.924\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.93989906889656\n",
      "    rl_1: 107.29404548074518\n",
      "  time_since_restore: 3105.216725587845\n",
      "  time_this_iter_s: 23.986621618270874\n",
      "  time_total_s: 3105.216725587845\n",
      "  timestamp: 1550796513\n",
      "  timesteps_since_restore: 1240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1240000\n",
      "  training_iteration: 124\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3105 s, 124 iter, 1240000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-48-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.4950495049505\n",
      "  episode_reward_max: 208.51627507865103\n",
      "  episode_reward_mean: 150.225537627938\n",
      "  episode_reward_min: -127.90528628016834\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 10412\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3157.058\n",
      "    load_time_ms: 2.452\n",
      "    num_steps_sampled: 1250000\n",
      "    num_steps_trained: 1250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.009265582946607e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0646454095840454\n",
      "      kl: 0.015234129503369331\n",
      "      policy_loss: -0.0071007488295435905\n",
      "      total_loss: 57.67074203491211\n",
      "      vf_explained_var: 0.8798170685768127\n",
      "      vf_loss: 57.67784881591797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.851859946171657e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8236384391784668\n",
      "      kl: 0.007460466586053371\n",
      "      policy_loss: -0.0022438177838921547\n",
      "      total_loss: 84.0185546875\n",
      "      vf_explained_var: 0.9294463992118835\n",
      "      vf_loss: 84.02079772949219\n",
      "    sample_time_ms: 20525.608\n",
      "    update_time_ms: 6.969\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.71327799683231\n",
      "    rl_1: 101.5122596311057\n",
      "  time_since_restore: 3128.7895092964172\n",
      "  time_this_iter_s: 23.572783708572388\n",
      "  time_total_s: 3128.7895092964172\n",
      "  timestamp: 1550796536\n",
      "  timesteps_since_restore: 1250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1250000\n",
      "  training_iteration: 125\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3128 s, 125 iter, 1250000 ts, 150 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-49-20\n",
      "  done: false\n",
      "  episode_len_mean: 98.15686274509804\n",
      "  episode_reward_max: 215.06037524012984\n",
      "  episode_reward_mean: 150.68614308642586\n",
      "  episode_reward_min: -120.69408537782095\n",
      "  episodes_this_iter: 102\n",
      "  episodes_total: 10514\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3163.729\n",
      "    load_time_ms: 2.436\n",
      "    num_steps_sampled: 1260000\n",
      "    num_steps_trained: 1260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.009265582946607e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9632760286331177\n",
      "      kl: 0.0075135244987905025\n",
      "      policy_loss: -0.0018818050157278776\n",
      "      total_loss: 33.64670181274414\n",
      "      vf_explained_var: 0.9414314031600952\n",
      "      vf_loss: 33.64858627319336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9259299730858284e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.723179042339325\n",
      "      kl: 0.007388704922050238\n",
      "      policy_loss: -0.002737197792157531\n",
      "      total_loss: 53.235450744628906\n",
      "      vf_explained_var: 0.9645535349845886\n",
      "      vf_loss: 53.23819351196289\n",
      "    sample_time_ms: 20543.941\n",
      "    update_time_ms: 7.149\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.659336697312405\n",
      "    rl_1: 102.02680638911346\n",
      "  time_since_restore: 3152.9249906539917\n",
      "  time_this_iter_s: 24.135481357574463\n",
      "  time_total_s: 3152.9249906539917\n",
      "  timestamp: 1550796560\n",
      "  timesteps_since_restore: 1260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1260000\n",
      "  training_iteration: 126\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3152 s, 126 iter, 1260000 ts, 151 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-49-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.54\n",
      "  episode_reward_max: 214.06951943963006\n",
      "  episode_reward_mean: 152.96447120113353\n",
      "  episode_reward_min: -148.57461542416718\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 10613\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3152.513\n",
      "    load_time_ms: 2.439\n",
      "    num_steps_sampled: 1270000\n",
      "    num_steps_trained: 1270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5046327914733034e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0457156896591187\n",
      "      kl: 0.010261035524308681\n",
      "      policy_loss: -0.004531819839030504\n",
      "      total_loss: 42.57329177856445\n",
      "      vf_explained_var: 0.9160679578781128\n",
      "      vf_loss: 42.57781982421875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.629649865429142e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8446327447891235\n",
      "      kl: 0.007333135697990656\n",
      "      policy_loss: -0.0037592286244034767\n",
      "      total_loss: 66.50505828857422\n",
      "      vf_explained_var: 0.9469401240348816\n",
      "      vf_loss: 66.50880432128906\n",
      "    sample_time_ms: 20521.565\n",
      "    update_time_ms: 6.948\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.02341657471874\n",
      "    rl_1: 100.94105462641477\n",
      "  time_since_restore: 3176.3532078266144\n",
      "  time_this_iter_s: 23.42821717262268\n",
      "  time_total_s: 3176.3532078266144\n",
      "  timestamp: 1550796584\n",
      "  timesteps_since_restore: 1270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1270000\n",
      "  training_iteration: 127\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3176 s, 127 iter, 1270000 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-50-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.83783783783784\n",
      "  episode_reward_max: 210.85772614990347\n",
      "  episode_reward_mean: 137.88201348860576\n",
      "  episode_reward_min: -155.58483041504877\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 10724\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.891\n",
      "    load_time_ms: 2.339\n",
      "    num_steps_sampled: 1280000\n",
      "    num_steps_trained: 1280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5046327914733034e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8915249705314636\n",
      "      kl: 0.004815345164388418\n",
      "      policy_loss: -0.0027473263908177614\n",
      "      total_loss: 92.47479248046875\n",
      "      vf_explained_var: 0.8803145289421082\n",
      "      vf_loss: 92.47755432128906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.814824932714571e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6866633296012878\n",
      "      kl: 0.00646202452480793\n",
      "      policy_loss: -0.004312520381063223\n",
      "      total_loss: 154.11062622070312\n",
      "      vf_explained_var: 0.8944844007492065\n",
      "      vf_loss: 154.11492919921875\n",
      "    sample_time_ms: 20525.911\n",
      "    update_time_ms: 6.924\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.93443332494844\n",
      "    rl_1: 93.94758016365736\n",
      "  time_since_restore: 3199.9504244327545\n",
      "  time_this_iter_s: 23.597216606140137\n",
      "  time_total_s: 3199.9504244327545\n",
      "  timestamp: 1550796608\n",
      "  timesteps_since_restore: 1280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1280000\n",
      "  training_iteration: 128\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3199 s, 128 iter, 1280000 ts, 138 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-50-31\n",
      "  done: false\n",
      "  episode_len_mean: 93.18691588785046\n",
      "  episode_reward_max: 211.19291357616046\n",
      "  episode_reward_mean: 148.8366511096416\n",
      "  episode_reward_min: -160.01908389007687\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 10831\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.905\n",
      "    load_time_ms: 2.343\n",
      "    num_steps_sampled: 1290000\n",
      "    num_steps_trained: 1290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.523163957366517e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9359583854675293\n",
      "      kl: 0.007438104134052992\n",
      "      policy_loss: -0.004446067847311497\n",
      "      total_loss: 48.351158142089844\n",
      "      vf_explained_var: 0.9075686931610107\n",
      "      vf_loss: 48.355594635009766\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.4074124663572855e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7287559509277344\n",
      "      kl: 0.010003931820392609\n",
      "      policy_loss: -0.0033135581761598587\n",
      "      total_loss: 84.83194732666016\n",
      "      vf_explained_var: 0.9274726510047913\n",
      "      vf_loss: 84.83525848388672\n",
      "    sample_time_ms: 20599.912\n",
      "    update_time_ms: 6.921\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.48151718008279\n",
      "    rl_1: 100.35513392955878\n",
      "  time_since_restore: 3223.7435109615326\n",
      "  time_this_iter_s: 23.793086528778076\n",
      "  time_total_s: 3223.7435109615326\n",
      "  timestamp: 1550796631\n",
      "  timesteps_since_restore: 1290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1290000\n",
      "  training_iteration: 129\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3223 s, 129 iter, 1290000 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-50-55\n",
      "  done: false\n",
      "  episode_len_mean: 94.62264150943396\n",
      "  episode_reward_max: 211.47166334904776\n",
      "  episode_reward_mean: 154.05684372416968\n",
      "  episode_reward_min: -138.84916721172465\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 10937\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.801\n",
      "    load_time_ms: 2.218\n",
      "    num_steps_sampled: 1300000\n",
      "    num_steps_trained: 1300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7615819786832586e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9800622463226318\n",
      "      kl: 0.00941486656665802\n",
      "      policy_loss: -0.005152552854269743\n",
      "      total_loss: 33.259239196777344\n",
      "      vf_explained_var: 0.9234380722045898\n",
      "      vf_loss: 33.2643928527832\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.4074124663572855e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7694149613380432\n",
      "      kl: 0.0058513483963906765\n",
      "      policy_loss: -0.0019002621993422508\n",
      "      total_loss: 55.42877197265625\n",
      "      vf_explained_var: 0.9457833766937256\n",
      "      vf_loss: 55.4306755065918\n",
      "    sample_time_ms: 20584.334\n",
      "    update_time_ms: 7.222\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.907161368497995\n",
      "    rl_1: 104.14968235567169\n",
      "  time_since_restore: 3247.5465025901794\n",
      "  time_this_iter_s: 23.80299162864685\n",
      "  time_total_s: 3247.5465025901794\n",
      "  timestamp: 1550796655\n",
      "  timesteps_since_restore: 1300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1300000\n",
      "  training_iteration: 130\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3247 s, 130 iter, 1300000 ts, 154 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-51-19\n",
      "  done: false\n",
      "  episode_len_mean: 99.65\n",
      "  episode_reward_max: 216.847540702051\n",
      "  episode_reward_mean: 155.00802978960374\n",
      "  episode_reward_min: 42.06240630398073\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 11037\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.892\n",
      "    load_time_ms: 2.248\n",
      "    num_steps_sampled: 1310000\n",
      "    num_steps_trained: 1310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8807909893416293e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0345016717910767\n",
      "      kl: 0.007698534522205591\n",
      "      policy_loss: -0.003954131621867418\n",
      "      total_loss: 4.648106575012207\n",
      "      vf_explained_var: 0.9881518483161926\n",
      "      vf_loss: 4.652060508728027\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2037062331786428e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.83230060338974\n",
      "      kl: 0.004980986472219229\n",
      "      policy_loss: -0.0035261977463960648\n",
      "      total_loss: 11.223203659057617\n",
      "      vf_explained_var: 0.9881715178489685\n",
      "      vf_loss: 11.226729393005371\n",
      "    sample_time_ms: 20598.119\n",
      "    update_time_ms: 7.477\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.581890604419556\n",
      "    rl_1: 104.42613918518414\n",
      "  time_since_restore: 3271.3253095149994\n",
      "  time_this_iter_s: 23.778806924819946\n",
      "  time_total_s: 3271.3253095149994\n",
      "  timestamp: 1550796679\n",
      "  timesteps_since_restore: 1310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1310000\n",
      "  training_iteration: 131\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3271 s, 131 iter, 1310000 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-51-43\n",
      "  done: false\n",
      "  episode_len_mean: 93.4766355140187\n",
      "  episode_reward_max: 210.37724970935565\n",
      "  episode_reward_mean: 150.76878894215793\n",
      "  episode_reward_min: -144.59943647094764\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 11144\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.321\n",
      "    load_time_ms: 2.331\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.403954246058914e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9551297426223755\n",
      "      kl: 0.006757201161235571\n",
      "      policy_loss: -0.004090689588338137\n",
      "      total_loss: 32.711822509765625\n",
      "      vf_explained_var: 0.933657705783844\n",
      "      vf_loss: 32.715919494628906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.018531165893214e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7123648524284363\n",
      "      kl: 0.005384214222431183\n",
      "      policy_loss: -0.0032915151678025723\n",
      "      total_loss: 55.94451141357422\n",
      "      vf_explained_var: 0.9529818892478943\n",
      "      vf_loss: 55.94781494140625\n",
      "    sample_time_ms: 20613.459\n",
      "    update_time_ms: 7.579\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 47.90261855221394\n",
      "    rl_1: 102.86617038994399\n",
      "  time_since_restore: 3294.8937935829163\n",
      "  time_this_iter_s: 23.56848406791687\n",
      "  time_total_s: 3294.8937935829163\n",
      "  timestamp: 1550796703\n",
      "  timesteps_since_restore: 1320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 132\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3294 s, 132 iter, 1320000 ts, 151 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-52-06\n",
      "  done: false\n",
      "  episode_len_mean: 92.55555555555556\n",
      "  episode_reward_max: 205.77528115662986\n",
      "  episode_reward_mean: 152.54905631922486\n",
      "  episode_reward_min: 112.23710704602495\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 11252\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.766\n",
      "    load_time_ms: 2.349\n",
      "    num_steps_sampled: 1330000\n",
      "    num_steps_trained: 1330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.701977123029457e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9381758570671082\n",
      "      kl: 0.014106275513768196\n",
      "      policy_loss: -0.006135599222034216\n",
      "      total_loss: 4.663829803466797\n",
      "      vf_explained_var: 0.9865544438362122\n",
      "      vf_loss: 4.669965744018555\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.009265582946607e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6981682181358337\n",
      "      kl: 0.008806990459561348\n",
      "      policy_loss: -0.004177164752036333\n",
      "      total_loss: 9.621867179870605\n",
      "      vf_explained_var: 0.990840494632721\n",
      "      vf_loss: 9.626044273376465\n",
      "    sample_time_ms: 20538.109\n",
      "    update_time_ms: 7.469\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.75642383969612\n",
      "    rl_1: 103.79263247952876\n",
      "  time_since_restore: 3318.1869690418243\n",
      "  time_this_iter_s: 23.29317545890808\n",
      "  time_total_s: 3318.1869690418243\n",
      "  timestamp: 1550796726\n",
      "  timesteps_since_restore: 1330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1330000\n",
      "  training_iteration: 133\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3318 s, 133 iter, 1330000 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-52-29\n",
      "  done: false\n",
      "  episode_len_mean: 91.97272727272727\n",
      "  episode_reward_max: 208.97714077187607\n",
      "  episode_reward_mean: 157.47652268956782\n",
      "  episode_reward_min: -131.80339780801737\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 11362\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.828\n",
      "    load_time_ms: 2.482\n",
      "    num_steps_sampled: 1340000\n",
      "    num_steps_trained: 1340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.701977123029457e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.897986114025116\n",
      "      kl: 0.005961236078292131\n",
      "      policy_loss: -0.001923959469422698\n",
      "      total_loss: 30.346019744873047\n",
      "      vf_explained_var: 0.9202823042869568\n",
      "      vf_loss: 30.34794044494629\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5046327914733034e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6798184514045715\n",
      "      kl: 0.00947122648358345\n",
      "      policy_loss: -0.0017770201666280627\n",
      "      total_loss: 50.28684997558594\n",
      "      vf_explained_var: 0.9516305327415466\n",
      "      vf_loss: 50.288631439208984\n",
      "    sample_time_ms: 20474.985\n",
      "    update_time_ms: 7.738\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.746972749863886\n",
      "    rl_1: 103.72954993970397\n",
      "  time_since_restore: 3341.5578167438507\n",
      "  time_this_iter_s: 23.370847702026367\n",
      "  time_total_s: 3341.5578167438507\n",
      "  timestamp: 1550796749\n",
      "  timesteps_since_restore: 1340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1340000\n",
      "  training_iteration: 134\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3341 s, 134 iter, 1340000 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-52-52\n",
      "  done: false\n",
      "  episode_len_mean: 92.27777777777777\n",
      "  episode_reward_max: 214.83843914616887\n",
      "  episode_reward_mean: 160.63596060970852\n",
      "  episode_reward_min: 110.01537044831736\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 11470\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.812\n",
      "    load_time_ms: 2.43\n",
      "    num_steps_sampled: 1350000\n",
      "    num_steps_trained: 1350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3509892621639607e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9036900401115417\n",
      "      kl: 0.009391574189066887\n",
      "      policy_loss: -0.004836085718125105\n",
      "      total_loss: 3.0003647804260254\n",
      "      vf_explained_var: 0.9909865260124207\n",
      "      vf_loss: 3.0052008628845215\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.523163957366517e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6918697357177734\n",
      "      kl: 0.008444790728390217\n",
      "      policy_loss: -0.00347792636603117\n",
      "      total_loss: 7.24308967590332\n",
      "      vf_explained_var: 0.9922581911087036\n",
      "      vf_loss: 7.246567249298096\n",
      "    sample_time_ms: 20407.936\n",
      "    update_time_ms: 7.43\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.579302522154315\n",
      "    rl_1: 105.05665808755421\n",
      "  time_since_restore: 3364.485315322876\n",
      "  time_this_iter_s: 22.92749857902527\n",
      "  time_total_s: 3364.485315322876\n",
      "  timestamp: 1550796772\n",
      "  timesteps_since_restore: 1350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1350000\n",
      "  training_iteration: 135\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3364 s, 135 iter, 1350000 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-53-16\n",
      "  done: false\n",
      "  episode_len_mean: 95.42307692307692\n",
      "  episode_reward_max: 214.97881380756223\n",
      "  episode_reward_mean: 147.69980355273046\n",
      "  episode_reward_min: -157.51318668391383\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 11574\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3115.767\n",
      "    load_time_ms: 2.434\n",
      "    num_steps_sampled: 1360000\n",
      "    num_steps_trained: 1360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1754946310819804e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.971032977104187\n",
      "      kl: 0.007161102257668972\n",
      "      policy_loss: -0.002799804089590907\n",
      "      total_loss: 65.51554107666016\n",
      "      vf_explained_var: 0.8852478265762329\n",
      "      vf_loss: 65.51834106445312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7615819786832586e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7399435043334961\n",
      "      kl: 0.006003530230373144\n",
      "      policy_loss: -0.0007755334372632205\n",
      "      total_loss: 109.41490173339844\n",
      "      vf_explained_var: 0.91019606590271\n",
      "      vf_loss: 109.41566467285156\n",
      "    sample_time_ms: 20314.843\n",
      "    update_time_ms: 7.088\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.070005850251604\n",
      "    rl_1: 98.62979770247888\n",
      "  time_since_restore: 3387.505026578903\n",
      "  time_this_iter_s: 23.01971125602722\n",
      "  time_total_s: 3387.505026578903\n",
      "  timestamp: 1550796796\n",
      "  timesteps_since_restore: 1360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1360000\n",
      "  training_iteration: 136\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3387 s, 136 iter, 1360000 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-53-40\n",
      "  done: false\n",
      "  episode_len_mean: 95.61904761904762\n",
      "  episode_reward_max: 210.55688216060804\n",
      "  episode_reward_mean: 156.49627901059944\n",
      "  episode_reward_min: -137.18008494773375\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 11679\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3120.129\n",
      "    load_time_ms: 2.483\n",
      "    num_steps_sampled: 1370000\n",
      "    num_steps_trained: 1370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.87746614891758e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.958552360534668\n",
      "      kl: 0.005261050071567297\n",
      "      policy_loss: -0.0017767989775165915\n",
      "      total_loss: 27.394868850708008\n",
      "      vf_explained_var: 0.9390802383422852\n",
      "      vf_loss: 27.39664077758789\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8807909893416293e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.733008861541748\n",
      "      kl: 0.006285139359533787\n",
      "      policy_loss: -0.0008918264647945762\n",
      "      total_loss: 45.986351013183594\n",
      "      vf_explained_var: 0.9579066634178162\n",
      "      vf_loss: 45.98724365234375\n",
      "    sample_time_ms: 20399.434\n",
      "    update_time_ms: 7.156\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.56011400134144\n",
      "    rl_1: 102.93616500925798\n",
      "  time_since_restore: 3411.825963973999\n",
      "  time_this_iter_s: 24.320937395095825\n",
      "  time_total_s: 3411.825963973999\n",
      "  timestamp: 1550796820\n",
      "  timesteps_since_restore: 1370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1370000\n",
      "  training_iteration: 137\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3411 s, 137 iter, 1370000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-54-04\n",
      "  done: false\n",
      "  episode_len_mean: 90.72727272727273\n",
      "  episode_reward_max: 212.08557045774558\n",
      "  episode_reward_mean: 144.86384399892063\n",
      "  episode_reward_min: -146.01427333084126\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 11789\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3121.756\n",
      "    load_time_ms: 2.5\n",
      "    num_steps_sampled: 1380000\n",
      "    num_steps_trained: 1380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.93873307445879e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8808607459068298\n",
      "      kl: 0.004549169447273016\n",
      "      policy_loss: -0.0021560597233474255\n",
      "      total_loss: 157.68885803222656\n",
      "      vf_explained_var: 0.7580146789550781\n",
      "      vf_loss: 157.69102478027344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.403954246058914e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6616157293319702\n",
      "      kl: 0.004063266795128584\n",
      "      policy_loss: -0.0032949286978691816\n",
      "      total_loss: 247.0128936767578\n",
      "      vf_explained_var: 0.8031759262084961\n",
      "      vf_loss: 247.01620483398438\n",
      "    sample_time_ms: 20401.954\n",
      "    update_time_ms: 7.279\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.33260206975924\n",
      "    rl_1: 95.53124192916137\n",
      "  time_since_restore: 3435.466330051422\n",
      "  time_this_iter_s: 23.640366077423096\n",
      "  time_total_s: 3435.466330051422\n",
      "  timestamp: 1550796844\n",
      "  timesteps_since_restore: 1380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1380000\n",
      "  training_iteration: 138\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3435 s, 138 iter, 1380000 ts, 145 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-54-27\n",
      "  done: false\n",
      "  episode_len_mean: 92.25925925925925\n",
      "  episode_reward_max: 217.669733008323\n",
      "  episode_reward_mean: 163.10471371994475\n",
      "  episode_reward_min: 112.57871581422685\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 11897\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.185\n",
      "    load_time_ms: 2.544\n",
      "    num_steps_sampled: 1390000\n",
      "    num_steps_trained: 1390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4693735437217167e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8791145086288452\n",
      "      kl: 0.00465833256021142\n",
      "      policy_loss: -0.0034040147438645363\n",
      "      total_loss: 4.272100448608398\n",
      "      vf_explained_var: 0.9885259866714478\n",
      "      vf_loss: 4.275503158569336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.701977123029457e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6325011849403381\n",
      "      kl: 0.01000400260090828\n",
      "      policy_loss: -0.002995759481564164\n",
      "      total_loss: 7.78955602645874\n",
      "      vf_explained_var: 0.9913467764854431\n",
      "      vf_loss: 7.792550086975098\n",
      "    sample_time_ms: 20313.491\n",
      "    update_time_ms: 7.363\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.324235417326726\n",
      "    rl_1: 105.78047830261802\n",
      "  time_since_restore: 3458.3999483585358\n",
      "  time_this_iter_s: 22.933618307113647\n",
      "  time_total_s: 3458.3999483585358\n",
      "  timestamp: 1550796867\n",
      "  timesteps_since_restore: 1390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1390000\n",
      "  training_iteration: 139\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3458 s, 139 iter, 1390000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-54-50\n",
      "  done: false\n",
      "  episode_len_mean: 91.53211009174312\n",
      "  episode_reward_max: 213.01073204493125\n",
      "  episode_reward_mean: 152.96498665151236\n",
      "  episode_reward_min: -118.24823525485651\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 12006\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.988\n",
      "    load_time_ms: 2.558\n",
      "    num_steps_sampled: 1400000\n",
      "    num_steps_trained: 1400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.346867718608583e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9320637583732605\n",
      "      kl: 0.007695375941693783\n",
      "      policy_loss: -0.004032399971038103\n",
      "      total_loss: 31.419004440307617\n",
      "      vf_explained_var: 0.9237247109413147\n",
      "      vf_loss: 31.42304229736328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.701977123029457e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6357570290565491\n",
      "      kl: 0.002578847575932741\n",
      "      policy_loss: -0.001403303467668593\n",
      "      total_loss: 42.70268630981445\n",
      "      vf_explained_var: 0.955678403377533\n",
      "      vf_loss: 42.70409393310547\n",
      "    sample_time_ms: 20281.271\n",
      "    update_time_ms: 7.034\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.858045778052045\n",
      "    rl_1: 102.10694087346029\n",
      "  time_since_restore: 3482.044567346573\n",
      "  time_this_iter_s: 23.64461898803711\n",
      "  time_total_s: 3482.044567346573\n",
      "  timestamp: 1550796890\n",
      "  timesteps_since_restore: 1400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1400000\n",
      "  training_iteration: 140\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3482 s, 140 iter, 1400000 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-55-14\n",
      "  done: false\n",
      "  episode_len_mean: 91.89090909090909\n",
      "  episode_reward_max: 213.14076467378314\n",
      "  episode_reward_mean: 147.91154302054053\n",
      "  episode_reward_min: -134.0287072604227\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 12116\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.207\n",
      "    load_time_ms: 2.663\n",
      "    num_steps_sampled: 1410000\n",
      "    num_steps_trained: 1410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6733637943810755e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9777494668960571\n",
      "      kl: 0.002941027283668518\n",
      "      policy_loss: -0.002485893899574876\n",
      "      total_loss: 52.695674896240234\n",
      "      vf_explained_var: 0.8820112347602844\n",
      "      vf_loss: 52.69816589355469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3509892621639607e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7159357070922852\n",
      "      kl: 0.012538254261016846\n",
      "      policy_loss: -0.0040860865265131\n",
      "      total_loss: 85.55265045166016\n",
      "      vf_explained_var: 0.9169800877571106\n",
      "      vf_loss: 85.55672454833984\n",
      "    sample_time_ms: 20241.337\n",
      "    update_time_ms: 6.863\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.47262115004134\n",
      "    rl_1: 99.43892187049924\n",
      "  time_since_restore: 3505.344782590866\n",
      "  time_this_iter_s: 23.300215244293213\n",
      "  time_total_s: 3505.344782590866\n",
      "  timestamp: 1550796914\n",
      "  timesteps_since_restore: 1410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1410000\n",
      "  training_iteration: 141\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3505 s, 141 iter, 1410000 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-55-37\n",
      "  done: false\n",
      "  episode_len_mean: 91.17272727272727\n",
      "  episode_reward_max: 214.18167730288468\n",
      "  episode_reward_mean: 157.8737328682102\n",
      "  episode_reward_min: -131.18961795787936\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 12226\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.233\n",
      "    load_time_ms: 2.537\n",
      "    num_steps_sampled: 1420000\n",
      "    num_steps_trained: 1420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8366818971905377e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8808021545410156\n",
      "      kl: 0.006512590683996677\n",
      "      policy_loss: -0.0035024587996304035\n",
      "      total_loss: 55.43588638305664\n",
      "      vf_explained_var: 0.8768174648284912\n",
      "      vf_loss: 55.43939208984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3509892621639607e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6582815647125244\n",
      "      kl: 0.006760444492101669\n",
      "      policy_loss: -0.0017473292537033558\n",
      "      total_loss: 82.63478088378906\n",
      "      vf_explained_var: 0.9197056889533997\n",
      "      vf_loss: 82.63654327392578\n",
      "    sample_time_ms: 20224.386\n",
      "    update_time_ms: 6.762\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.68369611599411\n",
      "    rl_1: 104.19003675221612\n",
      "  time_since_restore: 3528.749081134796\n",
      "  time_this_iter_s: 23.404298543930054\n",
      "  time_total_s: 3528.749081134796\n",
      "  timestamp: 1550796937\n",
      "  timesteps_since_restore: 1420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1420000\n",
      "  training_iteration: 142\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3528 s, 142 iter, 1420000 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-56-01\n",
      "  done: false\n",
      "  episode_len_mean: 91.53211009174312\n",
      "  episode_reward_max: 206.30296746454275\n",
      "  episode_reward_mean: 156.52913889666303\n",
      "  episode_reward_min: -122.35318702994707\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 12335\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.376\n",
      "    load_time_ms: 2.479\n",
      "    num_steps_sampled: 1430000\n",
      "    num_steps_trained: 1430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.184110135184851e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9044374227523804\n",
      "      kl: 0.006336801685392857\n",
      "      policy_loss: -0.0034830921795219183\n",
      "      total_loss: 28.617671966552734\n",
      "      vf_explained_var: 0.9329243302345276\n",
      "      vf_loss: 28.621152877807617\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1754946310819804e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6822396516799927\n",
      "      kl: 0.0063048857264220715\n",
      "      policy_loss: -0.002365623600780964\n",
      "      total_loss: 40.552398681640625\n",
      "      vf_explained_var: 0.9597412347793579\n",
      "      vf_loss: 40.55476760864258\n",
      "    sample_time_ms: 20240.476\n",
      "    update_time_ms: 6.743\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.04934060309007\n",
      "    rl_1: 103.47979829357296\n",
      "  time_since_restore: 3552.22344994545\n",
      "  time_this_iter_s: 23.474368810653687\n",
      "  time_total_s: 3552.22344994545\n",
      "  timestamp: 1550796961\n",
      "  timesteps_since_restore: 1430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1430000\n",
      "  training_iteration: 143\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3552 s, 143 iter, 1430000 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 91.97169811320755\n",
      "  episode_reward_max: 209.73249070527137\n",
      "  episode_reward_mean: 163.65039877258695\n",
      "  episode_reward_min: 119.66896522213474\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 12441\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.692\n",
      "    load_time_ms: 2.396\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.5920550675924255e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9019799828529358\n",
      "      kl: 0.0063727847300469875\n",
      "      policy_loss: -0.005399740766733885\n",
      "      total_loss: 3.3153228759765625\n",
      "      vf_explained_var: 0.9905455112457275\n",
      "      vf_loss: 3.3207225799560547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.87746614891758e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6940777897834778\n",
      "      kl: 0.00566375395283103\n",
      "      policy_loss: -0.003375179599970579\n",
      "      total_loss: 6.0807671546936035\n",
      "      vf_explained_var: 0.993457019329071\n",
      "      vf_loss: 6.084142208099365\n",
      "    sample_time_ms: 20248.366\n",
      "    update_time_ms: 6.768\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.80492021009546\n",
      "    rl_1: 106.84547856249148\n",
      "  time_since_restore: 3575.6642575263977\n",
      "  time_this_iter_s: 23.440807580947876\n",
      "  time_total_s: 3575.6642575263977\n",
      "  timestamp: 1550796984\n",
      "  timesteps_since_restore: 1440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 144\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3575 s, 144 iter, 1440000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-56-48\n",
      "  done: false\n",
      "  episode_len_mean: 97.28571428571429\n",
      "  episode_reward_max: 212.95652179969258\n",
      "  episode_reward_mean: 144.37572063655637\n",
      "  episode_reward_min: -147.43210515292972\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 12546\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.128\n",
      "    load_time_ms: 2.385\n",
      "    num_steps_sampled: 1450000\n",
      "    num_steps_trained: 1450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2953268845640504e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9516928195953369\n",
      "      kl: 0.006627310533076525\n",
      "      policy_loss: -0.004734646063297987\n",
      "      total_loss: 133.99319458007812\n",
      "      vf_explained_var: 0.7797834873199463\n",
      "      vf_loss: 133.99794006347656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.93873307445879e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.765268862247467\n",
      "      kl: 0.004023600369691849\n",
      "      policy_loss: -0.0011891216272488236\n",
      "      total_loss: 226.32676696777344\n",
      "      vf_explained_var: 0.813492476940155\n",
      "      vf_loss: 226.32797241210938\n",
      "    sample_time_ms: 20301.378\n",
      "    update_time_ms: 7.009\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.458863733453256\n",
      "    rl_1: 94.91685690310308\n",
      "  time_since_restore: 3599.1162889003754\n",
      "  time_this_iter_s: 23.45203137397766\n",
      "  time_total_s: 3599.1162889003754\n",
      "  timestamp: 1550797008\n",
      "  timesteps_since_restore: 1450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1450000\n",
      "  training_iteration: 145\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3599 s, 145 iter, 1450000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-57-11\n",
      "  done: false\n",
      "  episode_len_mean: 92.25925925925925\n",
      "  episode_reward_max: 213.15871624977785\n",
      "  episode_reward_mean: 155.51637534697485\n",
      "  episode_reward_min: -133.80324314421284\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 12654\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.145\n",
      "    load_time_ms: 2.405\n",
      "    num_steps_sampled: 1460000\n",
      "    num_steps_trained: 1460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1476634422820252e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.911217212677002\n",
      "      kl: 0.00863111112266779\n",
      "      policy_loss: -0.004163398407399654\n",
      "      total_loss: 45.945838928222656\n",
      "      vf_explained_var: 0.9080153107643127\n",
      "      vf_loss: 45.94999313354492\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4693735437217167e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6622878313064575\n",
      "      kl: 0.008485455997288227\n",
      "      policy_loss: -0.0007375161512754858\n",
      "      total_loss: 76.47880554199219\n",
      "      vf_explained_var: 0.9244982600212097\n",
      "      vf_loss: 76.47955322265625\n",
      "    sample_time_ms: 20317.846\n",
      "    update_time_ms: 7.389\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.0674238783413\n",
      "    rl_1: 102.44895146863355\n",
      "  time_since_restore: 3622.3147637844086\n",
      "  time_this_iter_s: 23.198474884033203\n",
      "  time_total_s: 3622.3147637844086\n",
      "  timestamp: 1550797031\n",
      "  timesteps_since_restore: 1460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1460000\n",
      "  training_iteration: 146\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3622 s, 146 iter, 1460000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-57-34\n",
      "  done: false\n",
      "  episode_len_mean: 94.17924528301887\n",
      "  episode_reward_max: 215.0583419197048\n",
      "  episode_reward_mean: 152.7502417533213\n",
      "  episode_reward_min: -135.38637183271493\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 12760\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.263\n",
      "    load_time_ms: 2.391\n",
      "    num_steps_sampled: 1470000\n",
      "    num_steps_trained: 1470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.74532370373175e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9686636328697205\n",
      "      kl: 0.006526000797748566\n",
      "      policy_loss: -0.003938158042728901\n",
      "      total_loss: 48.78371047973633\n",
      "      vf_explained_var: 0.892771303653717\n",
      "      vf_loss: 48.78764724731445\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.346867718608583e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7223332524299622\n",
      "      kl: 0.010436109267175198\n",
      "      policy_loss: -0.003334436332806945\n",
      "      total_loss: 78.02621459960938\n",
      "      vf_explained_var: 0.9283026456832886\n",
      "      vf_loss: 78.02955627441406\n",
      "    sample_time_ms: 20190.675\n",
      "    update_time_ms: 7.298\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.055361723938745\n",
      "    rl_1: 100.69488002938255\n",
      "  time_since_restore: 3645.370898246765\n",
      "  time_this_iter_s: 23.056134462356567\n",
      "  time_total_s: 3645.370898246765\n",
      "  timestamp: 1550797054\n",
      "  timesteps_since_restore: 1470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1470000\n",
      "  training_iteration: 147\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3645 s, 147 iter, 1470000 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-57-57\n",
      "  done: false\n",
      "  episode_len_mean: 92.0952380952381\n",
      "  episode_reward_max: 207.4898923017425\n",
      "  episode_reward_mean: 156.9415635630275\n",
      "  episode_reward_min: -141.99381924669513\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 12865\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.381\n",
      "    load_time_ms: 2.52\n",
      "    num_steps_sampled: 1480000\n",
      "    num_steps_trained: 1480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.872661851865875e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9546504616737366\n",
      "      kl: 0.004549319390207529\n",
      "      policy_loss: -0.0022557794582098722\n",
      "      total_loss: 29.038915634155273\n",
      "      vf_explained_var: 0.9444270133972168\n",
      "      vf_loss: 29.041166305541992\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.346867718608583e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.749255359172821\n",
      "      kl: 0.005463526584208012\n",
      "      policy_loss: -0.002100201090797782\n",
      "      total_loss: 50.28035354614258\n",
      "      vf_explained_var: 0.9661099314689636\n",
      "      vf_loss: 50.28245162963867\n",
      "    sample_time_ms: 20147.88\n",
      "    update_time_ms: 7.399\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.71312701914577\n",
      "    rl_1: 103.2284365438817\n",
      "  time_since_restore: 3668.5583403110504\n",
      "  time_this_iter_s: 23.18744206428528\n",
      "  time_total_s: 3668.5583403110504\n",
      "  timestamp: 1550797077\n",
      "  timesteps_since_restore: 1480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1480000\n",
      "  training_iteration: 148\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3668 s, 148 iter, 1480000 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-58-21\n",
      "  done: false\n",
      "  episode_len_mean: 103.6\n",
      "  episode_reward_max: 210.3176008210934\n",
      "  episode_reward_mean: 159.5571456467647\n",
      "  episode_reward_min: 58.1787723306112\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 12965\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3152.156\n",
      "    load_time_ms: 2.507\n",
      "    num_steps_sampled: 1490000\n",
      "    num_steps_trained: 1490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4293244336113134e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0244933366775513\n",
      "      kl: 0.004253038205206394\n",
      "      policy_loss: -0.0027995181735605\n",
      "      total_loss: 5.768479347229004\n",
      "      vf_explained_var: 0.986255943775177\n",
      "      vf_loss: 5.7712788581848145\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6733637943810755e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8088067770004272\n",
      "      kl: 0.004973617848008871\n",
      "      policy_loss: -0.0016133063472807407\n",
      "      total_loss: 17.135543823242188\n",
      "      vf_explained_var: 0.9838667511940002\n",
      "      vf_loss: 17.137157440185547\n",
      "    sample_time_ms: 20170.798\n",
      "    update_time_ms: 7.678\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.25375022483133\n",
      "    rl_1: 104.30339542193339\n",
      "  time_since_restore: 3691.903977394104\n",
      "  time_this_iter_s: 23.34563708305359\n",
      "  time_total_s: 3691.903977394104\n",
      "  timestamp: 1550797101\n",
      "  timesteps_since_restore: 1490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1490000\n",
      "  training_iteration: 149\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3691 s, 149 iter, 1490000 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-58-44\n",
      "  done: false\n",
      "  episode_len_mean: 90.36936936936937\n",
      "  episode_reward_max: 207.06467006501225\n",
      "  episode_reward_mean: 156.49971680513815\n",
      "  episode_reward_min: -136.67140530528525\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 13076\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.449\n",
      "    load_time_ms: 2.582\n",
      "    num_steps_sampled: 1500000\n",
      "    num_steps_trained: 1500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.146622168056567e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.889825701713562\n",
      "      kl: 0.0059203216806054115\n",
      "      policy_loss: -0.0037762124557048082\n",
      "      total_loss: 35.77511978149414\n",
      "      vf_explained_var: 0.9208309650421143\n",
      "      vf_loss: 35.77889633178711\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8366818971905377e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6551491022109985\n",
      "      kl: 0.013477216474711895\n",
      "      policy_loss: -0.002133625326678157\n",
      "      total_loss: 56.32372283935547\n",
      "      vf_explained_var: 0.9467498660087585\n",
      "      vf_loss: 56.32585144042969\n",
      "    sample_time_ms: 20129.512\n",
      "    update_time_ms: 7.907\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.35973429714892\n",
      "    rl_1: 102.13998250798932\n",
      "  time_since_restore: 3714.943006515503\n",
      "  time_this_iter_s: 23.039029121398926\n",
      "  time_total_s: 3714.943006515503\n",
      "  timestamp: 1550797124\n",
      "  timesteps_since_restore: 1500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1500000\n",
      "  training_iteration: 150\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3714 s, 150 iter, 1500000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-59-07\n",
      "  done: false\n",
      "  episode_len_mean: 90.2090909090909\n",
      "  episode_reward_max: 212.04966062406956\n",
      "  episode_reward_mean: 154.71657780788505\n",
      "  episode_reward_min: -137.46410954358555\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 13186\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.443\n",
      "    load_time_ms: 2.429\n",
      "    num_steps_sampled: 1510000\n",
      "    num_steps_trained: 1510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6433760072445244e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8688276410102844\n",
      "      kl: 0.005000991281121969\n",
      "      policy_loss: -0.0018690568394958973\n",
      "      total_loss: 71.92787170410156\n",
      "      vf_explained_var: 0.8522152900695801\n",
      "      vf_loss: 71.92974090576172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8366818971905377e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6725509166717529\n",
      "      kl: 0.02022376097738743\n",
      "      policy_loss: -0.008609257638454437\n",
      "      total_loss: 113.51119232177734\n",
      "      vf_explained_var: 0.8945664763450623\n",
      "      vf_loss: 113.51979064941406\n",
      "    sample_time_ms: 20087.675\n",
      "    update_time_ms: 7.807\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.87047052772201\n",
      "    rl_1: 100.84610728016305\n",
      "  time_since_restore: 3737.8097512722015\n",
      "  time_this_iter_s: 22.86674475669861\n",
      "  time_total_s: 3737.8097512722015\n",
      "  timestamp: 1550797147\n",
      "  timesteps_since_restore: 1510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1510000\n",
      "  training_iteration: 151\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3737 s, 151 iter, 1510000 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-59-29\n",
      "  done: false\n",
      "  episode_len_mean: 90.92792792792793\n",
      "  episode_reward_max: 208.7991790410705\n",
      "  episode_reward_mean: 158.82680903788463\n",
      "  episode_reward_min: 104.61865196513617\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 13297\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.832\n",
      "    load_time_ms: 2.474\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8216880036222622e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8735949397087097\n",
      "      kl: 0.008680818602442741\n",
      "      policy_loss: -0.003985194955021143\n",
      "      total_loss: 2.7040512561798096\n",
      "      vf_explained_var: 0.9918519258499146\n",
      "      vf_loss: 2.7080366611480713\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8366818971905377e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6888818144798279\n",
      "      kl: 0.00529088219627738\n",
      "      policy_loss: -0.0031789918430149555\n",
      "      total_loss: 5.276775360107422\n",
      "      vf_explained_var: 0.99409419298172\n",
      "      vf_loss: 5.279954433441162\n",
      "    sample_time_ms: 20026.463\n",
      "    update_time_ms: 7.849\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.888244854566274\n",
      "    rl_1: 103.93856418331836\n",
      "  time_since_restore: 3760.628075838089\n",
      "  time_this_iter_s: 22.81832456588745\n",
      "  time_total_s: 3760.628075838089\n",
      "  timestamp: 1550797169\n",
      "  timesteps_since_restore: 1520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 152\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3760 s, 152 iter, 1520000 ts, 159 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_01-59-53\n",
      "  done: false\n",
      "  episode_len_mean: 89.94594594594595\n",
      "  episode_reward_max: 212.861758874096\n",
      "  episode_reward_mean: 153.46387725373245\n",
      "  episode_reward_min: -147.7426845059535\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 13408\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.272\n",
      "    load_time_ms: 2.48\n",
      "    num_steps_sampled: 1530000\n",
      "    num_steps_trained: 1530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.407790785948902e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8415666222572327\n",
      "      kl: 0.004999960772693157\n",
      "      policy_loss: -0.0023049619048833847\n",
      "      total_loss: 94.72466278076172\n",
      "      vf_explained_var: 0.8177213668823242\n",
      "      vf_loss: 94.72697448730469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.184110135184851e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6682661771774292\n",
      "      kl: 0.00658042635768652\n",
      "      policy_loss: -0.0034730450715869665\n",
      "      total_loss: 153.1385040283203\n",
      "      vf_explained_var: 0.8609680533409119\n",
      "      vf_loss: 153.14198303222656\n",
      "    sample_time_ms: 20016.113\n",
      "    update_time_ms: 7.916\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.59811063050825\n",
      "    rl_1: 99.86576662322425\n",
      "  time_since_restore: 3783.9956333637238\n",
      "  time_this_iter_s: 23.367557525634766\n",
      "  time_total_s: 3783.9956333637238\n",
      "  timestamp: 1550797193\n",
      "  timesteps_since_restore: 1530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1530000\n",
      "  training_iteration: 153\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3783 s, 153 iter, 1530000 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-00-16\n",
      "  done: false\n",
      "  episode_len_mean: 90.28828828828829\n",
      "  episode_reward_max: 212.4418043807838\n",
      "  episode_reward_mean: 155.9697318030816\n",
      "  episode_reward_min: -138.6990919661331\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 13519\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.365\n",
      "    load_time_ms: 2.454\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.203895392974451e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8176217675209045\n",
      "      kl: 0.007831190712749958\n",
      "      policy_loss: -0.0014210117515176535\n",
      "      total_loss: 77.06037139892578\n",
      "      vf_explained_var: 0.8399685621261597\n",
      "      vf_loss: 77.0617904663086\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.5920550675924255e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.673167884349823\n",
      "      kl: 0.011136338114738464\n",
      "      policy_loss: -0.004076515324413776\n",
      "      total_loss: 114.6156997680664\n",
      "      vf_explained_var: 0.8922082185745239\n",
      "      vf_loss: 114.61978149414062\n",
      "    sample_time_ms: 20015.276\n",
      "    update_time_ms: 7.818\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.2482891350528\n",
      "    rl_1: 101.72144266802879\n",
      "  time_since_restore: 3807.426589488983\n",
      "  time_this_iter_s: 23.4309561252594\n",
      "  time_total_s: 3807.426589488983\n",
      "  timestamp: 1550797216\n",
      "  timesteps_since_restore: 1540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 154\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3807 s, 154 iter, 1540000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-00-39\n",
      "  done: false\n",
      "  episode_len_mean: 91.02752293577981\n",
      "  episode_reward_max: 208.7400335752029\n",
      "  episode_reward_mean: 155.62530180425966\n",
      "  episode_reward_min: -130.26837932078308\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 13628\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.126\n",
      "    load_time_ms: 2.499\n",
      "    num_steps_sampled: 1550000\n",
      "    num_steps_trained: 1550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.802596928649634e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8481937050819397\n",
      "      kl: 0.00461295573040843\n",
      "      policy_loss: -0.0010071381693705916\n",
      "      total_loss: 23.220598220825195\n",
      "      vf_explained_var: 0.9462305903434753\n",
      "      vf_loss: 23.22160530090332\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.5920550675924255e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.657001793384552\n",
      "      kl: 0.006176071707159281\n",
      "      policy_loss: -0.001527311047539115\n",
      "      total_loss: 35.363800048828125\n",
      "      vf_explained_var: 0.9648417234420776\n",
      "      vf_loss: 35.36532974243164\n",
      "    sample_time_ms: 19974.779\n",
      "    update_time_ms: 7.833\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.39347084348384\n",
      "    rl_1: 103.2318309607758\n",
      "  time_since_restore: 3830.580597639084\n",
      "  time_this_iter_s: 23.154008150100708\n",
      "  time_total_s: 3830.580597639084\n",
      "  timestamp: 1550797239\n",
      "  timesteps_since_restore: 1550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1550000\n",
      "  training_iteration: 155\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3830 s, 155 iter, 1550000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-01-03\n",
      "  done: false\n",
      "  episode_len_mean: 94.51886792452831\n",
      "  episode_reward_max: 211.72692605962857\n",
      "  episode_reward_mean: 157.75726523877012\n",
      "  episode_reward_min: -139.65139037895406\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 13734\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.151\n",
      "    load_time_ms: 2.523\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.401298464324817e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8845170140266418\n",
      "      kl: 0.008270248770713806\n",
      "      policy_loss: -0.0025433783885091543\n",
      "      total_loss: 24.557607650756836\n",
      "      vf_explained_var: 0.9459018111228943\n",
      "      vf_loss: 24.560150146484375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2953268845640504e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7398260235786438\n",
      "      kl: 0.0037127782125025988\n",
      "      policy_loss: -0.0008549560443498194\n",
      "      total_loss: 41.88964080810547\n",
      "      vf_explained_var: 0.9565377831459045\n",
      "      vf_loss: 41.890499114990234\n",
      "    sample_time_ms: 20002.14\n",
      "    update_time_ms: 7.746\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.07324112807733\n",
      "    rl_1: 102.68402411069283\n",
      "  time_since_restore: 3854.0413269996643\n",
      "  time_this_iter_s: 23.460729360580444\n",
      "  time_total_s: 3854.0413269996643\n",
      "  timestamp: 1550797263\n",
      "  timesteps_since_restore: 1560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 156\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3854 s, 156 iter, 1560000 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-01-26\n",
      "  done: false\n",
      "  episode_len_mean: 88.66371681415929\n",
      "  episode_reward_max: 214.6796662404754\n",
      "  episode_reward_mean: 147.2007575031313\n",
      "  episode_reward_min: -142.7699093896744\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 13847\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.996\n",
      "    load_time_ms: 2.516\n",
      "    num_steps_sampled: 1570000\n",
      "    num_steps_trained: 1570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7903431057929993\n",
      "      kl: 0.009362115524709225\n",
      "      policy_loss: -0.0028042683843523264\n",
      "      total_loss: 105.72875213623047\n",
      "      vf_explained_var: 0.8032039999961853\n",
      "      vf_loss: 105.7315673828125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1476634422820252e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6512321829795837\n",
      "      kl: 0.005972013343125582\n",
      "      policy_loss: -0.000649722118396312\n",
      "      total_loss: 166.2340087890625\n",
      "      vf_explained_var: 0.849757969379425\n",
      "      vf_loss: 166.23464965820312\n",
      "    sample_time_ms: 20006.872\n",
      "    update_time_ms: 7.77\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.32323447389501\n",
      "    rl_1: 95.8775230292363\n",
      "  time_since_restore: 3877.124255180359\n",
      "  time_this_iter_s: 23.08292818069458\n",
      "  time_total_s: 3877.124255180359\n",
      "  timestamp: 1550797286\n",
      "  timesteps_since_restore: 1570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1570000\n",
      "  training_iteration: 157\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3877 s, 157 iter, 1570000 ts, 147 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 93.03703703703704\n",
      "  episode_reward_max: 208.96466286908736\n",
      "  episode_reward_mean: 148.6632637087893\n",
      "  episode_reward_min: -151.0197202015418\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 13955\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3158.999\n",
      "    load_time_ms: 2.487\n",
      "    num_steps_sampled: 1580000\n",
      "    num_steps_trained: 1580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8351167440414429\n",
      "      kl: 0.007479771040380001\n",
      "      policy_loss: -0.0031175795011222363\n",
      "      total_loss: 67.12535095214844\n",
      "      vf_explained_var: 0.8445329666137695\n",
      "      vf_loss: 67.12845611572266\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.74532370373175e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6949010491371155\n",
      "      kl: 0.003820253536105156\n",
      "      policy_loss: -0.0017202236922457814\n",
      "      total_loss: 105.69163513183594\n",
      "      vf_explained_var: 0.8900442123413086\n",
      "      vf_loss: 105.69337463378906\n",
      "    sample_time_ms: 20021.486\n",
      "    update_time_ms: 7.615\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.49775372091489\n",
      "    rl_1: 98.16550998787437\n",
      "  time_since_restore: 3900.6355543136597\n",
      "  time_this_iter_s: 23.51129913330078\n",
      "  time_total_s: 3900.6355543136597\n",
      "  timestamp: 1550797310\n",
      "  timesteps_since_restore: 1580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1580000\n",
      "  training_iteration: 158\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3900 s, 158 iter, 1580000 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-02-13\n",
      "  done: false\n",
      "  episode_len_mean: 94.17924528301887\n",
      "  episode_reward_max: 213.96871515610727\n",
      "  episode_reward_mean: 157.30918284277953\n",
      "  episode_reward_min: -95.53924838848805\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 14061\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.955\n",
      "    load_time_ms: 2.445\n",
      "    num_steps_sampled: 1590000\n",
      "    num_steps_trained: 1590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8344274759292603\n",
      "      kl: 0.008961559273302555\n",
      "      policy_loss: -0.003666885429993272\n",
      "      total_loss: 9.744352340698242\n",
      "      vf_explained_var: 0.9803738594055176\n",
      "      vf_loss: 9.74802017211914\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.872661851865875e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.722255289554596\n",
      "      kl: 0.0076791513711214066\n",
      "      policy_loss: -0.002785625634714961\n",
      "      total_loss: 13.790925025939941\n",
      "      vf_explained_var: 0.9905956983566284\n",
      "      vf_loss: 13.793710708618164\n",
      "    sample_time_ms: 20041.456\n",
      "    update_time_ms: 7.238\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.62555150531223\n",
      "    rl_1: 102.68363133746733\n",
      "  time_since_restore: 3923.9724655151367\n",
      "  time_this_iter_s: 23.33691120147705\n",
      "  time_total_s: 3923.9724655151367\n",
      "  timestamp: 1550797333\n",
      "  timesteps_since_restore: 1590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1590000\n",
      "  training_iteration: 159\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3923 s, 159 iter, 1590000 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-02-36\n",
      "  done: false\n",
      "  episode_len_mean: 92.52777777777777\n",
      "  episode_reward_max: 213.40632648116653\n",
      "  episode_reward_mean: 158.73113021733937\n",
      "  episode_reward_min: -122.36234082954762\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 14169\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.277\n",
      "    load_time_ms: 2.381\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7690914869308472\n",
      "      kl: 0.0071768066845834255\n",
      "      policy_loss: -0.0025051177944988012\n",
      "      total_loss: 36.95050048828125\n",
      "      vf_explained_var: 0.923986554145813\n",
      "      vf_loss: 36.952999114990234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4293244336113134e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6819833517074585\n",
      "      kl: 0.004764669109135866\n",
      "      policy_loss: -0.0016808813670650125\n",
      "      total_loss: 53.939239501953125\n",
      "      vf_explained_var: 0.9510679244995117\n",
      "      vf_loss: 53.9409294128418\n",
      "    sample_time_ms: 20037.476\n",
      "    update_time_ms: 7.44\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.32233144826693\n",
      "    rl_1: 103.40879876907242\n",
      "  time_since_restore: 3946.9846436977386\n",
      "  time_this_iter_s: 23.01217818260193\n",
      "  time_total_s: 3946.9846436977386\n",
      "  timestamp: 1550797356\n",
      "  timesteps_since_restore: 1600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 160\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3946 s, 160 iter, 1600000 ts, 159 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-02-59\n",
      "  done: false\n",
      "  episode_len_mean: 93.98113207547169\n",
      "  episode_reward_max: 211.865157356731\n",
      "  episode_reward_mean: 158.89559735962786\n",
      "  episode_reward_min: -141.74573206393913\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 14275\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.153\n",
      "    load_time_ms: 2.485\n",
      "    num_steps_sampled: 1610000\n",
      "    num_steps_trained: 1610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.852921187877655\n",
      "      kl: 0.011507625691592693\n",
      "      policy_loss: -0.009432164952158928\n",
      "      total_loss: 24.758651733398438\n",
      "      vf_explained_var: 0.9479237794876099\n",
      "      vf_loss: 24.768083572387695\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.146622168056567e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.704394519329071\n",
      "      kl: 0.01242261752486229\n",
      "      policy_loss: -0.002585460664704442\n",
      "      total_loss: 48.33335876464844\n",
      "      vf_explained_var: 0.949018120765686\n",
      "      vf_loss: 48.33594512939453\n",
      "    sample_time_ms: 20061.525\n",
      "    update_time_ms: 7.576\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.76869684287077\n",
      "    rl_1: 101.1269005167571\n",
      "  time_since_restore: 3970.115973711014\n",
      "  time_this_iter_s: 23.131330013275146\n",
      "  time_total_s: 3970.115973711014\n",
      "  timestamp: 1550797379\n",
      "  timesteps_since_restore: 1610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1610000\n",
      "  training_iteration: 161\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3970 s, 161 iter, 1610000 ts, 159 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-03-22\n",
      "  done: false\n",
      "  episode_len_mean: 97.2135922330097\n",
      "  episode_reward_max: 213.10571846307516\n",
      "  episode_reward_mean: 154.97396673674368\n",
      "  episode_reward_min: -128.27650723893288\n",
      "  episodes_this_iter: 103\n",
      "  episodes_total: 14378\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.654\n",
      "    load_time_ms: 2.561\n",
      "    num_steps_sampled: 1620000\n",
      "    num_steps_trained: 1620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9501671195030212\n",
      "      kl: 0.005955543834716082\n",
      "      policy_loss: -0.0024191562552005053\n",
      "      total_loss: 40.6891975402832\n",
      "      vf_explained_var: 0.9245246648788452\n",
      "      vf_loss: 40.69161605834961\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.146622168056567e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.818098783493042\n",
      "      kl: 0.008031873032450676\n",
      "      policy_loss: -0.003756047924980521\n",
      "      total_loss: 58.50356674194336\n",
      "      vf_explained_var: 0.9509164690971375\n",
      "      vf_loss: 58.50733184814453\n",
      "    sample_time_ms: 20071.668\n",
      "    update_time_ms: 7.433\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.79389765653167\n",
      "    rl_1: 101.180069080212\n",
      "  time_since_restore: 3993.0976157188416\n",
      "  time_this_iter_s: 22.98164200782776\n",
      "  time_total_s: 3993.0976157188416\n",
      "  timestamp: 1550797402\n",
      "  timesteps_since_restore: 1620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1620000\n",
      "  training_iteration: 162\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 3993 s, 162 iter, 1620000 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-03-45\n",
      "  done: false\n",
      "  episode_len_mean: 89.76785714285714\n",
      "  episode_reward_max: 212.51468153966795\n",
      "  episode_reward_mean: 158.07055672563442\n",
      "  episode_reward_min: -136.8454344459896\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 14490\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3151.596\n",
      "    load_time_ms: 2.53\n",
      "    num_steps_sampled: 1630000\n",
      "    num_steps_trained: 1630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.821759045124054\n",
      "      kl: 0.006282716058194637\n",
      "      policy_loss: -0.005307702347636223\n",
      "      total_loss: 13.649924278259277\n",
      "      vf_explained_var: 0.9671854972839355\n",
      "      vf_loss: 13.655232429504395\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6433760072445244e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6804977655410767\n",
      "      kl: 0.005895722191780806\n",
      "      policy_loss: -0.0006312061450444162\n",
      "      total_loss: 20.88231086730957\n",
      "      vf_explained_var: 0.9793133735656738\n",
      "      vf_loss: 20.88294219970703\n",
      "    sample_time_ms: 20032.649\n",
      "    update_time_ms: 7.346\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.82249724406749\n",
      "    rl_1: 102.24805948156693\n",
      "  time_since_restore: 4016.1014540195465\n",
      "  time_this_iter_s: 23.003838300704956\n",
      "  time_total_s: 4016.1014540195465\n",
      "  timestamp: 1550797425\n",
      "  timesteps_since_restore: 1630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1630000\n",
      "  training_iteration: 163\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4016 s, 163 iter, 1630000 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-04-09\n",
      "  done: false\n",
      "  episode_len_mean: 91.38532110091744\n",
      "  episode_reward_max: 211.60733276980642\n",
      "  episode_reward_mean: 164.01205081550043\n",
      "  episode_reward_min: 95.45693419069447\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 14599\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3151.076\n",
      "    load_time_ms: 2.544\n",
      "    num_steps_sampled: 1640000\n",
      "    num_steps_trained: 1640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8032996654510498\n",
      "      kl: 0.007878405973315239\n",
      "      policy_loss: -0.0025545135140419006\n",
      "      total_loss: 2.822016477584839\n",
      "      vf_explained_var: 0.9925610423088074\n",
      "      vf_loss: 2.824571371078491\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8216880036222622e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6626675128936768\n",
      "      kl: 0.00930787157267332\n",
      "      policy_loss: -0.0032299458980560303\n",
      "      total_loss: 6.621167182922363\n",
      "      vf_explained_var: 0.9928433299064636\n",
      "      vf_loss: 6.624396800994873\n",
      "    sample_time_ms: 20035.573\n",
      "    update_time_ms: 7.431\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.10751116002933\n",
      "    rl_1: 105.90453965547107\n",
      "  time_since_restore: 4039.561546564102\n",
      "  time_this_iter_s: 23.460092544555664\n",
      "  time_total_s: 4039.561546564102\n",
      "  timestamp: 1550797449\n",
      "  timesteps_since_restore: 1640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1640000\n",
      "  training_iteration: 164\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4039 s, 164 iter, 1640000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-04-32\n",
      "  done: false\n",
      "  episode_len_mean: 90.08108108108108\n",
      "  episode_reward_max: 211.94422031385798\n",
      "  episode_reward_mean: 161.2660109031587\n",
      "  episode_reward_min: -132.94352168218984\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 14710\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.58\n",
      "    load_time_ms: 2.538\n",
      "    num_steps_sampled: 1650000\n",
      "    num_steps_trained: 1650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7975943088531494\n",
      "      kl: 0.008673271164298058\n",
      "      policy_loss: -0.007300197146832943\n",
      "      total_loss: 26.21456527709961\n",
      "      vf_explained_var: 0.9416259527206421\n",
      "      vf_loss: 26.221866607666016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.407790785948902e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6546157598495483\n",
      "      kl: 0.007947714999318123\n",
      "      policy_loss: -0.0018594914581626654\n",
      "      total_loss: 43.17610168457031\n",
      "      vf_explained_var: 0.9578471779823303\n",
      "      vf_loss: 43.17796325683594\n",
      "    sample_time_ms: 20045.099\n",
      "    update_time_ms: 7.152\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.88680994402219\n",
      "    rl_1: 102.37920095913653\n",
      "  time_since_restore: 4062.7233743667603\n",
      "  time_this_iter_s: 23.16182780265808\n",
      "  time_total_s: 4062.7233743667603\n",
      "  timestamp: 1550797472\n",
      "  timesteps_since_restore: 1650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1650000\n",
      "  training_iteration: 165\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4062 s, 165 iter, 1650000 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-04-56\n",
      "  done: false\n",
      "  episode_len_mean: 90.54954954954955\n",
      "  episode_reward_max: 212.72313213711405\n",
      "  episode_reward_mean: 164.46069207659545\n",
      "  episode_reward_min: 98.45409174387947\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 14821\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.012\n",
      "    load_time_ms: 2.519\n",
      "    num_steps_sampled: 1660000\n",
      "    num_steps_trained: 1660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7863959074020386\n",
      "      kl: 0.006915762089192867\n",
      "      policy_loss: -0.004025778733193874\n",
      "      total_loss: 3.090902090072632\n",
      "      vf_explained_var: 0.9919828772544861\n",
      "      vf_loss: 3.0949275493621826\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.203895392974451e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6371304988861084\n",
      "      kl: 0.00422413507476449\n",
      "      policy_loss: -0.002632114104926586\n",
      "      total_loss: 5.6336541175842285\n",
      "      vf_explained_var: 0.9939324855804443\n",
      "      vf_loss: 5.63628625869751\n",
      "    sample_time_ms: 20056.839\n",
      "    update_time_ms: 6.957\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 59.49194250290133\n",
      "    rl_1: 104.96874957369414\n",
      "  time_since_restore: 4086.305631160736\n",
      "  time_this_iter_s: 23.58225679397583\n",
      "  time_total_s: 4086.305631160736\n",
      "  timestamp: 1550797496\n",
      "  timesteps_since_restore: 1660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1660000\n",
      "  training_iteration: 166\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4086 s, 166 iter, 1660000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-05-19\n",
      "  done: false\n",
      "  episode_len_mean: 90.76146788990826\n",
      "  episode_reward_max: 215.37760495084726\n",
      "  episode_reward_mean: 158.02674614078734\n",
      "  episode_reward_min: -132.8529462024226\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 14930\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.293\n",
      "    load_time_ms: 2.512\n",
      "    num_steps_sampled: 1670000\n",
      "    num_steps_trained: 1670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7710868716239929\n",
      "      kl: 0.005526745226234198\n",
      "      policy_loss: -0.0025272315833717585\n",
      "      total_loss: 80.9801254272461\n",
      "      vf_explained_var: 0.842261791229248\n",
      "      vf_loss: 80.9826431274414\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.802596928649634e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6908466815948486\n",
      "      kl: 0.021320268511772156\n",
      "      policy_loss: -0.007617162540555\n",
      "      total_loss: 119.89280700683594\n",
      "      vf_explained_var: 0.8888786435127258\n",
      "      vf_loss: 119.9004135131836\n",
      "    sample_time_ms: 20029.251\n",
      "    update_time_ms: 7.109\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.73593527171156\n",
      "    rl_1: 101.2908108690758\n",
      "  time_since_restore: 4109.108674049377\n",
      "  time_this_iter_s: 22.803042888641357\n",
      "  time_total_s: 4109.108674049377\n",
      "  timestamp: 1550797519\n",
      "  timesteps_since_restore: 1670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1670000\n",
      "  training_iteration: 167\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4109 s, 167 iter, 1670000 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-05-42\n",
      "  done: false\n",
      "  episode_len_mean: 89.30973451327434\n",
      "  episode_reward_max: 209.76659622643965\n",
      "  episode_reward_mean: 155.71350898169018\n",
      "  episode_reward_min: -126.0188363939215\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 15043\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.665\n",
      "    load_time_ms: 2.45\n",
      "    num_steps_sampled: 1680000\n",
      "    num_steps_trained: 1680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7876729369163513\n",
      "      kl: 0.006954117678105831\n",
      "      policy_loss: -0.0032986945006996393\n",
      "      total_loss: 59.524864196777344\n",
      "      vf_explained_var: 0.8796334862709045\n",
      "      vf_loss: 59.52816390991211\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.802596928649634e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6723304390907288\n",
      "      kl: 0.003470923285931349\n",
      "      policy_loss: -0.00195816857740283\n",
      "      total_loss: 85.51908111572266\n",
      "      vf_explained_var: 0.9217074513435364\n",
      "      vf_loss: 85.52104949951172\n",
      "    sample_time_ms: 19996.636\n",
      "    update_time_ms: 6.996\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.406357945679474\n",
      "    rl_1: 100.3071510360107\n",
      "  time_since_restore: 4132.156958818436\n",
      "  time_this_iter_s: 23.048284769058228\n",
      "  time_total_s: 4132.156958818436\n",
      "  timestamp: 1550797542\n",
      "  timesteps_since_restore: 1680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1680000\n",
      "  training_iteration: 168\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4132 s, 168 iter, 1680000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-06-05\n",
      "  done: false\n",
      "  episode_len_mean: 90.87272727272727\n",
      "  episode_reward_max: 214.29757986670066\n",
      "  episode_reward_mean: 162.13101951091286\n",
      "  episode_reward_min: 103.15586671487614\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 15153\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.286\n",
      "    load_time_ms: 2.455\n",
      "    num_steps_sampled: 1690000\n",
      "    num_steps_trained: 1690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8109620213508606\n",
      "      kl: 0.006172766909003258\n",
      "      policy_loss: -0.002877028426155448\n",
      "      total_loss: 3.9344587326049805\n",
      "      vf_explained_var: 0.9892885088920593\n",
      "      vf_loss: 3.9373364448547363\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.401298464324817e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6908622980117798\n",
      "      kl: 0.009869484230875969\n",
      "      policy_loss: -0.004699515178799629\n",
      "      total_loss: 6.456722736358643\n",
      "      vf_explained_var: 0.9929652214050293\n",
      "      vf_loss: 6.461422443389893\n",
      "    sample_time_ms: 20004.378\n",
      "    update_time_ms: 7.104\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.47472732397479\n",
      "    rl_1: 104.65629218693809\n",
      "  time_since_restore: 4155.601838588715\n",
      "  time_this_iter_s: 23.44487977027893\n",
      "  time_total_s: 4155.601838588715\n",
      "  timestamp: 1550797565\n",
      "  timesteps_since_restore: 1690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1690000\n",
      "  training_iteration: 169\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4155 s, 169 iter, 1690000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-06-29\n",
      "  done: false\n",
      "  episode_len_mean: 93.20560747663552\n",
      "  episode_reward_max: 214.6981636039369\n",
      "  episode_reward_mean: 157.70760085172293\n",
      "  episode_reward_min: -142.39986846586845\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 15260\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.486\n",
      "    load_time_ms: 2.46\n",
      "    num_steps_sampled: 1700000\n",
      "    num_steps_trained: 1700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8442091345787048\n",
      "      kl: 0.007426272612065077\n",
      "      policy_loss: -0.00477562565356493\n",
      "      total_loss: 83.20228576660156\n",
      "      vf_explained_var: 0.8605515956878662\n",
      "      vf_loss: 83.20706176757812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7010113596916199\n",
      "      kl: 0.0056831506080925465\n",
      "      policy_loss: -0.003237613942474127\n",
      "      total_loss: 124.39225006103516\n",
      "      vf_explained_var: 0.881798267364502\n",
      "      vf_loss: 124.39549255371094\n",
      "    sample_time_ms: 20034.831\n",
      "    update_time_ms: 6.686\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.824329134954105\n",
      "    rl_1: 100.88327171676882\n",
      "  time_since_restore: 4178.91764497757\n",
      "  time_this_iter_s: 23.31580638885498\n",
      "  time_total_s: 4178.91764497757\n",
      "  timestamp: 1550797589\n",
      "  timesteps_since_restore: 1700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1700000\n",
      "  training_iteration: 170\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4178 s, 170 iter, 1700000 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-06-52\n",
      "  done: false\n",
      "  episode_len_mean: 91.59633027522936\n",
      "  episode_reward_max: 213.95949178653836\n",
      "  episode_reward_mean: 164.94716214339022\n",
      "  episode_reward_min: -131.3146117305933\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 15369\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.969\n",
      "    load_time_ms: 2.371\n",
      "    num_steps_sampled: 1710000\n",
      "    num_steps_trained: 1710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.80491703748703\n",
      "      kl: 0.005015574861317873\n",
      "      policy_loss: -0.003415087005123496\n",
      "      total_loss: 29.014225006103516\n",
      "      vf_explained_var: 0.9365620017051697\n",
      "      vf_loss: 29.017637252807617\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6715342402458191\n",
      "      kl: 0.009217062965035439\n",
      "      policy_loss: -0.0025295137893408537\n",
      "      total_loss: 42.89276885986328\n",
      "      vf_explained_var: 0.9568691849708557\n",
      "      vf_loss: 42.89529800415039\n",
      "    sample_time_ms: 20028.279\n",
      "    update_time_ms: 6.868\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 59.862808268769186\n",
      "    rl_1: 105.08435387462106\n",
      "  time_since_restore: 4202.147748231888\n",
      "  time_this_iter_s: 23.230103254318237\n",
      "  time_total_s: 4202.147748231888\n",
      "  timestamp: 1550797612\n",
      "  timesteps_since_restore: 1710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1710000\n",
      "  training_iteration: 171\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4202 s, 171 iter, 1710000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-07-15\n",
      "  done: false\n",
      "  episode_len_mean: 89.75675675675676\n",
      "  episode_reward_max: 212.1336437093273\n",
      "  episode_reward_mean: 164.75624385553147\n",
      "  episode_reward_min: 117.54715699392194\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 15480\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.731\n",
      "    load_time_ms: 2.264\n",
      "    num_steps_sampled: 1720000\n",
      "    num_steps_trained: 1720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8179154992103577\n",
      "      kl: 0.005322863347828388\n",
      "      policy_loss: -0.0038296435959637165\n",
      "      total_loss: 3.920846700668335\n",
      "      vf_explained_var: 0.9898643493652344\n",
      "      vf_loss: 3.9246766567230225\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6506518125534058\n",
      "      kl: 0.006935949437320232\n",
      "      policy_loss: -0.004028904717415571\n",
      "      total_loss: 6.0086669921875\n",
      "      vf_explained_var: 0.9934817552566528\n",
      "      vf_loss: 6.012696266174316\n",
      "    sample_time_ms: 20011.341\n",
      "    update_time_ms: 7.115\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.11541438171886\n",
      "    rl_1: 104.64082947381263\n",
      "  time_since_restore: 4224.863609790802\n",
      "  time_this_iter_s: 22.715861558914185\n",
      "  time_total_s: 4224.863609790802\n",
      "  timestamp: 1550797635\n",
      "  timesteps_since_restore: 1720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1720000\n",
      "  training_iteration: 172\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4224 s, 172 iter, 1720000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-07-38\n",
      "  done: false\n",
      "  episode_len_mean: 93.47222222222223\n",
      "  episode_reward_max: 212.20638636884436\n",
      "  episode_reward_mean: 162.51221213627065\n",
      "  episode_reward_min: -158.1399236589664\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 15588\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.198\n",
      "    load_time_ms: 2.255\n",
      "    num_steps_sampled: 1730000\n",
      "    num_steps_trained: 1730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8325326442718506\n",
      "      kl: 0.008242140524089336\n",
      "      policy_loss: -0.0023322622291743755\n",
      "      total_loss: 9.454742431640625\n",
      "      vf_explained_var: 0.983005702495575\n",
      "      vf_loss: 9.457077026367188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6572638154029846\n",
      "      kl: 0.005242849234491587\n",
      "      policy_loss: -0.0012638550251722336\n",
      "      total_loss: 28.817041397094727\n",
      "      vf_explained_var: 0.9748204350471497\n",
      "      vf_loss: 28.81830596923828\n",
      "    sample_time_ms: 20030.417\n",
      "    update_time_ms: 7.122\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.33344700773593\n",
      "    rl_1: 104.17876512853478\n",
      "  time_since_restore: 4248.032719135284\n",
      "  time_this_iter_s: 23.169109344482422\n",
      "  time_total_s: 4248.032719135284\n",
      "  timestamp: 1550797658\n",
      "  timesteps_since_restore: 1730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1730000\n",
      "  training_iteration: 173\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4248 s, 173 iter, 1730000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-08-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.5\n",
      "  episode_reward_max: 213.60266948883358\n",
      "  episode_reward_mean: 167.7043381353224\n",
      "  episode_reward_min: 110.65509956768241\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 15698\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.333\n",
      "    load_time_ms: 2.206\n",
      "    num_steps_sampled: 1740000\n",
      "    num_steps_trained: 1740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7689841389656067\n",
      "      kl: 0.010623264126479626\n",
      "      policy_loss: -0.004469988867640495\n",
      "      total_loss: 2.915031671524048\n",
      "      vf_explained_var: 0.99294513463974\n",
      "      vf_loss: 2.919501781463623\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.606920063495636\n",
      "      kl: 0.003297876799479127\n",
      "      policy_loss: -0.002740143798291683\n",
      "      total_loss: 5.343310356140137\n",
      "      vf_explained_var: 0.994566798210144\n",
      "      vf_loss: 5.346048831939697\n",
      "    sample_time_ms: 19983.363\n",
      "    update_time_ms: 6.829\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.51492639436034\n",
      "    rl_1: 106.189411740962\n",
      "  time_since_restore: 4271.016766309738\n",
      "  time_this_iter_s: 22.984047174453735\n",
      "  time_total_s: 4271.016766309738\n",
      "  timestamp: 1550797681\n",
      "  timesteps_since_restore: 1740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1740000\n",
      "  training_iteration: 174\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4271 s, 174 iter, 1740000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-08-24\n",
      "  done: false\n",
      "  episode_len_mean: 89.29464285714286\n",
      "  episode_reward_max: 209.41640501583998\n",
      "  episode_reward_mean: 157.79880242512436\n",
      "  episode_reward_min: -160.6989924356549\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 15810\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.557\n",
      "    load_time_ms: 2.165\n",
      "    num_steps_sampled: 1750000\n",
      "    num_steps_trained: 1750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7811149954795837\n",
      "      kl: 0.008918333798646927\n",
      "      policy_loss: -0.0027872263453900814\n",
      "      total_loss: 44.24068069458008\n",
      "      vf_explained_var: 0.9107779264450073\n",
      "      vf_loss: 44.243473052978516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5984841585159302\n",
      "      kl: 0.006823728326708078\n",
      "      policy_loss: -0.002025672933086753\n",
      "      total_loss: 69.47727966308594\n",
      "      vf_explained_var: 0.9358087182044983\n",
      "      vf_loss: 69.47930145263672\n",
      "    sample_time_ms: 19964.076\n",
      "    update_time_ms: 6.878\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.374854006100364\n",
      "    rl_1: 100.423948419024\n",
      "  time_since_restore: 4293.958384037018\n",
      "  time_this_iter_s: 22.941617727279663\n",
      "  time_total_s: 4293.958384037018\n",
      "  timestamp: 1550797704\n",
      "  timesteps_since_restore: 1750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1750000\n",
      "  training_iteration: 175\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4293 s, 175 iter, 1750000 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-08-47\n",
      "  done: false\n",
      "  episode_len_mean: 89.92792792792793\n",
      "  episode_reward_max: 215.37654724685103\n",
      "  episode_reward_mean: 166.8412615845855\n",
      "  episode_reward_min: -133.78624929903236\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 15921\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.482\n",
      "    load_time_ms: 2.164\n",
      "    num_steps_sampled: 1760000\n",
      "    num_steps_trained: 1760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.773932933807373\n",
      "      kl: 0.004680480342358351\n",
      "      policy_loss: -0.0010466937674209476\n",
      "      total_loss: 28.563695907592773\n",
      "      vf_explained_var: 0.9403985142707825\n",
      "      vf_loss: 28.564741134643555\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6054676175117493\n",
      "      kl: 0.002300033811479807\n",
      "      policy_loss: -0.0007919146446511149\n",
      "      total_loss: 43.63601303100586\n",
      "      vf_explained_var: 0.9577540159225464\n",
      "      vf_loss: 43.63681411743164\n",
      "    sample_time_ms: 19929.076\n",
      "    update_time_ms: 6.966\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.69263590491927\n",
      "    rl_1: 105.14862567966627\n",
      "  time_since_restore: 4317.1817715168\n",
      "  time_this_iter_s: 23.223387479782104\n",
      "  time_total_s: 4317.1817715168\n",
      "  timestamp: 1550797727\n",
      "  timesteps_since_restore: 1760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1760000\n",
      "  training_iteration: 176\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4317 s, 176 iter, 1760000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-09-11\n",
      "  done: false\n",
      "  episode_len_mean: 89.08035714285714\n",
      "  episode_reward_max: 207.6022938580618\n",
      "  episode_reward_mean: 166.42869050086034\n",
      "  episode_reward_min: 125.20015536382452\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 16033\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.035\n",
      "    load_time_ms: 2.22\n",
      "    num_steps_sampled: 1770000\n",
      "    num_steps_trained: 1770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7544023394584656\n",
      "      kl: 0.008434913121163845\n",
      "      policy_loss: -0.004171143751591444\n",
      "      total_loss: 2.9344472885131836\n",
      "      vf_explained_var: 0.9917538166046143\n",
      "      vf_loss: 2.9386186599731445\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5657342076301575\n",
      "      kl: 0.004997683688998222\n",
      "      policy_loss: -0.0024424816947430372\n",
      "      total_loss: 4.056977272033691\n",
      "      vf_explained_var: 0.9957119822502136\n",
      "      vf_loss: 4.05941915512085\n",
      "    sample_time_ms: 20020.669\n",
      "    update_time_ms: 6.815\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.23493442096609\n",
      "    rl_1: 105.19375607989426\n",
      "  time_since_restore: 4340.875173091888\n",
      "  time_this_iter_s: 23.6934015750885\n",
      "  time_total_s: 4340.875173091888\n",
      "  timestamp: 1550797751\n",
      "  timesteps_since_restore: 1770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1770000\n",
      "  training_iteration: 177\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4340 s, 177 iter, 1770000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-09-34\n",
      "  done: false\n",
      "  episode_len_mean: 94.79245283018868\n",
      "  episode_reward_max: 214.38813920002326\n",
      "  episode_reward_mean: 166.21164098762418\n",
      "  episode_reward_min: -114.82881727303571\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 16139\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.702\n",
      "    load_time_ms: 2.249\n",
      "    num_steps_sampled: 1780000\n",
      "    num_steps_trained: 1780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8572778701782227\n",
      "      kl: 0.008182564750313759\n",
      "      policy_loss: -0.0020502423867583275\n",
      "      total_loss: 11.719609260559082\n",
      "      vf_explained_var: 0.9801730513572693\n",
      "      vf_loss: 11.721659660339355\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6921014785766602\n",
      "      kl: 0.005860181991010904\n",
      "      policy_loss: -0.0015279361978173256\n",
      "      total_loss: 18.16962242126465\n",
      "      vf_explained_var: 0.9849424362182617\n",
      "      vf_loss: 18.171154022216797\n",
      "    sample_time_ms: 20073.398\n",
      "    update_time_ms: 6.829\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.97511069191186\n",
      "    rl_1: 103.23653029571234\n",
      "  time_since_restore: 4364.435351133347\n",
      "  time_this_iter_s: 23.56017804145813\n",
      "  time_total_s: 4364.435351133347\n",
      "  timestamp: 1550797774\n",
      "  timesteps_since_restore: 1780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1780000\n",
      "  training_iteration: 178\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4364 s, 178 iter, 1780000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-09-58\n",
      "  done: false\n",
      "  episode_len_mean: 90.9\n",
      "  episode_reward_max: 215.69628437169786\n",
      "  episode_reward_mean: 167.31469405330725\n",
      "  episode_reward_min: -130.56797242043626\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 16249\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.678\n",
      "    load_time_ms: 2.309\n",
      "    num_steps_sampled: 1790000\n",
      "    num_steps_trained: 1790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7464504837989807\n",
      "      kl: 0.007952696643769741\n",
      "      policy_loss: -0.0045673237182199955\n",
      "      total_loss: 29.12738800048828\n",
      "      vf_explained_var: 0.939525306224823\n",
      "      vf_loss: 29.1319522857666\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5596476197242737\n",
      "      kl: 0.004356143530458212\n",
      "      policy_loss: -0.0007784173940308392\n",
      "      total_loss: 43.07258224487305\n",
      "      vf_explained_var: 0.9603959918022156\n",
      "      vf_loss: 43.07335662841797\n",
      "    sample_time_ms: 20071.404\n",
      "    update_time_ms: 6.747\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.918803872534596\n",
      "    rl_1: 106.39589018077268\n",
      "  time_since_restore: 4387.853732585907\n",
      "  time_this_iter_s: 23.418381452560425\n",
      "  time_total_s: 4387.853732585907\n",
      "  timestamp: 1550797798\n",
      "  timesteps_since_restore: 1790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1790000\n",
      "  training_iteration: 179\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4387 s, 179 iter, 1790000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-10-21\n",
      "  done: false\n",
      "  episode_len_mean: 87.91228070175438\n",
      "  episode_reward_max: 214.4643142481328\n",
      "  episode_reward_mean: 152.9905630824355\n",
      "  episode_reward_min: -178.1845085440903\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 16363\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.043\n",
      "    load_time_ms: 2.367\n",
      "    num_steps_sampled: 1800000\n",
      "    num_steps_trained: 1800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7593287229537964\n",
      "      kl: 0.011253923177719116\n",
      "      policy_loss: -0.005245225969702005\n",
      "      total_loss: 135.35824584960938\n",
      "      vf_explained_var: 0.7767233848571777\n",
      "      vf_loss: 135.36346435546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5493819117546082\n",
      "      kl: 0.005639167502522469\n",
      "      policy_loss: -0.0022391502279788256\n",
      "      total_loss: 224.47921752929688\n",
      "      vf_explained_var: 0.8159884214401245\n",
      "      vf_loss: 224.48147583007812\n",
      "    sample_time_ms: 20048.641\n",
      "    update_time_ms: 6.817\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.398840116821205\n",
      "    rl_1: 96.59172296561432\n",
      "  time_since_restore: 4411.120875597\n",
      "  time_this_iter_s: 23.26714301109314\n",
      "  time_total_s: 4411.120875597\n",
      "  timestamp: 1550797821\n",
      "  timesteps_since_restore: 1800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1800000\n",
      "  training_iteration: 180\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4411 s, 180 iter, 1800000 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-10-44\n",
      "  done: false\n",
      "  episode_len_mean: 87.29824561403508\n",
      "  episode_reward_max: 215.2646251694496\n",
      "  episode_reward_mean: 152.90860148809625\n",
      "  episode_reward_min: -164.71733188586336\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 16477\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3123.465\n",
      "    load_time_ms: 2.343\n",
      "    num_steps_sampled: 1810000\n",
      "    num_steps_trained: 1810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7690457701683044\n",
      "      kl: 0.010692871175706387\n",
      "      policy_loss: -0.005679330788552761\n",
      "      total_loss: 104.5296859741211\n",
      "      vf_explained_var: 0.8248303532600403\n",
      "      vf_loss: 104.53536224365234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5740330815315247\n",
      "      kl: 0.006168347783386707\n",
      "      policy_loss: -0.002046413253992796\n",
      "      total_loss: 167.21881103515625\n",
      "      vf_explained_var: 0.8510255813598633\n",
      "      vf_loss: 167.22085571289062\n",
      "    sample_time_ms: 20070.62\n",
      "    update_time_ms: 6.551\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.27450130191503\n",
      "    rl_1: 95.6341001861812\n",
      "  time_since_restore: 4434.360563516617\n",
      "  time_this_iter_s: 23.2396879196167\n",
      "  time_total_s: 4434.360563516617\n",
      "  timestamp: 1550797844\n",
      "  timesteps_since_restore: 1810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1810000\n",
      "  training_iteration: 181\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4434 s, 181 iter, 1810000 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-11-08\n",
      "  done: false\n",
      "  episode_len_mean: 88.65486725663717\n",
      "  episode_reward_max: 215.6399479810528\n",
      "  episode_reward_mean: 152.49144789624214\n",
      "  episode_reward_min: -136.61933237229746\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 16590\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3123.557\n",
      "    load_time_ms: 2.357\n",
      "    num_steps_sampled: 1820000\n",
      "    num_steps_trained: 1820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7849622964859009\n",
      "      kl: 0.004010363481938839\n",
      "      policy_loss: -0.0011751275742426515\n",
      "      total_loss: 76.8558120727539\n",
      "      vf_explained_var: 0.8634381294250488\n",
      "      vf_loss: 76.85698699951172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5704345107078552\n",
      "      kl: 0.00971283856779337\n",
      "      policy_loss: -0.003550067078322172\n",
      "      total_loss: 111.13914489746094\n",
      "      vf_explained_var: 0.8969528079032898\n",
      "      vf_loss: 111.14271545410156\n",
      "    sample_time_ms: 20129.308\n",
      "    update_time_ms: 6.842\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.13748324985718\n",
      "    rl_1: 96.35396464638495\n",
      "  time_since_restore: 4457.665388584137\n",
      "  time_this_iter_s: 23.30482506752014\n",
      "  time_total_s: 4457.665388584137\n",
      "  timestamp: 1550797868\n",
      "  timesteps_since_restore: 1820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1820000\n",
      "  training_iteration: 182\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4457 s, 182 iter, 1820000 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-11-31\n",
      "  done: false\n",
      "  episode_len_mean: 89.08928571428571\n",
      "  episode_reward_max: 216.72193419043242\n",
      "  episode_reward_mean: 163.55031762243328\n",
      "  episode_reward_min: -131.9698031784914\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 16702\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.101\n",
      "    load_time_ms: 2.44\n",
      "    num_steps_sampled: 1830000\n",
      "    num_steps_trained: 1830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7603341341018677\n",
      "      kl: 0.00721904169768095\n",
      "      policy_loss: -0.003582978155463934\n",
      "      total_loss: 26.22678565979004\n",
      "      vf_explained_var: 0.9506919980049133\n",
      "      vf_loss: 26.230371475219727\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5189943313598633\n",
      "      kl: 0.008188690058887005\n",
      "      policy_loss: -0.0031374564860016108\n",
      "      total_loss: 38.284423828125\n",
      "      vf_explained_var: 0.963485598564148\n",
      "      vf_loss: 38.287567138671875\n",
      "    sample_time_ms: 20157.25\n",
      "    update_time_ms: 6.991\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.22942413807312\n",
      "    rl_1: 102.32089348436016\n",
      "  time_since_restore: 4481.12570977211\n",
      "  time_this_iter_s: 23.460321187973022\n",
      "  time_total_s: 4481.12570977211\n",
      "  timestamp: 1550797891\n",
      "  timesteps_since_restore: 1830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1830000\n",
      "  training_iteration: 183\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4481 s, 183 iter, 1830000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-11-55\n",
      "  done: false\n",
      "  episode_len_mean: 89.53571428571429\n",
      "  episode_reward_max: 215.92319109629682\n",
      "  episode_reward_mean: 164.65112534394464\n",
      "  episode_reward_min: -133.16262477511336\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 16814\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3121.79\n",
      "    load_time_ms: 2.504\n",
      "    num_steps_sampled: 1840000\n",
      "    num_steps_trained: 1840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7379061579704285\n",
      "      kl: 0.011720430105924606\n",
      "      policy_loss: -0.002657693112269044\n",
      "      total_loss: 36.658966064453125\n",
      "      vf_explained_var: 0.9308630228042603\n",
      "      vf_loss: 36.661624908447266\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5198227763175964\n",
      "      kl: 0.008276869542896748\n",
      "      policy_loss: -0.0032604967709630728\n",
      "      total_loss: 46.48032760620117\n",
      "      vf_explained_var: 0.952502429485321\n",
      "      vf_loss: 46.483585357666016\n",
      "    sample_time_ms: 20177.466\n",
      "    update_time_ms: 7.315\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.36869414242848\n",
      "    rl_1: 103.28243120151615\n",
      "  time_since_restore: 4504.291265249252\n",
      "  time_this_iter_s: 23.165555477142334\n",
      "  time_total_s: 4504.291265249252\n",
      "  timestamp: 1550797915\n",
      "  timesteps_since_restore: 1840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1840000\n",
      "  training_iteration: 184\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4504 s, 184 iter, 1840000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-12-18\n",
      "  done: false\n",
      "  episode_len_mean: 88.2566371681416\n",
      "  episode_reward_max: 215.70688841037867\n",
      "  episode_reward_mean: 162.99075053207537\n",
      "  episode_reward_min: -142.41165538848648\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 16927\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3120.558\n",
      "    load_time_ms: 2.505\n",
      "    num_steps_sampled: 1850000\n",
      "    num_steps_trained: 1850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7158814072608948\n",
      "      kl: 0.008593151345849037\n",
      "      policy_loss: -0.0031834898982197046\n",
      "      total_loss: 78.55755615234375\n",
      "      vf_explained_var: 0.8612498641014099\n",
      "      vf_loss: 78.56072998046875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5140158534049988\n",
      "      kl: 0.0068566096015274525\n",
      "      policy_loss: -0.0021507791243493557\n",
      "      total_loss: 119.65037536621094\n",
      "      vf_explained_var: 0.8889025449752808\n",
      "      vf_loss: 119.65254211425781\n",
      "    sample_time_ms: 20236.977\n",
      "    update_time_ms: 7.663\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.92987904967405\n",
      "    rl_1: 101.0608714824013\n",
      "  time_since_restore: 4527.819668292999\n",
      "  time_this_iter_s: 23.52840304374695\n",
      "  time_total_s: 4527.819668292999\n",
      "  timestamp: 1550797938\n",
      "  timesteps_since_restore: 1850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1850000\n",
      "  training_iteration: 185\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4527 s, 185 iter, 1850000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-12-42\n",
      "  done: false\n",
      "  episode_len_mean: 88.47787610619469\n",
      "  episode_reward_max: 216.35194924802985\n",
      "  episode_reward_mean: 165.8398146245977\n",
      "  episode_reward_min: -133.53598336633684\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 17040\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.782\n",
      "    load_time_ms: 2.687\n",
      "    num_steps_sampled: 1860000\n",
      "    num_steps_trained: 1860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7043128609657288\n",
      "      kl: 0.011715690605342388\n",
      "      policy_loss: -0.0034644592087715864\n",
      "      total_loss: 31.53215789794922\n",
      "      vf_explained_var: 0.932409942150116\n",
      "      vf_loss: 31.53562355041504\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.537887692451477\n",
      "      kl: 0.006752038840204477\n",
      "      policy_loss: -0.0027996962890028954\n",
      "      total_loss: 46.43065643310547\n",
      "      vf_explained_var: 0.9528768658638\n",
      "      vf_loss: 46.43345642089844\n",
      "    sample_time_ms: 20257.932\n",
      "    update_time_ms: 7.51\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.521632000494805\n",
      "    rl_1: 103.31818262410285\n",
      "  time_since_restore: 4551.275116443634\n",
      "  time_this_iter_s: 23.455448150634766\n",
      "  time_total_s: 4551.275116443634\n",
      "  timestamp: 1550797962\n",
      "  timesteps_since_restore: 1860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1860000\n",
      "  training_iteration: 186\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4551 s, 186 iter, 1860000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-13-05\n",
      "  done: false\n",
      "  episode_len_mean: 88.5575221238938\n",
      "  episode_reward_max: 214.44186453894955\n",
      "  episode_reward_mean: 157.37564109274734\n",
      "  episode_reward_min: -144.1205046701626\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 17153\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.429\n",
      "    load_time_ms: 2.623\n",
      "    num_steps_sampled: 1870000\n",
      "    num_steps_trained: 1870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7094933390617371\n",
      "      kl: 0.006195007357746363\n",
      "      policy_loss: -0.004040684551000595\n",
      "      total_loss: 46.51169204711914\n",
      "      vf_explained_var: 0.9090339541435242\n",
      "      vf_loss: 46.51573181152344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5337292551994324\n",
      "      kl: 0.008692322298884392\n",
      "      policy_loss: -0.003643828211352229\n",
      "      total_loss: 72.97067260742188\n",
      "      vf_explained_var: 0.928575873374939\n",
      "      vf_loss: 72.97430419921875\n",
      "    sample_time_ms: 20198.381\n",
      "    update_time_ms: 7.561\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.72413114066555\n",
      "    rl_1: 100.65150995208184\n",
      "  time_since_restore: 4574.36793589592\n",
      "  time_this_iter_s: 23.092819452285767\n",
      "  time_total_s: 4574.36793589592\n",
      "  timestamp: 1550797985\n",
      "  timesteps_since_restore: 1870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1870000\n",
      "  training_iteration: 187\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4574 s, 187 iter, 1870000 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-13-28\n",
      "  done: false\n",
      "  episode_len_mean: 88.91071428571429\n",
      "  episode_reward_max: 212.13020494946696\n",
      "  episode_reward_mean: 167.65826000797142\n",
      "  episode_reward_min: 122.91793101951777\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 17265\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3120.0\n",
      "    load_time_ms: 2.65\n",
      "    num_steps_sampled: 1880000\n",
      "    num_steps_trained: 1880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.652424693107605\n",
      "      kl: 0.006671779323369265\n",
      "      policy_loss: -0.0010815627174451947\n",
      "      total_loss: 3.824265241622925\n",
      "      vf_explained_var: 0.9904670715332031\n",
      "      vf_loss: 3.8253467082977295\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5159081816673279\n",
      "      kl: 0.006089323665946722\n",
      "      policy_loss: -0.002493959618732333\n",
      "      total_loss: 4.451925754547119\n",
      "      vf_explained_var: 0.9951788187026978\n",
      "      vf_loss: 4.45442008972168\n",
      "    sample_time_ms: 20169.989\n",
      "    update_time_ms: 7.793\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.42300569105959\n",
      "    rl_1: 104.23525431691179\n",
      "  time_since_restore: 4597.623360872269\n",
      "  time_this_iter_s: 23.255424976348877\n",
      "  time_total_s: 4597.623360872269\n",
      "  timestamp: 1550798008\n",
      "  timesteps_since_restore: 1880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1880000\n",
      "  training_iteration: 188\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4597 s, 188 iter, 1880000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-13-51\n",
      "  done: false\n",
      "  episode_len_mean: 88.53508771929825\n",
      "  episode_reward_max: 215.70503200884409\n",
      "  episode_reward_mean: 158.7645564508858\n",
      "  episode_reward_min: -140.92865732729075\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 17379\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.439\n",
      "    load_time_ms: 2.581\n",
      "    num_steps_sampled: 1890000\n",
      "    num_steps_trained: 1890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6803385615348816\n",
      "      kl: 0.011232897639274597\n",
      "      policy_loss: -0.0055358633399009705\n",
      "      total_loss: 57.01021194458008\n",
      "      vf_explained_var: 0.8971301317214966\n",
      "      vf_loss: 57.015743255615234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5281153917312622\n",
      "      kl: 0.003031425876542926\n",
      "      policy_loss: -0.0009423219016753137\n",
      "      total_loss: 86.75434112548828\n",
      "      vf_explained_var: 0.9224247932434082\n",
      "      vf_loss: 86.75528717041016\n",
      "    sample_time_ms: 20139.142\n",
      "    update_time_ms: 7.746\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 59.15468534952033\n",
      "    rl_1: 99.60987110136551\n",
      "  time_since_restore: 4620.900622367859\n",
      "  time_this_iter_s: 23.27726149559021\n",
      "  time_total_s: 4620.900622367859\n",
      "  timestamp: 1550798031\n",
      "  timesteps_since_restore: 1890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1890000\n",
      "  training_iteration: 189\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4620 s, 189 iter, 1890000 ts, 159 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-14-15\n",
      "  done: false\n",
      "  episode_len_mean: 89.32142857142857\n",
      "  episode_reward_max: 213.39380890285258\n",
      "  episode_reward_mean: 167.3347243311568\n",
      "  episode_reward_min: -119.85187765873303\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 17491\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3121.737\n",
      "    load_time_ms: 2.507\n",
      "    num_steps_sampled: 1900000\n",
      "    num_steps_trained: 1900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6389119029045105\n",
      "      kl: 0.0073425378650426865\n",
      "      policy_loss: -0.0025706165470182896\n",
      "      total_loss: 15.245226860046387\n",
      "      vf_explained_var: 0.9694201946258545\n",
      "      vf_loss: 15.247796058654785\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4601336121559143\n",
      "      kl: 0.009596928022801876\n",
      "      policy_loss: -0.00389218982309103\n",
      "      total_loss: 20.95747947692871\n",
      "      vf_explained_var: 0.9799606204032898\n",
      "      vf_loss: 20.961374282836914\n",
      "    sample_time_ms: 20150.205\n",
      "    update_time_ms: 7.959\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.08325555014325\n",
      "    rl_1: 105.25146878101351\n",
      "  time_since_restore: 4644.117251634598\n",
      "  time_this_iter_s: 23.21662926673889\n",
      "  time_total_s: 4644.117251634598\n",
      "  timestamp: 1550798055\n",
      "  timesteps_since_restore: 1900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1900000\n",
      "  training_iteration: 190\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4644 s, 190 iter, 1900000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-14-39\n",
      "  done: false\n",
      "  episode_len_mean: 89.16216216216216\n",
      "  episode_reward_max: 211.08829435879443\n",
      "  episode_reward_mean: 163.26296811854203\n",
      "  episode_reward_min: -174.40114926608726\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 17602\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.223\n",
      "    load_time_ms: 2.579\n",
      "    num_steps_sampled: 1910000\n",
      "    num_steps_trained: 1910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6753010153770447\n",
      "      kl: 0.003972998820245266\n",
      "      policy_loss: -0.002929141279309988\n",
      "      total_loss: 69.2374496459961\n",
      "      vf_explained_var: 0.8789560198783875\n",
      "      vf_loss: 69.24039459228516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4606057107448578\n",
      "      kl: 0.010190854780375957\n",
      "      policy_loss: -0.0015192037681117654\n",
      "      total_loss: 111.52566528320312\n",
      "      vf_explained_var: 0.8995448350906372\n",
      "      vf_loss: 111.52716827392578\n",
      "    sample_time_ms: 20210.841\n",
      "    update_time_ms: 7.988\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.93527206512623\n",
      "    rl_1: 102.3276960534158\n",
      "  time_since_restore: 4668.0517773628235\n",
      "  time_this_iter_s: 23.934525728225708\n",
      "  time_total_s: 4668.0517773628235\n",
      "  timestamp: 1550798079\n",
      "  timesteps_since_restore: 1910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1910000\n",
      "  training_iteration: 191\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4668 s, 191 iter, 1910000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-15-02\n",
      "  done: false\n",
      "  episode_len_mean: 89.70535714285714\n",
      "  episode_reward_max: 214.64036536253792\n",
      "  episode_reward_mean: 156.6808153933063\n",
      "  episode_reward_min: -149.83891931058258\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 17714\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.245\n",
      "    load_time_ms: 2.575\n",
      "    num_steps_sampled: 1920000\n",
      "    num_steps_trained: 1920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7463290691375732\n",
      "      kl: 0.00688647897914052\n",
      "      policy_loss: -0.00045663496712222695\n",
      "      total_loss: 57.805274963378906\n",
      "      vf_explained_var: 0.8871949911117554\n",
      "      vf_loss: 57.80574417114258\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5200235247612\n",
      "      kl: 0.009129981510341167\n",
      "      policy_loss: -0.004849690478295088\n",
      "      total_loss: 81.09309387207031\n",
      "      vf_explained_var: 0.919884204864502\n",
      "      vf_loss: 81.09794616699219\n",
      "    sample_time_ms: 20229.89\n",
      "    update_time_ms: 7.53\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.94483763475593\n",
      "    rl_1: 99.73597775855035\n",
      "  time_since_restore: 4691.573461771011\n",
      "  time_this_iter_s: 23.521684408187866\n",
      "  time_total_s: 4691.573461771011\n",
      "  timestamp: 1550798102\n",
      "  timesteps_since_restore: 1920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1920000\n",
      "  training_iteration: 192\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4691 s, 192 iter, 1920000 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-15-26\n",
      "  done: false\n",
      "  episode_len_mean: 89.07142857142857\n",
      "  episode_reward_max: 210.41280481517575\n",
      "  episode_reward_mean: 166.55009296228081\n",
      "  episode_reward_min: -118.88073700785706\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 17826\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.1\n",
      "    load_time_ms: 2.517\n",
      "    num_steps_sampled: 1930000\n",
      "    num_steps_trained: 1930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.685920238494873\n",
      "      kl: 0.00803757831454277\n",
      "      policy_loss: -0.004809076432138681\n",
      "      total_loss: 36.31515884399414\n",
      "      vf_explained_var: 0.9276769757270813\n",
      "      vf_loss: 36.31996536254883\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4787159562110901\n",
      "      kl: 0.0028072150889784098\n",
      "      policy_loss: -0.001494653639383614\n",
      "      total_loss: 45.046871185302734\n",
      "      vf_explained_var: 0.9547649025917053\n",
      "      vf_loss: 45.048370361328125\n",
      "    sample_time_ms: 20221.929\n",
      "    update_time_ms: 7.343\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.59969867875804\n",
      "    rl_1: 104.95039428352277\n",
      "  time_since_restore: 4714.927553892136\n",
      "  time_this_iter_s: 23.354092121124268\n",
      "  time_total_s: 4714.927553892136\n",
      "  timestamp: 1550798126\n",
      "  timesteps_since_restore: 1930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1930000\n",
      "  training_iteration: 193\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4714 s, 193 iter, 1930000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-15-49\n",
      "  done: false\n",
      "  episode_len_mean: 89.21428571428571\n",
      "  episode_reward_max: 211.35778927200286\n",
      "  episode_reward_mean: 163.97973249874914\n",
      "  episode_reward_min: -156.93587174778574\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 17938\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.229\n",
      "    load_time_ms: 2.528\n",
      "    num_steps_sampled: 1940000\n",
      "    num_steps_trained: 1940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7111602425575256\n",
      "      kl: 0.0050569199956953526\n",
      "      policy_loss: -0.001746509107761085\n",
      "      total_loss: 26.29106330871582\n",
      "      vf_explained_var: 0.9481363296508789\n",
      "      vf_loss: 26.29281234741211\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5149300694465637\n",
      "      kl: 0.003387984586879611\n",
      "      policy_loss: -0.0008295882144011557\n",
      "      total_loss: 33.35246658325195\n",
      "      vf_explained_var: 0.9689614772796631\n",
      "      vf_loss: 33.35329818725586\n",
      "    sample_time_ms: 20204.808\n",
      "    update_time_ms: 7.114\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.300148711299656\n",
      "    rl_1: 102.67958378744947\n",
      "  time_since_restore: 4737.943229436874\n",
      "  time_this_iter_s: 23.01567554473877\n",
      "  time_total_s: 4737.943229436874\n",
      "  timestamp: 1550798149\n",
      "  timesteps_since_restore: 1940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1940000\n",
      "  training_iteration: 194\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4737 s, 194 iter, 1940000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-16-12\n",
      "  done: false\n",
      "  episode_len_mean: 89.16964285714286\n",
      "  episode_reward_max: 205.95488989382395\n",
      "  episode_reward_mean: 165.39601860505394\n",
      "  episode_reward_min: -176.09920575306677\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 18050\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.606\n",
      "    load_time_ms: 2.529\n",
      "    num_steps_sampled: 1950000\n",
      "    num_steps_trained: 1950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6852743625640869\n",
      "      kl: 0.005021451972424984\n",
      "      policy_loss: -0.00333999702706933\n",
      "      total_loss: 27.105918884277344\n",
      "      vf_explained_var: 0.9458467364311218\n",
      "      vf_loss: 27.109256744384766\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.48932382464408875\n",
      "      kl: 0.008574261330068111\n",
      "      policy_loss: -0.0007254090742208064\n",
      "      total_loss: 40.641841888427734\n",
      "      vf_explained_var: 0.9604204893112183\n",
      "      vf_loss: 40.64257049560547\n",
      "    sample_time_ms: 20165.159\n",
      "    update_time_ms: 6.762\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.72332416132094\n",
      "    rl_1: 103.67269444373301\n",
      "  time_since_restore: 4761.077813625336\n",
      "  time_this_iter_s: 23.134584188461304\n",
      "  time_total_s: 4761.077813625336\n",
      "  timestamp: 1550798172\n",
      "  timesteps_since_restore: 1950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1950000\n",
      "  training_iteration: 195\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4761 s, 195 iter, 1950000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-16-35\n",
      "  done: false\n",
      "  episode_len_mean: 89.70535714285714\n",
      "  episode_reward_max: 210.11216907136537\n",
      "  episode_reward_mean: 169.3910266330238\n",
      "  episode_reward_min: 117.29561205503065\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 18162\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.949\n",
      "    load_time_ms: 2.334\n",
      "    num_steps_sampled: 1960000\n",
      "    num_steps_trained: 1960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6834962368011475\n",
      "      kl: 0.0061294096522033215\n",
      "      policy_loss: -0.0045458232052624226\n",
      "      total_loss: 4.11467981338501\n",
      "      vf_explained_var: 0.9901418685913086\n",
      "      vf_loss: 4.119224548339844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4946497976779938\n",
      "      kl: 0.003569244174286723\n",
      "      policy_loss: -0.002302012871950865\n",
      "      total_loss: 5.516570091247559\n",
      "      vf_explained_var: 0.9941930770874023\n",
      "      vf_loss: 5.518871784210205\n",
      "    sample_time_ms: 20113.329\n",
      "    update_time_ms: 6.798\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.42433867389683\n",
      "    rl_1: 105.96668795912701\n",
      "  time_since_restore: 4783.996682882309\n",
      "  time_this_iter_s: 22.918869256973267\n",
      "  time_total_s: 4783.996682882309\n",
      "  timestamp: 1550798195\n",
      "  timesteps_since_restore: 1960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1960000\n",
      "  training_iteration: 196\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4783 s, 196 iter, 1960000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-16-58\n",
      "  done: false\n",
      "  episode_len_mean: 88.73214285714286\n",
      "  episode_reward_max: 210.8705583331812\n",
      "  episode_reward_mean: 167.4598367333435\n",
      "  episode_reward_min: 127.23936596354925\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 18274\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.404\n",
      "    load_time_ms: 2.363\n",
      "    num_steps_sampled: 1970000\n",
      "    num_steps_trained: 1970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6659582853317261\n",
      "      kl: 0.007151211146265268\n",
      "      policy_loss: -0.0035389107652008533\n",
      "      total_loss: 3.672977924346924\n",
      "      vf_explained_var: 0.9911161661148071\n",
      "      vf_loss: 3.6765174865722656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4753938913345337\n",
      "      kl: 0.005331001710146666\n",
      "      policy_loss: -0.0024304776452481747\n",
      "      total_loss: 4.945047378540039\n",
      "      vf_explained_var: 0.9947445392608643\n",
      "      vf_loss: 4.9474778175354\n",
      "    sample_time_ms: 20137.01\n",
      "    update_time_ms: 6.921\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.82584088659899\n",
      "    rl_1: 104.63399584674448\n",
      "  time_since_restore: 4807.352274656296\n",
      "  time_this_iter_s: 23.355591773986816\n",
      "  time_total_s: 4807.352274656296\n",
      "  timestamp: 1550798218\n",
      "  timesteps_since_restore: 1970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1970000\n",
      "  training_iteration: 197\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4807 s, 197 iter, 1970000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-17-22\n",
      "  done: false\n",
      "  episode_len_mean: 87.81739130434782\n",
      "  episode_reward_max: 209.86328631166953\n",
      "  episode_reward_mean: 163.6981236866352\n",
      "  episode_reward_min: -176.63145224086733\n",
      "  episodes_this_iter: 115\n",
      "  episodes_total: 18389\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3154.046\n",
      "    load_time_ms: 2.25\n",
      "    num_steps_sampled: 1980000\n",
      "    num_steps_trained: 1980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6623648405075073\n",
      "      kl: 0.012436660937964916\n",
      "      policy_loss: -0.002524803625419736\n",
      "      total_loss: 49.351619720458984\n",
      "      vf_explained_var: 0.9128179550170898\n",
      "      vf_loss: 49.3541374206543\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.47325730323791504\n",
      "      kl: 0.007478885818272829\n",
      "      policy_loss: -0.0013430147664621472\n",
      "      total_loss: 74.39527130126953\n",
      "      vf_explained_var: 0.9344102740287781\n",
      "      vf_loss: 74.39662170410156\n",
      "    sample_time_ms: 20147.235\n",
      "    update_time_ms: 6.654\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.871054790381294\n",
      "    rl_1: 101.82706889625395\n",
      "  time_since_restore: 4830.901396751404\n",
      "  time_this_iter_s: 23.549122095108032\n",
      "  time_total_s: 4830.901396751404\n",
      "  timestamp: 1550798242\n",
      "  timesteps_since_restore: 1980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1980000\n",
      "  training_iteration: 198\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4830 s, 198 iter, 1980000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-17-46\n",
      "  done: false\n",
      "  episode_len_mean: 87.71929824561404\n",
      "  episode_reward_max: 213.82980480073732\n",
      "  episode_reward_mean: 157.17574209799912\n",
      "  episode_reward_min: -173.36179213368428\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 18503\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.268\n",
      "    load_time_ms: 2.257\n",
      "    num_steps_sampled: 1990000\n",
      "    num_steps_trained: 1990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6460427045822144\n",
      "      kl: 0.006038806401193142\n",
      "      policy_loss: -0.0006328849121928215\n",
      "      total_loss: 72.68799591064453\n",
      "      vf_explained_var: 0.8690792918205261\n",
      "      vf_loss: 72.68862915039062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4721519649028778\n",
      "      kl: 0.003543572500348091\n",
      "      policy_loss: 0.0005659560556523502\n",
      "      total_loss: 114.9634780883789\n",
      "      vf_explained_var: 0.8917628526687622\n",
      "      vf_loss: 114.9629135131836\n",
      "    sample_time_ms: 20203.58\n",
      "    update_time_ms: 6.765\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.27320967759414\n",
      "    rl_1: 98.902532420405\n",
      "  time_since_restore: 4854.564492225647\n",
      "  time_this_iter_s: 23.663095474243164\n",
      "  time_total_s: 4854.564492225647\n",
      "  timestamp: 1550798266\n",
      "  timesteps_since_restore: 1990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1990000\n",
      "  training_iteration: 199\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4854 s, 199 iter, 1990000 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-18-09\n",
      "  done: false\n",
      "  episode_len_mean: 88.38938053097345\n",
      "  episode_reward_max: 213.39408298804491\n",
      "  episode_reward_mean: 165.73051647228849\n",
      "  episode_reward_min: -171.81886684627446\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 18616\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.809\n",
      "    load_time_ms: 2.318\n",
      "    num_steps_sampled: 2000000\n",
      "    num_steps_trained: 2000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6303781867027283\n",
      "      kl: 0.005323466844856739\n",
      "      policy_loss: -0.0010842429473996162\n",
      "      total_loss: 24.93182373046875\n",
      "      vf_explained_var: 0.9528501629829407\n",
      "      vf_loss: 24.93290901184082\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.45596635341644287\n",
      "      kl: 0.010808252729475498\n",
      "      policy_loss: 0.0001706345792626962\n",
      "      total_loss: 40.82017135620117\n",
      "      vf_explained_var: 0.9610024690628052\n",
      "      vf_loss: 40.820003509521484\n",
      "    sample_time_ms: 20214.049\n",
      "    update_time_ms: 6.674\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.236499770575215\n",
      "    rl_1: 103.49401670171326\n",
      "  time_since_restore: 4877.862369298935\n",
      "  time_this_iter_s: 23.297877073287964\n",
      "  time_total_s: 4877.862369298935\n",
      "  timestamp: 1550798289\n",
      "  timesteps_since_restore: 2000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2000000\n",
      "  training_iteration: 200\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4877 s, 200 iter, 2000000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-18-32\n",
      "  done: false\n",
      "  episode_len_mean: 88.98214285714286\n",
      "  episode_reward_max: 212.13114148206412\n",
      "  episode_reward_mean: 166.95841409068308\n",
      "  episode_reward_min: 125.98835372597682\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 18728\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.816\n",
      "    load_time_ms: 2.296\n",
      "    num_steps_sampled: 2010000\n",
      "    num_steps_trained: 2010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6829609274864197\n",
      "      kl: 0.00789479911327362\n",
      "      policy_loss: -0.004035520367324352\n",
      "      total_loss: 5.462971210479736\n",
      "      vf_explained_var: 0.9862086772918701\n",
      "      vf_loss: 5.467006683349609\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.49773988127708435\n",
      "      kl: 0.004815425258129835\n",
      "      policy_loss: -0.0019945043604820967\n",
      "      total_loss: 6.048549175262451\n",
      "      vf_explained_var: 0.9934809803962708\n",
      "      vf_loss: 6.050543785095215\n",
      "    sample_time_ms: 20140.017\n",
      "    update_time_ms: 6.653\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.05745632000798\n",
      "    rl_1: 103.90095777067508\n",
      "  time_since_restore: 4900.978064060211\n",
      "  time_this_iter_s: 23.115694761276245\n",
      "  time_total_s: 4900.978064060211\n",
      "  timestamp: 1550798312\n",
      "  timesteps_since_restore: 2010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2010000\n",
      "  training_iteration: 201\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4900 s, 201 iter, 2010000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-18-55\n",
      "  done: false\n",
      "  episode_len_mean: 88.04424778761062\n",
      "  episode_reward_max: 208.59163857350592\n",
      "  episode_reward_mean: 162.57263703582748\n",
      "  episode_reward_min: -175.60171137899977\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 18841\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.933\n",
      "    load_time_ms: 2.275\n",
      "    num_steps_sampled: 2020000\n",
      "    num_steps_trained: 2020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6353432536125183\n",
      "      kl: 0.007934676483273506\n",
      "      policy_loss: -0.0027875287923961878\n",
      "      total_loss: 50.862457275390625\n",
      "      vf_explained_var: 0.9074357151985168\n",
      "      vf_loss: 50.8652458190918\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4587033987045288\n",
      "      kl: 0.0063592055812478065\n",
      "      policy_loss: -0.0004907799884676933\n",
      "      total_loss: 78.9203109741211\n",
      "      vf_explained_var: 0.92753666639328\n",
      "      vf_loss: 78.92081451416016\n",
      "    sample_time_ms: 20091.283\n",
      "    update_time_ms: 6.626\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.14289804496857\n",
      "    rl_1: 101.4297389908589\n",
      "  time_since_restore: 4924.001033782959\n",
      "  time_this_iter_s: 23.022969722747803\n",
      "  time_total_s: 4924.001033782959\n",
      "  timestamp: 1550798335\n",
      "  timesteps_since_restore: 2020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2020000\n",
      "  training_iteration: 202\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4924 s, 202 iter, 2020000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-19-18\n",
      "  done: false\n",
      "  episode_len_mean: 88.90350877192982\n",
      "  episode_reward_max: 215.08316748258645\n",
      "  episode_reward_mean: 169.7238580012053\n",
      "  episode_reward_min: 126.14641293533799\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 18955\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.824\n",
      "    load_time_ms: 2.33\n",
      "    num_steps_sampled: 2030000\n",
      "    num_steps_trained: 2030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6322177648544312\n",
      "      kl: 0.0061147790402174\n",
      "      policy_loss: -0.003266045590862632\n",
      "      total_loss: 4.667880535125732\n",
      "      vf_explained_var: 0.9891038537025452\n",
      "      vf_loss: 4.671146392822266\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.44788026809692383\n",
      "      kl: 0.005808743182569742\n",
      "      policy_loss: -0.0019994713366031647\n",
      "      total_loss: 5.110208034515381\n",
      "      vf_explained_var: 0.994541585445404\n",
      "      vf_loss: 5.112206935882568\n",
      "    sample_time_ms: 20069.483\n",
      "    update_time_ms: 6.706\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.80070622007719\n",
      "    rl_1: 104.92315178112811\n",
      "  time_since_restore: 4947.139678955078\n",
      "  time_this_iter_s: 23.13864517211914\n",
      "  time_total_s: 4947.139678955078\n",
      "  timestamp: 1550798358\n",
      "  timesteps_since_restore: 2030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2030000\n",
      "  training_iteration: 203\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4947 s, 203 iter, 2030000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-19-41\n",
      "  done: false\n",
      "  episode_len_mean: 88.34821428571429\n",
      "  episode_reward_max: 213.1444197500374\n",
      "  episode_reward_mean: 161.29557170312134\n",
      "  episode_reward_min: -180.51808825674235\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 19067\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3123.408\n",
      "    load_time_ms: 2.305\n",
      "    num_steps_sampled: 2040000\n",
      "    num_steps_trained: 2040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6388368606567383\n",
      "      kl: 0.0059913648292422295\n",
      "      policy_loss: -0.0028658632654696703\n",
      "      total_loss: 41.75140380859375\n",
      "      vf_explained_var: 0.9236658811569214\n",
      "      vf_loss: 41.7542724609375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4555948078632355\n",
      "      kl: 0.008017316460609436\n",
      "      policy_loss: -0.004302448593080044\n",
      "      total_loss: 74.13323974609375\n",
      "      vf_explained_var: 0.9326832890510559\n",
      "      vf_loss: 74.13753509521484\n",
      "    sample_time_ms: 20080.579\n",
      "    update_time_ms: 6.64\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 59.74372788701843\n",
      "    rl_1: 101.5518438161029\n",
      "  time_since_restore: 4970.251154661179\n",
      "  time_this_iter_s: 23.111475706100464\n",
      "  time_total_s: 4970.251154661179\n",
      "  timestamp: 1550798381\n",
      "  timesteps_since_restore: 2040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2040000\n",
      "  training_iteration: 204\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4970 s, 204 iter, 2040000 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-20-05\n",
      "  done: false\n",
      "  episode_len_mean: 88.54867256637168\n",
      "  episode_reward_max: 216.03828538440723\n",
      "  episode_reward_mean: 167.9409984701639\n",
      "  episode_reward_min: 120.98124761841059\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 19180\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.321\n",
      "    load_time_ms: 2.396\n",
      "    num_steps_sampled: 2050000\n",
      "    num_steps_trained: 2050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6296117901802063\n",
      "      kl: 0.009111898951232433\n",
      "      policy_loss: -0.005573394242674112\n",
      "      total_loss: 5.040783405303955\n",
      "      vf_explained_var: 0.9880606532096863\n",
      "      vf_loss: 5.046356201171875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.45187219977378845\n",
      "      kl: 0.0052635143510997295\n",
      "      policy_loss: 0.0007873217691667378\n",
      "      total_loss: 5.11137056350708\n",
      "      vf_explained_var: 0.9946461319923401\n",
      "      vf_loss: 5.11058235168457\n",
      "    sample_time_ms: 20129.73\n",
      "    update_time_ms: 6.646\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.343413537560444\n",
      "    rl_1: 104.59758493260345\n",
      "  time_since_restore: 4993.9071393013\n",
      "  time_this_iter_s: 23.65598464012146\n",
      "  time_total_s: 4993.9071393013\n",
      "  timestamp: 1550798405\n",
      "  timesteps_since_restore: 2050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2050000\n",
      "  training_iteration: 205\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 4993 s, 205 iter, 2050000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-20-29\n",
      "  done: false\n",
      "  episode_len_mean: 88.09649122807018\n",
      "  episode_reward_max: 209.59044266445522\n",
      "  episode_reward_mean: 163.9432084289166\n",
      "  episode_reward_min: -169.9861588299444\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 19294\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.71\n",
      "    load_time_ms: 2.441\n",
      "    num_steps_sampled: 2060000\n",
      "    num_steps_trained: 2060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6146478056907654\n",
      "      kl: 0.010895011946558952\n",
      "      policy_loss: -0.006436766590923071\n",
      "      total_loss: 74.48516845703125\n",
      "      vf_explained_var: 0.867401123046875\n",
      "      vf_loss: 74.49161529541016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.46780142188072205\n",
      "      kl: 0.00782222393900156\n",
      "      policy_loss: -0.0021489933133125305\n",
      "      total_loss: 118.71320343017578\n",
      "      vf_explained_var: 0.8968474268913269\n",
      "      vf_loss: 118.7153549194336\n",
      "    sample_time_ms: 20172.542\n",
      "    update_time_ms: 6.62\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.071397530721356\n",
      "    rl_1: 101.87181089819522\n",
      "  time_since_restore: 5017.267571926117\n",
      "  time_this_iter_s: 23.360432624816895\n",
      "  time_total_s: 5017.267571926117\n",
      "  timestamp: 1550798429\n",
      "  timesteps_since_restore: 2060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2060000\n",
      "  training_iteration: 206\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5017 s, 206 iter, 2060000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-20-52\n",
      "  done: false\n",
      "  episode_len_mean: 88.2566371681416\n",
      "  episode_reward_max: 211.69311356239479\n",
      "  episode_reward_mean: 169.55656825291933\n",
      "  episode_reward_min: -167.96534409999524\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 19407\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.918\n",
      "    load_time_ms: 2.394\n",
      "    num_steps_sampled: 2070000\n",
      "    num_steps_trained: 2070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.621242105960846\n",
      "      kl: 0.006296579260379076\n",
      "      policy_loss: -0.0021209721453487873\n",
      "      total_loss: 18.943235397338867\n",
      "      vf_explained_var: 0.9656976461410522\n",
      "      vf_loss: 18.94535255432129\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.497881144285202\n",
      "      kl: 0.003356719622388482\n",
      "      policy_loss: -0.0010253108339384198\n",
      "      total_loss: 29.416446685791016\n",
      "      vf_explained_var: 0.9733982682228088\n",
      "      vf_loss: 29.417476654052734\n",
      "    sample_time_ms: 20155.087\n",
      "    update_time_ms: 6.536\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.86320182312525\n",
      "    rl_1: 103.69336642979408\n",
      "  time_since_restore: 5040.6213710308075\n",
      "  time_this_iter_s: 23.35379910469055\n",
      "  time_total_s: 5040.6213710308075\n",
      "  timestamp: 1550798452\n",
      "  timesteps_since_restore: 2070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2070000\n",
      "  training_iteration: 207\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5040 s, 207 iter, 2070000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-21-15\n",
      "  done: false\n",
      "  episode_len_mean: 88.47787610619469\n",
      "  episode_reward_max: 210.3196083326909\n",
      "  episode_reward_mean: 165.56483199455164\n",
      "  episode_reward_min: -132.05901580219665\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 19520\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.126\n",
      "    load_time_ms: 2.393\n",
      "    num_steps_sampled: 2080000\n",
      "    num_steps_trained: 2080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6032430529594421\n",
      "      kl: 0.022934960201382637\n",
      "      policy_loss: -0.006374560762196779\n",
      "      total_loss: 59.83938217163086\n",
      "      vf_explained_var: 0.8953324556350708\n",
      "      vf_loss: 59.845760345458984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.43874940276145935\n",
      "      kl: 0.00955187901854515\n",
      "      policy_loss: -0.0019019138999283314\n",
      "      total_loss: 84.73882293701172\n",
      "      vf_explained_var: 0.9212018251419067\n",
      "      vf_loss: 84.74073028564453\n",
      "    sample_time_ms: 20131.733\n",
      "    update_time_ms: 6.666\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.41429904656295\n",
      "    rl_1: 103.15053294798865\n",
      "  time_since_restore: 5063.768743991852\n",
      "  time_this_iter_s: 23.14737296104431\n",
      "  time_total_s: 5063.768743991852\n",
      "  timestamp: 1550798475\n",
      "  timesteps_since_restore: 2080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2080000\n",
      "  training_iteration: 208\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5063 s, 208 iter, 2080000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-21-39\n",
      "  done: false\n",
      "  episode_len_mean: 89.20535714285714\n",
      "  episode_reward_max: 215.5493523554634\n",
      "  episode_reward_mean: 165.77707644970067\n",
      "  episode_reward_min: 119.37408760873953\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 19632\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.834\n",
      "    load_time_ms: 2.429\n",
      "    num_steps_sampled: 2090000\n",
      "    num_steps_trained: 2090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6154721975326538\n",
      "      kl: 0.006914742290973663\n",
      "      policy_loss: -0.002916731173172593\n",
      "      total_loss: 3.9363465309143066\n",
      "      vf_explained_var: 0.9910588264465332\n",
      "      vf_loss: 3.939262866973877\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4327850937843323\n",
      "      kl: 0.004747208673506975\n",
      "      policy_loss: -0.0019907462410628796\n",
      "      total_loss: 4.791604518890381\n",
      "      vf_explained_var: 0.9947925209999084\n",
      "      vf_loss: 4.793595314025879\n",
      "    sample_time_ms: 20097.459\n",
      "    update_time_ms: 6.55\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.56911291590562\n",
      "    rl_1: 105.20796353379511\n",
      "  time_since_restore: 5087.116520643234\n",
      "  time_this_iter_s: 23.347776651382446\n",
      "  time_total_s: 5087.116520643234\n",
      "  timestamp: 1550798499\n",
      "  timesteps_since_restore: 2090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2090000\n",
      "  training_iteration: 209\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5087 s, 209 iter, 2090000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-22-02\n",
      "  done: false\n",
      "  episode_len_mean: 89.28318584070796\n",
      "  episode_reward_max: 215.43264681485306\n",
      "  episode_reward_mean: 171.37471126086675\n",
      "  episode_reward_min: -169.27765497633354\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 19745\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.007\n",
      "    load_time_ms: 2.366\n",
      "    num_steps_sampled: 2100000\n",
      "    num_steps_trained: 2100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6355801820755005\n",
      "      kl: 0.016208697110414505\n",
      "      policy_loss: -0.005895405076444149\n",
      "      total_loss: 21.37068748474121\n",
      "      vf_explained_var: 0.9591745138168335\n",
      "      vf_loss: 21.3765811920166\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.49228301644325256\n",
      "      kl: 0.0043961284682154655\n",
      "      policy_loss: -0.0009472867823205888\n",
      "      total_loss: 26.49860954284668\n",
      "      vf_explained_var: 0.9763588309288025\n",
      "      vf_loss: 26.49955940246582\n",
      "    sample_time_ms: 20069.938\n",
      "    update_time_ms: 6.536\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.0407945003025\n",
      "    rl_1: 106.33391676056425\n",
      "  time_since_restore: 5110.1615443229675\n",
      "  time_this_iter_s: 23.045023679733276\n",
      "  time_total_s: 5110.1615443229675\n",
      "  timestamp: 1550798522\n",
      "  timesteps_since_restore: 2100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2100000\n",
      "  training_iteration: 210\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5110 s, 210 iter, 2100000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-22-25\n",
      "  done: false\n",
      "  episode_len_mean: 88.70535714285714\n",
      "  episode_reward_max: 212.33553561382922\n",
      "  episode_reward_mean: 168.82016853327158\n",
      "  episode_reward_min: -163.12087762087225\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 19857\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.798\n",
      "    load_time_ms: 2.316\n",
      "    num_steps_sampled: 2110000\n",
      "    num_steps_trained: 2110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6377472281455994\n",
      "      kl: 0.009113937616348267\n",
      "      policy_loss: -0.004656463395804167\n",
      "      total_loss: 25.055618286132812\n",
      "      vf_explained_var: 0.954269528388977\n",
      "      vf_loss: 25.060272216796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.508520781993866\n",
      "      kl: 0.004910831339657307\n",
      "      policy_loss: -0.002828417345881462\n",
      "      total_loss: 43.672889709472656\n",
      "      vf_explained_var: 0.9584075808525085\n",
      "      vf_loss: 43.675716400146484\n",
      "    sample_time_ms: 20104.004\n",
      "    update_time_ms: 6.617\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.46747917328058\n",
      "    rl_1: 104.35268935999102\n",
      "  time_since_restore: 5133.642672777176\n",
      "  time_this_iter_s: 23.481128454208374\n",
      "  time_total_s: 5133.642672777176\n",
      "  timestamp: 1550798545\n",
      "  timesteps_since_restore: 2110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2110000\n",
      "  training_iteration: 211\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5133 s, 211 iter, 2110000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-22-48\n",
      "  done: false\n",
      "  episode_len_mean: 89.72321428571429\n",
      "  episode_reward_max: 214.9795582233789\n",
      "  episode_reward_mean: 168.54770678339446\n",
      "  episode_reward_min: 120.68014507729934\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 19969\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.999\n",
      "    load_time_ms: 2.39\n",
      "    num_steps_sampled: 2120000\n",
      "    num_steps_trained: 2120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6606876850128174\n",
      "      kl: 0.010349531657993793\n",
      "      policy_loss: -0.0036746168043464422\n",
      "      total_loss: 3.9171106815338135\n",
      "      vf_explained_var: 0.99063640832901\n",
      "      vf_loss: 3.920785427093506\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5410332083702087\n",
      "      kl: 0.0037549547851085663\n",
      "      policy_loss: -0.00021710828877985477\n",
      "      total_loss: 4.021443843841553\n",
      "      vf_explained_var: 0.995756208896637\n",
      "      vf_loss: 4.021661281585693\n",
      "    sample_time_ms: 20110.158\n",
      "    update_time_ms: 6.753\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.46199952538342\n",
      "    rl_1: 104.085707258011\n",
      "  time_since_restore: 5156.733132362366\n",
      "  time_this_iter_s: 23.09045958518982\n",
      "  time_total_s: 5156.733132362366\n",
      "  timestamp: 1550798568\n",
      "  timesteps_since_restore: 2120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2120000\n",
      "  training_iteration: 212\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5156 s, 212 iter, 2120000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-23-12\n",
      "  done: false\n",
      "  episode_len_mean: 88.92857142857143\n",
      "  episode_reward_max: 214.23951599311187\n",
      "  episode_reward_mean: 164.0915226804406\n",
      "  episode_reward_min: -181.1927238324401\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 20081\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.358\n",
      "    load_time_ms: 2.422\n",
      "    num_steps_sampled: 2130000\n",
      "    num_steps_trained: 2130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6543264389038086\n",
      "      kl: 0.0072007752023637295\n",
      "      policy_loss: -0.001619108603335917\n",
      "      total_loss: 42.2281494140625\n",
      "      vf_explained_var: 0.9223282933235168\n",
      "      vf_loss: 42.22977066040039\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5130800604820251\n",
      "      kl: 0.010438278317451477\n",
      "      policy_loss: -0.0010181029792875051\n",
      "      total_loss: 70.7320785522461\n",
      "      vf_explained_var: 0.936282217502594\n",
      "      vf_loss: 70.73309326171875\n",
      "    sample_time_ms: 20124.426\n",
      "    update_time_ms: 6.884\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.52915200790873\n",
      "    rl_1: 102.56237067253186\n",
      "  time_since_restore: 5180.04634141922\n",
      "  time_this_iter_s: 23.313209056854248\n",
      "  time_total_s: 5180.04634141922\n",
      "  timestamp: 1550798592\n",
      "  timesteps_since_restore: 2130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2130000\n",
      "  training_iteration: 213\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5180 s, 213 iter, 2130000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-23-35\n",
      "  done: false\n",
      "  episode_len_mean: 89.69369369369369\n",
      "  episode_reward_max: 214.98340241317717\n",
      "  episode_reward_mean: 171.24174176913132\n",
      "  episode_reward_min: 124.5403933872939\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 20192\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.96\n",
      "    load_time_ms: 2.456\n",
      "    num_steps_sampled: 2140000\n",
      "    num_steps_trained: 2140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6491482853889465\n",
      "      kl: 0.00818944163620472\n",
      "      policy_loss: -0.0038608962204307318\n",
      "      total_loss: 3.469597101211548\n",
      "      vf_explained_var: 0.992051362991333\n",
      "      vf_loss: 3.4734578132629395\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4869436025619507\n",
      "      kl: 0.004960146266967058\n",
      "      policy_loss: -0.0023672848474234343\n",
      "      total_loss: 3.576422691345215\n",
      "      vf_explained_var: 0.9963140487670898\n",
      "      vf_loss: 3.5787901878356934\n",
      "    sample_time_ms: 20174.049\n",
      "    update_time_ms: 7.273\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.62824089502706\n",
      "    rl_1: 106.61350087410428\n",
      "  time_since_restore: 5203.684408903122\n",
      "  time_this_iter_s: 23.638067483901978\n",
      "  time_total_s: 5203.684408903122\n",
      "  timestamp: 1550798615\n",
      "  timesteps_since_restore: 2140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2140000\n",
      "  training_iteration: 214\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5203 s, 214 iter, 2140000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-23-59\n",
      "  done: false\n",
      "  episode_len_mean: 88.72566371681415\n",
      "  episode_reward_max: 216.25672660648056\n",
      "  episode_reward_mean: 168.3986482114569\n",
      "  episode_reward_min: -120.55745288477479\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 20305\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.282\n",
      "    load_time_ms: 2.372\n",
      "    num_steps_sampled: 2150000\n",
      "    num_steps_trained: 2150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6372653245925903\n",
      "      kl: 0.008053267374634743\n",
      "      policy_loss: -0.0017769323894754052\n",
      "      total_loss: 31.254980087280273\n",
      "      vf_explained_var: 0.9453150629997253\n",
      "      vf_loss: 31.25676155090332\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4884297847747803\n",
      "      kl: 0.00991152599453926\n",
      "      policy_loss: -0.003940283320844173\n",
      "      total_loss: 42.214351654052734\n",
      "      vf_explained_var: 0.9615826606750488\n",
      "      vf_loss: 42.21828842163086\n",
      "    sample_time_ms: 20185.926\n",
      "    update_time_ms: 7.409\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.51060387149253\n",
      "    rl_1: 103.88804433996435\n",
      "  time_since_restore: 5227.442363023758\n",
      "  time_this_iter_s: 23.757954120635986\n",
      "  time_total_s: 5227.442363023758\n",
      "  timestamp: 1550798639\n",
      "  timesteps_since_restore: 2150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2150000\n",
      "  training_iteration: 215\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5227 s, 215 iter, 2150000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-24-23\n",
      "  done: false\n",
      "  episode_len_mean: 86.30172413793103\n",
      "  episode_reward_max: 212.6851531165761\n",
      "  episode_reward_mean: 147.65957187879152\n",
      "  episode_reward_min: -179.1461490158423\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 20421\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.481\n",
      "    load_time_ms: 2.322\n",
      "    num_steps_sampled: 2160000\n",
      "    num_steps_trained: 2160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6833605766296387\n",
      "      kl: 0.01411494705826044\n",
      "      policy_loss: -0.008441045880317688\n",
      "      total_loss: 214.64926147460938\n",
      "      vf_explained_var: 0.7114110589027405\n",
      "      vf_loss: 214.65769958496094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.49126115441322327\n",
      "      kl: 0.006868885830044746\n",
      "      policy_loss: 0.0001388006639899686\n",
      "      total_loss: 328.8144226074219\n",
      "      vf_explained_var: 0.7402574419975281\n",
      "      vf_loss: 328.8143005371094\n",
      "    sample_time_ms: 20195.132\n",
      "    update_time_ms: 7.56\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.265964842721985\n",
      "    rl_1: 91.3936070360695\n",
      "  time_since_restore: 5250.907588481903\n",
      "  time_this_iter_s: 23.46522545814514\n",
      "  time_total_s: 5250.907588481903\n",
      "  timestamp: 1550798663\n",
      "  timesteps_since_restore: 2160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2160000\n",
      "  training_iteration: 216\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5250 s, 216 iter, 2160000 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-24-46\n",
      "  done: false\n",
      "  episode_len_mean: 89.01769911504425\n",
      "  episode_reward_max: 212.6365385152341\n",
      "  episode_reward_mean: 168.7490351471718\n",
      "  episode_reward_min: 122.5730567658533\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 20534\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.562\n",
      "    load_time_ms: 2.353\n",
      "    num_steps_sampled: 2170000\n",
      "    num_steps_trained: 2170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6769140362739563\n",
      "      kl: 0.008809183724224567\n",
      "      policy_loss: -0.0030273704323917627\n",
      "      total_loss: 5.4209208488464355\n",
      "      vf_explained_var: 0.9879631400108337\n",
      "      vf_loss: 5.423948287963867\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.48078781366348267\n",
      "      kl: 0.004479875788092613\n",
      "      policy_loss: -0.0026700699236243963\n",
      "      total_loss: 4.770659923553467\n",
      "      vf_explained_var: 0.9946960210800171\n",
      "      vf_loss: 4.773330211639404\n",
      "    sample_time_ms: 20215.691\n",
      "    update_time_ms: 7.465\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.26399878801448\n",
      "    rl_1: 104.48503635915735\n",
      "  time_since_restore: 5274.276775360107\n",
      "  time_this_iter_s: 23.369186878204346\n",
      "  time_total_s: 5274.276775360107\n",
      "  timestamp: 1550798686\n",
      "  timesteps_since_restore: 2170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2170000\n",
      "  training_iteration: 217\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5274 s, 217 iter, 2170000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-25-09\n",
      "  done: false\n",
      "  episode_len_mean: 87.93805309734513\n",
      "  episode_reward_max: 214.21602485593857\n",
      "  episode_reward_mean: 159.53033133701953\n",
      "  episode_reward_min: -175.63620233466077\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 20647\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.355\n",
      "    load_time_ms: 2.363\n",
      "    num_steps_sampled: 2180000\n",
      "    num_steps_trained: 2180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6020184755325317\n",
      "      kl: 0.017749816179275513\n",
      "      policy_loss: -0.0018924662144854665\n",
      "      total_loss: 112.83783721923828\n",
      "      vf_explained_var: 0.8280298113822937\n",
      "      vf_loss: 112.8397445678711\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.45055893063545227\n",
      "      kl: 0.007814053446054459\n",
      "      policy_loss: -0.0014958601677790284\n",
      "      total_loss: 174.43519592285156\n",
      "      vf_explained_var: 0.8539784550666809\n",
      "      vf_loss: 174.43667602539062\n",
      "    sample_time_ms: 20203.464\n",
      "    update_time_ms: 7.308\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.262975399661045\n",
      "    rl_1: 99.26735593735849\n",
      "  time_since_restore: 5297.30277466774\n",
      "  time_this_iter_s: 23.025999307632446\n",
      "  time_total_s: 5297.30277466774\n",
      "  timestamp: 1550798709\n",
      "  timesteps_since_restore: 2180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2180000\n",
      "  training_iteration: 218\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5297 s, 218 iter, 2180000 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-25-32\n",
      "  done: false\n",
      "  episode_len_mean: 89.10714285714286\n",
      "  episode_reward_max: 213.56516093920965\n",
      "  episode_reward_mean: 168.75791673205387\n",
      "  episode_reward_min: 114.58585388122115\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 20759\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.86\n",
      "    load_time_ms: 2.327\n",
      "    num_steps_sampled: 2190000\n",
      "    num_steps_trained: 2190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6421779990196228\n",
      "      kl: 0.005835238378494978\n",
      "      policy_loss: -0.002567928982898593\n",
      "      total_loss: 5.190748691558838\n",
      "      vf_explained_var: 0.9881885051727295\n",
      "      vf_loss: 5.193316459655762\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.47519731521606445\n",
      "      kl: 0.008905623108148575\n",
      "      policy_loss: -0.004414035938680172\n",
      "      total_loss: 4.331037521362305\n",
      "      vf_explained_var: 0.9951971173286438\n",
      "      vf_loss: 4.335451602935791\n",
      "    sample_time_ms: 20189.315\n",
      "    update_time_ms: 7.359\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.88238523319885\n",
      "    rl_1: 103.87553149885503\n",
      "  time_since_restore: 5320.514695644379\n",
      "  time_this_iter_s: 23.211920976638794\n",
      "  time_total_s: 5320.514695644379\n",
      "  timestamp: 1550798732\n",
      "  timesteps_since_restore: 2190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2190000\n",
      "  training_iteration: 219\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5320 s, 219 iter, 2190000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-25-55\n",
      "  done: false\n",
      "  episode_len_mean: 88.26548672566372\n",
      "  episode_reward_max: 214.95855746998544\n",
      "  episode_reward_mean: 164.4818139851543\n",
      "  episode_reward_min: -153.26395639859612\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 20872\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.239\n",
      "    load_time_ms: 2.334\n",
      "    num_steps_sampled: 2200000\n",
      "    num_steps_trained: 2200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6182886958122253\n",
      "      kl: 0.012506430968642235\n",
      "      policy_loss: -0.006767910905182362\n",
      "      total_loss: 84.62210083007812\n",
      "      vf_explained_var: 0.8637718558311462\n",
      "      vf_loss: 84.62886047363281\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.42689943313598633\n",
      "      kl: 0.008927951566874981\n",
      "      policy_loss: -0.0007498487830162048\n",
      "      total_loss: 119.66133117675781\n",
      "      vf_explained_var: 0.8929574489593506\n",
      "      vf_loss: 119.66209411621094\n",
      "    sample_time_ms: 20161.129\n",
      "    update_time_ms: 7.299\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.276030017210346\n",
      "    rl_1: 101.20578396794396\n",
      "  time_since_restore: 5343.288767814636\n",
      "  time_this_iter_s: 22.77407217025757\n",
      "  time_total_s: 5343.288767814636\n",
      "  timestamp: 1550798755\n",
      "  timesteps_since_restore: 2200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2200000\n",
      "  training_iteration: 220\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5343 s, 220 iter, 2200000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-26-18\n",
      "  done: false\n",
      "  episode_len_mean: 89.73214285714286\n",
      "  episode_reward_max: 218.4147265224256\n",
      "  episode_reward_mean: 174.88029762670502\n",
      "  episode_reward_min: 125.67691055171679\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 20984\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.022\n",
      "    load_time_ms: 2.336\n",
      "    num_steps_sampled: 2210000\n",
      "    num_steps_trained: 2210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5980921387672424\n",
      "      kl: 0.009682642295956612\n",
      "      policy_loss: -0.003215257078409195\n",
      "      total_loss: 3.898378849029541\n",
      "      vf_explained_var: 0.9913787841796875\n",
      "      vf_loss: 3.901594877243042\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.433013379573822\n",
      "      kl: 0.005410126876085997\n",
      "      policy_loss: -0.0016794558614492416\n",
      "      total_loss: 4.105260848999023\n",
      "      vf_explained_var: 0.9957146048545837\n",
      "      vf_loss: 4.106940746307373\n",
      "    sample_time_ms: 20122.472\n",
      "    update_time_ms: 7.693\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.97059928562244\n",
      "    rl_1: 107.90969834108259\n",
      "  time_since_restore: 5366.365344762802\n",
      "  time_this_iter_s: 23.076576948165894\n",
      "  time_total_s: 5366.365344762802\n",
      "  timestamp: 1550798778\n",
      "  timesteps_since_restore: 2210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2210000\n",
      "  training_iteration: 221\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5366 s, 221 iter, 2210000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-26-42\n",
      "  done: false\n",
      "  episode_len_mean: 88.58407079646018\n",
      "  episode_reward_max: 218.43119634508597\n",
      "  episode_reward_mean: 163.53076826553837\n",
      "  episode_reward_min: -130.3233620591564\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 21097\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.8\n",
      "    load_time_ms: 2.257\n",
      "    num_steps_sampled: 2220000\n",
      "    num_steps_trained: 2220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.654577910900116\n",
      "      kl: 0.007154380436986685\n",
      "      policy_loss: -0.0031091971322894096\n",
      "      total_loss: 46.91513442993164\n",
      "      vf_explained_var: 0.9122934341430664\n",
      "      vf_loss: 46.91823959350586\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4530719816684723\n",
      "      kl: 0.004286718089133501\n",
      "      policy_loss: -0.0014977488899603486\n",
      "      total_loss: 65.79569244384766\n",
      "      vf_explained_var: 0.9372679591178894\n",
      "      vf_loss: 65.79717254638672\n",
      "    sample_time_ms: 20174.438\n",
      "    update_time_ms: 7.56\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.76211200475614\n",
      "    rl_1: 101.76865626078224\n",
      "  time_since_restore: 5389.99807882309\n",
      "  time_this_iter_s: 23.632734060287476\n",
      "  time_total_s: 5389.99807882309\n",
      "  timestamp: 1550798802\n",
      "  timesteps_since_restore: 2220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2220000\n",
      "  training_iteration: 222\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5389 s, 222 iter, 2220000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-27-06\n",
      "  done: false\n",
      "  episode_len_mean: 89.24107142857143\n",
      "  episode_reward_max: 216.15242138586305\n",
      "  episode_reward_mean: 173.59150733969116\n",
      "  episode_reward_min: 121.3810058896962\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 21209\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3121.526\n",
      "    load_time_ms: 2.274\n",
      "    num_steps_sampled: 2230000\n",
      "    num_steps_trained: 2230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6326403617858887\n",
      "      kl: 0.004824521951377392\n",
      "      policy_loss: -0.002415616065263748\n",
      "      total_loss: 3.354374885559082\n",
      "      vf_explained_var: 0.9926914572715759\n",
      "      vf_loss: 3.356790781021118\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4452930986881256\n",
      "      kl: 0.008139126934111118\n",
      "      policy_loss: -0.003060299204662442\n",
      "      total_loss: 3.148146629333496\n",
      "      vf_explained_var: 0.9967551827430725\n",
      "      vf_loss: 3.1512064933776855\n",
      "    sample_time_ms: 20215.08\n",
      "    update_time_ms: 7.376\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.64245300788919\n",
      "    rl_1: 105.94905433180195\n",
      "  time_since_restore: 5413.686514377594\n",
      "  time_this_iter_s: 23.688435554504395\n",
      "  time_total_s: 5413.686514377594\n",
      "  timestamp: 1550798826\n",
      "  timesteps_since_restore: 2230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2230000\n",
      "  training_iteration: 223\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5413 s, 223 iter, 2230000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-27-29\n",
      "  done: false\n",
      "  episode_len_mean: 89.12389380530973\n",
      "  episode_reward_max: 210.39527572940136\n",
      "  episode_reward_mean: 166.39786286726473\n",
      "  episode_reward_min: -125.67335569089293\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 21322\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3119.22\n",
      "    load_time_ms: 2.209\n",
      "    num_steps_sampled: 2240000\n",
      "    num_steps_trained: 2240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6748723983764648\n",
      "      kl: 0.009944170713424683\n",
      "      policy_loss: -0.003534108866006136\n",
      "      total_loss: 31.72800636291504\n",
      "      vf_explained_var: 0.9383010864257812\n",
      "      vf_loss: 31.731538772583008\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.44046738743782043\n",
      "      kl: 0.006688798312097788\n",
      "      policy_loss: -0.001103312592022121\n",
      "      total_loss: 44.14268493652344\n",
      "      vf_explained_var: 0.957056999206543\n",
      "      vf_loss: 44.14378356933594\n",
      "    sample_time_ms: 20159.849\n",
      "    update_time_ms: 6.953\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.78385227476429\n",
      "    rl_1: 103.61401059250045\n",
      "  time_since_restore: 5436.742849111557\n",
      "  time_this_iter_s: 23.056334733963013\n",
      "  time_total_s: 5436.742849111557\n",
      "  timestamp: 1550798849\n",
      "  timesteps_since_restore: 2240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2240000\n",
      "  training_iteration: 224\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5436 s, 224 iter, 2240000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-27-52\n",
      "  done: false\n",
      "  episode_len_mean: 89.27927927927928\n",
      "  episode_reward_max: 216.49950202873845\n",
      "  episode_reward_mean: 172.49361782486667\n",
      "  episode_reward_min: -130.5235689517239\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 21433\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.838\n",
      "    load_time_ms: 2.242\n",
      "    num_steps_sampled: 2250000\n",
      "    num_steps_trained: 2250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6343989968299866\n",
      "      kl: 0.0055646738037467\n",
      "      policy_loss: -0.00028175354236736894\n",
      "      total_loss: 32.41543197631836\n",
      "      vf_explained_var: 0.9395701289176941\n",
      "      vf_loss: 32.41571807861328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4356847405433655\n",
      "      kl: 0.0070564886555075645\n",
      "      policy_loss: -0.001116695930249989\n",
      "      total_loss: 45.69713592529297\n",
      "      vf_explained_var: 0.956145167350769\n",
      "      vf_loss: 45.69825744628906\n",
      "    sample_time_ms: 20081.134\n",
      "    update_time_ms: 6.829\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.29849157941474\n",
      "    rl_1: 104.19512624545197\n",
      "  time_since_restore: 5459.859450817108\n",
      "  time_this_iter_s: 23.116601705551147\n",
      "  time_total_s: 5459.859450817108\n",
      "  timestamp: 1550798872\n",
      "  timesteps_since_restore: 2250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2250000\n",
      "  training_iteration: 225\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5459 s, 225 iter, 2250000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-28-15\n",
      "  done: false\n",
      "  episode_len_mean: 88.39473684210526\n",
      "  episode_reward_max: 213.10150274265177\n",
      "  episode_reward_mean: 170.1978689411695\n",
      "  episode_reward_min: -140.0711756777204\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 21547\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.392\n",
      "    load_time_ms: 2.253\n",
      "    num_steps_sampled: 2260000\n",
      "    num_steps_trained: 2260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6421538591384888\n",
      "      kl: 0.007452826481312513\n",
      "      policy_loss: -0.0017504733987152576\n",
      "      total_loss: 31.334110260009766\n",
      "      vf_explained_var: 0.9403199553489685\n",
      "      vf_loss: 31.335859298706055\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.41461440920829773\n",
      "      kl: 0.009385268203914165\n",
      "      policy_loss: -0.00018292288586962968\n",
      "      total_loss: 45.05289840698242\n",
      "      vf_explained_var: 0.9559493660926819\n",
      "      vf_loss: 45.05308532714844\n",
      "    sample_time_ms: 20048.45\n",
      "    update_time_ms: 6.897\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.7631612149957\n",
      "    rl_1: 103.43470772617376\n",
      "  time_since_restore: 5482.982876777649\n",
      "  time_this_iter_s: 23.12342596054077\n",
      "  time_total_s: 5482.982876777649\n",
      "  timestamp: 1550798895\n",
      "  timesteps_since_restore: 2260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2260000\n",
      "  training_iteration: 226\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5482 s, 226 iter, 2260000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-28-38\n",
      "  done: false\n",
      "  episode_len_mean: 88.6875\n",
      "  episode_reward_max: 213.48864333867263\n",
      "  episode_reward_mean: 170.6186068305863\n",
      "  episode_reward_min: 124.18080607434614\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 21659\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.523\n",
      "    load_time_ms: 2.215\n",
      "    num_steps_sampled: 2270000\n",
      "    num_steps_trained: 2270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6297855377197266\n",
      "      kl: 0.006957262754440308\n",
      "      policy_loss: -0.003848522901535034\n",
      "      total_loss: 3.022418737411499\n",
      "      vf_explained_var: 0.9930505156517029\n",
      "      vf_loss: 3.0262670516967773\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.37995341420173645\n",
      "      kl: 0.006133108399808407\n",
      "      policy_loss: -0.001541710109449923\n",
      "      total_loss: 4.192537307739258\n",
      "      vf_explained_var: 0.9954838156700134\n",
      "      vf_loss: 4.194077968597412\n",
      "    sample_time_ms: 20000.278\n",
      "    update_time_ms: 6.935\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.70540928511721\n",
      "    rl_1: 104.91319754546905\n",
      "  time_since_restore: 5505.929589986801\n",
      "  time_this_iter_s: 22.94671320915222\n",
      "  time_total_s: 5505.929589986801\n",
      "  timestamp: 1550798918\n",
      "  timesteps_since_restore: 2270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2270000\n",
      "  training_iteration: 227\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5505 s, 227 iter, 2270000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-29-01\n",
      "  done: false\n",
      "  episode_len_mean: 88.69911504424779\n",
      "  episode_reward_max: 217.013511275513\n",
      "  episode_reward_mean: 165.01638060901828\n",
      "  episode_reward_min: -164.38459282312544\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 21772\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.15\n",
      "    load_time_ms: 2.219\n",
      "    num_steps_sampled: 2280000\n",
      "    num_steps_trained: 2280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6140716075897217\n",
      "      kl: 0.007869039662182331\n",
      "      policy_loss: -0.002354628872126341\n",
      "      total_loss: 56.78322982788086\n",
      "      vf_explained_var: 0.8987309336662292\n",
      "      vf_loss: 56.78556442260742\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3715623915195465\n",
      "      kl: 0.010288039222359657\n",
      "      policy_loss: -0.003841541474685073\n",
      "      total_loss: 83.07994079589844\n",
      "      vf_explained_var: 0.923858642578125\n",
      "      vf_loss: 83.08379364013672\n",
      "    sample_time_ms: 19984.852\n",
      "    update_time_ms: 6.929\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.64499480698844\n",
      "    rl_1: 102.37138580202986\n",
      "  time_since_restore: 5528.814240217209\n",
      "  time_this_iter_s: 22.884650230407715\n",
      "  time_total_s: 5528.814240217209\n",
      "  timestamp: 1550798941\n",
      "  timesteps_since_restore: 2280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2280000\n",
      "  training_iteration: 228\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5528 s, 228 iter, 2280000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-29-25\n",
      "  done: false\n",
      "  episode_len_mean: 88.20353982300885\n",
      "  episode_reward_max: 213.47795758630573\n",
      "  episode_reward_mean: 163.6461970444814\n",
      "  episode_reward_min: -177.82486441883285\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 21885\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.608\n",
      "    load_time_ms: 2.225\n",
      "    num_steps_sampled: 2290000\n",
      "    num_steps_trained: 2290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6301600933074951\n",
      "      kl: 0.009850947186350822\n",
      "      policy_loss: -0.0020449822768568993\n",
      "      total_loss: 75.77823638916016\n",
      "      vf_explained_var: 0.8734759092330933\n",
      "      vf_loss: 75.78028869628906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39166730642318726\n",
      "      kl: 0.0034483224153518677\n",
      "      policy_loss: -0.002945282030850649\n",
      "      total_loss: 114.30582427978516\n",
      "      vf_explained_var: 0.9009883403778076\n",
      "      vf_loss: 114.30876922607422\n",
      "    sample_time_ms: 19995.91\n",
      "    update_time_ms: 6.92\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.21505993294953\n",
      "    rl_1: 101.43113711153185\n",
      "  time_since_restore: 5552.110652685165\n",
      "  time_this_iter_s: 23.296412467956543\n",
      "  time_total_s: 5552.110652685165\n",
      "  timestamp: 1550798965\n",
      "  timesteps_since_restore: 2290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2290000\n",
      "  training_iteration: 229\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5552 s, 229 iter, 2290000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-29-48\n",
      "  done: false\n",
      "  episode_len_mean: 89.67857142857143\n",
      "  episode_reward_max: 216.4180974424489\n",
      "  episode_reward_mean: 170.41257368626125\n",
      "  episode_reward_min: 126.48260782381655\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 21997\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.024\n",
      "    load_time_ms: 2.211\n",
      "    num_steps_sampled: 2300000\n",
      "    num_steps_trained: 2300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6438950300216675\n",
      "      kl: 0.00710958382114768\n",
      "      policy_loss: -0.0017577593680471182\n",
      "      total_loss: 3.781451463699341\n",
      "      vf_explained_var: 0.991541862487793\n",
      "      vf_loss: 3.7832093238830566\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3823469877243042\n",
      "      kl: 0.00547753507271409\n",
      "      policy_loss: -0.0023075148928910494\n",
      "      total_loss: 4.036457061767578\n",
      "      vf_explained_var: 0.9957516193389893\n",
      "      vf_loss: 4.038764953613281\n",
      "    sample_time_ms: 20022.079\n",
      "    update_time_ms: 7.02\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.4125257764758\n",
      "    rl_1: 106.00004790978548\n",
      "  time_since_restore: 5575.130722761154\n",
      "  time_this_iter_s: 23.02007007598877\n",
      "  time_total_s: 5575.130722761154\n",
      "  timestamp: 1550798988\n",
      "  timesteps_since_restore: 2300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2300000\n",
      "  training_iteration: 230\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5575 s, 230 iter, 2300000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-30-10\n",
      "  done: false\n",
      "  episode_len_mean: 88.91071428571429\n",
      "  episode_reward_max: 213.67609258327784\n",
      "  episode_reward_mean: 167.23304992780118\n",
      "  episode_reward_min: 128.63318354646515\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 22109\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.923\n",
      "    load_time_ms: 2.326\n",
      "    num_steps_sampled: 2310000\n",
      "    num_steps_trained: 2310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6418018341064453\n",
      "      kl: 0.006153940223157406\n",
      "      policy_loss: -0.001873937901109457\n",
      "      total_loss: 3.729299783706665\n",
      "      vf_explained_var: 0.9912610054016113\n",
      "      vf_loss: 3.7311737537384033\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3627226650714874\n",
      "      kl: 0.008562751114368439\n",
      "      policy_loss: -0.001992317382246256\n",
      "      total_loss: 2.8170418739318848\n",
      "      vf_explained_var: 0.9970425367355347\n",
      "      vf_loss: 2.8190343379974365\n",
      "    sample_time_ms: 19993.418\n",
      "    update_time_ms: 6.689\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.92819054855574\n",
      "    rl_1: 104.30485937924543\n",
      "  time_since_restore: 5597.957417488098\n",
      "  time_this_iter_s: 22.82669472694397\n",
      "  time_total_s: 5597.957417488098\n",
      "  timestamp: 1550799010\n",
      "  timesteps_since_restore: 2310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2310000\n",
      "  training_iteration: 231\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5597 s, 231 iter, 2310000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-30-34\n",
      "  done: false\n",
      "  episode_len_mean: 89.42857142857143\n",
      "  episode_reward_max: 218.12073334015827\n",
      "  episode_reward_mean: 172.2603225715237\n",
      "  episode_reward_min: 121.20039971138198\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 22221\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.277\n",
      "    load_time_ms: 2.348\n",
      "    num_steps_sampled: 2320000\n",
      "    num_steps_trained: 2320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6477803587913513\n",
      "      kl: 0.0056965043768286705\n",
      "      policy_loss: -0.002414563437923789\n",
      "      total_loss: 2.598972797393799\n",
      "      vf_explained_var: 0.9940742254257202\n",
      "      vf_loss: 2.601386785507202\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.37605559825897217\n",
      "      kl: 0.008241426199674606\n",
      "      policy_loss: -0.0019476574379950762\n",
      "      total_loss: 3.051720142364502\n",
      "      vf_explained_var: 0.9969159364700317\n",
      "      vf_loss: 3.0536677837371826\n",
      "    sample_time_ms: 19979.668\n",
      "    update_time_ms: 6.947\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.75802513435477\n",
      "    rl_1: 106.50229743716889\n",
      "  time_since_restore: 5621.411382436752\n",
      "  time_this_iter_s: 23.453964948654175\n",
      "  time_total_s: 5621.411382436752\n",
      "  timestamp: 1550799034\n",
      "  timesteps_since_restore: 2320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2320000\n",
      "  training_iteration: 232\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5621 s, 232 iter, 2320000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-30-57\n",
      "  done: false\n",
      "  episode_len_mean: 87.9298245614035\n",
      "  episode_reward_max: 217.59638945838148\n",
      "  episode_reward_mean: 167.17410928246426\n",
      "  episode_reward_min: -174.39920562150792\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 22335\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.045\n",
      "    load_time_ms: 2.23\n",
      "    num_steps_sampled: 2330000\n",
      "    num_steps_trained: 2330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6087730526924133\n",
      "      kl: 0.008534599095582962\n",
      "      policy_loss: -0.0016106466064229608\n",
      "      total_loss: 34.90728759765625\n",
      "      vf_explained_var: 0.9381338357925415\n",
      "      vf_loss: 34.908897399902344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39899876713752747\n",
      "      kl: 0.018862327560782433\n",
      "      policy_loss: -0.004916102159768343\n",
      "      total_loss: 51.08153533935547\n",
      "      vf_explained_var: 0.9547439217567444\n",
      "      vf_loss: 51.08645248413086\n",
      "    sample_time_ms: 19923.888\n",
      "    update_time_ms: 7.234\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.55682483256557\n",
      "    rl_1: 102.61728444989863\n",
      "  time_since_restore: 5644.55893611908\n",
      "  time_this_iter_s: 23.14755368232727\n",
      "  time_total_s: 5644.55893611908\n",
      "  timestamp: 1550799057\n",
      "  timesteps_since_restore: 2330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2330000\n",
      "  training_iteration: 233\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5644 s, 233 iter, 2330000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-31-20\n",
      "  done: false\n",
      "  episode_len_mean: 87.05263157894737\n",
      "  episode_reward_max: 212.85182395597909\n",
      "  episode_reward_mean: 153.15545648248516\n",
      "  episode_reward_min: -176.39722602476525\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 22449\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3157.945\n",
      "    load_time_ms: 2.208\n",
      "    num_steps_sampled: 2340000\n",
      "    num_steps_trained: 2340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6504833698272705\n",
      "      kl: 0.011565951630473137\n",
      "      policy_loss: -0.0055586909875273705\n",
      "      total_loss: 171.7294158935547\n",
      "      vf_explained_var: 0.75278639793396\n",
      "      vf_loss: 171.73497009277344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.42926397919654846\n",
      "      kl: 0.007180435582995415\n",
      "      policy_loss: -0.0020370222628116608\n",
      "      total_loss: 240.75\n",
      "      vf_explained_var: 0.8050520420074463\n",
      "      vf_loss: 240.7520294189453\n",
      "    sample_time_ms: 19913.678\n",
      "    update_time_ms: 7.683\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.687105850581005\n",
      "    rl_1: 94.46835063190412\n",
      "  time_since_restore: 5667.726375579834\n",
      "  time_this_iter_s: 23.167439460754395\n",
      "  time_total_s: 5667.726375579834\n",
      "  timestamp: 1550799080\n",
      "  timesteps_since_restore: 2340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2340000\n",
      "  training_iteration: 234\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5667 s, 234 iter, 2340000 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-31-44\n",
      "  done: false\n",
      "  episode_len_mean: 87.59649122807018\n",
      "  episode_reward_max: 212.96921928267483\n",
      "  episode_reward_mean: 159.58978653553973\n",
      "  episode_reward_min: -185.32658151955755\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 22563\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.938\n",
      "    load_time_ms: 2.176\n",
      "    num_steps_sampled: 2350000\n",
      "    num_steps_trained: 2350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6337860822677612\n",
      "      kl: 0.013494334183633327\n",
      "      policy_loss: -0.004278395790606737\n",
      "      total_loss: 70.1110610961914\n",
      "      vf_explained_var: 0.8789625763893127\n",
      "      vf_loss: 70.1153335571289\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.41613802313804626\n",
      "      kl: 0.006771532353013754\n",
      "      policy_loss: -0.004321653861552477\n",
      "      total_loss: 90.80615997314453\n",
      "      vf_explained_var: 0.9197763204574585\n",
      "      vf_loss: 90.81048583984375\n",
      "    sample_time_ms: 19941.33\n",
      "    update_time_ms: 7.75\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 59.42776042634122\n",
      "    rl_1: 100.16202610919846\n",
      "  time_since_restore: 5690.979210615158\n",
      "  time_this_iter_s: 23.252835035324097\n",
      "  time_total_s: 5690.979210615158\n",
      "  timestamp: 1550799104\n",
      "  timesteps_since_restore: 2350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2350000\n",
      "  training_iteration: 235\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5690 s, 235 iter, 2350000 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-32-07\n",
      "  done: false\n",
      "  episode_len_mean: 89.19469026548673\n",
      "  episode_reward_max: 214.80322785890073\n",
      "  episode_reward_mean: 169.98178281212026\n",
      "  episode_reward_min: 124.18786963933816\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 22676\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.1\n",
      "    load_time_ms: 2.156\n",
      "    num_steps_sampled: 2360000\n",
      "    num_steps_trained: 2360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6410403847694397\n",
      "      kl: 0.005794212222099304\n",
      "      policy_loss: -0.0019592882599681616\n",
      "      total_loss: 5.2295427322387695\n",
      "      vf_explained_var: 0.9883043169975281\n",
      "      vf_loss: 5.23150110244751\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.45991942286491394\n",
      "      kl: 0.005976393818855286\n",
      "      policy_loss: -0.0011132475920021534\n",
      "      total_loss: 5.485199928283691\n",
      "      vf_explained_var: 0.9946863055229187\n",
      "      vf_loss: 5.486313343048096\n",
      "    sample_time_ms: 19914.014\n",
      "    update_time_ms: 7.56\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.6313812016935\n",
      "    rl_1: 104.35040161042676\n",
      "  time_since_restore: 5713.799175262451\n",
      "  time_this_iter_s: 22.81996464729309\n",
      "  time_total_s: 5713.799175262451\n",
      "  timestamp: 1550799127\n",
      "  timesteps_since_restore: 2360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2360000\n",
      "  training_iteration: 236\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5713 s, 236 iter, 2360000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-32-30\n",
      "  done: false\n",
      "  episode_len_mean: 90.32432432432432\n",
      "  episode_reward_max: 214.2652033111309\n",
      "  episode_reward_mean: 168.90551672039214\n",
      "  episode_reward_min: 126.71423444932712\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 22787\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.755\n",
      "    load_time_ms: 2.173\n",
      "    num_steps_sampled: 2370000\n",
      "    num_steps_trained: 2370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.67416912317276\n",
      "      kl: 0.0047103362157940865\n",
      "      policy_loss: -0.0007424653158523142\n",
      "      total_loss: 3.9558982849121094\n",
      "      vf_explained_var: 0.9903882145881653\n",
      "      vf_loss: 3.9566400051116943\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.46526962518692017\n",
      "      kl: 0.004280431661754847\n",
      "      policy_loss: -0.0009480913286097348\n",
      "      total_loss: 3.9146833419799805\n",
      "      vf_explained_var: 0.9959126114845276\n",
      "      vf_loss: 3.9156317710876465\n",
      "    sample_time_ms: 19994.014\n",
      "    update_time_ms: 7.591\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.78378204331314\n",
      "    rl_1: 105.12173467707898\n",
      "  time_since_restore: 5737.49421620369\n",
      "  time_this_iter_s: 23.695040941238403\n",
      "  time_total_s: 5737.49421620369\n",
      "  timestamp: 1550799150\n",
      "  timesteps_since_restore: 2370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2370000\n",
      "  training_iteration: 237\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5737 s, 237 iter, 2370000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-32-54\n",
      "  done: false\n",
      "  episode_len_mean: 89.01785714285714\n",
      "  episode_reward_max: 214.14872327330346\n",
      "  episode_reward_mean: 166.69462926671955\n",
      "  episode_reward_min: -167.73267120152659\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 22899\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.834\n",
      "    load_time_ms: 2.159\n",
      "    num_steps_sampled: 2380000\n",
      "    num_steps_trained: 2380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6525329947471619\n",
      "      kl: 0.010793063789606094\n",
      "      policy_loss: -0.00416180957108736\n",
      "      total_loss: 57.92219543457031\n",
      "      vf_explained_var: 0.9025542140007019\n",
      "      vf_loss: 57.92634582519531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.42576879262924194\n",
      "      kl: 0.012784743681550026\n",
      "      policy_loss: -0.0012002234579995275\n",
      "      total_loss: 88.30648040771484\n",
      "      vf_explained_var: 0.9240615963935852\n",
      "      vf_loss: 88.30767822265625\n",
      "    sample_time_ms: 20069.63\n",
      "    update_time_ms: 7.658\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.34132690230362\n",
      "    rl_1: 102.3533023644159\n",
      "  time_since_restore: 5761.097822666168\n",
      "  time_this_iter_s: 23.603606462478638\n",
      "  time_total_s: 5761.097822666168\n",
      "  timestamp: 1550799174\n",
      "  timesteps_since_restore: 2380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2380000\n",
      "  training_iteration: 238\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5761 s, 238 iter, 2380000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-33-17\n",
      "  done: false\n",
      "  episode_len_mean: 89.32142857142857\n",
      "  episode_reward_max: 214.27135468306503\n",
      "  episode_reward_mean: 172.01913997587295\n",
      "  episode_reward_min: 128.4107056338207\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 23011\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.23\n",
      "    load_time_ms: 2.188\n",
      "    num_steps_sampled: 2390000\n",
      "    num_steps_trained: 2390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6310656070709229\n",
      "      kl: 0.007277566008269787\n",
      "      policy_loss: -0.0033851563930511475\n",
      "      total_loss: 3.1462080478668213\n",
      "      vf_explained_var: 0.9931235909461975\n",
      "      vf_loss: 3.1495933532714844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39627623558044434\n",
      "      kl: 0.007056374568492174\n",
      "      policy_loss: -0.0009451953810639679\n",
      "      total_loss: 3.512066125869751\n",
      "      vf_explained_var: 0.9962882995605469\n",
      "      vf_loss: 3.5130109786987305\n",
      "    sample_time_ms: 20058.61\n",
      "    update_time_ms: 8.012\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.63396303179873\n",
      "    rl_1: 105.38517694407422\n",
      "  time_since_restore: 5784.277825832367\n",
      "  time_this_iter_s: 23.18000316619873\n",
      "  time_total_s: 5784.277825832367\n",
      "  timestamp: 1550799197\n",
      "  timesteps_since_restore: 2390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2390000\n",
      "  training_iteration: 239\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5784 s, 239 iter, 2390000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-33-40\n",
      "  done: false\n",
      "  episode_len_mean: 89.24107142857143\n",
      "  episode_reward_max: 209.4308556065503\n",
      "  episode_reward_mean: 171.7754825593106\n",
      "  episode_reward_min: -180.35609823887475\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 23123\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.481\n",
      "    load_time_ms: 2.223\n",
      "    num_steps_sampled: 2400000\n",
      "    num_steps_trained: 2400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6445105075836182\n",
      "      kl: 0.004254501778632402\n",
      "      policy_loss: -0.0035444865934550762\n",
      "      total_loss: 16.46341896057129\n",
      "      vf_explained_var: 0.9700103998184204\n",
      "      vf_loss: 16.466962814331055\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.41225647926330566\n",
      "      kl: 0.0043784985318779945\n",
      "      policy_loss: -0.0017492437036707997\n",
      "      total_loss: 19.27045249938965\n",
      "      vf_explained_var: 0.9832020998001099\n",
      "      vf_loss: 19.272199630737305\n",
      "    sample_time_ms: 20084.02\n",
      "    update_time_ms: 7.809\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.15002315278056\n",
      "    rl_1: 105.62545940653003\n",
      "  time_since_restore: 5807.564011335373\n",
      "  time_this_iter_s: 23.28618550300598\n",
      "  time_total_s: 5807.564011335373\n",
      "  timestamp: 1550799220\n",
      "  timesteps_since_restore: 2400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2400000\n",
      "  training_iteration: 240\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5807 s, 240 iter, 2400000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-34-04\n",
      "  done: false\n",
      "  episode_len_mean: 88.97345132743362\n",
      "  episode_reward_max: 207.65653236398614\n",
      "  episode_reward_mean: 162.0470998155954\n",
      "  episode_reward_min: -134.78939881628386\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 23236\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.243\n",
      "    load_time_ms: 2.21\n",
      "    num_steps_sampled: 2410000\n",
      "    num_steps_trained: 2410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7036471962928772\n",
      "      kl: 0.011290407739579678\n",
      "      policy_loss: -0.007107349578291178\n",
      "      total_loss: 82.96434020996094\n",
      "      vf_explained_var: 0.8621463179588318\n",
      "      vf_loss: 82.97144317626953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.41190239787101746\n",
      "      kl: 0.011495494283735752\n",
      "      policy_loss: -0.0019115799805149436\n",
      "      total_loss: 120.43800354003906\n",
      "      vf_explained_var: 0.8960886001586914\n",
      "      vf_loss: 120.43992614746094\n",
      "    sample_time_ms: 20182.728\n",
      "    update_time_ms: 7.95\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.01572446516149\n",
      "    rl_1: 101.03137535043388\n",
      "  time_since_restore: 5831.377432584763\n",
      "  time_this_iter_s: 23.81342124938965\n",
      "  time_total_s: 5831.377432584763\n",
      "  timestamp: 1550799244\n",
      "  timesteps_since_restore: 2410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2410000\n",
      "  training_iteration: 241\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5831 s, 241 iter, 2410000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-34-28\n",
      "  done: false\n",
      "  episode_len_mean: 88.50442477876106\n",
      "  episode_reward_max: 219.5486651636659\n",
      "  episode_reward_mean: 166.96706085961165\n",
      "  episode_reward_min: -136.0156473852956\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 23349\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.326\n",
      "    load_time_ms: 2.191\n",
      "    num_steps_sampled: 2420000\n",
      "    num_steps_trained: 2420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7290792465209961\n",
      "      kl: 0.006692768074572086\n",
      "      policy_loss: -0.0020967572927474976\n",
      "      total_loss: 49.48454284667969\n",
      "      vf_explained_var: 0.9173679947853088\n",
      "      vf_loss: 49.48664474487305\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4379642605781555\n",
      "      kl: 0.004879555199295282\n",
      "      policy_loss: -0.001138768857344985\n",
      "      total_loss: 70.80829620361328\n",
      "      vf_explained_var: 0.933951735496521\n",
      "      vf_loss: 70.80943298339844\n",
      "    sample_time_ms: 20181.167\n",
      "    update_time_ms: 7.952\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.51709522507471\n",
      "    rl_1: 101.44996563453694\n",
      "  time_since_restore: 5854.846224784851\n",
      "  time_this_iter_s: 23.4687922000885\n",
      "  time_total_s: 5854.846224784851\n",
      "  timestamp: 1550799268\n",
      "  timesteps_since_restore: 2420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2420000\n",
      "  training_iteration: 242\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5854 s, 242 iter, 2420000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-34-51\n",
      "  done: false\n",
      "  episode_len_mean: 89.53153153153153\n",
      "  episode_reward_max: 216.9970132400672\n",
      "  episode_reward_mean: 167.07002181516083\n",
      "  episode_reward_min: -132.40145217647486\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 23460\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3154.837\n",
      "    load_time_ms: 2.181\n",
      "    num_steps_sampled: 2430000\n",
      "    num_steps_trained: 2430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6912952065467834\n",
      "      kl: 0.009230615571141243\n",
      "      policy_loss: -0.00477280979976058\n",
      "      total_loss: 36.60406494140625\n",
      "      vf_explained_var: 0.9283934235572815\n",
      "      vf_loss: 36.60883331298828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4223516285419464\n",
      "      kl: 0.006411857437342405\n",
      "      policy_loss: -0.0002598862920422107\n",
      "      total_loss: 44.67511749267578\n",
      "      vf_explained_var: 0.9563313722610474\n",
      "      vf_loss: 44.675376892089844\n",
      "    sample_time_ms: 20196.391\n",
      "    update_time_ms: 7.783\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.91216065672988\n",
      "    rl_1: 103.15786115843092\n",
      "  time_since_restore: 5878.348303794861\n",
      "  time_this_iter_s: 23.502079010009766\n",
      "  time_total_s: 5878.348303794861\n",
      "  timestamp: 1550799291\n",
      "  timesteps_since_restore: 2430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2430000\n",
      "  training_iteration: 243\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5878 s, 243 iter, 2430000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-35-14\n",
      "  done: false\n",
      "  episode_len_mean: 88.83185840707965\n",
      "  episode_reward_max: 220.64252240239693\n",
      "  episode_reward_mean: 165.84183061483546\n",
      "  episode_reward_min: -177.39476760386452\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 23573\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.678\n",
      "    load_time_ms: 2.211\n",
      "    num_steps_sampled: 2440000\n",
      "    num_steps_trained: 2440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6905473470687866\n",
      "      kl: 0.011413347907364368\n",
      "      policy_loss: -0.004180690739303827\n",
      "      total_loss: 54.08203887939453\n",
      "      vf_explained_var: 0.8965460062026978\n",
      "      vf_loss: 54.08620834350586\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.42118772864341736\n",
      "      kl: 0.005576505791395903\n",
      "      policy_loss: 0.0008861532551236451\n",
      "      total_loss: 74.68592834472656\n",
      "      vf_explained_var: 0.9322970509529114\n",
      "      vf_loss: 74.6850357055664\n",
      "    sample_time_ms: 20205.079\n",
      "    update_time_ms: 7.617\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.93125361671607\n",
      "    rl_1: 102.91057699811938\n",
      "  time_since_restore: 5901.409498214722\n",
      "  time_this_iter_s: 23.06119441986084\n",
      "  time_total_s: 5901.409498214722\n",
      "  timestamp: 1550799314\n",
      "  timesteps_since_restore: 2440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2440000\n",
      "  training_iteration: 244\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5901 s, 244 iter, 2440000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-35-38\n",
      "  done: false\n",
      "  episode_len_mean: 89.34234234234235\n",
      "  episode_reward_max: 213.86126144808532\n",
      "  episode_reward_mean: 170.82422736869466\n",
      "  episode_reward_min: 125.05916355513554\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 23684\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.187\n",
      "    load_time_ms: 2.25\n",
      "    num_steps_sampled: 2450000\n",
      "    num_steps_trained: 2450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6669011116027832\n",
      "      kl: 0.008227027021348476\n",
      "      policy_loss: -0.004457819741219282\n",
      "      total_loss: 2.579486131668091\n",
      "      vf_explained_var: 0.9939388632774353\n",
      "      vf_loss: 2.5839438438415527\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4037545621395111\n",
      "      kl: 0.01102450117468834\n",
      "      policy_loss: -0.0031025141943246126\n",
      "      total_loss: 2.8921377658843994\n",
      "      vf_explained_var: 0.9968815445899963\n",
      "      vf_loss: 2.8952407836914062\n",
      "    sample_time_ms: 20226.395\n",
      "    update_time_ms: 7.557\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.95797836467757\n",
      "    rl_1: 104.86624900401708\n",
      "  time_since_restore: 5924.879589557648\n",
      "  time_this_iter_s: 23.470091342926025\n",
      "  time_total_s: 5924.879589557648\n",
      "  timestamp: 1550799338\n",
      "  timesteps_since_restore: 2450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2450000\n",
      "  training_iteration: 245\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5924 s, 245 iter, 2450000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-36-01\n",
      "  done: false\n",
      "  episode_len_mean: 89.36607142857143\n",
      "  episode_reward_max: 217.5390248832298\n",
      "  episode_reward_mean: 166.9612631275336\n",
      "  episode_reward_min: -133.11253531712464\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 23796\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.284\n",
      "    load_time_ms: 2.321\n",
      "    num_steps_sampled: 2460000\n",
      "    num_steps_trained: 2460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6314201354980469\n",
      "      kl: 0.00889328308403492\n",
      "      policy_loss: -0.004171981476247311\n",
      "      total_loss: 41.057621002197266\n",
      "      vf_explained_var: 0.9220219254493713\n",
      "      vf_loss: 41.06178665161133\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.38693633675575256\n",
      "      kl: 0.0043648178689181805\n",
      "      policy_loss: -0.0003909060324076563\n",
      "      total_loss: 56.43345642089844\n",
      "      vf_explained_var: 0.9484720230102539\n",
      "      vf_loss: 56.43386459350586\n",
      "    sample_time_ms: 20237.697\n",
      "    update_time_ms: 7.903\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.44523606776933\n",
      "    rl_1: 102.51602705976428\n",
      "  time_since_restore: 5947.850991487503\n",
      "  time_this_iter_s: 22.971401929855347\n",
      "  time_total_s: 5947.850991487503\n",
      "  timestamp: 1550799361\n",
      "  timesteps_since_restore: 2460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2460000\n",
      "  training_iteration: 246\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5947 s, 246 iter, 2460000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-36-25\n",
      "  done: false\n",
      "  episode_len_mean: 88.30973451327434\n",
      "  episode_reward_max: 211.54799145731565\n",
      "  episode_reward_mean: 163.4069262886785\n",
      "  episode_reward_min: -173.31578325556666\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 23909\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.195\n",
      "    load_time_ms: 2.315\n",
      "    num_steps_sampled: 2470000\n",
      "    num_steps_trained: 2470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7034493684768677\n",
      "      kl: 0.008128613233566284\n",
      "      policy_loss: -0.0010936667677015066\n",
      "      total_loss: 40.66719436645508\n",
      "      vf_explained_var: 0.9262393116950989\n",
      "      vf_loss: 40.66828918457031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.40169963240623474\n",
      "      kl: 0.01889145001769066\n",
      "      policy_loss: -0.0027276307810097933\n",
      "      total_loss: 58.0197639465332\n",
      "      vf_explained_var: 0.9446759223937988\n",
      "      vf_loss: 58.022499084472656\n",
      "    sample_time_ms: 20240.772\n",
      "    update_time_ms: 7.825\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.109937808291384\n",
      "    rl_1: 100.29698848038711\n",
      "  time_since_restore: 5971.5836045742035\n",
      "  time_this_iter_s: 23.73261308670044\n",
      "  time_total_s: 5971.5836045742035\n",
      "  timestamp: 1550799385\n",
      "  timesteps_since_restore: 2470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2470000\n",
      "  training_iteration: 247\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5971 s, 247 iter, 2470000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-36-48\n",
      "  done: false\n",
      "  episode_len_mean: 89.23008849557522\n",
      "  episode_reward_max: 214.0894895188145\n",
      "  episode_reward_mean: 162.5089289684801\n",
      "  episode_reward_min: -170.609363037822\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 24022\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.123\n",
      "    load_time_ms: 2.318\n",
      "    num_steps_sampled: 2480000\n",
      "    num_steps_trained: 2480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6423102617263794\n",
      "      kl: 0.01107647642493248\n",
      "      policy_loss: -0.005286036524921656\n",
      "      total_loss: 38.30567169189453\n",
      "      vf_explained_var: 0.9297766089439392\n",
      "      vf_loss: 38.31095504760742\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32332444190979004\n",
      "      kl: 0.006398647092282772\n",
      "      policy_loss: -0.0022214422933757305\n",
      "      total_loss: 57.37148666381836\n",
      "      vf_explained_var: 0.9478771686553955\n",
      "      vf_loss: 57.37371063232422\n",
      "    sample_time_ms: 20232.113\n",
      "    update_time_ms: 7.843\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 59.97022311882276\n",
      "    rl_1: 102.53870584965728\n",
      "  time_since_restore: 5995.1200494766235\n",
      "  time_this_iter_s: 23.536444902420044\n",
      "  time_total_s: 5995.1200494766235\n",
      "  timestamp: 1550799408\n",
      "  timesteps_since_restore: 2480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2480000\n",
      "  training_iteration: 248\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 5995 s, 248 iter, 2480000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-37-11\n",
      "  done: false\n",
      "  episode_len_mean: 89.91891891891892\n",
      "  episode_reward_max: 215.94149893892046\n",
      "  episode_reward_mean: 167.23295863893526\n",
      "  episode_reward_min: 125.39770583305106\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 24133\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.295\n",
      "    load_time_ms: 2.285\n",
      "    num_steps_sampled: 2490000\n",
      "    num_steps_trained: 2490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6411206126213074\n",
      "      kl: 0.007656117435544729\n",
      "      policy_loss: -0.0037524814251810312\n",
      "      total_loss: 3.2173011302948\n",
      "      vf_explained_var: 0.9922972321510315\n",
      "      vf_loss: 3.2210538387298584\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.31069326400756836\n",
      "      kl: 0.0069956849329173565\n",
      "      policy_loss: -0.002066885819658637\n",
      "      total_loss: 4.21259069442749\n",
      "      vf_explained_var: 0.9955930709838867\n",
      "      vf_loss: 4.214657306671143\n",
      "    sample_time_ms: 20216.287\n",
      "    update_time_ms: 7.525\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.06554360122858\n",
      "    rl_1: 105.16741503770669\n",
      "  time_since_restore: 6018.137025356293\n",
      "  time_this_iter_s: 23.01697587966919\n",
      "  time_total_s: 6018.137025356293\n",
      "  timestamp: 1550799431\n",
      "  timesteps_since_restore: 2490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2490000\n",
      "  training_iteration: 249\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6018 s, 249 iter, 2490000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-37-35\n",
      "  done: false\n",
      "  episode_len_mean: 89.41071428571429\n",
      "  episode_reward_max: 214.02481687695834\n",
      "  episode_reward_mean: 168.2568484553799\n",
      "  episode_reward_min: -169.13180010937953\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 24245\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.212\n",
      "    load_time_ms: 2.27\n",
      "    num_steps_sampled: 2500000\n",
      "    num_steps_trained: 2500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6126981377601624\n",
      "      kl: 0.007137584965676069\n",
      "      policy_loss: -0.005718528758734465\n",
      "      total_loss: 22.88241958618164\n",
      "      vf_explained_var: 0.9581132531166077\n",
      "      vf_loss: 22.888141632080078\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2974317967891693\n",
      "      kl: 0.004531290847808123\n",
      "      policy_loss: -8.946093294071034e-05\n",
      "      total_loss: 39.620025634765625\n",
      "      vf_explained_var: 0.9647676348686218\n",
      "      vf_loss: 39.6201171875\n",
      "    sample_time_ms: 20218.522\n",
      "    update_time_ms: 7.714\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.267239197569175\n",
      "    rl_1: 104.9896092578107\n",
      "  time_since_restore: 6041.4675171375275\n",
      "  time_this_iter_s: 23.33049178123474\n",
      "  time_total_s: 6041.4675171375275\n",
      "  timestamp: 1550799455\n",
      "  timesteps_since_restore: 2500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2500000\n",
      "  training_iteration: 250\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6041 s, 250 iter, 2500000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-37-58\n",
      "  done: false\n",
      "  episode_len_mean: 90.38181818181818\n",
      "  episode_reward_max: 216.20320400424168\n",
      "  episode_reward_mean: 168.7423113241323\n",
      "  episode_reward_min: 123.62882970028461\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 24355\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.109\n",
      "    load_time_ms: 2.171\n",
      "    num_steps_sampled: 2510000\n",
      "    num_steps_trained: 2510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6533511281013489\n",
      "      kl: 0.012255333364009857\n",
      "      policy_loss: -0.00477631064131856\n",
      "      total_loss: 3.887749195098877\n",
      "      vf_explained_var: 0.991450309753418\n",
      "      vf_loss: 3.8925251960754395\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3104251027107239\n",
      "      kl: 0.006217626854777336\n",
      "      policy_loss: -0.000583979650400579\n",
      "      total_loss: 4.128228187561035\n",
      "      vf_explained_var: 0.9956587553024292\n",
      "      vf_loss: 4.128812313079834\n",
      "    sample_time_ms: 20162.115\n",
      "    update_time_ms: 7.406\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.396593071277636\n",
      "    rl_1: 105.3457182528546\n",
      "  time_since_restore: 6064.700557231903\n",
      "  time_this_iter_s: 23.23304009437561\n",
      "  time_total_s: 6064.700557231903\n",
      "  timestamp: 1550799478\n",
      "  timesteps_since_restore: 2510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2510000\n",
      "  training_iteration: 251\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6064 s, 251 iter, 2510000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-38-22\n",
      "  done: false\n",
      "  episode_len_mean: 89.83928571428571\n",
      "  episode_reward_max: 219.03443378217005\n",
      "  episode_reward_mean: 170.01274025334115\n",
      "  episode_reward_min: 125.8691179561705\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 24467\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3159.097\n",
      "    load_time_ms: 2.28\n",
      "    num_steps_sampled: 2520000\n",
      "    num_steps_trained: 2520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6201270818710327\n",
      "      kl: 0.006277936510741711\n",
      "      policy_loss: -0.003914499189704657\n",
      "      total_loss: 3.032569169998169\n",
      "      vf_explained_var: 0.9932816624641418\n",
      "      vf_loss: 3.0364835262298584\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.271051824092865\n",
      "      kl: 0.005911578424274921\n",
      "      policy_loss: -0.0021116663701832294\n",
      "      total_loss: 3.14699125289917\n",
      "      vf_explained_var: 0.9967915415763855\n",
      "      vf_loss: 3.1491031646728516\n",
      "    sample_time_ms: 20167.68\n",
      "    update_time_ms: 7.119\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.14981016319506\n",
      "    rl_1: 105.86293009014605\n",
      "  time_since_restore: 6088.384775161743\n",
      "  time_this_iter_s: 23.684217929840088\n",
      "  time_total_s: 6088.384775161743\n",
      "  timestamp: 1550799502\n",
      "  timesteps_since_restore: 2520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2520000\n",
      "  training_iteration: 252\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6088 s, 252 iter, 2520000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-38-45\n",
      "  done: false\n",
      "  episode_len_mean: 88.98214285714286\n",
      "  episode_reward_max: 216.854047838686\n",
      "  episode_reward_mean: 167.26753014185138\n",
      "  episode_reward_min: -125.73344669523478\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 24579\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.96\n",
      "    load_time_ms: 2.31\n",
      "    num_steps_sampled: 2530000\n",
      "    num_steps_trained: 2530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6584666967391968\n",
      "      kl: 0.011186649091541767\n",
      "      policy_loss: -0.0054213861003518105\n",
      "      total_loss: 39.802978515625\n",
      "      vf_explained_var: 0.9304692149162292\n",
      "      vf_loss: 39.80839157104492\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.30111125111579895\n",
      "      kl: 0.009688982740044594\n",
      "      policy_loss: -0.0055589377880096436\n",
      "      total_loss: 52.98714828491211\n",
      "      vf_explained_var: 0.9526055455207825\n",
      "      vf_loss: 52.992706298828125\n",
      "    sample_time_ms: 20140.277\n",
      "    update_time_ms: 7.276\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.367176322127065\n",
      "    rl_1: 103.90035381972429\n",
      "  time_since_restore: 6111.38286614418\n",
      "  time_this_iter_s: 22.998090982437134\n",
      "  time_total_s: 6111.38286614418\n",
      "  timestamp: 1550799525\n",
      "  timesteps_since_restore: 2530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2530000\n",
      "  training_iteration: 253\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6111 s, 253 iter, 2530000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-39-09\n",
      "  done: false\n",
      "  episode_len_mean: 89.24107142857143\n",
      "  episode_reward_max: 218.83391518702635\n",
      "  episode_reward_mean: 174.4169976212585\n",
      "  episode_reward_min: 132.02343991605574\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 24691\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.435\n",
      "    load_time_ms: 2.294\n",
      "    num_steps_sampled: 2540000\n",
      "    num_steps_trained: 2540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6496796011924744\n",
      "      kl: 0.00612652488052845\n",
      "      policy_loss: -0.0030224784277379513\n",
      "      total_loss: 2.8301799297332764\n",
      "      vf_explained_var: 0.9938803315162659\n",
      "      vf_loss: 2.8332021236419678\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.31158867478370667\n",
      "      kl: 0.0038812721613794565\n",
      "      policy_loss: 0.0006286537391133606\n",
      "      total_loss: 2.9274356365203857\n",
      "      vf_explained_var: 0.9969558715820312\n",
      "      vf_loss: 2.926807403564453\n",
      "    sample_time_ms: 20194.006\n",
      "    update_time_ms: 7.102\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.73744705512061\n",
      "    rl_1: 105.6795505661379\n",
      "  time_since_restore: 6134.976570367813\n",
      "  time_this_iter_s: 23.593704223632812\n",
      "  time_total_s: 6134.976570367813\n",
      "  timestamp: 1550799549\n",
      "  timesteps_since_restore: 2540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2540000\n",
      "  training_iteration: 254\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6134 s, 254 iter, 2540000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-39-32\n",
      "  done: false\n",
      "  episode_len_mean: 89.19469026548673\n",
      "  episode_reward_max: 211.53673357919152\n",
      "  episode_reward_mean: 168.5192429632058\n",
      "  episode_reward_min: -177.18100213471223\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 24804\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.375\n",
      "    load_time_ms: 2.295\n",
      "    num_steps_sampled: 2550000\n",
      "    num_steps_trained: 2550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7008447051048279\n",
      "      kl: 0.012733625248074532\n",
      "      policy_loss: -0.00780462846159935\n",
      "      total_loss: 34.489601135253906\n",
      "      vf_explained_var: 0.9403284788131714\n",
      "      vf_loss: 34.497406005859375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.38849809765815735\n",
      "      kl: 0.008416171185672283\n",
      "      policy_loss: -0.000767105259001255\n",
      "      total_loss: 50.969120025634766\n",
      "      vf_explained_var: 0.9539194107055664\n",
      "      vf_loss: 50.969886779785156\n",
      "    sample_time_ms: 20163.642\n",
      "    update_time_ms: 7.104\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.15052408930659\n",
      "    rl_1: 102.36871887389921\n",
      "  time_since_restore: 6158.153334140778\n",
      "  time_this_iter_s: 23.176763772964478\n",
      "  time_total_s: 6158.153334140778\n",
      "  timestamp: 1550799572\n",
      "  timesteps_since_restore: 2550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2550000\n",
      "  training_iteration: 255\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6158 s, 255 iter, 2550000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-39-55\n",
      "  done: false\n",
      "  episode_len_mean: 89.61261261261261\n",
      "  episode_reward_max: 220.20125884130147\n",
      "  episode_reward_mean: 166.93331178820952\n",
      "  episode_reward_min: 123.6129103788037\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 24915\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.583\n",
      "    load_time_ms: 2.299\n",
      "    num_steps_sampled: 2560000\n",
      "    num_steps_trained: 2560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7100347280502319\n",
      "      kl: 0.00864277221262455\n",
      "      policy_loss: -0.005694023333489895\n",
      "      total_loss: 3.2545695304870605\n",
      "      vf_explained_var: 0.9926953315734863\n",
      "      vf_loss: 3.260263204574585\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3339230418205261\n",
      "      kl: 0.006711337249726057\n",
      "      policy_loss: -0.0015639106277376413\n",
      "      total_loss: 3.162055492401123\n",
      "      vf_explained_var: 0.9966176152229309\n",
      "      vf_loss: 3.1636197566986084\n",
      "    sample_time_ms: 20164.107\n",
      "    update_time_ms: 6.748\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.0140157026894\n",
      "    rl_1: 103.91929608552012\n",
      "  time_since_restore: 6181.116890192032\n",
      "  time_this_iter_s: 22.963556051254272\n",
      "  time_total_s: 6181.116890192032\n",
      "  timestamp: 1550799595\n",
      "  timesteps_since_restore: 2560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2560000\n",
      "  training_iteration: 256\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6181 s, 256 iter, 2560000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-40-18\n",
      "  done: false\n",
      "  episode_len_mean: 89.3963963963964\n",
      "  episode_reward_max: 215.31636758281525\n",
      "  episode_reward_mean: 167.28628874294012\n",
      "  episode_reward_min: -127.32482599043487\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 25026\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.759\n",
      "    load_time_ms: 2.428\n",
      "    num_steps_sampled: 2570000\n",
      "    num_steps_trained: 2570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6722812652587891\n",
      "      kl: 0.008968872018158436\n",
      "      policy_loss: -0.003466802416369319\n",
      "      total_loss: 30.358592987060547\n",
      "      vf_explained_var: 0.945908784866333\n",
      "      vf_loss: 30.362051010131836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3371087610721588\n",
      "      kl: 0.006487981416285038\n",
      "      policy_loss: -0.0002911823394242674\n",
      "      total_loss: 41.01173782348633\n",
      "      vf_explained_var: 0.9604208469390869\n",
      "      vf_loss: 41.012027740478516\n",
      "    sample_time_ms: 20062.169\n",
      "    update_time_ms: 6.873\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.45205309327959\n",
      "    rl_1: 102.8342356496605\n",
      "  time_since_restore: 6203.8183007240295\n",
      "  time_this_iter_s: 22.70141053199768\n",
      "  time_total_s: 6203.8183007240295\n",
      "  timestamp: 1550799618\n",
      "  timesteps_since_restore: 2570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2570000\n",
      "  training_iteration: 257\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6203 s, 257 iter, 2570000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-40-41\n",
      "  done: false\n",
      "  episode_len_mean: 88.6140350877193\n",
      "  episode_reward_max: 216.0101827568575\n",
      "  episode_reward_mean: 169.9586189165475\n",
      "  episode_reward_min: -169.53799750360514\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 25140\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.89\n",
      "    load_time_ms: 2.433\n",
      "    num_steps_sampled: 2580000\n",
      "    num_steps_trained: 2580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6623785495758057\n",
      "      kl: 0.009400486014783382\n",
      "      policy_loss: -0.004071678034961224\n",
      "      total_loss: 34.22979736328125\n",
      "      vf_explained_var: 0.937052309513092\n",
      "      vf_loss: 34.23387145996094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.36958128213882446\n",
      "      kl: 0.00671408511698246\n",
      "      policy_loss: -0.002298930427059531\n",
      "      total_loss: 40.76799011230469\n",
      "      vf_explained_var: 0.9605514407157898\n",
      "      vf_loss: 40.77029037475586\n",
      "    sample_time_ms: 20021.215\n",
      "    update_time_ms: 6.886\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.98380702474884\n",
      "    rl_1: 102.97481189179858\n",
      "  time_since_restore: 6226.997478961945\n",
      "  time_this_iter_s: 23.17917823791504\n",
      "  time_total_s: 6226.997478961945\n",
      "  timestamp: 1550799641\n",
      "  timesteps_since_restore: 2580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2580000\n",
      "  training_iteration: 258\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6226 s, 258 iter, 2580000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-41-04\n",
      "  done: false\n",
      "  episode_len_mean: 90.2090909090909\n",
      "  episode_reward_max: 215.00781832478714\n",
      "  episode_reward_mean: 169.51221811693364\n",
      "  episode_reward_min: 124.86895095031952\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 25250\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.714\n",
      "    load_time_ms: 2.547\n",
      "    num_steps_sampled: 2590000\n",
      "    num_steps_trained: 2590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7157672047615051\n",
      "      kl: 0.007678483612835407\n",
      "      policy_loss: -0.0025193113833665848\n",
      "      total_loss: 3.311328172683716\n",
      "      vf_explained_var: 0.9927563071250916\n",
      "      vf_loss: 3.313847064971924\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.38112103939056396\n",
      "      kl: 0.007264628540724516\n",
      "      policy_loss: -0.0035423904191702604\n",
      "      total_loss: 3.143958330154419\n",
      "      vf_explained_var: 0.9967182874679565\n",
      "      vf_loss: 3.14750075340271\n",
      "    sample_time_ms: 20026.077\n",
      "    update_time_ms: 6.937\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.80309827922247\n",
      "    rl_1: 104.7091198377112\n",
      "  time_since_restore: 6250.071949005127\n",
      "  time_this_iter_s: 23.074470043182373\n",
      "  time_total_s: 6250.071949005127\n",
      "  timestamp: 1550799664\n",
      "  timesteps_since_restore: 2590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2590000\n",
      "  training_iteration: 259\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6250 s, 259 iter, 2590000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-41-27\n",
      "  done: false\n",
      "  episode_len_mean: 88.94690265486726\n",
      "  episode_reward_max: 219.68104680473482\n",
      "  episode_reward_mean: 168.8125832211654\n",
      "  episode_reward_min: 122.01421556845864\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 25363\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.836\n",
      "    load_time_ms: 2.633\n",
      "    num_steps_sampled: 2600000\n",
      "    num_steps_trained: 2600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6630837917327881\n",
      "      kl: 0.006612969562411308\n",
      "      policy_loss: -0.0024585432838648558\n",
      "      total_loss: 3.1172049045562744\n",
      "      vf_explained_var: 0.9927682280540466\n",
      "      vf_loss: 3.1196632385253906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.34670454263687134\n",
      "      kl: 0.007383332587778568\n",
      "      policy_loss: -0.0016851708060130477\n",
      "      total_loss: 2.922757387161255\n",
      "      vf_explained_var: 0.9968849420547485\n",
      "      vf_loss: 2.9244420528411865\n",
      "    sample_time_ms: 20040.779\n",
      "    update_time_ms: 6.794\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.3588029409038\n",
      "    rl_1: 103.4537802802616\n",
      "  time_since_restore: 6273.522247076035\n",
      "  time_this_iter_s: 23.450298070907593\n",
      "  time_total_s: 6273.522247076035\n",
      "  timestamp: 1550799687\n",
      "  timesteps_since_restore: 2600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2600000\n",
      "  training_iteration: 260\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6273 s, 260 iter, 2600000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-41-51\n",
      "  done: false\n",
      "  episode_len_mean: 87.93805309734513\n",
      "  episode_reward_max: 217.92888735857264\n",
      "  episode_reward_mean: 159.85251634004908\n",
      "  episode_reward_min: -187.55380913831004\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 25476\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3151.605\n",
      "    load_time_ms: 2.643\n",
      "    num_steps_sampled: 2610000\n",
      "    num_steps_trained: 2610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.659888505935669\n",
      "      kl: 0.010211308486759663\n",
      "      policy_loss: -0.003912945743650198\n",
      "      total_loss: 80.1318588256836\n",
      "      vf_explained_var: 0.8661090135574341\n",
      "      vf_loss: 80.13578796386719\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3426065146923065\n",
      "      kl: 0.006462707184255123\n",
      "      policy_loss: -0.0024804319255053997\n",
      "      total_loss: 114.88129425048828\n",
      "      vf_explained_var: 0.9022332429885864\n",
      "      vf_loss: 114.8837890625\n",
      "    sample_time_ms: 20046.456\n",
      "    update_time_ms: 7.091\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.10689419680688\n",
      "    rl_1: 98.74562214324216\n",
      "  time_since_restore: 6296.964020013809\n",
      "  time_this_iter_s: 23.441772937774658\n",
      "  time_total_s: 6296.964020013809\n",
      "  timestamp: 1550799711\n",
      "  timesteps_since_restore: 2610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2610000\n",
      "  training_iteration: 261\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6296 s, 261 iter, 2610000 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-42-14\n",
      "  done: false\n",
      "  episode_len_mean: 89.375\n",
      "  episode_reward_max: 217.80427529963876\n",
      "  episode_reward_mean: 172.18368581895646\n",
      "  episode_reward_min: 129.6644673758318\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 25588\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.244\n",
      "    load_time_ms: 2.619\n",
      "    num_steps_sampled: 2620000\n",
      "    num_steps_trained: 2620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6347119212150574\n",
      "      kl: 0.006965838372707367\n",
      "      policy_loss: -0.0018943246686831117\n",
      "      total_loss: 2.9454128742218018\n",
      "      vf_explained_var: 0.9934626817703247\n",
      "      vf_loss: 2.947307586669922\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3434651792049408\n",
      "      kl: 0.007440449204295874\n",
      "      policy_loss: -0.0011807880364358425\n",
      "      total_loss: 2.424015522003174\n",
      "      vf_explained_var: 0.9974647760391235\n",
      "      vf_loss: 2.425196409225464\n",
      "    sample_time_ms: 20016.304\n",
      "    update_time_ms: 7.096\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.02686728444084\n",
      "    rl_1: 105.15681853451564\n",
      "  time_since_restore: 6320.1515855789185\n",
      "  time_this_iter_s: 23.187565565109253\n",
      "  time_total_s: 6320.1515855789185\n",
      "  timestamp: 1550799734\n",
      "  timesteps_since_restore: 2620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2620000\n",
      "  training_iteration: 262\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6320 s, 262 iter, 2620000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-42-38\n",
      "  done: false\n",
      "  episode_len_mean: 89.44642857142857\n",
      "  episode_reward_max: 218.77901332584213\n",
      "  episode_reward_mean: 173.63685437415742\n",
      "  episode_reward_min: 129.68745935718317\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 25700\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.35\n",
      "    load_time_ms: 2.662\n",
      "    num_steps_sampled: 2630000\n",
      "    num_steps_trained: 2630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6070104241371155\n",
      "      kl: 0.011282369494438171\n",
      "      policy_loss: -0.004316483158618212\n",
      "      total_loss: 3.2865049839019775\n",
      "      vf_explained_var: 0.9928665161132812\n",
      "      vf_loss: 3.290821075439453\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3435527980327606\n",
      "      kl: 0.00486545916646719\n",
      "      policy_loss: -0.0009615203598514199\n",
      "      total_loss: 2.5478568077087402\n",
      "      vf_explained_var: 0.9973278641700745\n",
      "      vf_loss: 2.5488181114196777\n",
      "    sample_time_ms: 20058.042\n",
      "    update_time_ms: 7.173\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.15847026794516\n",
      "    rl_1: 105.47838410621226\n",
      "  time_since_restore: 6343.605757713318\n",
      "  time_this_iter_s: 23.454172134399414\n",
      "  time_total_s: 6343.605757713318\n",
      "  timestamp: 1550799758\n",
      "  timesteps_since_restore: 2630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2630000\n",
      "  training_iteration: 263\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6343 s, 263 iter, 2630000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-43-01\n",
      "  done: false\n",
      "  episode_len_mean: 88.66371681415929\n",
      "  episode_reward_max: 214.30400331774652\n",
      "  episode_reward_mean: 168.83337556019956\n",
      "  episode_reward_min: -177.39231910551587\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 25813\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.935\n",
      "    load_time_ms: 2.753\n",
      "    num_steps_sampled: 2640000\n",
      "    num_steps_trained: 2640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5739549994468689\n",
      "      kl: 0.00608897116035223\n",
      "      policy_loss: -0.0016600453527644277\n",
      "      total_loss: 43.2584342956543\n",
      "      vf_explained_var: 0.9259238839149475\n",
      "      vf_loss: 43.260093688964844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32769128680229187\n",
      "      kl: 0.0076970383524894714\n",
      "      policy_loss: -0.002468075603246689\n",
      "      total_loss: 66.82283782958984\n",
      "      vf_explained_var: 0.9398102164268494\n",
      "      vf_loss: 66.82530975341797\n",
      "    sample_time_ms: 20017.438\n",
      "    update_time_ms: 7.154\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.2521511905933\n",
      "    rl_1: 103.58122436960623\n",
      "  time_since_restore: 6366.79024720192\n",
      "  time_this_iter_s: 23.184489488601685\n",
      "  time_total_s: 6366.79024720192\n",
      "  timestamp: 1550799781\n",
      "  timesteps_since_restore: 2640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2640000\n",
      "  training_iteration: 264\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6366 s, 264 iter, 2640000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-43-24\n",
      "  done: false\n",
      "  episode_len_mean: 88.12389380530973\n",
      "  episode_reward_max: 216.52906728010015\n",
      "  episode_reward_mean: 160.0746825881564\n",
      "  episode_reward_min: -178.52399071026036\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 25926\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.113\n",
      "    load_time_ms: 2.757\n",
      "    num_steps_sampled: 2650000\n",
      "    num_steps_trained: 2650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6461805105209351\n",
      "      kl: 0.008876198902726173\n",
      "      policy_loss: -0.0020405149552971125\n",
      "      total_loss: 50.99529266357422\n",
      "      vf_explained_var: 0.9067398309707642\n",
      "      vf_loss: 50.997337341308594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3485974967479706\n",
      "      kl: 0.011853566393256187\n",
      "      policy_loss: -0.002917114645242691\n",
      "      total_loss: 74.29317474365234\n",
      "      vf_explained_var: 0.9317196607589722\n",
      "      vf_loss: 74.29608917236328\n",
      "    sample_time_ms: 20042.579\n",
      "    update_time_ms: 7.102\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.02895643020883\n",
      "    rl_1: 100.04572615794757\n",
      "  time_since_restore: 6390.213036298752\n",
      "  time_this_iter_s: 23.422789096832275\n",
      "  time_total_s: 6390.213036298752\n",
      "  timestamp: 1550799804\n",
      "  timesteps_since_restore: 2650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2650000\n",
      "  training_iteration: 265\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6390 s, 265 iter, 2650000 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-43-47\n",
      "  done: false\n",
      "  episode_len_mean: 89.96396396396396\n",
      "  episode_reward_max: 216.81431323716106\n",
      "  episode_reward_mean: 171.86120948431181\n",
      "  episode_reward_min: 131.35696989941252\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 26037\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.059\n",
      "    load_time_ms: 2.682\n",
      "    num_steps_sampled: 2660000\n",
      "    num_steps_trained: 2660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6336541175842285\n",
      "      kl: 0.005521375685930252\n",
      "      policy_loss: -0.0018822825513780117\n",
      "      total_loss: 3.897855758666992\n",
      "      vf_explained_var: 0.9913411140441895\n",
      "      vf_loss: 3.8997368812561035\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.36766326427459717\n",
      "      kl: 0.007435102481395006\n",
      "      policy_loss: -0.0009960245806723833\n",
      "      total_loss: 3.1016550064086914\n",
      "      vf_explained_var: 0.9967322945594788\n",
      "      vf_loss: 3.1026508808135986\n",
      "    sample_time_ms: 20061.569\n",
      "    update_time_ms: 7.292\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.08463934134708\n",
      "    rl_1: 104.77657014296466\n",
      "  time_since_restore: 6413.3561906814575\n",
      "  time_this_iter_s: 23.14315438270569\n",
      "  time_total_s: 6413.3561906814575\n",
      "  timestamp: 1550799827\n",
      "  timesteps_since_restore: 2660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2660000\n",
      "  training_iteration: 266\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6413 s, 266 iter, 2660000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-44-11\n",
      "  done: false\n",
      "  episode_len_mean: 89.00884955752213\n",
      "  episode_reward_max: 206.83378696618544\n",
      "  episode_reward_mean: 163.63286549374587\n",
      "  episode_reward_min: -177.9134627830693\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 26150\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.781\n",
      "    load_time_ms: 2.54\n",
      "    num_steps_sampled: 2670000\n",
      "    num_steps_trained: 2670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6539179682731628\n",
      "      kl: 0.007788123097270727\n",
      "      policy_loss: -0.003051331965252757\n",
      "      total_loss: 44.049041748046875\n",
      "      vf_explained_var: 0.92926424741745\n",
      "      vf_loss: 44.05209732055664\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3283568024635315\n",
      "      kl: 0.006634511984884739\n",
      "      policy_loss: 0.00107823614962399\n",
      "      total_loss: 69.76575469970703\n",
      "      vf_explained_var: 0.9380987286567688\n",
      "      vf_loss: 69.76467895507812\n",
      "    sample_time_ms: 20129.426\n",
      "    update_time_ms: 7.298\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.123467223309255\n",
      "    rl_1: 102.50939827043662\n",
      "  time_since_restore: 6436.759634256363\n",
      "  time_this_iter_s: 23.403443574905396\n",
      "  time_total_s: 6436.759634256363\n",
      "  timestamp: 1550799851\n",
      "  timesteps_since_restore: 2670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2670000\n",
      "  training_iteration: 267\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6436 s, 267 iter, 2670000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-44-34\n",
      "  done: false\n",
      "  episode_len_mean: 89.02678571428571\n",
      "  episode_reward_max: 218.86915222966684\n",
      "  episode_reward_mean: 167.4400915963523\n",
      "  episode_reward_min: -169.15981278586085\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 26262\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.915\n",
      "    load_time_ms: 2.546\n",
      "    num_steps_sampled: 2680000\n",
      "    num_steps_trained: 2680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6459810137748718\n",
      "      kl: 0.007769221905618906\n",
      "      policy_loss: -0.0013694019289687276\n",
      "      total_loss: 25.485820770263672\n",
      "      vf_explained_var: 0.9542567729949951\n",
      "      vf_loss: 25.48719024658203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.37244173884391785\n",
      "      kl: 0.005033647175878286\n",
      "      policy_loss: -0.0006409960915334523\n",
      "      total_loss: 43.991695404052734\n",
      "      vf_explained_var: 0.9579943418502808\n",
      "      vf_loss: 43.99233627319336\n",
      "    sample_time_ms: 20108.928\n",
      "    update_time_ms: 7.308\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.99681428875317\n",
      "    rl_1: 103.44327730759912\n",
      "  time_since_restore: 6459.665278434753\n",
      "  time_this_iter_s: 22.905644178390503\n",
      "  time_total_s: 6459.665278434753\n",
      "  timestamp: 1550799874\n",
      "  timesteps_since_restore: 2680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2680000\n",
      "  training_iteration: 268\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6459 s, 268 iter, 2680000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-44-57\n",
      "  done: false\n",
      "  episode_len_mean: 88.83185840707965\n",
      "  episode_reward_max: 215.57484100321253\n",
      "  episode_reward_mean: 163.72569418786702\n",
      "  episode_reward_min: -171.68284763048172\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 26375\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.459\n",
      "    load_time_ms: 2.524\n",
      "    num_steps_sampled: 2690000\n",
      "    num_steps_trained: 2690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6792794466018677\n",
      "      kl: 0.017105715349316597\n",
      "      policy_loss: -0.00716448575258255\n",
      "      total_loss: 49.77020263671875\n",
      "      vf_explained_var: 0.9232005476951599\n",
      "      vf_loss: 49.777366638183594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.38966676592826843\n",
      "      kl: 0.007798837497830391\n",
      "      policy_loss: -0.0018321374664083123\n",
      "      total_loss: 74.77828979492188\n",
      "      vf_explained_var: 0.9344679713249207\n",
      "      vf_loss: 74.78012084960938\n",
      "    sample_time_ms: 20146.586\n",
      "    update_time_ms: 7.183\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.47289337841822\n",
      "    rl_1: 100.25280080944877\n",
      "  time_since_restore: 6483.118973970413\n",
      "  time_this_iter_s: 23.45369553565979\n",
      "  time_total_s: 6483.118973970413\n",
      "  timestamp: 1550799897\n",
      "  timesteps_since_restore: 2690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2690000\n",
      "  training_iteration: 269\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6483 s, 269 iter, 2690000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-45-21\n",
      "  done: false\n",
      "  episode_len_mean: 89.55855855855856\n",
      "  episode_reward_max: 213.21614317424687\n",
      "  episode_reward_mean: 167.92653557300753\n",
      "  episode_reward_min: -127.84779385938893\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 26486\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.256\n",
      "    load_time_ms: 2.462\n",
      "    num_steps_sampled: 2700000\n",
      "    num_steps_trained: 2700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6546441316604614\n",
      "      kl: 0.01099341083317995\n",
      "      policy_loss: -0.004004354123026133\n",
      "      total_loss: 59.505069732666016\n",
      "      vf_explained_var: 0.9004074335098267\n",
      "      vf_loss: 59.50905990600586\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.35098811984062195\n",
      "      kl: 0.006865611299872398\n",
      "      policy_loss: -0.0027165613137185574\n",
      "      total_loss: 79.81349182128906\n",
      "      vf_explained_var: 0.9286482334136963\n",
      "      vf_loss: 79.81620788574219\n",
      "    sample_time_ms: 20121.005\n",
      "    update_time_ms: 7.127\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.7117431650884\n",
      "    rl_1: 104.21479240791908\n",
      "  time_since_restore: 6506.4801614284515\n",
      "  time_this_iter_s: 23.36118745803833\n",
      "  time_total_s: 6506.4801614284515\n",
      "  timestamp: 1550799921\n",
      "  timesteps_since_restore: 2700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2700000\n",
      "  training_iteration: 270\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6506 s, 270 iter, 2700000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-45-45\n",
      "  done: false\n",
      "  episode_len_mean: 88.94690265486726\n",
      "  episode_reward_max: 214.34989596723082\n",
      "  episode_reward_mean: 168.7647353165449\n",
      "  episode_reward_min: -175.11546421266593\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 26599\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.736\n",
      "    load_time_ms: 2.584\n",
      "    num_steps_sampled: 2710000\n",
      "    num_steps_trained: 2710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6915955543518066\n",
      "      kl: 0.010022441856563091\n",
      "      policy_loss: -0.004333439748734236\n",
      "      total_loss: 32.8980827331543\n",
      "      vf_explained_var: 0.9419801235198975\n",
      "      vf_loss: 32.90242004394531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3460127115249634\n",
      "      kl: 0.006125330459326506\n",
      "      policy_loss: -0.0017879895167425275\n",
      "      total_loss: 36.760780334472656\n",
      "      vf_explained_var: 0.9651599526405334\n",
      "      vf_loss: 36.76256561279297\n",
      "    sample_time_ms: 20164.525\n",
      "    update_time_ms: 6.815\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.40503963022023\n",
      "    rl_1: 103.3596956863247\n",
      "  time_since_restore: 6530.212242841721\n",
      "  time_this_iter_s: 23.732081413269043\n",
      "  time_total_s: 6530.212242841721\n",
      "  timestamp: 1550799945\n",
      "  timesteps_since_restore: 2710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2710000\n",
      "  training_iteration: 271\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6530 s, 271 iter, 2710000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-46-08\n",
      "  done: false\n",
      "  episode_len_mean: 89.98198198198199\n",
      "  episode_reward_max: 216.77556225658228\n",
      "  episode_reward_mean: 174.74052522980904\n",
      "  episode_reward_min: 120.49620311988697\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 26710\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.847\n",
      "    load_time_ms: 2.652\n",
      "    num_steps_sampled: 2720000\n",
      "    num_steps_trained: 2720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6590675115585327\n",
      "      kl: 0.007399662863463163\n",
      "      policy_loss: -0.0020405363757163286\n",
      "      total_loss: 3.2641444206237793\n",
      "      vf_explained_var: 0.9929159283638\n",
      "      vf_loss: 3.2661845684051514\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3403424620628357\n",
      "      kl: 0.005090511869639158\n",
      "      policy_loss: -0.002170814201235771\n",
      "      total_loss: 3.3242955207824707\n",
      "      vf_explained_var: 0.9966026544570923\n",
      "      vf_loss: 3.3264660835266113\n",
      "    sample_time_ms: 20148.719\n",
      "    update_time_ms: 6.985\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.99409470492806\n",
      "    rl_1: 106.74643052488096\n",
      "  time_since_restore: 6553.228111028671\n",
      "  time_this_iter_s: 23.015868186950684\n",
      "  time_total_s: 6553.228111028671\n",
      "  timestamp: 1550799968\n",
      "  timesteps_since_restore: 2720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2720000\n",
      "  training_iteration: 272\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6553 s, 272 iter, 2720000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-46-31\n",
      "  done: false\n",
      "  episode_len_mean: 89.41071428571429\n",
      "  episode_reward_max: 216.32160129731116\n",
      "  episode_reward_mean: 162.20951030361226\n",
      "  episode_reward_min: -169.72470916200083\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 26822\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.123\n",
      "    load_time_ms: 2.713\n",
      "    num_steps_sampled: 2730000\n",
      "    num_steps_trained: 2730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7504908442497253\n",
      "      kl: 0.018866848200559616\n",
      "      policy_loss: -0.005797405261546373\n",
      "      total_loss: 95.01087188720703\n",
      "      vf_explained_var: 0.8584223985671997\n",
      "      vf_loss: 95.01668548583984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3986327350139618\n",
      "      kl: 0.023021355271339417\n",
      "      policy_loss: -0.004867336247116327\n",
      "      total_loss: 137.81317138671875\n",
      "      vf_explained_var: 0.8863471746444702\n",
      "      vf_loss: 137.8180389404297\n",
      "    sample_time_ms: 20106.903\n",
      "    update_time_ms: 6.843\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.17938096706776\n",
      "    rl_1: 100.03012933654449\n",
      "  time_since_restore: 6576.2422659397125\n",
      "  time_this_iter_s: 23.01415491104126\n",
      "  time_total_s: 6576.2422659397125\n",
      "  timestamp: 1550799991\n",
      "  timesteps_since_restore: 2730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2730000\n",
      "  training_iteration: 273\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6576 s, 273 iter, 2730000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-46-54\n",
      "  done: false\n",
      "  episode_len_mean: 88.24778761061947\n",
      "  episode_reward_max: 220.1867903882124\n",
      "  episode_reward_mean: 151.36652141887458\n",
      "  episode_reward_min: -176.36654991555747\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 26935\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.054\n",
      "    load_time_ms: 2.625\n",
      "    num_steps_sampled: 2740000\n",
      "    num_steps_trained: 2740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7587020397186279\n",
      "      kl: 0.007360955700278282\n",
      "      policy_loss: -0.0036357808858156204\n",
      "      total_loss: 151.4949188232422\n",
      "      vf_explained_var: 0.7727323770523071\n",
      "      vf_loss: 151.49856567382812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.33846983313560486\n",
      "      kl: 0.01410706527531147\n",
      "      policy_loss: -0.0032407110556960106\n",
      "      total_loss: 220.3917999267578\n",
      "      vf_explained_var: 0.8204957246780396\n",
      "      vf_loss: 220.39505004882812\n",
      "    sample_time_ms: 20151.729\n",
      "    update_time_ms: 6.794\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.25910136420063\n",
      "    rl_1: 97.10742005467392\n",
      "  time_since_restore: 6599.851621389389\n",
      "  time_this_iter_s: 23.609355449676514\n",
      "  time_total_s: 6599.851621389389\n",
      "  timestamp: 1550800014\n",
      "  timesteps_since_restore: 2740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2740000\n",
      "  training_iteration: 274\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6599 s, 274 iter, 2740000 ts, 151 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-47-17\n",
      "  done: false\n",
      "  episode_len_mean: 89.08035714285714\n",
      "  episode_reward_max: 216.91030349092202\n",
      "  episode_reward_mean: 173.3921550850399\n",
      "  episode_reward_min: 124.60408327459305\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 27047\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.85\n",
      "    load_time_ms: 2.586\n",
      "    num_steps_sampled: 2750000\n",
      "    num_steps_trained: 2750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6880559325218201\n",
      "      kl: 0.0075601194985210896\n",
      "      policy_loss: -0.004142117220908403\n",
      "      total_loss: 4.289284706115723\n",
      "      vf_explained_var: 0.9911593198776245\n",
      "      vf_loss: 4.293427467346191\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.31815847754478455\n",
      "      kl: 0.00790625624358654\n",
      "      policy_loss: -0.00221357773989439\n",
      "      total_loss: 3.8251800537109375\n",
      "      vf_explained_var: 0.9959000945091248\n",
      "      vf_loss: 3.827392578125\n",
      "    sample_time_ms: 20077.644\n",
      "    update_time_ms: 6.834\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.0345606330824\n",
      "    rl_1: 106.35759445195747\n",
      "  time_since_restore: 6622.529571056366\n",
      "  time_this_iter_s: 22.67794966697693\n",
      "  time_total_s: 6622.529571056366\n",
      "  timestamp: 1550800037\n",
      "  timesteps_since_restore: 2750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2750000\n",
      "  training_iteration: 275\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6622 s, 275 iter, 2750000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-47-40\n",
      "  done: false\n",
      "  episode_len_mean: 90.38738738738739\n",
      "  episode_reward_max: 218.7741954825732\n",
      "  episode_reward_mean: 172.74883163415674\n",
      "  episode_reward_min: 128.05236172329424\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 27158\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.573\n",
      "    load_time_ms: 2.601\n",
      "    num_steps_sampled: 2760000\n",
      "    num_steps_trained: 2760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.726742148399353\n",
      "      kl: 0.007142041809856892\n",
      "      policy_loss: -0.0025923855137079954\n",
      "      total_loss: 3.451401948928833\n",
      "      vf_explained_var: 0.9927359223365784\n",
      "      vf_loss: 3.4539947509765625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32593193650245667\n",
      "      kl: 0.006268000695854425\n",
      "      policy_loss: -0.0025970940478146076\n",
      "      total_loss: 3.5255987644195557\n",
      "      vf_explained_var: 0.9963473081588745\n",
      "      vf_loss: 3.5281951427459717\n",
      "    sample_time_ms: 20074.446\n",
      "    update_time_ms: 6.894\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.45001754224003\n",
      "    rl_1: 107.29881409191673\n",
      "  time_since_restore: 6645.641273975372\n",
      "  time_this_iter_s: 23.111702919006348\n",
      "  time_total_s: 6645.641273975372\n",
      "  timestamp: 1550800060\n",
      "  timesteps_since_restore: 2760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2760000\n",
      "  training_iteration: 276\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6645 s, 276 iter, 2760000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-48-03\n",
      "  done: false\n",
      "  episode_len_mean: 88.73451327433628\n",
      "  episode_reward_max: 211.45687002401436\n",
      "  episode_reward_mean: 167.62150961564313\n",
      "  episode_reward_min: -175.0294749263399\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 27271\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.433\n",
      "    load_time_ms: 2.653\n",
      "    num_steps_sampled: 2770000\n",
      "    num_steps_trained: 2770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7498897910118103\n",
      "      kl: 0.007998698391020298\n",
      "      policy_loss: -0.0036820988170802593\n",
      "      total_loss: 17.784574508666992\n",
      "      vf_explained_var: 0.9672287106513977\n",
      "      vf_loss: 17.78825569152832\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.33920130133628845\n",
      "      kl: 0.00861444417387247\n",
      "      policy_loss: -0.004682661034166813\n",
      "      total_loss: 30.1535701751709\n",
      "      vf_explained_var: 0.9721155166625977\n",
      "      vf_loss: 30.15825653076172\n",
      "    sample_time_ms: 20052.66\n",
      "    update_time_ms: 6.896\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.01467631242019\n",
      "    rl_1: 102.60683330322293\n",
      "  time_since_restore: 6668.815528869629\n",
      "  time_this_iter_s: 23.174254894256592\n",
      "  time_total_s: 6668.815528869629\n",
      "  timestamp: 1550800083\n",
      "  timesteps_since_restore: 2770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2770000\n",
      "  training_iteration: 277\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6668 s, 277 iter, 2770000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-48-28\n",
      "  done: false\n",
      "  episode_len_mean: 89.1875\n",
      "  episode_reward_max: 211.7869924918707\n",
      "  episode_reward_mean: 168.78856724866753\n",
      "  episode_reward_min: -166.28153203525974\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 27383\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.707\n",
      "    load_time_ms: 2.638\n",
      "    num_steps_sampled: 2780000\n",
      "    num_steps_trained: 2780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7410691380500793\n",
      "      kl: 0.006578419357538223\n",
      "      policy_loss: -0.002080501290038228\n",
      "      total_loss: 24.81932258605957\n",
      "      vf_explained_var: 0.9570119976997375\n",
      "      vf_loss: 24.8214054107666\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3354686498641968\n",
      "      kl: 0.011944739148020744\n",
      "      policy_loss: -0.001032253261655569\n",
      "      total_loss: 43.190452575683594\n",
      "      vf_explained_var: 0.9585312008857727\n",
      "      vf_loss: 43.19147872924805\n",
      "    sample_time_ms: 20163.247\n",
      "    update_time_ms: 6.915\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.0384820291774\n",
      "    rl_1: 103.75008521949015\n",
      "  time_since_restore: 6692.841050863266\n",
      "  time_this_iter_s: 24.025521993637085\n",
      "  time_total_s: 6692.841050863266\n",
      "  timestamp: 1550800108\n",
      "  timesteps_since_restore: 2780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2780000\n",
      "  training_iteration: 278\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6692 s, 278 iter, 2780000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-48-51\n",
      "  done: false\n",
      "  episode_len_mean: 85.90517241379311\n",
      "  episode_reward_max: 220.50245994615477\n",
      "  episode_reward_mean: 142.77419578647917\n",
      "  episode_reward_min: -178.59392536257425\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 27499\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.893\n",
      "    load_time_ms: 2.544\n",
      "    num_steps_sampled: 2790000\n",
      "    num_steps_trained: 2790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7380551695823669\n",
      "      kl: 0.014498219825327396\n",
      "      policy_loss: -0.00520507013425231\n",
      "      total_loss: 255.16061401367188\n",
      "      vf_explained_var: 0.677237868309021\n",
      "      vf_loss: 255.165771484375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.36269283294677734\n",
      "      kl: 0.015312890522181988\n",
      "      policy_loss: -0.006087931804358959\n",
      "      total_loss: 343.3226623535156\n",
      "      vf_explained_var: 0.7436116933822632\n",
      "      vf_loss: 343.32879638671875\n",
      "    sample_time_ms: 20164.074\n",
      "    update_time_ms: 7.251\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.006991104885245\n",
      "    rl_1: 88.76720468159392\n",
      "  time_since_restore: 6716.45606637001\n",
      "  time_this_iter_s: 23.615015506744385\n",
      "  time_total_s: 6716.45606637001\n",
      "  timestamp: 1550800131\n",
      "  timesteps_since_restore: 2790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2790000\n",
      "  training_iteration: 279\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6716 s, 279 iter, 2790000 ts, 143 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-49-14\n",
      "  done: false\n",
      "  episode_len_mean: 88.14035087719299\n",
      "  episode_reward_max: 214.82773182609589\n",
      "  episode_reward_mean: 161.72545037367078\n",
      "  episode_reward_min: -176.54230870552857\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 27613\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.021\n",
      "    load_time_ms: 2.507\n",
      "    num_steps_sampled: 2800000\n",
      "    num_steps_trained: 2800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6652410626411438\n",
      "      kl: 0.010341600514948368\n",
      "      policy_loss: -0.0038652473594993353\n",
      "      total_loss: 94.47032928466797\n",
      "      vf_explained_var: 0.8612826466560364\n",
      "      vf_loss: 94.47419738769531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2896846830844879\n",
      "      kl: 0.0075054229237139225\n",
      "      policy_loss: -0.002369213616475463\n",
      "      total_loss: 133.12335205078125\n",
      "      vf_explained_var: 0.8838949799537659\n",
      "      vf_loss: 133.125732421875\n",
      "    sample_time_ms: 20165.852\n",
      "    update_time_ms: 7.373\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.540257650957734\n",
      "    rl_1: 100.18519272271303\n",
      "  time_since_restore: 6739.645834207535\n",
      "  time_this_iter_s: 23.189767837524414\n",
      "  time_total_s: 6739.645834207535\n",
      "  timestamp: 1550800154\n",
      "  timesteps_since_restore: 2800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2800000\n",
      "  training_iteration: 280\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6739 s, 280 iter, 2800000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-49-37\n",
      "  done: false\n",
      "  episode_len_mean: 89.48214285714286\n",
      "  episode_reward_max: 216.68526902643228\n",
      "  episode_reward_mean: 171.92775104719172\n",
      "  episode_reward_min: 125.12411429216311\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 27725\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3119.579\n",
      "    load_time_ms: 2.379\n",
      "    num_steps_sampled: 2810000\n",
      "    num_steps_trained: 2810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6842132210731506\n",
      "      kl: 0.0068176803179085255\n",
      "      policy_loss: -0.0026453484315425158\n",
      "      total_loss: 4.09379243850708\n",
      "      vf_explained_var: 0.9909551739692688\n",
      "      vf_loss: 4.096437931060791\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.31879329681396484\n",
      "      kl: 0.005764430854469538\n",
      "      policy_loss: -0.00134604936465621\n",
      "      total_loss: 4.608966827392578\n",
      "      vf_explained_var: 0.9950937032699585\n",
      "      vf_loss: 4.610313415527344\n",
      "    sample_time_ms: 20073.768\n",
      "    update_time_ms: 7.352\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.58689810774631\n",
      "    rl_1: 105.34085293944541\n",
      "  time_since_restore: 6762.429893255234\n",
      "  time_this_iter_s: 22.784059047698975\n",
      "  time_total_s: 6762.429893255234\n",
      "  timestamp: 1550800177\n",
      "  timesteps_since_restore: 2810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2810000\n",
      "  training_iteration: 281\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6762 s, 281 iter, 2810000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-50-01\n",
      "  done: false\n",
      "  episode_len_mean: 88.09734513274336\n",
      "  episode_reward_max: 215.04591029030465\n",
      "  episode_reward_mean: 160.42428587374286\n",
      "  episode_reward_min: -181.1057070184407\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 27838\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.301\n",
      "    load_time_ms: 2.284\n",
      "    num_steps_sampled: 2820000\n",
      "    num_steps_trained: 2820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6879375576972961\n",
      "      kl: 0.00872547272592783\n",
      "      policy_loss: -0.002119053853675723\n",
      "      total_loss: 73.3531494140625\n",
      "      vf_explained_var: 0.8722885251045227\n",
      "      vf_loss: 73.35527038574219\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3222174644470215\n",
      "      kl: 0.005429544951766729\n",
      "      policy_loss: -0.0005764670786447823\n",
      "      total_loss: 101.51882934570312\n",
      "      vf_explained_var: 0.9069964289665222\n",
      "      vf_loss: 101.5194091796875\n",
      "    sample_time_ms: 20087.935\n",
      "    update_time_ms: 7.23\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.174763062604875\n",
      "    rl_1: 99.249522811138\n",
      "  time_since_restore: 6785.629692316055\n",
      "  time_this_iter_s: 23.199799060821533\n",
      "  time_total_s: 6785.629692316055\n",
      "  timestamp: 1550800201\n",
      "  timesteps_since_restore: 2820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2820000\n",
      "  training_iteration: 282\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6785 s, 282 iter, 2820000 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-50-24\n",
      "  done: false\n",
      "  episode_len_mean: 89.34821428571429\n",
      "  episode_reward_max: 219.35887152728745\n",
      "  episode_reward_mean: 167.9015100370539\n",
      "  episode_reward_min: -130.9674767305517\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 27950\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.666\n",
      "    load_time_ms: 2.193\n",
      "    num_steps_sampled: 2830000\n",
      "    num_steps_trained: 2830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7273743152618408\n",
      "      kl: 0.0148716289550066\n",
      "      policy_loss: -0.0049616494216024876\n",
      "      total_loss: 33.40460968017578\n",
      "      vf_explained_var: 0.932819664478302\n",
      "      vf_loss: 33.40958023071289\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.37787294387817383\n",
      "      kl: 0.011342977173626423\n",
      "      policy_loss: -0.00511552020907402\n",
      "      total_loss: 44.68686294555664\n",
      "      vf_explained_var: 0.9559513926506042\n",
      "      vf_loss: 44.69197463989258\n",
      "    sample_time_ms: 20136.13\n",
      "    update_time_ms: 7.13\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.86085528478779\n",
      "    rl_1: 103.0406547522661\n",
      "  time_since_restore: 6809.1259179115295\n",
      "  time_this_iter_s: 23.496225595474243\n",
      "  time_total_s: 6809.1259179115295\n",
      "  timestamp: 1550800224\n",
      "  timesteps_since_restore: 2830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2830000\n",
      "  training_iteration: 283\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6809 s, 283 iter, 2830000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-50-48\n",
      "  done: false\n",
      "  episode_len_mean: 89.90990990990991\n",
      "  episode_reward_max: 216.98470839037842\n",
      "  episode_reward_mean: 172.968042251291\n",
      "  episode_reward_min: 124.18981565042924\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 28061\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.789\n",
      "    load_time_ms: 2.248\n",
      "    num_steps_sampled: 2840000\n",
      "    num_steps_trained: 2840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6896933317184448\n",
      "      kl: 0.00805776659399271\n",
      "      policy_loss: -0.0031900028698146343\n",
      "      total_loss: 4.041816711425781\n",
      "      vf_explained_var: 0.9911876320838928\n",
      "      vf_loss: 4.045007228851318\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32806381583213806\n",
      "      kl: 0.007588277105242014\n",
      "      policy_loss: -0.0016563424142077565\n",
      "      total_loss: 3.9175219535827637\n",
      "      vf_explained_var: 0.9959694147109985\n",
      "      vf_loss: 3.919178009033203\n",
      "    sample_time_ms: 20162.496\n",
      "    update_time_ms: 7.427\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.98781674049346\n",
      "    rl_1: 106.98022551079755\n",
      "  time_since_restore: 6833.025167942047\n",
      "  time_this_iter_s: 23.899250030517578\n",
      "  time_total_s: 6833.025167942047\n",
      "  timestamp: 1550800248\n",
      "  timesteps_since_restore: 2840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2840000\n",
      "  training_iteration: 284\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6833 s, 284 iter, 2840000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-51-11\n",
      "  done: false\n",
      "  episode_len_mean: 88.04385964912281\n",
      "  episode_reward_max: 214.59446876793805\n",
      "  episode_reward_mean: 161.9523342220224\n",
      "  episode_reward_min: -166.5295077457677\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 28175\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.323\n",
      "    load_time_ms: 2.242\n",
      "    num_steps_sampled: 2850000\n",
      "    num_steps_trained: 2850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7255606055259705\n",
      "      kl: 0.008680031634867191\n",
      "      policy_loss: -0.00446805777028203\n",
      "      total_loss: 73.91722106933594\n",
      "      vf_explained_var: 0.8812211751937866\n",
      "      vf_loss: 73.92169952392578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3430840075016022\n",
      "      kl: 0.008742748759686947\n",
      "      policy_loss: -0.0005595347611233592\n",
      "      total_loss: 113.57530975341797\n",
      "      vf_explained_var: 0.8977534174919128\n",
      "      vf_loss: 113.57587432861328\n",
      "    sample_time_ms: 20219.284\n",
      "    update_time_ms: 7.43\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.97370236145325\n",
      "    rl_1: 98.97863186056914\n",
      "  time_since_restore: 6856.263030529022\n",
      "  time_this_iter_s: 23.237862586975098\n",
      "  time_total_s: 6856.263030529022\n",
      "  timestamp: 1550800271\n",
      "  timesteps_since_restore: 2850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2850000\n",
      "  training_iteration: 285\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6856 s, 285 iter, 2850000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-51-34\n",
      "  done: false\n",
      "  episode_len_mean: 88.97321428571429\n",
      "  episode_reward_max: 218.11616743313002\n",
      "  episode_reward_mean: 160.9740516174092\n",
      "  episode_reward_min: -169.69496886540978\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 28287\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.607\n",
      "    load_time_ms: 2.24\n",
      "    num_steps_sampled: 2860000\n",
      "    num_steps_trained: 2860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7793731689453125\n",
      "      kl: 0.01805812679231167\n",
      "      policy_loss: -0.005937796086072922\n",
      "      total_loss: 86.91384887695312\n",
      "      vf_explained_var: 0.8629089593887329\n",
      "      vf_loss: 86.91978454589844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.33787813782691956\n",
      "      kl: 0.007577643264085054\n",
      "      policy_loss: -0.0018036055844277143\n",
      "      total_loss: 126.40242004394531\n",
      "      vf_explained_var: 0.8899623155593872\n",
      "      vf_loss: 126.40421295166016\n",
      "    sample_time_ms: 20218.474\n",
      "    update_time_ms: 7.22\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.2556132820212\n",
      "    rl_1: 99.71843833538799\n",
      "  time_since_restore: 6879.363538503647\n",
      "  time_this_iter_s: 23.100507974624634\n",
      "  time_total_s: 6879.363538503647\n",
      "  timestamp: 1550800294\n",
      "  timesteps_since_restore: 2860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2860000\n",
      "  training_iteration: 286\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6879 s, 286 iter, 2860000 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-51-57\n",
      "  done: false\n",
      "  episode_len_mean: 89.75\n",
      "  episode_reward_max: 215.51279057869792\n",
      "  episode_reward_mean: 163.86406470912473\n",
      "  episode_reward_min: -163.98723276757968\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 28399\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.427\n",
      "    load_time_ms: 2.255\n",
      "    num_steps_sampled: 2870000\n",
      "    num_steps_trained: 2870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8015059232711792\n",
      "      kl: 0.005570967216044664\n",
      "      policy_loss: -0.0014995632227510214\n",
      "      total_loss: 42.23609161376953\n",
      "      vf_explained_var: 0.9319305419921875\n",
      "      vf_loss: 42.237586975097656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.34900563955307007\n",
      "      kl: 0.0054632555693387985\n",
      "      policy_loss: -0.0009119054884649813\n",
      "      total_loss: 74.39434814453125\n",
      "      vf_explained_var: 0.9295855760574341\n",
      "      vf_loss: 74.395263671875\n",
      "    sample_time_ms: 20150.695\n",
      "    update_time_ms: 7.211\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.30893290239222\n",
      "    rl_1: 102.55513180673246\n",
      "  time_since_restore: 6901.858241081238\n",
      "  time_this_iter_s: 22.494702577590942\n",
      "  time_total_s: 6901.858241081238\n",
      "  timestamp: 1550800317\n",
      "  timesteps_since_restore: 2870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2870000\n",
      "  training_iteration: 287\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6901 s, 287 iter, 2870000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-52-20\n",
      "  done: false\n",
      "  episode_len_mean: 90.71559633027523\n",
      "  episode_reward_max: 219.61178676067902\n",
      "  episode_reward_mean: 170.2944847884287\n",
      "  episode_reward_min: 120.7257194196742\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 28508\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.65\n",
      "    load_time_ms: 2.259\n",
      "    num_steps_sampled: 2880000\n",
      "    num_steps_trained: 2880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7755820751190186\n",
      "      kl: 0.008814724162220955\n",
      "      policy_loss: -0.0045327078551054\n",
      "      total_loss: 4.65317964553833\n",
      "      vf_explained_var: 0.9894262552261353\n",
      "      vf_loss: 4.657711505889893\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3372848927974701\n",
      "      kl: 0.005335446447134018\n",
      "      policy_loss: 0.0004472687724046409\n",
      "      total_loss: 4.868636131286621\n",
      "      vf_explained_var: 0.99488765001297\n",
      "      vf_loss: 4.868188858032227\n",
      "    sample_time_ms: 20071.076\n",
      "    update_time_ms: 7.281\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.13010344610464\n",
      "    rl_1: 106.16438134232398\n",
      "  time_since_restore: 6925.275895118713\n",
      "  time_this_iter_s: 23.417654037475586\n",
      "  time_total_s: 6925.275895118713\n",
      "  timestamp: 1550800340\n",
      "  timesteps_since_restore: 2880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2880000\n",
      "  training_iteration: 288\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6925 s, 288 iter, 2880000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-52-44\n",
      "  done: false\n",
      "  episode_len_mean: 89.65178571428571\n",
      "  episode_reward_max: 218.6005282575809\n",
      "  episode_reward_mean: 168.18898374535203\n",
      "  episode_reward_min: -164.70777934761497\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 28620\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.668\n",
      "    load_time_ms: 2.312\n",
      "    num_steps_sampled: 2890000\n",
      "    num_steps_trained: 2890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7540971040725708\n",
      "      kl: 0.008875264786183834\n",
      "      policy_loss: -0.0043465555645525455\n",
      "      total_loss: 56.08545684814453\n",
      "      vf_explained_var: 0.9011728167533875\n",
      "      vf_loss: 56.089805603027344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3337884843349457\n",
      "      kl: 0.006409534253180027\n",
      "      policy_loss: -0.0023069477174431086\n",
      "      total_loss: 82.8514175415039\n",
      "      vf_explained_var: 0.9250079393386841\n",
      "      vf_loss: 82.85372924804688\n",
      "    sample_time_ms: 20066.3\n",
      "    update_time_ms: 6.891\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.923352664714\n",
      "    rl_1: 103.26563108063799\n",
      "  time_since_restore: 6948.684503316879\n",
      "  time_this_iter_s: 23.408608198165894\n",
      "  time_total_s: 6948.684503316879\n",
      "  timestamp: 1550800364\n",
      "  timesteps_since_restore: 2890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2890000\n",
      "  training_iteration: 289\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6948 s, 289 iter, 2890000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-53-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.10810810810811\n",
      "  episode_reward_max: 218.5015231050061\n",
      "  episode_reward_mean: 169.36807588678127\n",
      "  episode_reward_min: 125.72170862525151\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 28731\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.18\n",
      "    load_time_ms: 2.4\n",
      "    num_steps_sampled: 2900000\n",
      "    num_steps_trained: 2900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7984143495559692\n",
      "      kl: 0.007247145287692547\n",
      "      policy_loss: -0.004240608308464289\n",
      "      total_loss: 3.574680805206299\n",
      "      vf_explained_var: 0.9917031526565552\n",
      "      vf_loss: 3.5789215564727783\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3543158173561096\n",
      "      kl: 0.006735155358910561\n",
      "      policy_loss: -0.0011835851473733783\n",
      "      total_loss: 3.4934120178222656\n",
      "      vf_explained_var: 0.996247410774231\n",
      "      vf_loss: 3.494595766067505\n",
      "    sample_time_ms: 20118.233\n",
      "    update_time_ms: 6.811\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.39417921860573\n",
      "    rl_1: 103.97389666817544\n",
      "  time_since_restore: 6972.409008979797\n",
      "  time_this_iter_s: 23.72450566291809\n",
      "  time_total_s: 6972.409008979797\n",
      "  timestamp: 1550800388\n",
      "  timesteps_since_restore: 2900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2900000\n",
      "  training_iteration: 290\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6972 s, 290 iter, 2900000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-53-31\n",
      "  done: false\n",
      "  episode_len_mean: 90.09009009009009\n",
      "  episode_reward_max: 215.08623390638022\n",
      "  episode_reward_mean: 170.71481950643587\n",
      "  episode_reward_min: 127.03717376673801\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 28842\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.803\n",
      "    load_time_ms: 2.404\n",
      "    num_steps_sampled: 2910000\n",
      "    num_steps_trained: 2910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7781485319137573\n",
      "      kl: 0.008282728493213654\n",
      "      policy_loss: -0.004611728712916374\n",
      "      total_loss: 3.1011931896209717\n",
      "      vf_explained_var: 0.9930015802383423\n",
      "      vf_loss: 3.105804681777954\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3487280607223511\n",
      "      kl: 0.00423125084489584\n",
      "      policy_loss: -0.0013475731248036027\n",
      "      total_loss: 3.3850464820861816\n",
      "      vf_explained_var: 0.9963886737823486\n",
      "      vf_loss: 3.386394500732422\n",
      "    sample_time_ms: 20187.839\n",
      "    update_time_ms: 6.825\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.19772502562871\n",
      "    rl_1: 104.51709448080715\n",
      "  time_since_restore: 6995.904583930969\n",
      "  time_this_iter_s: 23.495574951171875\n",
      "  time_total_s: 6995.904583930969\n",
      "  timestamp: 1550800411\n",
      "  timesteps_since_restore: 2910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2910000\n",
      "  training_iteration: 291\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 6995 s, 291 iter, 2910000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-53-54\n",
      "  done: false\n",
      "  episode_len_mean: 89.67857142857143\n",
      "  episode_reward_max: 213.5489284691738\n",
      "  episode_reward_mean: 169.02971230847973\n",
      "  episode_reward_min: -120.01252964138628\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 28954\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.36\n",
      "    load_time_ms: 2.432\n",
      "    num_steps_sampled: 2920000\n",
      "    num_steps_trained: 2920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7748653888702393\n",
      "      kl: 0.011422840878367424\n",
      "      policy_loss: -0.0034409428481012583\n",
      "      total_loss: 31.88663673400879\n",
      "      vf_explained_var: 0.9403179883956909\n",
      "      vf_loss: 31.890077590942383\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3258860111236572\n",
      "      kl: 0.01638679951429367\n",
      "      policy_loss: -0.005663861986249685\n",
      "      total_loss: 40.200626373291016\n",
      "      vf_explained_var: 0.9615859985351562\n",
      "      vf_loss: 40.20629119873047\n",
      "    sample_time_ms: 20190.179\n",
      "    update_time_ms: 6.92\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.5105063398949\n",
      "    rl_1: 103.51920596858486\n",
      "  time_since_restore: 7019.125749349594\n",
      "  time_this_iter_s: 23.221165418624878\n",
      "  time_total_s: 7019.125749349594\n",
      "  timestamp: 1550800434\n",
      "  timesteps_since_restore: 2920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2920000\n",
      "  training_iteration: 292\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7019 s, 292 iter, 2920000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-54-18\n",
      "  done: false\n",
      "  episode_len_mean: 87.55263157894737\n",
      "  episode_reward_max: 215.14709584582832\n",
      "  episode_reward_mean: 153.35122139369557\n",
      "  episode_reward_min: -177.13740578603202\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 29068\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.427\n",
      "    load_time_ms: 2.387\n",
      "    num_steps_sampled: 2930000\n",
      "    num_steps_trained: 2930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7831651568412781\n",
      "      kl: 0.0196317657828331\n",
      "      policy_loss: -0.007484667003154755\n",
      "      total_loss: 164.68325805664062\n",
      "      vf_explained_var: 0.7399466037750244\n",
      "      vf_loss: 164.69073486328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2891046702861786\n",
      "      kl: 0.007879097945988178\n",
      "      policy_loss: -0.0017354971496388316\n",
      "      total_loss: 200.9245147705078\n",
      "      vf_explained_var: 0.8330410718917847\n",
      "      vf_loss: 200.92625427246094\n",
      "    sample_time_ms: 20176.377\n",
      "    update_time_ms: 7.142\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.28214215710794\n",
      "    rl_1: 95.06907923658758\n",
      "  time_since_restore: 7042.479584932327\n",
      "  time_this_iter_s: 23.353835582733154\n",
      "  time_total_s: 7042.479584932327\n",
      "  timestamp: 1550800458\n",
      "  timesteps_since_restore: 2930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2930000\n",
      "  training_iteration: 293\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7042 s, 293 iter, 2930000 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-54-42\n",
      "  done: false\n",
      "  episode_len_mean: 90.83636363636364\n",
      "  episode_reward_max: 217.76179774907962\n",
      "  episode_reward_mean: 172.9979710444513\n",
      "  episode_reward_min: -38.689046126723525\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 29178\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.415\n",
      "    load_time_ms: 2.421\n",
      "    num_steps_sampled: 2940000\n",
      "    num_steps_trained: 2940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7703898549079895\n",
      "      kl: 0.0022179388906806707\n",
      "      policy_loss: -0.0015545348869636655\n",
      "      total_loss: 9.061652183532715\n",
      "      vf_explained_var: 0.9850521683692932\n",
      "      vf_loss: 9.063207626342773\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2973189353942871\n",
      "      kl: 0.003907824866473675\n",
      "      policy_loss: -0.0004070524882990867\n",
      "      total_loss: 11.600587844848633\n",
      "      vf_explained_var: 0.990351676940918\n",
      "      vf_loss: 11.600996017456055\n",
      "    sample_time_ms: 20139.331\n",
      "    update_time_ms: 7.043\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.64010869368475\n",
      "    rl_1: 106.35786235076655\n",
      "  time_since_restore: 7066.069610834122\n",
      "  time_this_iter_s: 23.590025901794434\n",
      "  time_total_s: 7066.069610834122\n",
      "  timestamp: 1550800482\n",
      "  timesteps_since_restore: 2940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2940000\n",
      "  training_iteration: 294\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7066 s, 294 iter, 2940000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-55-05\n",
      "  done: false\n",
      "  episode_len_mean: 90.41441441441441\n",
      "  episode_reward_max: 217.23479857419653\n",
      "  episode_reward_mean: 169.0478029887739\n",
      "  episode_reward_min: 125.49611511699567\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 29289\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.312\n",
      "    load_time_ms: 2.417\n",
      "    num_steps_sampled: 2950000\n",
      "    num_steps_trained: 2950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8017580509185791\n",
      "      kl: 0.01056694146245718\n",
      "      policy_loss: -0.004478799179196358\n",
      "      total_loss: 2.743974208831787\n",
      "      vf_explained_var: 0.9935659766197205\n",
      "      vf_loss: 2.74845290184021\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.34775567054748535\n",
      "      kl: 0.0069970618933439255\n",
      "      policy_loss: -0.0007630388718098402\n",
      "      total_loss: 3.316399574279785\n",
      "      vf_explained_var: 0.9964348077774048\n",
      "      vf_loss: 3.31716251373291\n",
      "    sample_time_ms: 20153.569\n",
      "    update_time_ms: 7.28\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.17745354325075\n",
      "    rl_1: 103.87034944552312\n",
      "  time_since_restore: 7089.44127702713\n",
      "  time_this_iter_s: 23.371666193008423\n",
      "  time_total_s: 7089.44127702713\n",
      "  timestamp: 1550800505\n",
      "  timesteps_since_restore: 2950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2950000\n",
      "  training_iteration: 295\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7089 s, 295 iter, 2950000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-55-28\n",
      "  done: false\n",
      "  episode_len_mean: 90.9\n",
      "  episode_reward_max: 218.34492711692442\n",
      "  episode_reward_mean: 170.76951092209356\n",
      "  episode_reward_min: -150.47649174406092\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 29399\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.355\n",
      "    load_time_ms: 2.442\n",
      "    num_steps_sampled: 2960000\n",
      "    num_steps_trained: 2960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8032004237174988\n",
      "      kl: 0.00742166256532073\n",
      "      policy_loss: -0.005354807712137699\n",
      "      total_loss: 29.6828670501709\n",
      "      vf_explained_var: 0.9507997035980225\n",
      "      vf_loss: 29.68821907043457\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2907390296459198\n",
      "      kl: 0.020745180547237396\n",
      "      policy_loss: -0.004258021246641874\n",
      "      total_loss: 51.34745407104492\n",
      "      vf_explained_var: 0.9552237391471863\n",
      "      vf_loss: 51.35171127319336\n",
      "    sample_time_ms: 20170.304\n",
      "    update_time_ms: 7.324\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.2811344424321\n",
      "    rl_1: 104.48837647966147\n",
      "  time_since_restore: 7112.721688508987\n",
      "  time_this_iter_s: 23.2804114818573\n",
      "  time_total_s: 7112.721688508987\n",
      "  timestamp: 1550800528\n",
      "  timesteps_since_restore: 2960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2960000\n",
      "  training_iteration: 296\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7112 s, 296 iter, 2960000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-55-52\n",
      "  done: false\n",
      "  episode_len_mean: 88.88392857142857\n",
      "  episode_reward_max: 220.77375205364328\n",
      "  episode_reward_mean: 163.46087957299738\n",
      "  episode_reward_min: -176.81981534379506\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 29511\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3159.54\n",
      "    load_time_ms: 2.407\n",
      "    num_steps_sampled: 2970000\n",
      "    num_steps_trained: 2970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.797096848487854\n",
      "      kl: 0.013575255870819092\n",
      "      policy_loss: -0.0065485695376992226\n",
      "      total_loss: 45.11405944824219\n",
      "      vf_explained_var: 0.9139481782913208\n",
      "      vf_loss: 45.120609283447266\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.31043609976768494\n",
      "      kl: 0.030539222061634064\n",
      "      policy_loss: -0.005201213993132114\n",
      "      total_loss: 55.664981842041016\n",
      "      vf_explained_var: 0.9471381902694702\n",
      "      vf_loss: 55.67019271850586\n",
      "    sample_time_ms: 20230.666\n",
      "    update_time_ms: 7.26\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.55593324165373\n",
      "    rl_1: 99.90494633134368\n",
      "  time_since_restore: 7136.0426387786865\n",
      "  time_this_iter_s: 23.320950269699097\n",
      "  time_total_s: 7136.0426387786865\n",
      "  timestamp: 1550800552\n",
      "  timesteps_since_restore: 2970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2970000\n",
      "  training_iteration: 297\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7136 s, 297 iter, 2970000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-56-15\n",
      "  done: false\n",
      "  episode_len_mean: 89.82142857142857\n",
      "  episode_reward_max: 214.1529674188621\n",
      "  episode_reward_mean: 172.7639171638659\n",
      "  episode_reward_min: -129.37805166549077\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 29623\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.411\n",
      "    load_time_ms: 2.421\n",
      "    num_steps_sampled: 2980000\n",
      "    num_steps_trained: 2980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8372505307197571\n",
      "      kl: 0.01077411137521267\n",
      "      policy_loss: -0.004460988566279411\n",
      "      total_loss: 34.041812896728516\n",
      "      vf_explained_var: 0.9406190514564514\n",
      "      vf_loss: 34.0462760925293\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.34195375442504883\n",
      "      kl: 0.011417895555496216\n",
      "      policy_loss: -0.004610975738614798\n",
      "      total_loss: 43.24589157104492\n",
      "      vf_explained_var: 0.9583019614219666\n",
      "      vf_loss: 43.25050735473633\n",
      "    sample_time_ms: 20193.28\n",
      "    update_time_ms: 7.41\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.240542592234\n",
      "    rl_1: 105.52337457163189\n",
      "  time_since_restore: 7158.900142908096\n",
      "  time_this_iter_s: 22.85750412940979\n",
      "  time_total_s: 7158.900142908096\n",
      "  timestamp: 1550800575\n",
      "  timesteps_since_restore: 2980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2980000\n",
      "  training_iteration: 298\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7158 s, 298 iter, 2980000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-56-39\n",
      "  done: false\n",
      "  episode_len_mean: 88.97321428571429\n",
      "  episode_reward_max: 218.57352857898567\n",
      "  episode_reward_mean: 166.12744799436248\n",
      "  episode_reward_min: -171.38177188443285\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 29735\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.565\n",
      "    load_time_ms: 2.365\n",
      "    num_steps_sampled: 2990000\n",
      "    num_steps_trained: 2990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8259228467941284\n",
      "      kl: 0.009408323094248772\n",
      "      policy_loss: -0.0035663163289427757\n",
      "      total_loss: 47.504669189453125\n",
      "      vf_explained_var: 0.9160345196723938\n",
      "      vf_loss: 47.50823974609375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3448077440261841\n",
      "      kl: 0.00917157344520092\n",
      "      policy_loss: -0.0004485110112000257\n",
      "      total_loss: 74.3489990234375\n",
      "      vf_explained_var: 0.9304829239845276\n",
      "      vf_loss: 74.34944915771484\n",
      "    sample_time_ms: 20243.512\n",
      "    update_time_ms: 7.831\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.47602003187556\n",
      "    rl_1: 101.6514279624869\n",
      "  time_since_restore: 7182.883994340897\n",
      "  time_this_iter_s: 23.983851432800293\n",
      "  time_total_s: 7182.883994340897\n",
      "  timestamp: 1550800599\n",
      "  timesteps_since_restore: 2990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 2990000\n",
      "  training_iteration: 299\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7182 s, 299 iter, 2990000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-57-02\n",
      "  done: false\n",
      "  episode_len_mean: 89.5625\n",
      "  episode_reward_max: 214.07123570826852\n",
      "  episode_reward_mean: 167.2805968719251\n",
      "  episode_reward_min: -161.61709725860595\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 29847\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.566\n",
      "    load_time_ms: 2.279\n",
      "    num_steps_sampled: 3000000\n",
      "    num_steps_trained: 3000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8271234035491943\n",
      "      kl: 0.008794151246547699\n",
      "      policy_loss: -0.006158644799143076\n",
      "      total_loss: 7.208885192871094\n",
      "      vf_explained_var: 0.9849072098731995\n",
      "      vf_loss: 7.21504545211792\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3562794029712677\n",
      "      kl: 0.005854818969964981\n",
      "      policy_loss: 0.0007866614614613354\n",
      "      total_loss: 8.333908081054688\n",
      "      vf_explained_var: 0.9916372895240784\n",
      "      vf_loss: 8.333122253417969\n",
      "    sample_time_ms: 20245.81\n",
      "    update_time_ms: 7.788\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.11230688331112\n",
      "    rl_1: 102.16828998861395\n",
      "  time_since_restore: 7206.6289348602295\n",
      "  time_this_iter_s: 23.744940519332886\n",
      "  time_total_s: 7206.6289348602295\n",
      "  timestamp: 1550800622\n",
      "  timesteps_since_restore: 3000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3000000\n",
      "  training_iteration: 300\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7206 s, 300 iter, 3000000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-57-25\n",
      "  done: false\n",
      "  episode_len_mean: 90.86238532110092\n",
      "  episode_reward_max: 213.6341415647367\n",
      "  episode_reward_mean: 172.1332969469354\n",
      "  episode_reward_min: 124.92178295893031\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 29956\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.82\n",
      "    load_time_ms: 2.337\n",
      "    num_steps_sampled: 3010000\n",
      "    num_steps_trained: 3010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8380545377731323\n",
      "      kl: 0.009847764857113361\n",
      "      policy_loss: -0.003987302538007498\n",
      "      total_loss: 4.016917705535889\n",
      "      vf_explained_var: 0.9911262392997742\n",
      "      vf_loss: 4.020905017852783\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.38063573837280273\n",
      "      kl: 0.00670747272670269\n",
      "      policy_loss: -0.0017163425218313932\n",
      "      total_loss: 3.017536163330078\n",
      "      vf_explained_var: 0.9968446493148804\n",
      "      vf_loss: 3.0192527770996094\n",
      "    sample_time_ms: 20178.62\n",
      "    update_time_ms: 7.955\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.47841462890285\n",
      "    rl_1: 105.65488231803252\n",
      "  time_since_restore: 7229.447024106979\n",
      "  time_this_iter_s: 22.818089246749878\n",
      "  time_total_s: 7229.447024106979\n",
      "  timestamp: 1550800645\n",
      "  timesteps_since_restore: 3010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3010000\n",
      "  training_iteration: 301\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7229 s, 301 iter, 3010000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-57-48\n",
      "  done: false\n",
      "  episode_len_mean: 91.32727272727273\n",
      "  episode_reward_max: 217.60639107353794\n",
      "  episode_reward_mean: 164.74979617627588\n",
      "  episode_reward_min: -163.89577797384285\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 30066\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.635\n",
      "    load_time_ms: 2.271\n",
      "    num_steps_sampled: 3020000\n",
      "    num_steps_trained: 3020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8453724980354309\n",
      "      kl: 0.0071238246746361256\n",
      "      policy_loss: -0.001378783956170082\n",
      "      total_loss: 31.115825653076172\n",
      "      vf_explained_var: 0.9425665140151978\n",
      "      vf_loss: 31.11720848083496\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.27547311782836914\n",
      "      kl: 0.02216600999236107\n",
      "      policy_loss: -0.0025927762035280466\n",
      "      total_loss: 40.62936782836914\n",
      "      vf_explained_var: 0.9621835350990295\n",
      "      vf_loss: 40.631961822509766\n",
      "    sample_time_ms: 20179.38\n",
      "    update_time_ms: 7.954\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.705565448027976\n",
      "    rl_1: 104.04423072824787\n",
      "  time_since_restore: 7252.650674581528\n",
      "  time_this_iter_s: 23.20365047454834\n",
      "  time_total_s: 7252.650674581528\n",
      "  timestamp: 1550800668\n",
      "  timesteps_since_restore: 3020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3020000\n",
      "  training_iteration: 302\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7252 s, 302 iter, 3020000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-58-12\n",
      "  done: false\n",
      "  episode_len_mean: 89.91071428571429\n",
      "  episode_reward_max: 215.90146291857022\n",
      "  episode_reward_mean: 163.1662802689404\n",
      "  episode_reward_min: -165.4669896357229\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 30178\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.324\n",
      "    load_time_ms: 2.367\n",
      "    num_steps_sampled: 3030000\n",
      "    num_steps_trained: 3030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8159416913986206\n",
      "      kl: 0.007795685436576605\n",
      "      policy_loss: -0.0034529452677816153\n",
      "      total_loss: 48.18473815917969\n",
      "      vf_explained_var: 0.9174121022224426\n",
      "      vf_loss: 48.18819046020508\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24974015355110168\n",
      "      kl: 0.00884478259831667\n",
      "      policy_loss: -0.0021508510690182447\n",
      "      total_loss: 61.73094940185547\n",
      "      vf_explained_var: 0.9438766241073608\n",
      "      vf_loss: 61.733097076416016\n",
      "    sample_time_ms: 20178.192\n",
      "    update_time_ms: 7.657\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.73391316684816\n",
      "    rl_1: 100.43236710209224\n",
      "  time_since_restore: 7276.005833864212\n",
      "  time_this_iter_s: 23.355159282684326\n",
      "  time_total_s: 7276.005833864212\n",
      "  timestamp: 1550800692\n",
      "  timesteps_since_restore: 3030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3030000\n",
      "  training_iteration: 303\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7276 s, 303 iter, 3030000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-58-35\n",
      "  done: false\n",
      "  episode_len_mean: 88.23214285714286\n",
      "  episode_reward_max: 217.7390234661231\n",
      "  episode_reward_mean: 158.2093466246556\n",
      "  episode_reward_min: -168.4161677744707\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 30290\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.38\n",
      "    load_time_ms: 2.346\n",
      "    num_steps_sampled: 3040000\n",
      "    num_steps_trained: 3040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8473609685897827\n",
      "      kl: 0.019345808774232864\n",
      "      policy_loss: -0.007017519325017929\n",
      "      total_loss: 133.79469299316406\n",
      "      vf_explained_var: 0.8196994066238403\n",
      "      vf_loss: 133.8017120361328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24400700628757477\n",
      "      kl: 0.010130398906767368\n",
      "      policy_loss: -0.0015029525384306908\n",
      "      total_loss: 190.1295623779297\n",
      "      vf_explained_var: 0.8433222770690918\n",
      "      vf_loss: 190.1310577392578\n",
      "    sample_time_ms: 20161.925\n",
      "    update_time_ms: 7.513\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.0687312237557\n",
      "    rl_1: 97.1406154008999\n",
      "  time_since_restore: 7299.368435621262\n",
      "  time_this_iter_s: 23.36260175704956\n",
      "  time_total_s: 7299.368435621262\n",
      "  timestamp: 1550800715\n",
      "  timesteps_since_restore: 3040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3040000\n",
      "  training_iteration: 304\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7299 s, 304 iter, 3040000 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-58-59\n",
      "  done: false\n",
      "  episode_len_mean: 90.73873873873873\n",
      "  episode_reward_max: 210.22176557047328\n",
      "  episode_reward_mean: 166.50631608449768\n",
      "  episode_reward_min: 118.09181204038654\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 30401\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.84\n",
      "    load_time_ms: 2.42\n",
      "    num_steps_sampled: 3050000\n",
      "    num_steps_trained: 3050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8879607915878296\n",
      "      kl: 0.011541498824954033\n",
      "      policy_loss: -0.005447035189718008\n",
      "      total_loss: 5.977430820465088\n",
      "      vf_explained_var: 0.9863438010215759\n",
      "      vf_loss: 5.982878684997559\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24496544897556305\n",
      "      kl: 0.005939210299402475\n",
      "      policy_loss: -0.002212659688666463\n",
      "      total_loss: 3.5345685482025146\n",
      "      vf_explained_var: 0.9960991144180298\n",
      "      vf_loss: 3.5367815494537354\n",
      "    sample_time_ms: 20170.158\n",
      "    update_time_ms: 7.725\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.80826554696417\n",
      "    rl_1: 103.69805053753348\n",
      "  time_since_restore: 7322.803066968918\n",
      "  time_this_iter_s: 23.43463134765625\n",
      "  time_total_s: 7322.803066968918\n",
      "  timestamp: 1550800739\n",
      "  timesteps_since_restore: 3050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3050000\n",
      "  training_iteration: 305\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7322 s, 305 iter, 3050000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-59-22\n",
      "  done: false\n",
      "  episode_len_mean: 90.35135135135135\n",
      "  episode_reward_max: 218.7163983968886\n",
      "  episode_reward_mean: 167.8822878595398\n",
      "  episode_reward_min: -118.05909526705385\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 30512\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3155.667\n",
      "    load_time_ms: 2.46\n",
      "    num_steps_sampled: 3060000\n",
      "    num_steps_trained: 3060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.881227433681488\n",
      "      kl: 0.011449266225099564\n",
      "      policy_loss: -0.0023577800020575523\n",
      "      total_loss: 31.050765991210938\n",
      "      vf_explained_var: 0.9422752261161804\n",
      "      vf_loss: 31.05312156677246\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2672308683395386\n",
      "      kl: 0.006775404326617718\n",
      "      policy_loss: -0.0028974334709346294\n",
      "      total_loss: 37.72210693359375\n",
      "      vf_explained_var: 0.964151918888092\n",
      "      vf_loss: 37.72500228881836\n",
      "    sample_time_ms: 20127.453\n",
      "    update_time_ms: 7.596\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.63485671355405\n",
      "    rl_1: 104.24743114598576\n",
      "  time_since_restore: 7345.833264827728\n",
      "  time_this_iter_s: 23.030197858810425\n",
      "  time_total_s: 7345.833264827728\n",
      "  timestamp: 1550800762\n",
      "  timesteps_since_restore: 3060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3060000\n",
      "  training_iteration: 306\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7345 s, 306 iter, 3060000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_02-59-45\n",
      "  done: false\n",
      "  episode_len_mean: 89.72972972972973\n",
      "  episode_reward_max: 215.49461321070262\n",
      "  episode_reward_mean: 165.50694819882875\n",
      "  episode_reward_min: -174.024390131632\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 30623\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.823\n",
      "    load_time_ms: 2.436\n",
      "    num_steps_sampled: 3070000\n",
      "    num_steps_trained: 3070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8553065061569214\n",
      "      kl: 0.006150498054921627\n",
      "      policy_loss: -0.0019062795909121633\n",
      "      total_loss: 25.426654815673828\n",
      "      vf_explained_var: 0.9512565732002258\n",
      "      vf_loss: 25.428560256958008\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.26859262585639954\n",
      "      kl: 0.007737352512776852\n",
      "      policy_loss: 0.0010461413767188787\n",
      "      total_loss: 41.02877426147461\n",
      "      vf_explained_var: 0.9601196050643921\n",
      "      vf_loss: 41.02772521972656\n",
      "    sample_time_ms: 20124.001\n",
      "    update_time_ms: 7.563\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.42088345956296\n",
      "    rl_1: 102.0860647392658\n",
      "  time_since_restore: 7368.911055803299\n",
      "  time_this_iter_s: 23.07779097557068\n",
      "  time_total_s: 7368.911055803299\n",
      "  timestamp: 1550800785\n",
      "  timesteps_since_restore: 3070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3070000\n",
      "  training_iteration: 307\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7368 s, 307 iter, 3070000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-00-08\n",
      "  done: false\n",
      "  episode_len_mean: 91.06363636363636\n",
      "  episode_reward_max: 214.3168535853092\n",
      "  episode_reward_mean: 173.41573708887432\n",
      "  episode_reward_min: 117.21703154663722\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 30733\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.895\n",
      "    load_time_ms: 2.432\n",
      "    num_steps_sampled: 3080000\n",
      "    num_steps_trained: 3080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8256769180297852\n",
      "      kl: 0.012279776856303215\n",
      "      policy_loss: -0.005202285014092922\n",
      "      total_loss: 4.700711727142334\n",
      "      vf_explained_var: 0.9899260401725769\n",
      "      vf_loss: 4.705913543701172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2404351830482483\n",
      "      kl: 0.005065816920250654\n",
      "      policy_loss: -0.001076525542885065\n",
      "      total_loss: 2.5885040760040283\n",
      "      vf_explained_var: 0.99732905626297\n",
      "      vf_loss: 2.589580535888672\n",
      "    sample_time_ms: 20150.452\n",
      "    update_time_ms: 7.238\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.72074567746172\n",
      "    rl_1: 106.69499141141262\n",
      "  time_since_restore: 7392.009959936142\n",
      "  time_this_iter_s: 23.098904132843018\n",
      "  time_total_s: 7392.009959936142\n",
      "  timestamp: 1550800808\n",
      "  timesteps_since_restore: 3080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3080000\n",
      "  training_iteration: 308\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7392 s, 308 iter, 3080000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-00-32\n",
      "  done: false\n",
      "  episode_len_mean: 90.17117117117117\n",
      "  episode_reward_max: 214.5899147594834\n",
      "  episode_reward_mean: 172.2644045047814\n",
      "  episode_reward_min: -128.00833230314245\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 30844\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.401\n",
      "    load_time_ms: 2.435\n",
      "    num_steps_sampled: 3090000\n",
      "    num_steps_trained: 3090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8053798079490662\n",
      "      kl: 0.010957042686641216\n",
      "      policy_loss: -0.0029464007820934057\n",
      "      total_loss: 34.081600189208984\n",
      "      vf_explained_var: 0.9386953711509705\n",
      "      vf_loss: 34.08454895019531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2695821523666382\n",
      "      kl: 0.014378208667039871\n",
      "      policy_loss: -0.0018964010523632169\n",
      "      total_loss: 40.97340393066406\n",
      "      vf_explained_var: 0.9620289206504822\n",
      "      vf_loss: 40.975303649902344\n",
      "    sample_time_ms: 20148.698\n",
      "    update_time_ms: 7.356\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.82343660607273\n",
      "    rl_1: 104.44096789870864\n",
      "  time_since_restore: 7415.94295835495\n",
      "  time_this_iter_s: 23.932998418807983\n",
      "  time_total_s: 7415.94295835495\n",
      "  timestamp: 1550800832\n",
      "  timesteps_since_restore: 3090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3090000\n",
      "  training_iteration: 309\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7415 s, 309 iter, 3090000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-00-55\n",
      "  done: false\n",
      "  episode_len_mean: 89.88288288288288\n",
      "  episode_reward_max: 215.7307557782544\n",
      "  episode_reward_mean: 167.24428931596378\n",
      "  episode_reward_min: -175.50051461618517\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 30955\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.876\n",
      "    load_time_ms: 2.467\n",
      "    num_steps_sampled: 3100000\n",
      "    num_steps_trained: 3100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8468536138534546\n",
      "      kl: 0.005166919436305761\n",
      "      policy_loss: -0.0017222166061401367\n",
      "      total_loss: 40.896663665771484\n",
      "      vf_explained_var: 0.9332339763641357\n",
      "      vf_loss: 40.898380279541016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.25747743248939514\n",
      "      kl: 0.010247007012367249\n",
      "      policy_loss: -0.003416314022615552\n",
      "      total_loss: 44.39262771606445\n",
      "      vf_explained_var: 0.9626691937446594\n",
      "      vf_loss: 44.39604187011719\n",
      "    sample_time_ms: 20091.92\n",
      "    update_time_ms: 8.028\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.88565876283927\n",
      "    rl_1: 103.35863055312456\n",
      "  time_since_restore: 7439.1224665641785\n",
      "  time_this_iter_s: 23.179508209228516\n",
      "  time_total_s: 7439.1224665641785\n",
      "  timestamp: 1550800855\n",
      "  timesteps_since_restore: 3100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3100000\n",
      "  training_iteration: 310\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7439 s, 310 iter, 3100000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-01-19\n",
      "  done: false\n",
      "  episode_len_mean: 92.57407407407408\n",
      "  episode_reward_max: 216.5391002759588\n",
      "  episode_reward_mean: 162.91031478288582\n",
      "  episode_reward_min: -179.61431782569045\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 31063\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.324\n",
      "    load_time_ms: 2.403\n",
      "    num_steps_sampled: 3110000\n",
      "    num_steps_trained: 3110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8713230490684509\n",
      "      kl: 0.009278605692088604\n",
      "      policy_loss: -0.004420121666043997\n",
      "      total_loss: 43.6282844543457\n",
      "      vf_explained_var: 0.9402055740356445\n",
      "      vf_loss: 43.6327018737793\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2660539448261261\n",
      "      kl: 0.016273850575089455\n",
      "      policy_loss: -0.00045371128362603486\n",
      "      total_loss: 71.70217895507812\n",
      "      vf_explained_var: 0.9414467215538025\n",
      "      vf_loss: 71.70262145996094\n",
      "    sample_time_ms: 20128.391\n",
      "    update_time_ms: 8.081\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.98305539959934\n",
      "    rl_1: 100.92725938328653\n",
      "  time_since_restore: 7462.310503005981\n",
      "  time_this_iter_s: 23.18803644180298\n",
      "  time_total_s: 7462.310503005981\n",
      "  timestamp: 1550800879\n",
      "  timesteps_since_restore: 3110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3110000\n",
      "  training_iteration: 311\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7462 s, 311 iter, 3110000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-01-42\n",
      "  done: false\n",
      "  episode_len_mean: 90.1891891891892\n",
      "  episode_reward_max: 215.82630233360612\n",
      "  episode_reward_mean: 170.03994084975227\n",
      "  episode_reward_min: 117.55646847008079\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 31174\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.214\n",
      "    load_time_ms: 2.377\n",
      "    num_steps_sampled: 3120000\n",
      "    num_steps_trained: 3120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8596704006195068\n",
      "      kl: 0.01014222577214241\n",
      "      policy_loss: -0.004236710723489523\n",
      "      total_loss: 3.9350104331970215\n",
      "      vf_explained_var: 0.9916582107543945\n",
      "      vf_loss: 3.939246892929077\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2667975425720215\n",
      "      kl: 0.005716146435588598\n",
      "      policy_loss: 0.0008576917462050915\n",
      "      total_loss: 2.4567253589630127\n",
      "      vf_explained_var: 0.9972502589225769\n",
      "      vf_loss: 2.4558675289154053\n",
      "    sample_time_ms: 20146.76\n",
      "    update_time_ms: 7.96\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.47460927706446\n",
      "    rl_1: 102.56533157268774\n",
      "  time_since_restore: 7485.70619392395\n",
      "  time_this_iter_s: 23.39569091796875\n",
      "  time_total_s: 7485.70619392395\n",
      "  timestamp: 1550800902\n",
      "  timesteps_since_restore: 3120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3120000\n",
      "  training_iteration: 312\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7485 s, 312 iter, 3120000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-02-05\n",
      "  done: false\n",
      "  episode_len_mean: 90.14545454545454\n",
      "  episode_reward_max: 222.49373871905703\n",
      "  episode_reward_mean: 169.01169867566605\n",
      "  episode_reward_min: -173.2929795423214\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 31284\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.576\n",
      "    load_time_ms: 2.317\n",
      "    num_steps_sampled: 3130000\n",
      "    num_steps_trained: 3130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8664219379425049\n",
      "      kl: 0.007390174549072981\n",
      "      policy_loss: -0.0035175015218555927\n",
      "      total_loss: 49.38862228393555\n",
      "      vf_explained_var: 0.9215568900108337\n",
      "      vf_loss: 49.39213562011719\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.25573527812957764\n",
      "      kl: 0.008302537724375725\n",
      "      policy_loss: -0.0007722668233327568\n",
      "      total_loss: 69.63235473632812\n",
      "      vf_explained_var: 0.9377583265304565\n",
      "      vf_loss: 69.63311767578125\n",
      "    sample_time_ms: 20115.235\n",
      "    update_time_ms: 7.946\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.99239021759982\n",
      "    rl_1: 103.01930845806623\n",
      "  time_since_restore: 7508.717528104782\n",
      "  time_this_iter_s: 23.01133418083191\n",
      "  time_total_s: 7508.717528104782\n",
      "  timestamp: 1550800925\n",
      "  timesteps_since_restore: 3130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3130000\n",
      "  training_iteration: 313\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7508 s, 313 iter, 3130000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 91.05405405405405\n",
      "  episode_reward_max: 217.70777905250242\n",
      "  episode_reward_mean: 167.43372905685146\n",
      "  episode_reward_min: -122.81310313635707\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 31395\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.924\n",
      "    load_time_ms: 2.309\n",
      "    num_steps_sampled: 3140000\n",
      "    num_steps_trained: 3140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8349704146385193\n",
      "      kl: 0.008916097693145275\n",
      "      policy_loss: -0.0008366313995793462\n",
      "      total_loss: 34.77378463745117\n",
      "      vf_explained_var: 0.9384254217147827\n",
      "      vf_loss: 34.774620056152344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.27456003427505493\n",
      "      kl: 0.019938485696911812\n",
      "      policy_loss: -0.0046132588759064674\n",
      "      total_loss: 42.02299499511719\n",
      "      vf_explained_var: 0.961463451385498\n",
      "      vf_loss: 42.02760314941406\n",
      "    sample_time_ms: 20089.583\n",
      "    update_time_ms: 7.895\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.79717505213833\n",
      "    rl_1: 102.63655400471318\n",
      "  time_since_restore: 7531.825777053833\n",
      "  time_this_iter_s: 23.108248949050903\n",
      "  time_total_s: 7531.825777053833\n",
      "  timestamp: 1550800948\n",
      "  timesteps_since_restore: 3140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3140000\n",
      "  training_iteration: 314\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7531 s, 314 iter, 3140000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-02-52\n",
      "  done: false\n",
      "  episode_len_mean: 91.58715596330275\n",
      "  episode_reward_max: 222.1042942983031\n",
      "  episode_reward_mean: 170.65823684772536\n",
      "  episode_reward_min: 132.8508287782268\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 31504\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3149.232\n",
      "    load_time_ms: 2.317\n",
      "    num_steps_sampled: 3150000\n",
      "    num_steps_trained: 3150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9361534118652344\n",
      "      kl: 0.004039893392473459\n",
      "      policy_loss: -0.0023788867983967066\n",
      "      total_loss: 4.702408790588379\n",
      "      vf_explained_var: 0.9896549582481384\n",
      "      vf_loss: 4.7047882080078125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3518300950527191\n",
      "      kl: 0.005838597659021616\n",
      "      policy_loss: -0.0018353838240727782\n",
      "      total_loss: 4.290464878082275\n",
      "      vf_explained_var: 0.9954329133033752\n",
      "      vf_loss: 4.292300701141357\n",
      "    sample_time_ms: 20057.383\n",
      "    update_time_ms: 7.647\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.96705306844514\n",
      "    rl_1: 104.69118377928028\n",
      "  time_since_restore: 7555.148962497711\n",
      "  time_this_iter_s: 23.323185443878174\n",
      "  time_total_s: 7555.148962497711\n",
      "  timestamp: 1550800972\n",
      "  timesteps_since_restore: 3150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3150000\n",
      "  training_iteration: 315\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7555 s, 315 iter, 3150000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-03-15\n",
      "  done: false\n",
      "  episode_len_mean: 93.06481481481481\n",
      "  episode_reward_max: 218.05724096766374\n",
      "  episode_reward_mean: 172.3249397189736\n",
      "  episode_reward_min: 41.25996627430853\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 31612\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.531\n",
      "    load_time_ms: 2.252\n",
      "    num_steps_sampled: 3160000\n",
      "    num_steps_trained: 3160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8913970589637756\n",
      "      kl: 0.012113867327570915\n",
      "      policy_loss: -0.0040737055242061615\n",
      "      total_loss: 4.0981550216674805\n",
      "      vf_explained_var: 0.9924176335334778\n",
      "      vf_loss: 4.102228164672852\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3223584294319153\n",
      "      kl: 0.010144518688321114\n",
      "      policy_loss: -0.0010691070929169655\n",
      "      total_loss: 3.2850866317749023\n",
      "      vf_explained_var: 0.9966979622840881\n",
      "      vf_loss: 3.286156177520752\n",
      "    sample_time_ms: 20073.156\n",
      "    update_time_ms: 7.87\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.63144377623448\n",
      "    rl_1: 103.69349594273912\n",
      "  time_since_restore: 7578.140861034393\n",
      "  time_this_iter_s: 22.99189853668213\n",
      "  time_total_s: 7578.140861034393\n",
      "  timestamp: 1550800995\n",
      "  timesteps_since_restore: 3160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3160000\n",
      "  training_iteration: 316\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7578 s, 316 iter, 3160000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-03-38\n",
      "  done: false\n",
      "  episode_len_mean: 98.24752475247524\n",
      "  episode_reward_max: 217.52217057281067\n",
      "  episode_reward_mean: 168.328307183631\n",
      "  episode_reward_min: -157.3734874484145\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 31713\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.727\n",
      "    load_time_ms: 2.26\n",
      "    num_steps_sampled: 3170000\n",
      "    num_steps_trained: 3170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9408566355705261\n",
      "      kl: 0.007936407811939716\n",
      "      policy_loss: -0.0022472464479506016\n",
      "      total_loss: 23.846179962158203\n",
      "      vf_explained_var: 0.9662821888923645\n",
      "      vf_loss: 23.848426818847656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.38395535945892334\n",
      "      kl: 0.006833960767835379\n",
      "      policy_loss: 0.001811033464036882\n",
      "      total_loss: 43.10313034057617\n",
      "      vf_explained_var: 0.9627046585083008\n",
      "      vf_loss: 43.101318359375\n",
      "    sample_time_ms: 20115.139\n",
      "    update_time_ms: 7.99\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.62583065624011\n",
      "    rl_1: 102.70247652739089\n",
      "  time_since_restore: 7601.618591785431\n",
      "  time_this_iter_s: 23.477730751037598\n",
      "  time_total_s: 7601.618591785431\n",
      "  timestamp: 1550801018\n",
      "  timesteps_since_restore: 3170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3170000\n",
      "  training_iteration: 317\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7601 s, 317 iter, 3170000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-04-02\n",
      "  done: false\n",
      "  episode_len_mean: 90.89189189189189\n",
      "  episode_reward_max: 221.40060518381443\n",
      "  episode_reward_mean: 179.34223343682717\n",
      "  episode_reward_min: 136.79723065753498\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 31824\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.843\n",
      "    load_time_ms: 2.332\n",
      "    num_steps_sampled: 3180000\n",
      "    num_steps_trained: 3180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8372807502746582\n",
      "      kl: 0.007074274122714996\n",
      "      policy_loss: -0.0034025709610432386\n",
      "      total_loss: 3.665240526199341\n",
      "      vf_explained_var: 0.9928736686706543\n",
      "      vf_loss: 3.6686432361602783\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2719421982765198\n",
      "      kl: 0.005966751836240292\n",
      "      policy_loss: -0.0004328396753408015\n",
      "      total_loss: 3.0328197479248047\n",
      "      vf_explained_var: 0.9969469904899597\n",
      "      vf_loss: 3.033252477645874\n",
      "    sample_time_ms: 20142.974\n",
      "    update_time_ms: 7.918\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.56806756919717\n",
      "    rl_1: 107.77416586762997\n",
      "  time_since_restore: 7624.998672008514\n",
      "  time_this_iter_s: 23.380080223083496\n",
      "  time_total_s: 7624.998672008514\n",
      "  timestamp: 1550801042\n",
      "  timesteps_since_restore: 3180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3180000\n",
      "  training_iteration: 318\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7624 s, 318 iter, 3180000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-04-25\n",
      "  done: false\n",
      "  episode_len_mean: 87.49557522123894\n",
      "  episode_reward_max: 224.0403643442971\n",
      "  episode_reward_mean: 158.33577891169574\n",
      "  episode_reward_min: -157.80581612809937\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 31937\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3123.804\n",
      "    load_time_ms: 2.335\n",
      "    num_steps_sampled: 3190000\n",
      "    num_steps_trained: 3190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8137027621269226\n",
      "      kl: 0.006347517482936382\n",
      "      policy_loss: -0.002522572875022888\n",
      "      total_loss: 185.83993530273438\n",
      "      vf_explained_var: 0.7528491616249084\n",
      "      vf_loss: 185.84246826171875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22737789154052734\n",
      "      kl: 0.009912939742207527\n",
      "      policy_loss: -0.004088920541107655\n",
      "      total_loss: 250.27989196777344\n",
      "      vf_explained_var: 0.8021942377090454\n",
      "      vf_loss: 250.2839813232422\n",
      "    sample_time_ms: 20080.655\n",
      "    update_time_ms: 7.469\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.95669433181711\n",
      "    rl_1: 95.37908457987865\n",
      "  time_since_restore: 7648.2618844509125\n",
      "  time_this_iter_s: 23.26321244239807\n",
      "  time_total_s: 7648.2618844509125\n",
      "  timestamp: 1550801065\n",
      "  timesteps_since_restore: 3190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3190000\n",
      "  training_iteration: 319\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7648 s, 319 iter, 3190000 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-04-48\n",
      "  done: false\n",
      "  episode_len_mean: 91.15454545454546\n",
      "  episode_reward_max: 223.77559607669684\n",
      "  episode_reward_mean: 169.68855123863338\n",
      "  episode_reward_min: -129.7445160754146\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 32047\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.098\n",
      "    load_time_ms: 2.3\n",
      "    num_steps_sampled: 3200000\n",
      "    num_steps_trained: 3200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8471457362174988\n",
      "      kl: 0.005764192435890436\n",
      "      policy_loss: -0.004712502937763929\n",
      "      total_loss: 35.51656723022461\n",
      "      vf_explained_var: 0.9344815015792847\n",
      "      vf_loss: 35.52128219604492\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.23188629746437073\n",
      "      kl: 0.012569988146424294\n",
      "      policy_loss: -0.0013899736804887652\n",
      "      total_loss: 44.80752944946289\n",
      "      vf_explained_var: 0.954135000705719\n",
      "      vf_loss: 44.808929443359375\n",
      "    sample_time_ms: 20072.528\n",
      "    update_time_ms: 7.043\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.31427938830647\n",
      "    rl_1: 103.37427185032689\n",
      "  time_since_restore: 7671.338129997253\n",
      "  time_this_iter_s: 23.076245546340942\n",
      "  time_total_s: 7671.338129997253\n",
      "  timestamp: 1550801088\n",
      "  timesteps_since_restore: 3200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3200000\n",
      "  training_iteration: 320\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7671 s, 320 iter, 3200000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-05-11\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_reward_max: 221.07577476273318\n",
      "  episode_reward_mean: 169.92900065529855\n",
      "  episode_reward_min: -165.5389296072889\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 32158\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.196\n",
      "    load_time_ms: 2.338\n",
      "    num_steps_sampled: 3210000\n",
      "    num_steps_trained: 3210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8303695917129517\n",
      "      kl: 0.008314722217619419\n",
      "      policy_loss: -0.005056213121861219\n",
      "      total_loss: 58.07155990600586\n",
      "      vf_explained_var: 0.9071990847587585\n",
      "      vf_loss: 58.07661056518555\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.16932974755764008\n",
      "      kl: 0.013167638331651688\n",
      "      policy_loss: -0.0010191664332523942\n",
      "      total_loss: 81.7969741821289\n",
      "      vf_explained_var: 0.9214330911636353\n",
      "      vf_loss: 81.79798126220703\n",
      "    sample_time_ms: 20096.279\n",
      "    update_time_ms: 6.774\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.84302612065744\n",
      "    rl_1: 104.08597453464112\n",
      "  time_since_restore: 7694.782079935074\n",
      "  time_this_iter_s: 23.443949937820435\n",
      "  time_total_s: 7694.782079935074\n",
      "  timestamp: 1550801111\n",
      "  timesteps_since_restore: 3210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3210000\n",
      "  training_iteration: 321\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7694 s, 321 iter, 3210000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-05-35\n",
      "  done: false\n",
      "  episode_len_mean: 92.74074074074075\n",
      "  episode_reward_max: 223.89231093921214\n",
      "  episode_reward_mean: 156.6719320114404\n",
      "  episode_reward_min: -151.69506812424748\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 32266\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.08\n",
      "    load_time_ms: 2.344\n",
      "    num_steps_sampled: 3220000\n",
      "    num_steps_trained: 3220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8908687233924866\n",
      "      kl: 0.00601444486528635\n",
      "      policy_loss: -0.004738070070743561\n",
      "      total_loss: 108.12736511230469\n",
      "      vf_explained_var: 0.824072003364563\n",
      "      vf_loss: 108.13211059570312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24853554368019104\n",
      "      kl: 0.010436083190143108\n",
      "      policy_loss: -0.0014391329605132341\n",
      "      total_loss: 142.53160095214844\n",
      "      vf_explained_var: 0.8649075031280518\n",
      "      vf_loss: 142.53306579589844\n",
      "    sample_time_ms: 20067.186\n",
      "    update_time_ms: 6.92\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 59.76496049624121\n",
      "    rl_1: 96.90697151519916\n",
      "  time_since_restore: 7717.927379846573\n",
      "  time_this_iter_s: 23.145299911499023\n",
      "  time_total_s: 7717.927379846573\n",
      "  timestamp: 1550801135\n",
      "  timesteps_since_restore: 3220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3220000\n",
      "  training_iteration: 322\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7717 s, 322 iter, 3220000 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-05-58\n",
      "  done: false\n",
      "  episode_len_mean: 89.96396396396396\n",
      "  episode_reward_max: 217.5009807833317\n",
      "  episode_reward_mean: 164.326450538247\n",
      "  episode_reward_min: -172.04224140991715\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 32377\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.279\n",
      "    load_time_ms: 2.351\n",
      "    num_steps_sampled: 3230000\n",
      "    num_steps_trained: 3230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8660174608230591\n",
      "      kl: 0.0063122911378741264\n",
      "      policy_loss: -0.002950881840661168\n",
      "      total_loss: 89.81362915039062\n",
      "      vf_explained_var: 0.8537985682487488\n",
      "      vf_loss: 89.81656646728516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.20518875122070312\n",
      "      kl: 0.007242299150675535\n",
      "      policy_loss: -0.0029550264589488506\n",
      "      total_loss: 112.80025482177734\n",
      "      vf_explained_var: 0.8975093364715576\n",
      "      vf_loss: 112.8031997680664\n",
      "    sample_time_ms: 20096.636\n",
      "    update_time_ms: 6.911\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.66359070045395\n",
      "    rl_1: 99.66285983779306\n",
      "  time_since_restore: 7741.248719930649\n",
      "  time_this_iter_s: 23.321340084075928\n",
      "  time_total_s: 7741.248719930649\n",
      "  timestamp: 1550801158\n",
      "  timesteps_since_restore: 3230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3230000\n",
      "  training_iteration: 323\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7741 s, 323 iter, 3230000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-06-22\n",
      "  done: false\n",
      "  episode_len_mean: 91.60550458715596\n",
      "  episode_reward_max: 218.07824710146693\n",
      "  episode_reward_mean: 168.12653720380314\n",
      "  episode_reward_min: -122.27240364819409\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 32486\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3151.742\n",
      "    load_time_ms: 2.279\n",
      "    num_steps_sampled: 3240000\n",
      "    num_steps_trained: 3240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9303050637245178\n",
      "      kl: 0.022352974861860275\n",
      "      policy_loss: -0.009390287101268768\n",
      "      total_loss: 38.76603317260742\n",
      "      vf_explained_var: 0.9294734597206116\n",
      "      vf_loss: 38.77542495727539\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1810411512851715\n",
      "      kl: 0.010049655102193356\n",
      "      policy_loss: -0.0023051470052450895\n",
      "      total_loss: 51.17654800415039\n",
      "      vf_explained_var: 0.9500840306282043\n",
      "      vf_loss: 51.17884826660156\n",
      "    sample_time_ms: 20107.244\n",
      "    update_time_ms: 6.943\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.56873829087873\n",
      "    rl_1: 104.55779891292445\n",
      "  time_since_restore: 7764.688820838928\n",
      "  time_this_iter_s: 23.44010090827942\n",
      "  time_total_s: 7764.688820838928\n",
      "  timestamp: 1550801182\n",
      "  timesteps_since_restore: 3240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3240000\n",
      "  training_iteration: 324\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7764 s, 324 iter, 3240000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-06-45\n",
      "  done: false\n",
      "  episode_len_mean: 91.55454545454545\n",
      "  episode_reward_max: 220.03401809093657\n",
      "  episode_reward_mean: 172.26211982520135\n",
      "  episode_reward_min: 129.0082135957488\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 32596\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.714\n",
      "    load_time_ms: 2.197\n",
      "    num_steps_sampled: 3250000\n",
      "    num_steps_trained: 3250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9386898875236511\n",
      "      kl: 0.006844683550298214\n",
      "      policy_loss: -0.003537952434271574\n",
      "      total_loss: 4.429915904998779\n",
      "      vf_explained_var: 0.9900391697883606\n",
      "      vf_loss: 4.433454513549805\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22490829229354858\n",
      "      kl: 0.007551775313913822\n",
      "      policy_loss: -0.0012696352787315845\n",
      "      total_loss: 4.016351699829102\n",
      "      vf_explained_var: 0.9956562519073486\n",
      "      vf_loss: 4.0176215171813965\n",
      "    sample_time_ms: 20117.683\n",
      "    update_time_ms: 6.917\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.05002622297759\n",
      "    rl_1: 105.21209360222379\n",
      "  time_since_restore: 7787.934192657471\n",
      "  time_this_iter_s: 23.24537181854248\n",
      "  time_total_s: 7787.934192657471\n",
      "  timestamp: 1550801205\n",
      "  timesteps_since_restore: 3250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3250000\n",
      "  training_iteration: 325\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7787 s, 325 iter, 3250000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-07-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.33333333333333\n",
      "  episode_reward_max: 217.6178853986351\n",
      "  episode_reward_mean: 169.8233779544586\n",
      "  episode_reward_min: 132.44728271967577\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 32707\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.927\n",
      "    load_time_ms: 2.194\n",
      "    num_steps_sampled: 3260000\n",
      "    num_steps_trained: 3260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8768151998519897\n",
      "      kl: 0.006529960781335831\n",
      "      policy_loss: -0.004982915241271257\n",
      "      total_loss: 3.1928632259368896\n",
      "      vf_explained_var: 0.9926952719688416\n",
      "      vf_loss: 3.1978461742401123\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.14032460749149323\n",
      "      kl: 0.008946765214204788\n",
      "      policy_loss: 4.087866182089783e-05\n",
      "      total_loss: 2.8024375438690186\n",
      "      vf_explained_var: 0.9969186782836914\n",
      "      vf_loss: 2.8023970127105713\n",
      "    sample_time_ms: 20148.577\n",
      "    update_time_ms: 6.722\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.90031931488693\n",
      "    rl_1: 103.92305863957168\n",
      "  time_since_restore: 7811.2352550029755\n",
      "  time_this_iter_s: 23.30106234550476\n",
      "  time_total_s: 7811.2352550029755\n",
      "  timestamp: 1550801228\n",
      "  timesteps_since_restore: 3260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3260000\n",
      "  training_iteration: 326\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7811 s, 326 iter, 3260000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-07-32\n",
      "  done: false\n",
      "  episode_len_mean: 89.63963963963964\n",
      "  episode_reward_max: 213.28211441426484\n",
      "  episode_reward_mean: 167.05684372453285\n",
      "  episode_reward_min: -179.59744155638288\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 32818\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.408\n",
      "    load_time_ms: 2.196\n",
      "    num_steps_sampled: 3270000\n",
      "    num_steps_trained: 3270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9180634617805481\n",
      "      kl: 0.007456635124981403\n",
      "      policy_loss: -0.0024820754770189524\n",
      "      total_loss: 63.256710052490234\n",
      "      vf_explained_var: 0.9036492705345154\n",
      "      vf_loss: 63.25918197631836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24604950845241547\n",
      "      kl: 0.011592794209718704\n",
      "      policy_loss: -0.0006917511345818639\n",
      "      total_loss: 95.7691421508789\n",
      "      vf_explained_var: 0.9136052131652832\n",
      "      vf_loss: 95.76983642578125\n",
      "    sample_time_ms: 20164.625\n",
      "    update_time_ms: 6.586\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.65688822140864\n",
      "    rl_1: 99.39995550312425\n",
      "  time_since_restore: 7834.940183401108\n",
      "  time_this_iter_s: 23.704928398132324\n",
      "  time_total_s: 7834.940183401108\n",
      "  timestamp: 1550801252\n",
      "  timesteps_since_restore: 3270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3270000\n",
      "  training_iteration: 327\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7834 s, 327 iter, 3270000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-07-56\n",
      "  done: false\n",
      "  episode_len_mean: 93.55660377358491\n",
      "  episode_reward_max: 218.84502319140756\n",
      "  episode_reward_mean: 164.98432576387643\n",
      "  episode_reward_min: -167.26424754635013\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 32924\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.163\n",
      "    load_time_ms: 2.113\n",
      "    num_steps_sampled: 3280000\n",
      "    num_steps_trained: 3280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9357826113700867\n",
      "      kl: 0.008343176916241646\n",
      "      policy_loss: -0.003933328669518232\n",
      "      total_loss: 43.41787338256836\n",
      "      vf_explained_var: 0.9369779825210571\n",
      "      vf_loss: 43.42180252075195\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2556529939174652\n",
      "      kl: 0.0069951643235981464\n",
      "      policy_loss: -0.0006708099972456694\n",
      "      total_loss: 66.39107513427734\n",
      "      vf_explained_var: 0.9423309564590454\n",
      "      vf_loss: 66.39173889160156\n",
      "    sample_time_ms: 20192.737\n",
      "    update_time_ms: 6.647\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.31818196249375\n",
      "    rl_1: 100.66614380138262\n",
      "  time_since_restore: 7858.59698343277\n",
      "  time_this_iter_s: 23.656800031661987\n",
      "  time_total_s: 7858.59698343277\n",
      "  timestamp: 1550801276\n",
      "  timesteps_since_restore: 3280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3280000\n",
      "  training_iteration: 328\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7858 s, 328 iter, 3280000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-08-19\n",
      "  done: false\n",
      "  episode_len_mean: 91.4\n",
      "  episode_reward_max: 217.79425339444268\n",
      "  episode_reward_mean: 167.63098992873506\n",
      "  episode_reward_min: -171.77486271642377\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 33034\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.282\n",
      "    load_time_ms: 2.182\n",
      "    num_steps_sampled: 3290000\n",
      "    num_steps_trained: 3290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9171212911605835\n",
      "      kl: 0.013589677400887012\n",
      "      policy_loss: -0.002748713130131364\n",
      "      total_loss: 45.129154205322266\n",
      "      vf_explained_var: 0.9230543375015259\n",
      "      vf_loss: 45.13189697265625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.17224378883838654\n",
      "      kl: 0.013050973415374756\n",
      "      policy_loss: -0.000980739830993116\n",
      "      total_loss: 59.57323455810547\n",
      "      vf_explained_var: 0.9453938007354736\n",
      "      vf_loss: 59.57419967651367\n",
      "    sample_time_ms: 20166.672\n",
      "    update_time_ms: 6.632\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.54158283534366\n",
      "    rl_1: 103.08940709339146\n",
      "  time_since_restore: 7881.593256473541\n",
      "  time_this_iter_s: 22.996273040771484\n",
      "  time_total_s: 7881.593256473541\n",
      "  timestamp: 1550801299\n",
      "  timesteps_since_restore: 3290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3290000\n",
      "  training_iteration: 329\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7881 s, 329 iter, 3290000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-08-42\n",
      "  done: false\n",
      "  episode_len_mean: 93.33333333333333\n",
      "  episode_reward_max: 217.64595243704343\n",
      "  episode_reward_mean: 168.6467802811921\n",
      "  episode_reward_min: -165.6701758954618\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 33142\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.744\n",
      "    load_time_ms: 2.198\n",
      "    num_steps_sampled: 3300000\n",
      "    num_steps_trained: 3300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9754568338394165\n",
      "      kl: 0.014492548070847988\n",
      "      policy_loss: -0.006666089408099651\n",
      "      total_loss: 27.90934181213379\n",
      "      vf_explained_var: 0.9584353566169739\n",
      "      vf_loss: 27.916006088256836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2845500707626343\n",
      "      kl: 0.007900738157331944\n",
      "      policy_loss: 0.00011856634228024632\n",
      "      total_loss: 37.45133590698242\n",
      "      vf_explained_var: 0.9670741558074951\n",
      "      vf_loss: 37.45121383666992\n",
      "    sample_time_ms: 20183.634\n",
      "    update_time_ms: 6.593\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.89428321693515\n",
      "    rl_1: 101.75249706425694\n",
      "  time_since_restore: 7904.894277572632\n",
      "  time_this_iter_s: 23.301021099090576\n",
      "  time_total_s: 7904.894277572632\n",
      "  timestamp: 1550801322\n",
      "  timesteps_since_restore: 3300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3300000\n",
      "  training_iteration: 330\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7904 s, 330 iter, 3300000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-09-05\n",
      "  done: false\n",
      "  episode_len_mean: 91.73148148148148\n",
      "  episode_reward_max: 221.1318498733402\n",
      "  episode_reward_mean: 172.96985675424264\n",
      "  episode_reward_min: 120.48646769738473\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 33250\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.398\n",
      "    load_time_ms: 2.211\n",
      "    num_steps_sampled: 3310000\n",
      "    num_steps_trained: 3310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9013871550559998\n",
      "      kl: 0.005026000551879406\n",
      "      policy_loss: -0.0042929882183671\n",
      "      total_loss: 3.6202666759490967\n",
      "      vf_explained_var: 0.9922154545783997\n",
      "      vf_loss: 3.624558687210083\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.128642275929451\n",
      "      kl: 0.011669361032545567\n",
      "      policy_loss: -0.003507727524265647\n",
      "      total_loss: 3.0549070835113525\n",
      "      vf_explained_var: 0.9968112111091614\n",
      "      vf_loss: 3.058415412902832\n",
      "    sample_time_ms: 20141.501\n",
      "    update_time_ms: 6.75\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.75215873471441\n",
      "    rl_1: 106.2176980195282\n",
      "  time_since_restore: 7927.89345407486\n",
      "  time_this_iter_s: 22.999176502227783\n",
      "  time_total_s: 7927.89345407486\n",
      "  timestamp: 1550801345\n",
      "  timesteps_since_restore: 3310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3310000\n",
      "  training_iteration: 331\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7927 s, 331 iter, 3310000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-09-28\n",
      "  done: false\n",
      "  episode_len_mean: 92.26851851851852\n",
      "  episode_reward_max: 221.7646802451523\n",
      "  episode_reward_mean: 170.04448210852553\n",
      "  episode_reward_min: 117.93868173563226\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 33358\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.974\n",
      "    load_time_ms: 2.22\n",
      "    num_steps_sampled: 3320000\n",
      "    num_steps_trained: 3320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9437985420227051\n",
      "      kl: 0.0053367456421256065\n",
      "      policy_loss: -0.0030266791582107544\n",
      "      total_loss: 3.989274740219116\n",
      "      vf_explained_var: 0.9919517636299133\n",
      "      vf_loss: 3.992300510406494\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.180921271443367\n",
      "      kl: 0.007075269240885973\n",
      "      policy_loss: -0.00044646323658525944\n",
      "      total_loss: 3.4101264476776123\n",
      "      vf_explained_var: 0.9964251518249512\n",
      "      vf_loss: 3.4105730056762695\n",
      "    sample_time_ms: 20083.173\n",
      "    update_time_ms: 6.607\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.13456711278751\n",
      "    rl_1: 103.909914995738\n",
      "  time_since_restore: 7950.429869174957\n",
      "  time_this_iter_s: 22.536415100097656\n",
      "  time_total_s: 7950.429869174957\n",
      "  timestamp: 1550801368\n",
      "  timesteps_since_restore: 3320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3320000\n",
      "  training_iteration: 332\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7950 s, 332 iter, 3320000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-09-51\n",
      "  done: false\n",
      "  episode_len_mean: 90.0625\n",
      "  episode_reward_max: 217.09551947226947\n",
      "  episode_reward_mean: 164.55545413515833\n",
      "  episode_reward_min: -120.77362312671185\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 33470\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3154.923\n",
      "    load_time_ms: 2.247\n",
      "    num_steps_sampled: 3330000\n",
      "    num_steps_trained: 3330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9175894260406494\n",
      "      kl: 0.005806440487504005\n",
      "      policy_loss: -0.0013227995950728655\n",
      "      total_loss: 31.1149845123291\n",
      "      vf_explained_var: 0.9405633211135864\n",
      "      vf_loss: 31.116304397583008\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1231108233332634\n",
      "      kl: 0.012628216296434402\n",
      "      policy_loss: -0.0037358070258051157\n",
      "      total_loss: 38.89361572265625\n",
      "      vf_explained_var: 0.9625141620635986\n",
      "      vf_loss: 38.8973503112793\n",
      "    sample_time_ms: 20110.561\n",
      "    update_time_ms: 6.794\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.54283262616367\n",
      "    rl_1: 102.01262150899463\n",
      "  time_since_restore: 7974.175931692123\n",
      "  time_this_iter_s: 23.746062517166138\n",
      "  time_total_s: 7974.175931692123\n",
      "  timestamp: 1550801391\n",
      "  timesteps_since_restore: 3330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3330000\n",
      "  training_iteration: 333\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7974 s, 333 iter, 3330000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-10-14\n",
      "  done: false\n",
      "  episode_len_mean: 91.75229357798165\n",
      "  episode_reward_max: 219.06612557996607\n",
      "  episode_reward_mean: 171.19993178909894\n",
      "  episode_reward_min: 128.62260836586026\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 33579\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.612\n",
      "    load_time_ms: 2.271\n",
      "    num_steps_sampled: 3340000\n",
      "    num_steps_trained: 3340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9102856516838074\n",
      "      kl: 0.0067246234975755215\n",
      "      policy_loss: -0.002930110553279519\n",
      "      total_loss: 2.7686831951141357\n",
      "      vf_explained_var: 0.9938166737556458\n",
      "      vf_loss: 2.771613359451294\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.13377240300178528\n",
      "      kl: 0.009414286352694035\n",
      "      policy_loss: -0.002055206336081028\n",
      "      total_loss: 2.789203643798828\n",
      "      vf_explained_var: 0.9969714283943176\n",
      "      vf_loss: 2.7912588119506836\n",
      "    sample_time_ms: 20070.257\n",
      "    update_time_ms: 6.984\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.19719664528361\n",
      "    rl_1: 105.00273514381533\n",
      "  time_since_restore: 7996.980028867722\n",
      "  time_this_iter_s: 22.804097175598145\n",
      "  time_total_s: 7996.980028867722\n",
      "  timestamp: 1550801414\n",
      "  timesteps_since_restore: 3340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3340000\n",
      "  training_iteration: 334\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 7996 s, 334 iter, 3340000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-10-38\n",
      "  done: false\n",
      "  episode_len_mean: 90.09909909909909\n",
      "  episode_reward_max: 215.39511591696382\n",
      "  episode_reward_mean: 174.6088501994413\n",
      "  episode_reward_min: 133.22153190413184\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 33690\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.858\n",
      "    load_time_ms: 2.28\n",
      "    num_steps_sampled: 3350000\n",
      "    num_steps_trained: 3350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8697085380554199\n",
      "      kl: 0.008524708449840546\n",
      "      policy_loss: -0.0043764798901975155\n",
      "      total_loss: 2.5670857429504395\n",
      "      vf_explained_var: 0.9946302771568298\n",
      "      vf_loss: 2.5714614391326904\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.11291904747486115\n",
      "      kl: 0.00627089710906148\n",
      "      policy_loss: 0.0003901499731000513\n",
      "      total_loss: 2.0966992378234863\n",
      "      vf_explained_var: 0.9977241158485413\n",
      "      vf_loss: 2.096309185028076\n",
      "    sample_time_ms: 20084.736\n",
      "    update_time_ms: 6.85\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.22686728005695\n",
      "    rl_1: 104.38198291938434\n",
      "  time_since_restore: 8020.430063486099\n",
      "  time_this_iter_s: 23.450034618377686\n",
      "  time_total_s: 8020.430063486099\n",
      "  timestamp: 1550801438\n",
      "  timesteps_since_restore: 3350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3350000\n",
      "  training_iteration: 335\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8020 s, 335 iter, 3350000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.67272727272727\n",
      "  episode_reward_max: 221.29956216509973\n",
      "  episode_reward_mean: 175.53854515425465\n",
      "  episode_reward_min: -151.2678828344355\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 33800\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.802\n",
      "    load_time_ms: 2.282\n",
      "    num_steps_sampled: 3360000\n",
      "    num_steps_trained: 3360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8908748030662537\n",
      "      kl: 0.004804631229490042\n",
      "      policy_loss: -0.001996370032429695\n",
      "      total_loss: 20.823135375976562\n",
      "      vf_explained_var: 0.9652772545814514\n",
      "      vf_loss: 20.825130462646484\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.15875910222530365\n",
      "      kl: 0.007604860235005617\n",
      "      policy_loss: -0.0021375997457653284\n",
      "      total_loss: 32.937591552734375\n",
      "      vf_explained_var: 0.9698570370674133\n",
      "      vf_loss: 32.939727783203125\n",
      "    sample_time_ms: 20075.255\n",
      "    update_time_ms: 6.998\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.02690597580789\n",
      "    rl_1: 105.51163917844683\n",
      "  time_since_restore: 8043.659999370575\n",
      "  time_this_iter_s: 23.229935884475708\n",
      "  time_total_s: 8043.659999370575\n",
      "  timestamp: 1550801461\n",
      "  timesteps_since_restore: 3360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3360000\n",
      "  training_iteration: 336\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8043 s, 336 iter, 3360000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-11-24\n",
      "  done: false\n",
      "  episode_len_mean: 89.13274336283186\n",
      "  episode_reward_max: 219.71679503305282\n",
      "  episode_reward_mean: 167.99539375574565\n",
      "  episode_reward_min: -142.90240629329915\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 33913\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.4\n",
      "    load_time_ms: 2.331\n",
      "    num_steps_sampled: 3370000\n",
      "    num_steps_trained: 3370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8493639826774597\n",
      "      kl: 0.009562860243022442\n",
      "      policy_loss: -0.00301999319344759\n",
      "      total_loss: 85.6628646850586\n",
      "      vf_explained_var: 0.8609133958816528\n",
      "      vf_loss: 85.66588592529297\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.0977843627333641\n",
      "      kl: 0.0151847368106246\n",
      "      policy_loss: 0.0010455104056745768\n",
      "      total_loss: 120.55536651611328\n",
      "      vf_explained_var: 0.8915629982948303\n",
      "      vf_loss: 120.55431365966797\n",
      "    sample_time_ms: 20003.142\n",
      "    update_time_ms: 7.126\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.28358485575586\n",
      "    rl_1: 101.7118088999898\n",
      "  time_since_restore: 8066.580468654633\n",
      "  time_this_iter_s: 22.920469284057617\n",
      "  time_total_s: 8066.580468654633\n",
      "  timestamp: 1550801484\n",
      "  timesteps_since_restore: 3370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3370000\n",
      "  training_iteration: 337\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8066 s, 337 iter, 3370000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-11-47\n",
      "  done: false\n",
      "  episode_len_mean: 90.21818181818182\n",
      "  episode_reward_max: 213.85363682072722\n",
      "  episode_reward_mean: 175.398747117023\n",
      "  episode_reward_min: 138.66593259958134\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 34023\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.868\n",
      "    load_time_ms: 2.331\n",
      "    num_steps_sampled: 3380000\n",
      "    num_steps_trained: 3380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8874301314353943\n",
      "      kl: 0.00574222719296813\n",
      "      policy_loss: -0.003169201547279954\n",
      "      total_loss: 3.577787399291992\n",
      "      vf_explained_var: 0.9923730492591858\n",
      "      vf_loss: 3.5809569358825684\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.15285047888755798\n",
      "      kl: 0.01225295476615429\n",
      "      policy_loss: 0.00020020503143314272\n",
      "      total_loss: 3.0082948207855225\n",
      "      vf_explained_var: 0.9967557191848755\n",
      "      vf_loss: 3.0080950260162354\n",
      "    sample_time_ms: 19921.708\n",
      "    update_time_ms: 7.521\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.4853722395682\n",
      "    rl_1: 104.91337487745476\n",
      "  time_since_restore: 8089.439135551453\n",
      "  time_this_iter_s: 22.85866689682007\n",
      "  time_total_s: 8089.439135551453\n",
      "  timestamp: 1550801507\n",
      "  timesteps_since_restore: 3380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3380000\n",
      "  training_iteration: 338\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8089 s, 338 iter, 3380000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-12-10\n",
      "  done: false\n",
      "  episode_len_mean: 94.25471698113208\n",
      "  episode_reward_max: 215.8661373978154\n",
      "  episode_reward_mean: 170.3837674537569\n",
      "  episode_reward_min: -136.85700457178385\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 34129\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.525\n",
      "    load_time_ms: 2.259\n",
      "    num_steps_sampled: 3390000\n",
      "    num_steps_trained: 3390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9309359788894653\n",
      "      kl: 0.006906365044414997\n",
      "      policy_loss: -0.002579750493168831\n",
      "      total_loss: 30.686752319335938\n",
      "      vf_explained_var: 0.9484580755233765\n",
      "      vf_loss: 30.6893367767334\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.238662987947464\n",
      "      kl: 0.01261924672871828\n",
      "      policy_loss: -0.004987855441868305\n",
      "      total_loss: 40.69060516357422\n",
      "      vf_explained_var: 0.9607914686203003\n",
      "      vf_loss: 40.695594787597656\n",
      "    sample_time_ms: 19963.764\n",
      "    update_time_ms: 7.512\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.24771451641091\n",
      "    rl_1: 102.136052937346\n",
      "  time_since_restore: 8112.879421949387\n",
      "  time_this_iter_s: 23.44028639793396\n",
      "  time_total_s: 8112.879421949387\n",
      "  timestamp: 1550801530\n",
      "  timesteps_since_restore: 3390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3390000\n",
      "  training_iteration: 339\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8112 s, 339 iter, 3390000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-12-34\n",
      "  done: false\n",
      "  episode_len_mean: 90.2072072072072\n",
      "  episode_reward_max: 221.0472507907882\n",
      "  episode_reward_mean: 172.4569133568074\n",
      "  episode_reward_min: -121.54529584562424\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 34240\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.588\n",
      "    load_time_ms: 2.245\n",
      "    num_steps_sampled: 3400000\n",
      "    num_steps_trained: 3400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8864238262176514\n",
      "      kl: 0.004896250087767839\n",
      "      policy_loss: -0.0013370501110330224\n",
      "      total_loss: 6.656990051269531\n",
      "      vf_explained_var: 0.9879134893417358\n",
      "      vf_loss: 6.658328533172607\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.13709880411624908\n",
      "      kl: 0.005672813858836889\n",
      "      policy_loss: -0.0010161023819819093\n",
      "      total_loss: 7.734012603759766\n",
      "      vf_explained_var: 0.992659866809845\n",
      "      vf_loss: 7.735029220581055\n",
      "    sample_time_ms: 19942.745\n",
      "    update_time_ms: 7.759\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.03839756800578\n",
      "    rl_1: 103.4185157888016\n",
      "  time_since_restore: 8135.9350464344025\n",
      "  time_this_iter_s: 23.05562448501587\n",
      "  time_total_s: 8135.9350464344025\n",
      "  timestamp: 1550801554\n",
      "  timesteps_since_restore: 3400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3400000\n",
      "  training_iteration: 340\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8135 s, 340 iter, 3400000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-12-57\n",
      "  done: false\n",
      "  episode_len_mean: 90.01801801801801\n",
      "  episode_reward_max: 215.1560733541094\n",
      "  episode_reward_mean: 171.97569230472783\n",
      "  episode_reward_min: -121.77800340103745\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 34351\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.164\n",
      "    load_time_ms: 2.198\n",
      "    num_steps_sampled: 3410000\n",
      "    num_steps_trained: 3410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9406437277793884\n",
      "      kl: 0.012957300990819931\n",
      "      policy_loss: -0.004585642367601395\n",
      "      total_loss: 31.677783966064453\n",
      "      vf_explained_var: 0.941724419593811\n",
      "      vf_loss: 31.682376861572266\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.12347149103879929\n",
      "      kl: 0.008825738914310932\n",
      "      policy_loss: -0.0030609567184001207\n",
      "      total_loss: 41.276954650878906\n",
      "      vf_explained_var: 0.9603131413459778\n",
      "      vf_loss: 41.28002166748047\n",
      "    sample_time_ms: 19978.654\n",
      "    update_time_ms: 7.908\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.74757901822467\n",
      "    rl_1: 104.22811328650313\n",
      "  time_since_restore: 8159.309958696365\n",
      "  time_this_iter_s: 23.37491226196289\n",
      "  time_total_s: 8159.309958696365\n",
      "  timestamp: 1550801577\n",
      "  timesteps_since_restore: 3410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3410000\n",
      "  training_iteration: 341\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8159 s, 341 iter, 3410000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-13-20\n",
      "  done: false\n",
      "  episode_len_mean: 90.09909909909909\n",
      "  episode_reward_max: 217.9155303555146\n",
      "  episode_reward_mean: 177.33466297276215\n",
      "  episode_reward_min: 137.40942039313998\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 34462\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.77\n",
      "    load_time_ms: 2.183\n",
      "    num_steps_sampled: 3420000\n",
      "    num_steps_trained: 3420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9058760404586792\n",
      "      kl: 0.006444532889872789\n",
      "      policy_loss: -0.00417244341224432\n",
      "      total_loss: 2.521893262863159\n",
      "      vf_explained_var: 0.9946971535682678\n",
      "      vf_loss: 2.5260658264160156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.10903614014387131\n",
      "      kl: 0.008808308281004429\n",
      "      policy_loss: 0.0009831624338403344\n",
      "      total_loss: 2.637385129928589\n",
      "      vf_explained_var: 0.997228741645813\n",
      "      vf_loss: 2.636401891708374\n",
      "    sample_time_ms: 20007.686\n",
      "    update_time_ms: 8.121\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.64953617994195\n",
      "    rl_1: 105.68512679282013\n",
      "  time_since_restore: 8182.294592618942\n",
      "  time_this_iter_s: 22.984633922576904\n",
      "  time_total_s: 8182.294592618942\n",
      "  timestamp: 1550801600\n",
      "  timesteps_since_restore: 3420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3420000\n",
      "  training_iteration: 342\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8182 s, 342 iter, 3420000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-13-43\n",
      "  done: false\n",
      "  episode_len_mean: 88.08849557522124\n",
      "  episode_reward_max: 215.75348024663472\n",
      "  episode_reward_mean: 164.3378297080334\n",
      "  episode_reward_min: -180.13245973126178\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 34575\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.454\n",
      "    load_time_ms: 2.128\n",
      "    num_steps_sampled: 3430000\n",
      "    num_steps_trained: 3430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9223519563674927\n",
      "      kl: 0.008703140541911125\n",
      "      policy_loss: -0.005720692686736584\n",
      "      total_loss: 129.12533569335938\n",
      "      vf_explained_var: 0.8215600252151489\n",
      "      vf_loss: 129.13108825683594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.11970183998346329\n",
      "      kl: 0.015313871204853058\n",
      "      policy_loss: -0.002644368913024664\n",
      "      total_loss: 187.46315002441406\n",
      "      vf_explained_var: 0.8399860858917236\n",
      "      vf_loss: 187.4658203125\n",
      "    sample_time_ms: 19957.156\n",
      "    update_time_ms: 7.968\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.26760509600015\n",
      "    rl_1: 97.07022461203326\n",
      "  time_since_restore: 8205.387132167816\n",
      "  time_this_iter_s: 23.0925395488739\n",
      "  time_total_s: 8205.387132167816\n",
      "  timestamp: 1550801623\n",
      "  timesteps_since_restore: 3430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3430000\n",
      "  training_iteration: 343\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8205 s, 343 iter, 3430000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-14-06\n",
      "  done: false\n",
      "  episode_len_mean: 89.47787610619469\n",
      "  episode_reward_max: 217.28930460966998\n",
      "  episode_reward_mean: 165.57701096420587\n",
      "  episode_reward_min: -177.41140256958045\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 34688\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.411\n",
      "    load_time_ms: 2.169\n",
      "    num_steps_sampled: 3440000\n",
      "    num_steps_trained: 3440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9107773900032043\n",
      "      kl: 0.008020966313779354\n",
      "      policy_loss: -0.0038170956540852785\n",
      "      total_loss: 93.8721923828125\n",
      "      vf_explained_var: 0.855932354927063\n",
      "      vf_loss: 93.87600708007812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.12566547095775604\n",
      "      kl: 0.012239787727594376\n",
      "      policy_loss: 0.00028306053718551993\n",
      "      total_loss: 117.54383087158203\n",
      "      vf_explained_var: 0.8967739939689636\n",
      "      vf_loss: 117.5435562133789\n",
      "    sample_time_ms: 19940.25\n",
      "    update_time_ms: 8.031\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.57297599727147\n",
      "    rl_1: 100.00403496693438\n",
      "  time_since_restore: 8228.03370642662\n",
      "  time_this_iter_s: 22.64657425880432\n",
      "  time_total_s: 8228.03370642662\n",
      "  timestamp: 1550801646\n",
      "  timesteps_since_restore: 3440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3440000\n",
      "  training_iteration: 344\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8228 s, 344 iter, 3440000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-14-29\n",
      "  done: false\n",
      "  episode_len_mean: 89.58558558558559\n",
      "  episode_reward_max: 214.86063568168345\n",
      "  episode_reward_mean: 164.93700971508034\n",
      "  episode_reward_min: -167.90928427641416\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 34799\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.691\n",
      "    load_time_ms: 2.162\n",
      "    num_steps_sampled: 3450000\n",
      "    num_steps_trained: 3450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9353564381599426\n",
      "      kl: 0.011141346767544746\n",
      "      policy_loss: -0.008544322103261948\n",
      "      total_loss: 71.53276062011719\n",
      "      vf_explained_var: 0.8878649473190308\n",
      "      vf_loss: 71.5412826538086\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1292402744293213\n",
      "      kl: 0.013992815278470516\n",
      "      policy_loss: -0.003070620819926262\n",
      "      total_loss: 113.3663101196289\n",
      "      vf_explained_var: 0.8975564241409302\n",
      "      vf_loss: 113.36937713623047\n",
      "    sample_time_ms: 19900.0\n",
      "    update_time_ms: 7.933\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.39949926800264\n",
      "    rl_1: 100.53751044707771\n",
      "  time_since_restore: 8251.035284519196\n",
      "  time_this_iter_s: 23.001578092575073\n",
      "  time_total_s: 8251.035284519196\n",
      "  timestamp: 1550801669\n",
      "  timesteps_since_restore: 3450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3450000\n",
      "  training_iteration: 345\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8251 s, 345 iter, 3450000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-14-52\n",
      "  done: false\n",
      "  episode_len_mean: 90.4090909090909\n",
      "  episode_reward_max: 221.91984299257888\n",
      "  episode_reward_mean: 174.25676389870813\n",
      "  episode_reward_min: 133.37223669062095\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 34909\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.079\n",
      "    load_time_ms: 2.175\n",
      "    num_steps_sampled: 3460000\n",
      "    num_steps_trained: 3460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9128788709640503\n",
      "      kl: 0.0060285283252596855\n",
      "      policy_loss: -0.0029655422549694777\n",
      "      total_loss: 3.432356595993042\n",
      "      vf_explained_var: 0.9924784898757935\n",
      "      vf_loss: 3.435321807861328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.10622090846300125\n",
      "      kl: 0.004808607045561075\n",
      "      policy_loss: -0.0016697457758709788\n",
      "      total_loss: 2.7565529346466064\n",
      "      vf_explained_var: 0.9970698356628418\n",
      "      vf_loss: 2.7582225799560547\n",
      "    sample_time_ms: 19901.587\n",
      "    update_time_ms: 7.862\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.52868628677561\n",
      "    rl_1: 104.72807761193248\n",
      "  time_since_restore: 8274.38105893135\n",
      "  time_this_iter_s: 23.34577441215515\n",
      "  time_total_s: 8274.38105893135\n",
      "  timestamp: 1550801692\n",
      "  timesteps_since_restore: 3460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3460000\n",
      "  training_iteration: 346\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8274 s, 346 iter, 3460000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-15-16\n",
      "  done: false\n",
      "  episode_len_mean: 91.28181818181818\n",
      "  episode_reward_max: 217.64375480553576\n",
      "  episode_reward_mean: 172.62560178459128\n",
      "  episode_reward_min: 135.00924721441822\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 35019\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.03\n",
      "    load_time_ms: 2.182\n",
      "    num_steps_sampled: 3470000\n",
      "    num_steps_trained: 3470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9781398177146912\n",
      "      kl: 0.007071422412991524\n",
      "      policy_loss: -0.0028920124750584364\n",
      "      total_loss: 3.3535075187683105\n",
      "      vf_explained_var: 0.992832601070404\n",
      "      vf_loss: 3.3563995361328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.19570164382457733\n",
      "      kl: 0.00885283574461937\n",
      "      policy_loss: 0.0005274683353491127\n",
      "      total_loss: 3.09749174118042\n",
      "      vf_explained_var: 0.9967604279518127\n",
      "      vf_loss: 3.096964120864868\n",
      "    sample_time_ms: 19930.089\n",
      "    update_time_ms: 8.251\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.92073068562937\n",
      "    rl_1: 105.70487109896186\n",
      "  time_since_restore: 8297.611392974854\n",
      "  time_this_iter_s: 23.230334043502808\n",
      "  time_total_s: 8297.611392974854\n",
      "  timestamp: 1550801716\n",
      "  timesteps_since_restore: 3470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3470000\n",
      "  training_iteration: 347\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8297 s, 347 iter, 3470000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-15-39\n",
      "  done: false\n",
      "  episode_len_mean: 94.01886792452831\n",
      "  episode_reward_max: 223.2578788717726\n",
      "  episode_reward_mean: 177.6724041851777\n",
      "  episode_reward_min: 40.335165741976944\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 35125\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3151.189\n",
      "    load_time_ms: 2.189\n",
      "    num_steps_sampled: 3480000\n",
      "    num_steps_trained: 3480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9534158706665039\n",
      "      kl: 0.008822600357234478\n",
      "      policy_loss: -0.003329604398459196\n",
      "      total_loss: 2.797224521636963\n",
      "      vf_explained_var: 0.9949037432670593\n",
      "      vf_loss: 2.8005545139312744\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2103366106748581\n",
      "      kl: 0.0053067742846906185\n",
      "      policy_loss: 0.0017516519874334335\n",
      "      total_loss: 2.9724903106689453\n",
      "      vf_explained_var: 0.9971415996551514\n",
      "      vf_loss: 2.970738649368286\n",
      "    sample_time_ms: 20021.102\n",
      "    update_time_ms: 8.024\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.79418764434787\n",
      "    rl_1: 106.87821654082983\n",
      "  time_since_restore: 8321.441492795944\n",
      "  time_this_iter_s: 23.8300998210907\n",
      "  time_total_s: 8321.441492795944\n",
      "  timestamp: 1550801739\n",
      "  timesteps_since_restore: 3480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3480000\n",
      "  training_iteration: 348\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8321 s, 348 iter, 3480000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-16-03\n",
      "  done: false\n",
      "  episode_len_mean: 93.0925925925926\n",
      "  episode_reward_max: 220.74430478315372\n",
      "  episode_reward_mean: 167.77093539002524\n",
      "  episode_reward_min: -148.1425216292504\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 35233\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.681\n",
      "    load_time_ms: 2.183\n",
      "    num_steps_sampled: 3490000\n",
      "    num_steps_trained: 3490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9385713338851929\n",
      "      kl: 0.010077998042106628\n",
      "      policy_loss: -0.0038749808445572853\n",
      "      total_loss: 61.69490051269531\n",
      "      vf_explained_var: 0.8960877656936646\n",
      "      vf_loss: 61.69878005981445\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2141382247209549\n",
      "      kl: 0.013901829719543457\n",
      "      policy_loss: -0.0007557766512036324\n",
      "      total_loss: 85.39462280273438\n",
      "      vf_explained_var: 0.9174023866653442\n",
      "      vf_loss: 85.3953857421875\n",
      "    sample_time_ms: 19997.507\n",
      "    update_time_ms: 7.982\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.90270219261093\n",
      "    rl_1: 99.86823319741431\n",
      "  time_since_restore: 8344.641798734665\n",
      "  time_this_iter_s: 23.200305938720703\n",
      "  time_total_s: 8344.641798734665\n",
      "  timestamp: 1550801763\n",
      "  timesteps_since_restore: 3490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3490000\n",
      "  training_iteration: 349\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8344 s, 349 iter, 3490000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-16-26\n",
      "  done: false\n",
      "  episode_len_mean: 91.55454545454545\n",
      "  episode_reward_max: 217.44159340065872\n",
      "  episode_reward_mean: 170.48518142781177\n",
      "  episode_reward_min: -106.57552666145676\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 35343\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3151.94\n",
      "    load_time_ms: 2.204\n",
      "    num_steps_sampled: 3500000\n",
      "    num_steps_trained: 3500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9168961644172668\n",
      "      kl: 0.005857947748154402\n",
      "      policy_loss: -0.0021300597582012415\n",
      "      total_loss: 12.705182075500488\n",
      "      vf_explained_var: 0.9794018268585205\n",
      "      vf_loss: 12.707311630249023\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1568400263786316\n",
      "      kl: 0.008420462720096111\n",
      "      policy_loss: -0.0014057577354833484\n",
      "      total_loss: 15.26297664642334\n",
      "      vf_explained_var: 0.9863317608833313\n",
      "      vf_loss: 15.26438045501709\n",
      "    sample_time_ms: 19976.942\n",
      "    update_time_ms: 7.696\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.81777771574565\n",
      "    rl_1: 103.66740371206613\n",
      "  time_since_restore: 8367.500198841095\n",
      "  time_this_iter_s: 22.858400106430054\n",
      "  time_total_s: 8367.500198841095\n",
      "  timestamp: 1550801786\n",
      "  timesteps_since_restore: 3500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3500000\n",
      "  training_iteration: 350\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8367 s, 350 iter, 3500000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-16-49\n",
      "  done: false\n",
      "  episode_len_mean: 91.46788990825688\n",
      "  episode_reward_max: 218.22535282556922\n",
      "  episode_reward_mean: 169.51182678878564\n",
      "  episode_reward_min: -57.09302525105943\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 35452\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3160.128\n",
      "    load_time_ms: 2.189\n",
      "    num_steps_sampled: 3510000\n",
      "    num_steps_trained: 3510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.904836893081665\n",
      "      kl: 0.012968309223651886\n",
      "      policy_loss: -0.005046501290053129\n",
      "      total_loss: 9.678462028503418\n",
      "      vf_explained_var: 0.982006847858429\n",
      "      vf_loss: 9.683507919311523\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.16037622094154358\n",
      "      kl: 0.007488742005079985\n",
      "      policy_loss: -0.0007639346877112985\n",
      "      total_loss: 12.203554153442383\n",
      "      vf_explained_var: 0.9881736636161804\n",
      "      vf_loss: 12.204317092895508\n",
      "    sample_time_ms: 19975.397\n",
      "    update_time_ms: 7.86\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.7503838266063\n",
      "    rl_1: 101.76144296217929\n",
      "  time_since_restore: 8390.945750236511\n",
      "  time_this_iter_s: 23.44555139541626\n",
      "  time_total_s: 8390.945750236511\n",
      "  timestamp: 1550801809\n",
      "  timesteps_since_restore: 3510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3510000\n",
      "  training_iteration: 351\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8390 s, 351 iter, 3510000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-17-13\n",
      "  done: false\n",
      "  episode_len_mean: 91.44954128440367\n",
      "  episode_reward_max: 217.29053797764797\n",
      "  episode_reward_mean: 169.32086319249103\n",
      "  episode_reward_min: -90.36602171420746\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 35561\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.505\n",
      "    load_time_ms: 2.208\n",
      "    num_steps_sampled: 3520000\n",
      "    num_steps_trained: 3520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8756183981895447\n",
      "      kl: 0.007994485087692738\n",
      "      policy_loss: -0.0027621330227702856\n",
      "      total_loss: 10.565885543823242\n",
      "      vf_explained_var: 0.980366051197052\n",
      "      vf_loss: 10.568648338317871\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.09360137581825256\n",
      "      kl: 0.011148371733725071\n",
      "      policy_loss: -0.0007654093205928802\n",
      "      total_loss: 11.635459899902344\n",
      "      vf_explained_var: 0.9892982840538025\n",
      "      vf_loss: 11.636225700378418\n",
      "    sample_time_ms: 20051.456\n",
      "    update_time_ms: 7.972\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.15525358730825\n",
      "    rl_1: 104.16560960518272\n",
      "  time_since_restore: 8414.514756202698\n",
      "  time_this_iter_s: 23.569005966186523\n",
      "  time_total_s: 8414.514756202698\n",
      "  timestamp: 1550801833\n",
      "  timesteps_since_restore: 3520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3520000\n",
      "  training_iteration: 352\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8414 s, 352 iter, 3520000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-17-36\n",
      "  done: false\n",
      "  episode_len_mean: 90.06306306306307\n",
      "  episode_reward_max: 215.9916388512111\n",
      "  episode_reward_mean: 177.35962321049618\n",
      "  episode_reward_min: 134.94242981249153\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 35672\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.526\n",
      "    load_time_ms: 2.208\n",
      "    num_steps_sampled: 3530000\n",
      "    num_steps_trained: 3530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.869674026966095\n",
      "      kl: 0.00564160430803895\n",
      "      policy_loss: -0.0022592185996472836\n",
      "      total_loss: 2.7062854766845703\n",
      "      vf_explained_var: 0.9948065280914307\n",
      "      vf_loss: 2.708545207977295\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.0863485336303711\n",
      "      kl: 0.009396671317517757\n",
      "      policy_loss: 0.00057765725068748\n",
      "      total_loss: 3.2140955924987793\n",
      "      vf_explained_var: 0.9966039657592773\n",
      "      vf_loss: 3.213517904281616\n",
      "    sample_time_ms: 20080.077\n",
      "    update_time_ms: 8.051\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.15767367993665\n",
      "    rl_1: 105.20194953055952\n",
      "  time_since_restore: 8437.914592504501\n",
      "  time_this_iter_s: 23.39983630180359\n",
      "  time_total_s: 8437.914592504501\n",
      "  timestamp: 1550801856\n",
      "  timesteps_since_restore: 3530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3530000\n",
      "  training_iteration: 353\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8437 s, 353 iter, 3530000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-17-59\n",
      "  done: false\n",
      "  episode_len_mean: 89.51785714285714\n",
      "  episode_reward_max: 219.06165997006326\n",
      "  episode_reward_mean: 177.16676504884552\n",
      "  episode_reward_min: 134.5462137607876\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 35784\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.484\n",
      "    load_time_ms: 2.145\n",
      "    num_steps_sampled: 3540000\n",
      "    num_steps_trained: 3540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8833532333374023\n",
      "      kl: 0.005817831493914127\n",
      "      policy_loss: -0.0027229038532823324\n",
      "      total_loss: 2.4911091327667236\n",
      "      vf_explained_var: 0.9950442910194397\n",
      "      vf_loss: 2.4938321113586426\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1181267574429512\n",
      "      kl: 0.008785896003246307\n",
      "      policy_loss: -0.0006948891677893698\n",
      "      total_loss: 2.4894967079162598\n",
      "      vf_explained_var: 0.9972543120384216\n",
      "      vf_loss: 2.49019193649292\n",
      "    sample_time_ms: 20097.131\n",
      "    update_time_ms: 8.159\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.87384257842086\n",
      "    rl_1: 103.29292247042466\n",
      "  time_since_restore: 8460.73354101181\n",
      "  time_this_iter_s: 22.81894850730896\n",
      "  time_total_s: 8460.73354101181\n",
      "  timestamp: 1550801879\n",
      "  timesteps_since_restore: 3540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3540000\n",
      "  training_iteration: 354\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8460 s, 354 iter, 3540000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-18-22\n",
      "  done: false\n",
      "  episode_len_mean: 88.52212389380531\n",
      "  episode_reward_max: 218.87540136493044\n",
      "  episode_reward_mean: 166.02374538733196\n",
      "  episode_reward_min: -177.26884699833062\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 35897\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.709\n",
      "    load_time_ms: 2.169\n",
      "    num_steps_sampled: 3550000\n",
      "    num_steps_trained: 3550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8614920377731323\n",
      "      kl: 0.004345229361206293\n",
      "      policy_loss: -0.0008581596193835139\n",
      "      total_loss: 104.99332427978516\n",
      "      vf_explained_var: 0.8452218770980835\n",
      "      vf_loss: 104.99417877197266\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.033660970628261566\n",
      "      kl: 0.016357192769646645\n",
      "      policy_loss: 0.00036052882205694914\n",
      "      total_loss: 129.40744018554688\n",
      "      vf_explained_var: 0.8847012519836426\n",
      "      vf_loss: 129.4070587158203\n",
      "    sample_time_ms: 20141.639\n",
      "    update_time_ms: 8.363\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.77959959710351\n",
      "    rl_1: 98.2441457902284\n",
      "  time_since_restore: 8484.162807226181\n",
      "  time_this_iter_s: 23.429266214370728\n",
      "  time_total_s: 8484.162807226181\n",
      "  timestamp: 1550801902\n",
      "  timesteps_since_restore: 3550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3550000\n",
      "  training_iteration: 355\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8484 s, 355 iter, 3550000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-18-46\n",
      "  done: false\n",
      "  episode_len_mean: 89.94545454545455\n",
      "  episode_reward_max: 223.1552401400117\n",
      "  episode_reward_mean: 161.96437487743773\n",
      "  episode_reward_min: -172.9967858110815\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 36007\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.792\n",
      "    load_time_ms: 2.261\n",
      "    num_steps_sampled: 3560000\n",
      "    num_steps_trained: 3560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8569813966751099\n",
      "      kl: 0.011624958366155624\n",
      "      policy_loss: -0.0034881739411503077\n",
      "      total_loss: 132.81832885742188\n",
      "      vf_explained_var: 0.8223170042037964\n",
      "      vf_loss: 132.8218231201172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.02901388518512249\n",
      "      kl: 0.013861811719834805\n",
      "      policy_loss: -0.0032576119992882013\n",
      "      total_loss: 159.96717834472656\n",
      "      vf_explained_var: 0.8665908575057983\n",
      "      vf_loss: 159.97044372558594\n",
      "    sample_time_ms: 20136.469\n",
      "    update_time_ms: 8.297\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.49438082655813\n",
      "    rl_1: 97.46999405087962\n",
      "  time_since_restore: 8507.351133584976\n",
      "  time_this_iter_s: 23.188326358795166\n",
      "  time_total_s: 8507.351133584976\n",
      "  timestamp: 1550801926\n",
      "  timesteps_since_restore: 3560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3560000\n",
      "  training_iteration: 356\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8507 s, 356 iter, 3560000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-19-09\n",
      "  done: false\n",
      "  episode_len_mean: 89.24107142857143\n",
      "  episode_reward_max: 221.91667751859015\n",
      "  episode_reward_mean: 162.537415699047\n",
      "  episode_reward_min: -133.54035191934943\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 36119\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.481\n",
      "    load_time_ms: 2.194\n",
      "    num_steps_sampled: 3570000\n",
      "    num_steps_trained: 3570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8297387361526489\n",
      "      kl: 0.006322108209133148\n",
      "      policy_loss: -0.0017073869239538908\n",
      "      total_loss: 92.78815460205078\n",
      "      vf_explained_var: 0.8712727427482605\n",
      "      vf_loss: 92.78984832763672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.034641724079847336\n",
      "      kl: 0.008716851472854614\n",
      "      policy_loss: -0.0011940357508137822\n",
      "      total_loss: 118.63480377197266\n",
      "      vf_explained_var: 0.8960995078086853\n",
      "      vf_loss: 118.63599395751953\n",
      "    sample_time_ms: 20129.376\n",
      "    update_time_ms: 7.911\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.49508549830485\n",
      "    rl_1: 97.04233020074214\n",
      "  time_since_restore: 8530.54113984108\n",
      "  time_this_iter_s: 23.190006256103516\n",
      "  time_total_s: 8530.54113984108\n",
      "  timestamp: 1550801949\n",
      "  timesteps_since_restore: 3570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3570000\n",
      "  training_iteration: 357\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8530 s, 357 iter, 3570000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-19-32\n",
      "  done: false\n",
      "  episode_len_mean: 91.4\n",
      "  episode_reward_max: 223.5020578627629\n",
      "  episode_reward_mean: 147.7666147635067\n",
      "  episode_reward_min: -157.0485034250374\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 36229\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.246\n",
      "    load_time_ms: 2.19\n",
      "    num_steps_sampled: 3580000\n",
      "    num_steps_trained: 3580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8647547960281372\n",
      "      kl: 0.008139115758240223\n",
      "      policy_loss: -0.002593677258118987\n",
      "      total_loss: 187.73516845703125\n",
      "      vf_explained_var: 0.777114987373352\n",
      "      vf_loss: 187.73777770996094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.10965617746114731\n",
      "      kl: 0.00948302075266838\n",
      "      policy_loss: -0.0003982821654062718\n",
      "      total_loss: 265.176513671875\n",
      "      vf_explained_var: 0.780737042427063\n",
      "      vf_loss: 265.1768493652344\n",
      "    sample_time_ms: 20098.347\n",
      "    update_time_ms: 7.714\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.078518278578294\n",
      "    rl_1: 89.68809648492838\n",
      "  time_since_restore: 8553.997287750244\n",
      "  time_this_iter_s: 23.45614790916443\n",
      "  time_total_s: 8553.997287750244\n",
      "  timestamp: 1550801972\n",
      "  timesteps_since_restore: 3580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3580000\n",
      "  training_iteration: 358\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8553 s, 358 iter, 3580000 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-19-56\n",
      "  done: false\n",
      "  episode_len_mean: 89.71428571428571\n",
      "  episode_reward_max: 222.07204389952565\n",
      "  episode_reward_mean: 169.5728227677655\n",
      "  episode_reward_min: -157.71584449447752\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 36341\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.957\n",
      "    load_time_ms: 2.229\n",
      "    num_steps_sampled: 3590000\n",
      "    num_steps_trained: 3590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.789088249206543\n",
      "      kl: 0.014625309035182\n",
      "      policy_loss: -0.005957959685474634\n",
      "      total_loss: 70.47856903076172\n",
      "      vf_explained_var: 0.8918440341949463\n",
      "      vf_loss: 70.48451232910156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.0296711977571249\n",
      "      kl: 0.011558451689779758\n",
      "      policy_loss: -0.0006276786443777382\n",
      "      total_loss: 100.22850036621094\n",
      "      vf_explained_var: 0.9025311470031738\n",
      "      vf_loss: 100.22913360595703\n",
      "    sample_time_ms: 20091.072\n",
      "    update_time_ms: 7.691\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.56823935923897\n",
      "    rl_1: 101.00458340852651\n",
      "  time_since_restore: 8577.143802165985\n",
      "  time_this_iter_s: 23.146514415740967\n",
      "  time_total_s: 8577.143802165985\n",
      "  timestamp: 1550801996\n",
      "  timesteps_since_restore: 3590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3590000\n",
      "  training_iteration: 359\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8577 s, 359 iter, 3590000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-20-19\n",
      "  done: false\n",
      "  episode_len_mean: 91.62385321100918\n",
      "  episode_reward_max: 221.9167510937725\n",
      "  episode_reward_mean: 163.93750793039766\n",
      "  episode_reward_min: -156.21565967833163\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 36450\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.172\n",
      "    load_time_ms: 2.206\n",
      "    num_steps_sampled: 3600000\n",
      "    num_steps_trained: 3600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8526279926300049\n",
      "      kl: 0.007506038993597031\n",
      "      policy_loss: -0.0019221729598939419\n",
      "      total_loss: 84.09172821044922\n",
      "      vf_explained_var: 0.8713836669921875\n",
      "      vf_loss: 84.09364318847656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.14576537907123566\n",
      "      kl: 0.016255363821983337\n",
      "      policy_loss: -0.0008867155411280692\n",
      "      total_loss: 118.46861267089844\n",
      "      vf_explained_var: 0.8858668804168701\n",
      "      vf_loss: 118.46949005126953\n",
      "    sample_time_ms: 20110.714\n",
      "    update_time_ms: 7.737\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.77711533683456\n",
      "    rl_1: 99.16039259356312\n",
      "  time_since_restore: 8600.209448814392\n",
      "  time_this_iter_s: 23.065646648406982\n",
      "  time_total_s: 8600.209448814392\n",
      "  timestamp: 1550802019\n",
      "  timesteps_since_restore: 3600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3600000\n",
      "  training_iteration: 360\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8600 s, 360 iter, 3600000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-20-43\n",
      "  done: false\n",
      "  episode_len_mean: 92.74766355140187\n",
      "  episode_reward_max: 217.09210129094237\n",
      "  episode_reward_mean: 166.62738327947068\n",
      "  episode_reward_min: -146.1239856541419\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 36557\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.491\n",
      "    load_time_ms: 2.256\n",
      "    num_steps_sampled: 3610000\n",
      "    num_steps_trained: 3610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8151922821998596\n",
      "      kl: 0.010733542032539845\n",
      "      policy_loss: -0.0040740035474300385\n",
      "      total_loss: 84.35131072998047\n",
      "      vf_explained_var: 0.86873459815979\n",
      "      vf_loss: 84.35538482666016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.11489101499319077\n",
      "      kl: 0.011476775631308556\n",
      "      policy_loss: -0.002689366228878498\n",
      "      total_loss: 109.04204559326172\n",
      "      vf_explained_var: 0.8926942348480225\n",
      "      vf_loss: 109.04473114013672\n",
      "    sample_time_ms: 20146.31\n",
      "    update_time_ms: 7.611\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.42216186932421\n",
      "    rl_1: 98.20522141014642\n",
      "  time_since_restore: 8623.951852083206\n",
      "  time_this_iter_s: 23.742403268814087\n",
      "  time_total_s: 8623.951852083206\n",
      "  timestamp: 1550802043\n",
      "  timesteps_since_restore: 3610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3610000\n",
      "  training_iteration: 361\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8623 s, 361 iter, 3610000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-21-06\n",
      "  done: false\n",
      "  episode_len_mean: 90.43243243243244\n",
      "  episode_reward_max: 221.57074181575163\n",
      "  episode_reward_mean: 162.28118761414348\n",
      "  episode_reward_min: -129.43165859056103\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 36668\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.811\n",
      "    load_time_ms: 2.372\n",
      "    num_steps_sampled: 3620000\n",
      "    num_steps_trained: 3620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7866462469100952\n",
      "      kl: 0.005132022779434919\n",
      "      policy_loss: -0.001291664899326861\n",
      "      total_loss: 58.43145751953125\n",
      "      vf_explained_var: 0.8994699120521545\n",
      "      vf_loss: 58.43274688720703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.06090111657977104\n",
      "      kl: 0.010734676383435726\n",
      "      policy_loss: -0.004639958962798119\n",
      "      total_loss: 85.41603088378906\n",
      "      vf_explained_var: 0.9163243174552917\n",
      "      vf_loss: 85.4206771850586\n",
      "    sample_time_ms: 20086.063\n",
      "    update_time_ms: 7.3\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.555935379389354\n",
      "    rl_1: 99.72525223475411\n",
      "  time_since_restore: 8646.931903600693\n",
      "  time_this_iter_s: 22.980051517486572\n",
      "  time_total_s: 8646.931903600693\n",
      "  timestamp: 1550802066\n",
      "  timesteps_since_restore: 3620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3620000\n",
      "  training_iteration: 362\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8646 s, 362 iter, 3620000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-21-29\n",
      "  done: false\n",
      "  episode_len_mean: 94.34905660377359\n",
      "  episode_reward_max: 220.70681403259985\n",
      "  episode_reward_mean: 171.50872953007948\n",
      "  episode_reward_min: -122.96074453007816\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 36774\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.692\n",
      "    load_time_ms: 2.358\n",
      "    num_steps_sampled: 3630000\n",
      "    num_steps_trained: 3630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8682830929756165\n",
      "      kl: 0.004847735166549683\n",
      "      policy_loss: -0.0025177926290780306\n",
      "      total_loss: 29.483577728271484\n",
      "      vf_explained_var: 0.9519606232643127\n",
      "      vf_loss: 29.486093521118164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.20755180716514587\n",
      "      kl: 0.00851450115442276\n",
      "      policy_loss: -0.0016513597220182419\n",
      "      total_loss: 34.559661865234375\n",
      "      vf_explained_var: 0.9671557545661926\n",
      "      vf_loss: 34.56131362915039\n",
      "    sample_time_ms: 20085.482\n",
      "    update_time_ms: 7.172\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.79483480843358\n",
      "    rl_1: 101.71389472164587\n",
      "  time_since_restore: 8670.303497314453\n",
      "  time_this_iter_s: 23.371593713760376\n",
      "  time_total_s: 8670.303497314453\n",
      "  timestamp: 1550802089\n",
      "  timesteps_since_restore: 3630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3630000\n",
      "  training_iteration: 363\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8670 s, 363 iter, 3630000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-21-52\n",
      "  done: false\n",
      "  episode_len_mean: 92.61111111111111\n",
      "  episode_reward_max: 219.46463900972822\n",
      "  episode_reward_mean: 158.40401919053863\n",
      "  episode_reward_min: -146.43566246750592\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 36882\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.526\n",
      "    load_time_ms: 2.396\n",
      "    num_steps_sampled: 3640000\n",
      "    num_steps_trained: 3640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7937811017036438\n",
      "      kl: 0.00884036161005497\n",
      "      policy_loss: -0.0015329448506236076\n",
      "      total_loss: 125.59025573730469\n",
      "      vf_explained_var: 0.8152249455451965\n",
      "      vf_loss: 125.591796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.14631731808185577\n",
      "      kl: 0.013649254105985165\n",
      "      policy_loss: -0.002536569954827428\n",
      "      total_loss: 169.19802856445312\n",
      "      vf_explained_var: 0.8458333015441895\n",
      "      vf_loss: 169.2005615234375\n",
      "    sample_time_ms: 20126.343\n",
      "    update_time_ms: 6.795\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.99658901317448\n",
      "    rl_1: 95.40743017736416\n",
      "  time_since_restore: 8693.602322101593\n",
      "  time_this_iter_s: 23.298824787139893\n",
      "  time_total_s: 8693.602322101593\n",
      "  timestamp: 1550802112\n",
      "  timesteps_since_restore: 3640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3640000\n",
      "  training_iteration: 364\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8693 s, 364 iter, 3640000 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-22-15\n",
      "  done: false\n",
      "  episode_len_mean: 89.63392857142857\n",
      "  episode_reward_max: 219.76302575003714\n",
      "  episode_reward_mean: 174.44912581040555\n",
      "  episode_reward_min: -130.29801348983884\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 36994\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.985\n",
      "    load_time_ms: 2.385\n",
      "    num_steps_sampled: 3650000\n",
      "    num_steps_trained: 3650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7229860424995422\n",
      "      kl: 0.00911990087479353\n",
      "      policy_loss: -0.004237710498273373\n",
      "      total_loss: 32.02930450439453\n",
      "      vf_explained_var: 0.9434638619422913\n",
      "      vf_loss: 32.03354263305664\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.040745045989751816\n",
      "      kl: 0.010440985672175884\n",
      "      policy_loss: -0.0008280776673927903\n",
      "      total_loss: 37.26363754272461\n",
      "      vf_explained_var: 0.9626322984695435\n",
      "      vf_loss: 37.264469146728516\n",
      "    sample_time_ms: 20008.509\n",
      "    update_time_ms: 7.136\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.45621490489783\n",
      "    rl_1: 102.99291090550776\n",
      "  time_since_restore: 8715.862256526947\n",
      "  time_this_iter_s: 22.259934425354004\n",
      "  time_total_s: 8715.862256526947\n",
      "  timestamp: 1550802135\n",
      "  timesteps_since_restore: 3650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3650000\n",
      "  training_iteration: 365\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8715 s, 365 iter, 3650000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-22-38\n",
      "  done: false\n",
      "  episode_len_mean: 93.88679245283019\n",
      "  episode_reward_max: 219.93483311628074\n",
      "  episode_reward_mean: 168.74523889525446\n",
      "  episode_reward_min: -120.81547997858763\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 37100\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.216\n",
      "    load_time_ms: 2.278\n",
      "    num_steps_sampled: 3660000\n",
      "    num_steps_trained: 3660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.776580810546875\n",
      "      kl: 0.005357037298381329\n",
      "      policy_loss: -0.0019108689157292247\n",
      "      total_loss: 45.134422302246094\n",
      "      vf_explained_var: 0.928485095500946\n",
      "      vf_loss: 45.13633728027344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1392027884721756\n",
      "      kl: 0.005823036190122366\n",
      "      policy_loss: -0.0010574173647910357\n",
      "      total_loss: 54.84332275390625\n",
      "      vf_explained_var: 0.9483648538589478\n",
      "      vf_loss: 54.844383239746094\n",
      "    sample_time_ms: 20027.681\n",
      "    update_time_ms: 7.155\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.18110675951682\n",
      "    rl_1: 100.56413213573758\n",
      "  time_since_restore: 8739.291143417358\n",
      "  time_this_iter_s: 23.428886890411377\n",
      "  time_total_s: 8739.291143417358\n",
      "  timestamp: 1550802158\n",
      "  timesteps_since_restore: 3660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3660000\n",
      "  training_iteration: 366\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8739 s, 366 iter, 3660000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-23-01\n",
      "  done: false\n",
      "  episode_len_mean: 89.1875\n",
      "  episode_reward_max: 225.43160647283526\n",
      "  episode_reward_mean: 170.71230531605428\n",
      "  episode_reward_min: -125.899870240506\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 37212\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.95\n",
      "    load_time_ms: 2.343\n",
      "    num_steps_sampled: 3670000\n",
      "    num_steps_trained: 3670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7072485089302063\n",
      "      kl: 0.014850017614662647\n",
      "      policy_loss: -0.007363608106970787\n",
      "      total_loss: 49.27475357055664\n",
      "      vf_explained_var: 0.9185655117034912\n",
      "      vf_loss: 49.2821159362793\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.020265724509954453\n",
      "      kl: 0.0075734020210802555\n",
      "      policy_loss: 5.6882985518313944e-05\n",
      "      total_loss: 64.52628326416016\n",
      "      vf_explained_var: 0.9396001100540161\n",
      "      vf_loss: 64.5262222290039\n",
      "    sample_time_ms: 20044.209\n",
      "    update_time_ms: 6.99\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.29995288082468\n",
      "    rl_1: 102.41235243522962\n",
      "  time_since_restore: 8762.60092329979\n",
      "  time_this_iter_s: 23.30977988243103\n",
      "  time_total_s: 8762.60092329979\n",
      "  timestamp: 1550802181\n",
      "  timesteps_since_restore: 3670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3670000\n",
      "  training_iteration: 367\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8762 s, 367 iter, 3670000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-23-25\n",
      "  done: false\n",
      "  episode_len_mean: 91.18181818181819\n",
      "  episode_reward_max: 213.59440202760945\n",
      "  episode_reward_mean: 171.40766590789752\n",
      "  episode_reward_min: -154.26423881990792\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 37322\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.802\n",
      "    load_time_ms: 2.355\n",
      "    num_steps_sampled: 3680000\n",
      "    num_steps_trained: 3680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7496868968009949\n",
      "      kl: 0.01108220312744379\n",
      "      policy_loss: -0.003753860481083393\n",
      "      total_loss: 27.979110717773438\n",
      "      vf_explained_var: 0.9486096501350403\n",
      "      vf_loss: 27.98285675048828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.018070906400680542\n",
      "      kl: 0.008027555420994759\n",
      "      policy_loss: -0.0013104556128382683\n",
      "      total_loss: 44.51106262207031\n",
      "      vf_explained_var: 0.9580298066139221\n",
      "      vf_loss: 44.51237106323242\n",
      "    sample_time_ms: 20031.577\n",
      "    update_time_ms: 7.084\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.48263927210417\n",
      "    rl_1: 104.92502663579334\n",
      "  time_since_restore: 8785.920996427536\n",
      "  time_this_iter_s: 23.320073127746582\n",
      "  time_total_s: 8785.920996427536\n",
      "  timestamp: 1550802205\n",
      "  timesteps_since_restore: 3680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3680000\n",
      "  training_iteration: 368\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8785 s, 368 iter, 3680000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-23-48\n",
      "  done: false\n",
      "  episode_len_mean: 90.19090909090909\n",
      "  episode_reward_max: 223.26066724263595\n",
      "  episode_reward_mean: 167.8098910737876\n",
      "  episode_reward_min: -158.91390987219583\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 37432\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.251\n",
      "    load_time_ms: 2.325\n",
      "    num_steps_sampled: 3690000\n",
      "    num_steps_trained: 3690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7647057771682739\n",
      "      kl: 0.012027231976389885\n",
      "      policy_loss: -0.00686149625107646\n",
      "      total_loss: 32.19609832763672\n",
      "      vf_explained_var: 0.9462273716926575\n",
      "      vf_loss: 32.20296096801758\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.08814690262079239\n",
      "      kl: 0.014179411344230175\n",
      "      policy_loss: -0.0033265165984630585\n",
      "      total_loss: 47.78462600708008\n",
      "      vf_explained_var: 0.9535656571388245\n",
      "      vf_loss: 47.7879524230957\n",
      "    sample_time_ms: 20042.219\n",
      "    update_time_ms: 7.167\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.36235887744581\n",
      "    rl_1: 99.44753219634178\n",
      "  time_since_restore: 8809.25807094574\n",
      "  time_this_iter_s: 23.337074518203735\n",
      "  time_total_s: 8809.25807094574\n",
      "  timestamp: 1550802228\n",
      "  timesteps_since_restore: 3690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3690000\n",
      "  training_iteration: 369\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8809 s, 369 iter, 3690000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-24-11\n",
      "  done: false\n",
      "  episode_len_mean: 91.52727272727273\n",
      "  episode_reward_max: 223.64063987127233\n",
      "  episode_reward_mean: 175.0166281687626\n",
      "  episode_reward_min: 126.1446778296198\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 37542\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.895\n",
      "    load_time_ms: 2.391\n",
      "    num_steps_sampled: 3700000\n",
      "    num_steps_trained: 3700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.760453999042511\n",
      "      kl: 0.008261550217866898\n",
      "      policy_loss: -0.004960278049111366\n",
      "      total_loss: 4.359848976135254\n",
      "      vf_explained_var: 0.9906589984893799\n",
      "      vf_loss: 4.364809989929199\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.04623125493526459\n",
      "      kl: 0.012541625648736954\n",
      "      policy_loss: 0.0017930695321410894\n",
      "      total_loss: 3.9267890453338623\n",
      "      vf_explained_var: 0.9957588315010071\n",
      "      vf_loss: 3.9249963760375977\n",
      "    sample_time_ms: 20011.29\n",
      "    update_time_ms: 7.534\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.85882014058528\n",
      "    rl_1: 105.15780802817731\n",
      "  time_since_restore: 8831.983683109283\n",
      "  time_this_iter_s: 22.7256121635437\n",
      "  time_total_s: 8831.983683109283\n",
      "  timestamp: 1550802251\n",
      "  timesteps_since_restore: 3700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3700000\n",
      "  training_iteration: 370\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8831 s, 370 iter, 3700000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-24-34\n",
      "  done: false\n",
      "  episode_len_mean: 91.2909090909091\n",
      "  episode_reward_max: 219.77915734938944\n",
      "  episode_reward_mean: 178.73013170072235\n",
      "  episode_reward_min: 139.82226224799092\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 37652\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.134\n",
      "    load_time_ms: 2.368\n",
      "    num_steps_sampled: 3710000\n",
      "    num_steps_trained: 3710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7415266036987305\n",
      "      kl: 0.00793767161667347\n",
      "      policy_loss: -0.004291804041713476\n",
      "      total_loss: 3.5786712169647217\n",
      "      vf_explained_var: 0.9932273626327515\n",
      "      vf_loss: 3.582963228225708\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.05537568777799606\n",
      "      kl: 0.010061209090054035\n",
      "      policy_loss: 0.003721345216035843\n",
      "      total_loss: 3.4338908195495605\n",
      "      vf_explained_var: 0.9963979721069336\n",
      "      vf_loss: 3.430169105529785\n",
      "    sample_time_ms: 19929.854\n",
      "    update_time_ms: 7.35\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.76733322940693\n",
      "    rl_1: 105.96279847131541\n",
      "  time_since_restore: 8854.863743782043\n",
      "  time_this_iter_s: 22.88006067276001\n",
      "  time_total_s: 8854.863743782043\n",
      "  timestamp: 1550802274\n",
      "  timesteps_since_restore: 3710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3710000\n",
      "  training_iteration: 371\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8854 s, 371 iter, 3710000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-24-57\n",
      "  done: false\n",
      "  episode_len_mean: 89.92792792792793\n",
      "  episode_reward_max: 215.62214766866273\n",
      "  episode_reward_mean: 161.99885330639384\n",
      "  episode_reward_min: -152.43669656610132\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 37763\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.458\n",
      "    load_time_ms: 2.246\n",
      "    num_steps_sampled: 3720000\n",
      "    num_steps_trained: 3720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7552191019058228\n",
      "      kl: 0.01200995221734047\n",
      "      policy_loss: -0.004473404958844185\n",
      "      total_loss: 114.59263610839844\n",
      "      vf_explained_var: 0.8187370300292969\n",
      "      vf_loss: 114.59709167480469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.02710776776075363\n",
      "      kl: 0.018146106973290443\n",
      "      policy_loss: 0.0009430600912310183\n",
      "      total_loss: 147.89553833007812\n",
      "      vf_explained_var: 0.8637611269950867\n",
      "      vf_loss: 147.89459228515625\n",
      "    sample_time_ms: 19961.657\n",
      "    update_time_ms: 7.48\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.81572126542277\n",
      "    rl_1: 98.18313204097107\n",
      "  time_since_restore: 8878.143286705017\n",
      "  time_this_iter_s: 23.279542922973633\n",
      "  time_total_s: 8878.143286705017\n",
      "  timestamp: 1550802297\n",
      "  timesteps_since_restore: 3720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3720000\n",
      "  training_iteration: 372\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8878 s, 372 iter, 3720000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-25-21\n",
      "  done: false\n",
      "  episode_len_mean: 91.70642201834862\n",
      "  episode_reward_max: 214.94866684516367\n",
      "  episode_reward_mean: 176.45353172319895\n",
      "  episode_reward_min: 122.10751377808153\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 37872\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.868\n",
      "    load_time_ms: 2.267\n",
      "    num_steps_sampled: 3730000\n",
      "    num_steps_trained: 3730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.73259037733078\n",
      "      kl: 0.003917773719877005\n",
      "      policy_loss: -0.002650720300152898\n",
      "      total_loss: 3.62752366065979\n",
      "      vf_explained_var: 0.9924616813659668\n",
      "      vf_loss: 3.630174398422241\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.03244183957576752\n",
      "      kl: 0.008936665952205658\n",
      "      policy_loss: -0.0017636420670896769\n",
      "      total_loss: 3.0061278343200684\n",
      "      vf_explained_var: 0.9966897368431091\n",
      "      vf_loss: 3.0078911781311035\n",
      "    sample_time_ms: 19958.152\n",
      "    update_time_ms: 7.791\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.05531254724497\n",
      "    rl_1: 105.39821917595401\n",
      "  time_since_restore: 8901.637585401535\n",
      "  time_this_iter_s: 23.494298696517944\n",
      "  time_total_s: 8901.637585401535\n",
      "  timestamp: 1550802321\n",
      "  timesteps_since_restore: 3730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3730000\n",
      "  training_iteration: 373\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8901 s, 373 iter, 3730000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-25-45\n",
      "  done: false\n",
      "  episode_len_mean: 91.01818181818182\n",
      "  episode_reward_max: 223.0415712567912\n",
      "  episode_reward_mean: 172.0365863414805\n",
      "  episode_reward_min: -155.117594050339\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 37982\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.963\n",
      "    load_time_ms: 2.229\n",
      "    num_steps_sampled: 3740000\n",
      "    num_steps_trained: 3740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7573410868644714\n",
      "      kl: 0.006449765060096979\n",
      "      policy_loss: -0.0021059857681393623\n",
      "      total_loss: 25.31364631652832\n",
      "      vf_explained_var: 0.9550420045852661\n",
      "      vf_loss: 25.31574821472168\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.009560978971421719\n",
      "      kl: 0.009785130620002747\n",
      "      policy_loss: -0.0022341401781886816\n",
      "      total_loss: 41.97859573364258\n",
      "      vf_explained_var: 0.9599557518959045\n",
      "      vf_loss: 41.980831146240234\n",
      "    sample_time_ms: 20002.518\n",
      "    update_time_ms: 7.965\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.41155956976478\n",
      "    rl_1: 103.62502677171575\n",
      "  time_since_restore: 8925.304701805115\n",
      "  time_this_iter_s: 23.667116403579712\n",
      "  time_total_s: 8925.304701805115\n",
      "  timestamp: 1550802345\n",
      "  timesteps_since_restore: 3740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3740000\n",
      "  training_iteration: 374\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8925 s, 374 iter, 3740000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-26-08\n",
      "  done: false\n",
      "  episode_len_mean: 89.67567567567568\n",
      "  episode_reward_max: 224.04998347773747\n",
      "  episode_reward_mean: 167.85972984462407\n",
      "  episode_reward_min: -141.02467080795407\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 38093\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.27\n",
      "    load_time_ms: 2.275\n",
      "    num_steps_sampled: 3750000\n",
      "    num_steps_trained: 3750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7508754730224609\n",
      "      kl: 0.006689560599625111\n",
      "      policy_loss: -0.0037755637895315886\n",
      "      total_loss: 112.24834442138672\n",
      "      vf_explained_var: 0.8367270231246948\n",
      "      vf_loss: 112.25214385986328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.02527179755270481\n",
      "      kl: 0.007438075728714466\n",
      "      policy_loss: -0.0005519420956261456\n",
      "      total_loss: 146.06846618652344\n",
      "      vf_explained_var: 0.8742111325263977\n",
      "      vf_loss: 146.0690155029297\n",
      "    sample_time_ms: 20118.644\n",
      "    update_time_ms: 7.512\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.95915413056387\n",
      "    rl_1: 99.90057571406021\n",
      "  time_since_restore: 8948.734753370285\n",
      "  time_this_iter_s: 23.430051565170288\n",
      "  time_total_s: 8948.734753370285\n",
      "  timestamp: 1550802368\n",
      "  timesteps_since_restore: 3750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3750000\n",
      "  training_iteration: 375\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8948 s, 375 iter, 3750000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-26-31\n",
      "  done: false\n",
      "  episode_len_mean: 90.57657657657657\n",
      "  episode_reward_max: 220.92028678453323\n",
      "  episode_reward_mean: 177.49651675235168\n",
      "  episode_reward_min: 138.54193554507037\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 38204\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.142\n",
      "    load_time_ms: 2.305\n",
      "    num_steps_sampled: 3760000\n",
      "    num_steps_trained: 3760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7267950773239136\n",
      "      kl: 0.005724810063838959\n",
      "      policy_loss: -0.004056623205542564\n",
      "      total_loss: 3.4226973056793213\n",
      "      vf_explained_var: 0.9930352568626404\n",
      "      vf_loss: 3.4267542362213135\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.015224087052047253\n",
      "      kl: 0.011255225166678429\n",
      "      policy_loss: 0.004372033290565014\n",
      "      total_loss: 3.321021556854248\n",
      "      vf_explained_var: 0.9964324831962585\n",
      "      vf_loss: 3.3166496753692627\n",
      "    sample_time_ms: 20103.1\n",
      "    update_time_ms: 7.624\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.5253757667132\n",
      "    rl_1: 104.97114098563853\n",
      "  time_since_restore: 8971.959478139877\n",
      "  time_this_iter_s: 23.224724769592285\n",
      "  time_total_s: 8971.959478139877\n",
      "  timestamp: 1550802391\n",
      "  timesteps_since_restore: 3760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3760000\n",
      "  training_iteration: 376\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8971 s, 376 iter, 3760000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-26-55\n",
      "  done: false\n",
      "  episode_len_mean: 91.04545454545455\n",
      "  episode_reward_max: 224.7259328505075\n",
      "  episode_reward_mean: 171.44212944694667\n",
      "  episode_reward_min: -106.13739026201557\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 38314\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.605\n",
      "    load_time_ms: 2.239\n",
      "    num_steps_sampled: 3770000\n",
      "    num_steps_trained: 3770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7512595057487488\n",
      "      kl: 0.008609501644968987\n",
      "      policy_loss: -0.003798816353082657\n",
      "      total_loss: 10.527827262878418\n",
      "      vf_explained_var: 0.9810541868209839\n",
      "      vf_loss: 10.531627655029297\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.011128307320177555\n",
      "      kl: 0.0073112985119223595\n",
      "      policy_loss: -0.0007563040708191693\n",
      "      total_loss: 12.326896667480469\n",
      "      vf_explained_var: 0.9887124300003052\n",
      "      vf_loss: 12.327651977539062\n",
      "    sample_time_ms: 20117.5\n",
      "    update_time_ms: 7.682\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.09105294844696\n",
      "    rl_1: 103.35107649849972\n",
      "  time_since_restore: 8995.399996995926\n",
      "  time_this_iter_s: 23.440518856048584\n",
      "  time_total_s: 8995.399996995926\n",
      "  timestamp: 1550802415\n",
      "  timesteps_since_restore: 3770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3770000\n",
      "  training_iteration: 377\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 8995 s, 377 iter, 3770000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-27-18\n",
      "  done: false\n",
      "  episode_len_mean: 93.60377358490567\n",
      "  episode_reward_max: 223.40737696044866\n",
      "  episode_reward_mean: 160.9787587890292\n",
      "  episode_reward_min: -164.18162749886824\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 38420\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.664\n",
      "    load_time_ms: 2.307\n",
      "    num_steps_sampled: 3780000\n",
      "    num_steps_trained: 3780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.794166088104248\n",
      "      kl: 0.01545995194464922\n",
      "      policy_loss: -0.005450649186968803\n",
      "      total_loss: 133.48924255371094\n",
      "      vf_explained_var: 0.8195785880088806\n",
      "      vf_loss: 133.49472045898438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.06919777393341064\n",
      "      kl: 0.010064282454550266\n",
      "      policy_loss: -0.0004401988408062607\n",
      "      total_loss: 191.9981231689453\n",
      "      vf_explained_var: 0.8391940593719482\n",
      "      vf_loss: 191.99856567382812\n",
      "    sample_time_ms: 20078.112\n",
      "    update_time_ms: 7.73\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.50574971139732\n",
      "    rl_1: 97.47300907763181\n",
      "  time_since_restore: 9018.337431430817\n",
      "  time_this_iter_s: 22.937434434890747\n",
      "  time_total_s: 9018.337431430817\n",
      "  timestamp: 1550802438\n",
      "  timesteps_since_restore: 3780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3780000\n",
      "  training_iteration: 378\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9018 s, 378 iter, 3780000 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-27-41\n",
      "  done: false\n",
      "  episode_len_mean: 95.24761904761905\n",
      "  episode_reward_max: 224.48119932746815\n",
      "  episode_reward_mean: 172.7343057917683\n",
      "  episode_reward_min: 10.402874013598371\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 38525\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.97\n",
      "    load_time_ms: 2.339\n",
      "    num_steps_sampled: 3790000\n",
      "    num_steps_trained: 3790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7823978066444397\n",
      "      kl: 0.0049646575935184956\n",
      "      policy_loss: -0.002853988902643323\n",
      "      total_loss: 5.118954181671143\n",
      "      vf_explained_var: 0.9904298782348633\n",
      "      vf_loss: 5.121808052062988\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.061751775443553925\n",
      "      kl: 0.009501720778644085\n",
      "      policy_loss: -0.0014078820822760463\n",
      "      total_loss: 4.902364253997803\n",
      "      vf_explained_var: 0.9949066042900085\n",
      "      vf_loss: 4.903772830963135\n",
      "    sample_time_ms: 20082.873\n",
      "    update_time_ms: 7.733\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.23776940319466\n",
      "    rl_1: 104.4965363885736\n",
      "  time_since_restore: 9041.614473819733\n",
      "  time_this_iter_s: 23.277042388916016\n",
      "  time_total_s: 9041.614473819733\n",
      "  timestamp: 1550802461\n",
      "  timesteps_since_restore: 3790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3790000\n",
      "  training_iteration: 379\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9041 s, 379 iter, 3790000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-28-04\n",
      "  done: false\n",
      "  episode_len_mean: 91.19090909090909\n",
      "  episode_reward_max: 215.62344958537236\n",
      "  episode_reward_mean: 173.15056730162294\n",
      "  episode_reward_min: 126.64993645419118\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 38635\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.777\n",
      "    load_time_ms: 2.311\n",
      "    num_steps_sampled: 3800000\n",
      "    num_steps_trained: 3800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.763934314250946\n",
      "      kl: 0.007054861634969711\n",
      "      policy_loss: -0.0033007990568876266\n",
      "      total_loss: 4.419835090637207\n",
      "      vf_explained_var: 0.9913668632507324\n",
      "      vf_loss: 4.423135757446289\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.03191807121038437\n",
      "      kl: 0.011106641963124275\n",
      "      policy_loss: 0.0003273342445027083\n",
      "      total_loss: 3.2897987365722656\n",
      "      vf_explained_var: 0.9963832497596741\n",
      "      vf_loss: 3.289471387863159\n",
      "    sample_time_ms: 20149.656\n",
      "    update_time_ms: 7.38\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.63090023850623\n",
      "    rl_1: 103.51966706311667\n",
      "  time_since_restore: 9065.015696048737\n",
      "  time_this_iter_s: 23.401222229003906\n",
      "  time_total_s: 9065.015696048737\n",
      "  timestamp: 1550802484\n",
      "  timesteps_since_restore: 3800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3800000\n",
      "  training_iteration: 380\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9065 s, 380 iter, 3800000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-28-28\n",
      "  done: false\n",
      "  episode_len_mean: 89.90090090090091\n",
      "  episode_reward_max: 215.00333843494\n",
      "  episode_reward_mean: 172.54254338933802\n",
      "  episode_reward_min: 137.14535054330318\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 38746\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.506\n",
      "    load_time_ms: 2.363\n",
      "    num_steps_sampled: 3810000\n",
      "    num_steps_trained: 3810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7450059056282043\n",
      "      kl: 0.007495896890759468\n",
      "      policy_loss: -0.003028539242222905\n",
      "      total_loss: 4.331373691558838\n",
      "      vf_explained_var: 0.9908279180526733\n",
      "      vf_loss: 4.334401607513428\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.0007023561629466712\n",
      "      kl: 0.011405476368963718\n",
      "      policy_loss: 8.11664795037359e-05\n",
      "      total_loss: 3.0406599044799805\n",
      "      vf_explained_var: 0.9967808127403259\n",
      "      vf_loss: 3.040578603744507\n",
      "    sample_time_ms: 20197.566\n",
      "    update_time_ms: 7.44\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.66053490117068\n",
      "    rl_1: 103.88200848816739\n",
      "  time_since_restore: 9088.373012065887\n",
      "  time_this_iter_s: 23.35731601715088\n",
      "  time_total_s: 9088.373012065887\n",
      "  timestamp: 1550802508\n",
      "  timesteps_since_restore: 3810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3810000\n",
      "  training_iteration: 381\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9088 s, 381 iter, 3810000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-28-52\n",
      "  done: false\n",
      "  episode_len_mean: 90.61261261261261\n",
      "  episode_reward_max: 221.0697021622713\n",
      "  episode_reward_mean: 169.26923002069196\n",
      "  episode_reward_min: -169.6919590234258\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 38857\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.228\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 3820000\n",
      "    num_steps_trained: 3820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.730116069316864\n",
      "      kl: 0.009676420129835606\n",
      "      policy_loss: -0.003291069995611906\n",
      "      total_loss: 61.54776382446289\n",
      "      vf_explained_var: 0.9083495736122131\n",
      "      vf_loss: 61.551055908203125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.040616475045681\n",
      "      kl: 0.016551868990063667\n",
      "      policy_loss: -0.002868099370971322\n",
      "      total_loss: 89.33031463623047\n",
      "      vf_explained_var: 0.9267613291740417\n",
      "      vf_loss: 89.33317565917969\n",
      "    sample_time_ms: 20226.335\n",
      "    update_time_ms: 7.406\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.91203235139712\n",
      "    rl_1: 102.35719766929486\n",
      "  time_since_restore: 9112.148841381073\n",
      "  time_this_iter_s: 23.775829315185547\n",
      "  time_total_s: 9112.148841381073\n",
      "  timestamp: 1550802532\n",
      "  timesteps_since_restore: 3820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3820000\n",
      "  training_iteration: 382\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9112 s, 382 iter, 3820000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-29-15\n",
      "  done: false\n",
      "  episode_len_mean: 90.08181818181818\n",
      "  episode_reward_max: 226.41584809590202\n",
      "  episode_reward_mean: 170.4437613831425\n",
      "  episode_reward_min: -149.5662474260518\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 38967\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.488\n",
      "    load_time_ms: 2.361\n",
      "    num_steps_sampled: 3830000\n",
      "    num_steps_trained: 3830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7032658457756042\n",
      "      kl: 0.007889256812632084\n",
      "      policy_loss: -0.004422204568982124\n",
      "      total_loss: 35.42122268676758\n",
      "      vf_explained_var: 0.9422441124916077\n",
      "      vf_loss: 35.4256477355957\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.03303368762135506\n",
      "      kl: 0.020557494834065437\n",
      "      policy_loss: 0.0018887401092797518\n",
      "      total_loss: 42.95179748535156\n",
      "      vf_explained_var: 0.9601671099662781\n",
      "      vf_loss: 42.94990921020508\n",
      "    sample_time_ms: 20208.002\n",
      "    update_time_ms: 7.248\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.74660102611745\n",
      "    rl_1: 101.69716035702497\n",
      "  time_since_restore: 9135.33283162117\n",
      "  time_this_iter_s: 23.183990240097046\n",
      "  time_total_s: 9135.33283162117\n",
      "  timestamp: 1550802555\n",
      "  timesteps_since_restore: 3830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3830000\n",
      "  training_iteration: 383\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9135 s, 383 iter, 3830000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-29-39\n",
      "  done: false\n",
      "  episode_len_mean: 90.58558558558559\n",
      "  episode_reward_max: 219.26532104786273\n",
      "  episode_reward_mean: 168.28143463846928\n",
      "  episode_reward_min: -127.8376803000628\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 39078\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.025\n",
      "    load_time_ms: 2.362\n",
      "    num_steps_sampled: 3840000\n",
      "    num_steps_trained: 3840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7368165850639343\n",
      "      kl: 0.009362149983644485\n",
      "      policy_loss: -0.004788575228303671\n",
      "      total_loss: 53.159141540527344\n",
      "      vf_explained_var: 0.9209462404251099\n",
      "      vf_loss: 53.163936614990234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.015702741220593452\n",
      "      kl: 0.01508785504847765\n",
      "      policy_loss: -0.002428443403914571\n",
      "      total_loss: 61.33473587036133\n",
      "      vf_explained_var: 0.9465173482894897\n",
      "      vf_loss: 61.337158203125\n",
      "    sample_time_ms: 20196.637\n",
      "    update_time_ms: 7.232\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.98884918442388\n",
      "    rl_1: 100.29258545404538\n",
      "  time_since_restore: 9158.881397247314\n",
      "  time_this_iter_s: 23.54856562614441\n",
      "  time_total_s: 9158.881397247314\n",
      "  timestamp: 1550802579\n",
      "  timesteps_since_restore: 3840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3840000\n",
      "  training_iteration: 384\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9158 s, 384 iter, 3840000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-30-02\n",
      "  done: false\n",
      "  episode_len_mean: 89.89189189189189\n",
      "  episode_reward_max: 223.6493767166172\n",
      "  episode_reward_mean: 178.55720923522787\n",
      "  episode_reward_min: 135.95643987496277\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 39189\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.131\n",
      "    load_time_ms: 2.348\n",
      "    num_steps_sampled: 3850000\n",
      "    num_steps_trained: 3850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6763870716094971\n",
      "      kl: 0.011580422520637512\n",
      "      policy_loss: -0.004997448064386845\n",
      "      total_loss: 3.9530937671661377\n",
      "      vf_explained_var: 0.99217289686203\n",
      "      vf_loss: 3.9580914974212646\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.054182518273591995\n",
      "      kl: 0.0070160687901079655\n",
      "      policy_loss: 0.00023564085131511092\n",
      "      total_loss: 3.00274920463562\n",
      "      vf_explained_var: 0.9967533946037292\n",
      "      vf_loss: 3.002513885498047\n",
      "    sample_time_ms: 20160.608\n",
      "    update_time_ms: 7.405\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.44202280005457\n",
      "    rl_1: 105.11518643517333\n",
      "  time_since_restore: 9181.92276096344\n",
      "  time_this_iter_s: 23.04136371612549\n",
      "  time_total_s: 9181.92276096344\n",
      "  timestamp: 1550802602\n",
      "  timesteps_since_restore: 3850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3850000\n",
      "  training_iteration: 385\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9181 s, 385 iter, 3850000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-30-24\n",
      "  done: false\n",
      "  episode_len_mean: 91.41818181818182\n",
      "  episode_reward_max: 220.58349267170118\n",
      "  episode_reward_mean: 179.02347057395977\n",
      "  episode_reward_min: 140.64953719484686\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 39299\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.525\n",
      "    load_time_ms: 2.387\n",
      "    num_steps_sampled: 3860000\n",
      "    num_steps_trained: 3860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6945413947105408\n",
      "      kl: 0.01105921808630228\n",
      "      policy_loss: -0.006613127887248993\n",
      "      total_loss: 3.7542920112609863\n",
      "      vf_explained_var: 0.9925642609596252\n",
      "      vf_loss: 3.760904550552368\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.013519196771085262\n",
      "      kl: 0.009284774772822857\n",
      "      policy_loss: -0.0006499965093098581\n",
      "      total_loss: 3.0871429443359375\n",
      "      vf_explained_var: 0.9967941641807556\n",
      "      vf_loss: 3.0877931118011475\n",
      "    sample_time_ms: 20121.898\n",
      "    update_time_ms: 7.235\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.71970234355908\n",
      "    rl_1: 106.30376823040066\n",
      "  time_since_restore: 9204.741555452347\n",
      "  time_this_iter_s: 22.81879448890686\n",
      "  time_total_s: 9204.741555452347\n",
      "  timestamp: 1550802624\n",
      "  timesteps_since_restore: 3860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3860000\n",
      "  training_iteration: 386\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9204 s, 386 iter, 3860000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-30-47\n",
      "  done: false\n",
      "  episode_len_mean: 90.77272727272727\n",
      "  episode_reward_max: 221.87464025398035\n",
      "  episode_reward_mean: 179.75688841268777\n",
      "  episode_reward_min: 141.45867413844834\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 39409\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.826\n",
      "    load_time_ms: 2.465\n",
      "    num_steps_sampled: 3870000\n",
      "    num_steps_trained: 3870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6550843119621277\n",
      "      kl: 0.008303819224238396\n",
      "      policy_loss: -0.004861773457378149\n",
      "      total_loss: 3.0581095218658447\n",
      "      vf_explained_var: 0.9942399263381958\n",
      "      vf_loss: 3.062971830368042\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.04163917899131775\n",
      "      kl: 0.010045474395155907\n",
      "      policy_loss: -0.0006865074974484742\n",
      "      total_loss: 2.3599541187286377\n",
      "      vf_explained_var: 0.9975625276565552\n",
      "      vf_loss: 2.360640525817871\n",
      "    sample_time_ms: 20064.853\n",
      "    update_time_ms: 7.419\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.9588605649872\n",
      "    rl_1: 105.79802784770055\n",
      "  time_since_restore: 9227.65001630783\n",
      "  time_this_iter_s: 22.90846085548401\n",
      "  time_total_s: 9227.65001630783\n",
      "  timestamp: 1550802647\n",
      "  timesteps_since_restore: 3870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3870000\n",
      "  training_iteration: 387\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9227 s, 387 iter, 3870000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-31-11\n",
      "  done: false\n",
      "  episode_len_mean: 91.26605504587155\n",
      "  episode_reward_max: 220.37432318773196\n",
      "  episode_reward_mean: 176.8285800964232\n",
      "  episode_reward_min: -161.63772123647016\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 39518\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.773\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 3880000\n",
      "    num_steps_trained: 3880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.679779589176178\n",
      "      kl: 0.00994968507438898\n",
      "      policy_loss: -0.0045744748786091805\n",
      "      total_loss: 32.46357727050781\n",
      "      vf_explained_var: 0.9449759125709534\n",
      "      vf_loss: 32.46814727783203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.036426518112421036\n",
      "      kl: 0.05114797130227089\n",
      "      policy_loss: 0.0013637188822031021\n",
      "      total_loss: 38.87266159057617\n",
      "      vf_explained_var: 0.9640623331069946\n",
      "      vf_loss: 38.87130355834961\n",
      "    sample_time_ms: 20106.969\n",
      "    update_time_ms: 7.433\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.73089533730244\n",
      "    rl_1: 105.09768475912075\n",
      "  time_since_restore: 9251.004984378815\n",
      "  time_this_iter_s: 23.354968070983887\n",
      "  time_total_s: 9251.004984378815\n",
      "  timestamp: 1550802671\n",
      "  timesteps_since_restore: 3880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3880000\n",
      "  training_iteration: 388\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9251 s, 388 iter, 3880000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-31-34\n",
      "  done: false\n",
      "  episode_len_mean: 90.72972972972973\n",
      "  episode_reward_max: 221.72010344272604\n",
      "  episode_reward_mean: 171.39865198483574\n",
      "  episode_reward_min: -156.80034302862936\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 39629\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.815\n",
      "    load_time_ms: 2.442\n",
      "    num_steps_sampled: 3890000\n",
      "    num_steps_trained: 3890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6670181751251221\n",
      "      kl: 0.006943848449736834\n",
      "      policy_loss: -0.0026047667488455772\n",
      "      total_loss: 29.92854118347168\n",
      "      vf_explained_var: 0.9520063996315002\n",
      "      vf_loss: 29.9311466217041\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0857592299580574\n",
      "      kl: 0.009265880100429058\n",
      "      policy_loss: -0.00014993791410233825\n",
      "      total_loss: 48.82649612426758\n",
      "      vf_explained_var: 0.9555381536483765\n",
      "      vf_loss: 48.8266487121582\n",
      "    sample_time_ms: 20087.29\n",
      "    update_time_ms: 8.006\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.63357098263796\n",
      "    rl_1: 102.76508100219777\n",
      "  time_since_restore: 9274.110197544098\n",
      "  time_this_iter_s: 23.105213165283203\n",
      "  time_total_s: 9274.110197544098\n",
      "  timestamp: 1550802694\n",
      "  timesteps_since_restore: 3890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3890000\n",
      "  training_iteration: 389\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9274 s, 389 iter, 3890000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-31-57\n",
      "  done: false\n",
      "  episode_len_mean: 90.26126126126127\n",
      "  episode_reward_max: 222.27175108318525\n",
      "  episode_reward_mean: 173.85509371216835\n",
      "  episode_reward_min: -152.89136950361365\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 39740\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.766\n",
      "    load_time_ms: 2.423\n",
      "    num_steps_sampled: 3900000\n",
      "    num_steps_trained: 3900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6778016090393066\n",
      "      kl: 0.010861286893486977\n",
      "      policy_loss: -0.004668664652854204\n",
      "      total_loss: 33.616878509521484\n",
      "      vf_explained_var: 0.9420603513717651\n",
      "      vf_loss: 33.621543884277344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07290895283222198\n",
      "      kl: 0.01992703601717949\n",
      "      policy_loss: 0.000625109241809696\n",
      "      total_loss: 41.99649429321289\n",
      "      vf_explained_var: 0.9599021673202515\n",
      "      vf_loss: 41.99587631225586\n",
      "    sample_time_ms: 20082.537\n",
      "    update_time_ms: 7.924\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.9389558550195\n",
      "    rl_1: 103.91613785714891\n",
      "  time_since_restore: 9297.46900177002\n",
      "  time_this_iter_s: 23.35880422592163\n",
      "  time_total_s: 9297.46900177002\n",
      "  timestamp: 1550802717\n",
      "  timesteps_since_restore: 3900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3900000\n",
      "  training_iteration: 390\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9297 s, 390 iter, 3900000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-32-21\n",
      "  done: false\n",
      "  episode_len_mean: 90.64545454545454\n",
      "  episode_reward_max: 223.1382505877512\n",
      "  episode_reward_mean: 173.84643862157813\n",
      "  episode_reward_min: 141.74250548258246\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 39850\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3159.381\n",
      "    load_time_ms: 2.379\n",
      "    num_steps_sampled: 3910000\n",
      "    num_steps_trained: 3910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6745370030403137\n",
      "      kl: 0.007436675019562244\n",
      "      policy_loss: -0.004052834119647741\n",
      "      total_loss: 3.3257129192352295\n",
      "      vf_explained_var: 0.9930235743522644\n",
      "      vf_loss: 3.329765558242798\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08039166778326035\n",
      "      kl: 0.013937714509665966\n",
      "      policy_loss: -0.001063354080542922\n",
      "      total_loss: 2.4852702617645264\n",
      "      vf_explained_var: 0.9973267912864685\n",
      "      vf_loss: 2.4863336086273193\n",
      "    sample_time_ms: 20044.793\n",
      "    update_time_ms: 8.183\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.23339178088857\n",
      "    rl_1: 103.61304684068958\n",
      "  time_since_restore: 9320.695427894592\n",
      "  time_this_iter_s: 23.226426124572754\n",
      "  time_total_s: 9320.695427894592\n",
      "  timestamp: 1550802741\n",
      "  timesteps_since_restore: 3910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3910000\n",
      "  training_iteration: 391\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9320 s, 391 iter, 3910000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-32-44\n",
      "  done: false\n",
      "  episode_len_mean: 90.94545454545455\n",
      "  episode_reward_max: 219.93876795537165\n",
      "  episode_reward_mean: 173.96915742947365\n",
      "  episode_reward_min: -151.34113716531334\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 39960\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.188\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 3920000\n",
      "    num_steps_trained: 3920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6356253027915955\n",
      "      kl: 0.014317410066723824\n",
      "      policy_loss: -0.002906203968450427\n",
      "      total_loss: 31.50019645690918\n",
      "      vf_explained_var: 0.9525172114372253\n",
      "      vf_loss: 31.503095626831055\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12246471643447876\n",
      "      kl: 0.01728801615536213\n",
      "      policy_loss: 0.0011985948076471686\n",
      "      total_loss: 47.64950942993164\n",
      "      vf_explained_var: 0.9571115374565125\n",
      "      vf_loss: 47.64830780029297\n",
      "    sample_time_ms: 19997.222\n",
      "    update_time_ms: 8.154\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.99291602604042\n",
      "    rl_1: 103.97624140343325\n",
      "  time_since_restore: 9343.82578587532\n",
      "  time_this_iter_s: 23.13035798072815\n",
      "  time_total_s: 9343.82578587532\n",
      "  timestamp: 1550802764\n",
      "  timesteps_since_restore: 3920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3920000\n",
      "  training_iteration: 392\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9343 s, 392 iter, 3920000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-33-07\n",
      "  done: false\n",
      "  episode_len_mean: 90.8\n",
      "  episode_reward_max: 222.09697872856225\n",
      "  episode_reward_mean: 177.19666617263755\n",
      "  episode_reward_min: 137.82391622304633\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 40070\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.684\n",
      "    load_time_ms: 2.436\n",
      "    num_steps_sampled: 3930000\n",
      "    num_steps_trained: 3930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6189603805541992\n",
      "      kl: 0.010104376822710037\n",
      "      policy_loss: -0.003622445510700345\n",
      "      total_loss: 3.3837249279022217\n",
      "      vf_explained_var: 0.9933009743690491\n",
      "      vf_loss: 3.387347459793091\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07178051769733429\n",
      "      kl: 0.015326025895774364\n",
      "      policy_loss: -0.0017354299779981375\n",
      "      total_loss: 2.5708658695220947\n",
      "      vf_explained_var: 0.9971452355384827\n",
      "      vf_loss: 2.572601795196533\n",
      "    sample_time_ms: 19997.097\n",
      "    update_time_ms: 8.187\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.79002979086543\n",
      "    rl_1: 103.40663638177207\n",
      "  time_since_restore: 9366.993319749832\n",
      "  time_this_iter_s: 23.16753387451172\n",
      "  time_total_s: 9366.993319749832\n",
      "  timestamp: 1550802787\n",
      "  timesteps_since_restore: 3930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3930000\n",
      "  training_iteration: 393\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9366 s, 393 iter, 3930000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-33-30\n",
      "  done: false\n",
      "  episode_len_mean: 90.78899082568807\n",
      "  episode_reward_max: 226.46288268848627\n",
      "  episode_reward_mean: 167.50650311399946\n",
      "  episode_reward_min: -157.80946238996745\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 40179\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.65\n",
      "    load_time_ms: 2.506\n",
      "    num_steps_sampled: 3940000\n",
      "    num_steps_trained: 3940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6636098027229309\n",
      "      kl: 0.011387558653950691\n",
      "      policy_loss: -0.004556708037853241\n",
      "      total_loss: 60.9318962097168\n",
      "      vf_explained_var: 0.9074697494506836\n",
      "      vf_loss: 60.93644714355469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11150680482387543\n",
      "      kl: 0.027692466974258423\n",
      "      policy_loss: 0.00318170920945704\n",
      "      total_loss: 76.47923278808594\n",
      "      vf_explained_var: 0.9341018795967102\n",
      "      vf_loss: 76.4760513305664\n",
      "    sample_time_ms: 19977.641\n",
      "    update_time_ms: 8.157\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.5757007638062\n",
      "    rl_1: 99.9308023501932\n",
      "  time_since_restore: 9390.329916238785\n",
      "  time_this_iter_s: 23.336596488952637\n",
      "  time_total_s: 9390.329916238785\n",
      "  timestamp: 1550802810\n",
      "  timesteps_since_restore: 3940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3940000\n",
      "  training_iteration: 394\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9390 s, 394 iter, 3940000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-33-54\n",
      "  done: false\n",
      "  episode_len_mean: 95.15094339622641\n",
      "  episode_reward_max: 217.63571481362277\n",
      "  episode_reward_mean: 169.3581843330206\n",
      "  episode_reward_min: -149.21330104005722\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 40285\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.677\n",
      "    load_time_ms: 2.473\n",
      "    num_steps_sampled: 3950000\n",
      "    num_steps_trained: 3950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.733020007610321\n",
      "      kl: 0.013373903930187225\n",
      "      policy_loss: -0.005012885667383671\n",
      "      total_loss: 64.88371276855469\n",
      "      vf_explained_var: 0.9017961025238037\n",
      "      vf_loss: 64.88872528076172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.010343844071030617\n",
      "      kl: 0.016781941056251526\n",
      "      policy_loss: -0.001449508941732347\n",
      "      total_loss: 77.10147857666016\n",
      "      vf_explained_var: 0.9325611591339111\n",
      "      vf_loss: 77.1029281616211\n",
      "    sample_time_ms: 19983.053\n",
      "    update_time_ms: 7.977\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.03578747255008\n",
      "    rl_1: 101.32239686047053\n",
      "  time_since_restore: 9413.452680110931\n",
      "  time_this_iter_s: 23.122763872146606\n",
      "  time_total_s: 9413.452680110931\n",
      "  timestamp: 1550802834\n",
      "  timesteps_since_restore: 3950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3950000\n",
      "  training_iteration: 395\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9413 s, 395 iter, 3950000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-34-17\n",
      "  done: false\n",
      "  episode_len_mean: 90.50909090909092\n",
      "  episode_reward_max: 221.16180474750692\n",
      "  episode_reward_mean: 176.7500324714728\n",
      "  episode_reward_min: -58.26308967072305\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 40395\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.769\n",
      "    load_time_ms: 2.397\n",
      "    num_steps_sampled: 3960000\n",
      "    num_steps_trained: 3960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6807502508163452\n",
      "      kl: 0.0073884520679712296\n",
      "      policy_loss: -0.0026158993132412434\n",
      "      total_loss: 8.120526313781738\n",
      "      vf_explained_var: 0.9863279461860657\n",
      "      vf_loss: 8.12314224243164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.060184724628925323\n",
      "      kl: 0.010571258142590523\n",
      "      policy_loss: -0.003862063167616725\n",
      "      total_loss: 9.42629337310791\n",
      "      vf_explained_var: 0.9907652735710144\n",
      "      vf_loss: 9.430156707763672\n",
      "    sample_time_ms: 20027.029\n",
      "    update_time_ms: 7.971\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.04612108836366\n",
      "    rl_1: 103.70391138310912\n",
      "  time_since_restore: 9436.749946117401\n",
      "  time_this_iter_s: 23.297266006469727\n",
      "  time_total_s: 9436.749946117401\n",
      "  timestamp: 1550802857\n",
      "  timesteps_since_restore: 3960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3960000\n",
      "  training_iteration: 396\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9436 s, 396 iter, 3960000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-34-41\n",
      "  done: false\n",
      "  episode_len_mean: 90.46846846846847\n",
      "  episode_reward_max: 222.59493973103147\n",
      "  episode_reward_mean: 179.94235077541973\n",
      "  episode_reward_min: 138.16355897989763\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 40506\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3156.607\n",
      "    load_time_ms: 2.409\n",
      "    num_steps_sampled: 3970000\n",
      "    num_steps_trained: 3970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6623684167861938\n",
      "      kl: 0.007744230329990387\n",
      "      policy_loss: -0.0023294794373214245\n",
      "      total_loss: 3.30084228515625\n",
      "      vf_explained_var: 0.9936718344688416\n",
      "      vf_loss: 3.3031718730926514\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11853945255279541\n",
      "      kl: 0.01302068680524826\n",
      "      policy_loss: -0.001426058355718851\n",
      "      total_loss: 3.247220277786255\n",
      "      vf_explained_var: 0.9966546893119812\n",
      "      vf_loss: 3.2486462593078613\n",
      "    sample_time_ms: 20102.691\n",
      "    update_time_ms: 7.74\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.91278234076124\n",
      "    rl_1: 106.02956843465849\n",
      "  time_since_restore: 9460.519715309143\n",
      "  time_this_iter_s: 23.769769191741943\n",
      "  time_total_s: 9460.519715309143\n",
      "  timestamp: 1550802881\n",
      "  timesteps_since_restore: 3970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3970000\n",
      "  training_iteration: 397\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9460 s, 397 iter, 3970000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-35-04\n",
      "  done: false\n",
      "  episode_len_mean: 93.41121495327103\n",
      "  episode_reward_max: 219.84784072566376\n",
      "  episode_reward_mean: 166.68943277402133\n",
      "  episode_reward_min: -167.42957348847793\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 40613\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3159.969\n",
      "    load_time_ms: 2.43\n",
      "    num_steps_sampled: 3980000\n",
      "    num_steps_trained: 3980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6840834021568298\n",
      "      kl: 0.018143871799111366\n",
      "      policy_loss: -0.005231753457337618\n",
      "      total_loss: 68.86524200439453\n",
      "      vf_explained_var: 0.9057140946388245\n",
      "      vf_loss: 68.87047576904297\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.0023756634909659624\n",
      "      kl: 0.011511380784213543\n",
      "      policy_loss: -0.003762506414204836\n",
      "      total_loss: 95.01145935058594\n",
      "      vf_explained_var: 0.9198530912399292\n",
      "      vf_loss: 95.01522064208984\n",
      "    sample_time_ms: 20095.131\n",
      "    update_time_ms: 7.673\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.80268363446825\n",
      "    rl_1: 97.88674913955306\n",
      "  time_since_restore: 9483.836337089539\n",
      "  time_this_iter_s: 23.316621780395508\n",
      "  time_total_s: 9483.836337089539\n",
      "  timestamp: 1550802904\n",
      "  timesteps_since_restore: 3980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3980000\n",
      "  training_iteration: 398\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9483 s, 398 iter, 3980000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-35-27\n",
      "  done: false\n",
      "  episode_len_mean: 95.08571428571429\n",
      "  episode_reward_max: 223.30032424877098\n",
      "  episode_reward_mean: 176.62984665449906\n",
      "  episode_reward_min: 12.492207152893386\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 40718\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3158.812\n",
      "    load_time_ms: 2.425\n",
      "    num_steps_sampled: 3990000\n",
      "    num_steps_trained: 3990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.665245532989502\n",
      "      kl: 0.007907410152256489\n",
      "      policy_loss: -0.00407592486590147\n",
      "      total_loss: 4.775267601013184\n",
      "      vf_explained_var: 0.9912448525428772\n",
      "      vf_loss: 4.779343605041504\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.05223511904478073\n",
      "      kl: 0.007909983396530151\n",
      "      policy_loss: -0.00041476351907476783\n",
      "      total_loss: 4.4000043869018555\n",
      "      vf_explained_var: 0.9957115054130554\n",
      "      vf_loss: 4.400419235229492\n",
      "    sample_time_ms: 20075.562\n",
      "    update_time_ms: 7.048\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.57350731707533\n",
      "    rl_1: 105.05633933742372\n",
      "  time_since_restore: 9506.727839708328\n",
      "  time_this_iter_s: 22.891502618789673\n",
      "  time_total_s: 9506.727839708328\n",
      "  timestamp: 1550802927\n",
      "  timesteps_since_restore: 3990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 3990000\n",
      "  training_iteration: 399\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9506 s, 399 iter, 3990000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-35-51\n",
      "  done: false\n",
      "  episode_len_mean: 91.54128440366972\n",
      "  episode_reward_max: 220.11177205213247\n",
      "  episode_reward_mean: 175.47028699917772\n",
      "  episode_reward_min: -50.236900424206254\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 40827\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3177.174\n",
      "    load_time_ms: 2.443\n",
      "    num_steps_sampled: 4000000\n",
      "    num_steps_trained: 4000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6475359201431274\n",
      "      kl: 0.01200912520289421\n",
      "      policy_loss: -0.002916331635788083\n",
      "      total_loss: 18.98166847229004\n",
      "      vf_explained_var: 0.9710702300071716\n",
      "      vf_loss: 18.984582901000977\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07277123630046844\n",
      "      kl: 0.008258970454335213\n",
      "      policy_loss: 0.00038231900543905795\n",
      "      total_loss: 19.920528411865234\n",
      "      vf_explained_var: 0.9822906255722046\n",
      "      vf_loss: 19.920146942138672\n",
      "    sample_time_ms: 20157.178\n",
      "    update_time_ms: 7.088\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.88622379474766\n",
      "    rl_1: 102.58406320443002\n",
      "  time_since_restore: 9531.08865070343\n",
      "  time_this_iter_s: 24.36081099510193\n",
      "  time_total_s: 9531.08865070343\n",
      "  timestamp: 1550802951\n",
      "  timesteps_since_restore: 4000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4000000\n",
      "  training_iteration: 400\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9531 s, 400 iter, 4000000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-36-15\n",
      "  done: false\n",
      "  episode_len_mean: 89.57142857142857\n",
      "  episode_reward_max: 221.67247308873453\n",
      "  episode_reward_mean: 163.13411946937558\n",
      "  episode_reward_min: -174.07283219241046\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 40939\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3154.142\n",
      "    load_time_ms: 2.412\n",
      "    num_steps_sampled: 4010000\n",
      "    num_steps_trained: 4010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6858779191970825\n",
      "      kl: 0.017205575481057167\n",
      "      policy_loss: -0.008159380406141281\n",
      "      total_loss: 131.23727416992188\n",
      "      vf_explained_var: 0.8240267634391785\n",
      "      vf_loss: 131.2454376220703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08782993257045746\n",
      "      kl: 0.022599872201681137\n",
      "      policy_loss: 0.0016106743132695556\n",
      "      total_loss: 184.39788818359375\n",
      "      vf_explained_var: 0.8461329340934753\n",
      "      vf_loss: 184.39630126953125\n",
      "    sample_time_ms: 20222.807\n",
      "    update_time_ms: 6.683\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.74660153950138\n",
      "    rl_1: 97.38751792987419\n",
      "  time_since_restore: 9554.737221240997\n",
      "  time_this_iter_s: 23.64857053756714\n",
      "  time_total_s: 9554.737221240997\n",
      "  timestamp: 1550802975\n",
      "  timesteps_since_restore: 4010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4010000\n",
      "  training_iteration: 401\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9554 s, 401 iter, 4010000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-36-38\n",
      "  done: false\n",
      "  episode_len_mean: 90.73636363636363\n",
      "  episode_reward_max: 222.34566445979166\n",
      "  episode_reward_mean: 171.7683123919164\n",
      "  episode_reward_min: -149.77459409718267\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 41049\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3165.195\n",
      "    load_time_ms: 2.39\n",
      "    num_steps_sampled: 4020000\n",
      "    num_steps_trained: 4020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6815119981765747\n",
      "      kl: 0.010675106197595596\n",
      "      policy_loss: -0.005568903870880604\n",
      "      total_loss: 42.24880599975586\n",
      "      vf_explained_var: 0.9391134977340698\n",
      "      vf_loss: 42.25436782836914\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06436043232679367\n",
      "      kl: 0.01226985827088356\n",
      "      policy_loss: -0.001660371432080865\n",
      "      total_loss: 54.84890365600586\n",
      "      vf_explained_var: 0.95249342918396\n",
      "      vf_loss: 54.85055923461914\n",
      "    sample_time_ms: 20220.585\n",
      "    update_time_ms: 6.684\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.78211539528175\n",
      "    rl_1: 101.98619699663467\n",
      "  time_since_restore: 9577.95090842247\n",
      "  time_this_iter_s: 23.21368718147278\n",
      "  time_total_s: 9577.95090842247\n",
      "  timestamp: 1550802998\n",
      "  timesteps_since_restore: 4020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4020000\n",
      "  training_iteration: 402\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9577 s, 402 iter, 4020000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-37-02\n",
      "  done: false\n",
      "  episode_len_mean: 90.18018018018019\n",
      "  episode_reward_max: 221.37447683615358\n",
      "  episode_reward_mean: 172.77423370597836\n",
      "  episode_reward_min: -150.63695318589453\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 41160\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3164.896\n",
      "    load_time_ms: 2.333\n",
      "    num_steps_sampled: 4030000\n",
      "    num_steps_trained: 4030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6611431837081909\n",
      "      kl: 0.010378141887485981\n",
      "      policy_loss: -0.003126000752672553\n",
      "      total_loss: 41.733787536621094\n",
      "      vf_explained_var: 0.9332947134971619\n",
      "      vf_loss: 41.736915588378906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08849263936281204\n",
      "      kl: 0.025722641497850418\n",
      "      policy_loss: -0.0024813064374029636\n",
      "      total_loss: 54.10491943359375\n",
      "      vf_explained_var: 0.9488917589187622\n",
      "      vf_loss: 54.10739517211914\n",
      "    sample_time_ms: 20263.715\n",
      "    update_time_ms: 6.533\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.79867278877433\n",
      "    rl_1: 101.97556091720399\n",
      "  time_since_restore: 9601.541509151459\n",
      "  time_this_iter_s: 23.590600728988647\n",
      "  time_total_s: 9601.541509151459\n",
      "  timestamp: 1550803022\n",
      "  timesteps_since_restore: 4030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4030000\n",
      "  training_iteration: 403\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9601 s, 403 iter, 4030000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-37-25\n",
      "  done: false\n",
      "  episode_len_mean: 90.97272727272727\n",
      "  episode_reward_max: 222.14330990261055\n",
      "  episode_reward_mean: 176.068122026668\n",
      "  episode_reward_min: 137.01342498384713\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 41270\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3167.533\n",
      "    load_time_ms: 2.322\n",
      "    num_steps_sampled: 4040000\n",
      "    num_steps_trained: 4040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6695374250411987\n",
      "      kl: 0.0076388404704630375\n",
      "      policy_loss: -0.001768768997862935\n",
      "      total_loss: 3.86735463142395\n",
      "      vf_explained_var: 0.992130696773529\n",
      "      vf_loss: 3.869124174118042\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10375332087278366\n",
      "      kl: 0.021340126171708107\n",
      "      policy_loss: 0.0012594290310516953\n",
      "      total_loss: 2.858372926712036\n",
      "      vf_explained_var: 0.996909499168396\n",
      "      vf_loss: 2.857113838195801\n",
      "    sample_time_ms: 20251.566\n",
      "    update_time_ms: 6.358\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.47653049164295\n",
      "    rl_1: 104.59159153502503\n",
      "  time_since_restore: 9624.777251720428\n",
      "  time_this_iter_s: 23.235742568969727\n",
      "  time_total_s: 9624.777251720428\n",
      "  timestamp: 1550803045\n",
      "  timesteps_since_restore: 4040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4040000\n",
      "  training_iteration: 404\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9624 s, 404 iter, 4040000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-37-48\n",
      "  done: false\n",
      "  episode_len_mean: 91.84545454545454\n",
      "  episode_reward_max: 219.1497931557059\n",
      "  episode_reward_mean: 174.99705837239816\n",
      "  episode_reward_min: -33.91378072095736\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 41380\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3167.064\n",
      "    load_time_ms: 2.411\n",
      "    num_steps_sampled: 4050000\n",
      "    num_steps_trained: 4050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6595895290374756\n",
      "      kl: 0.007855185307562351\n",
      "      policy_loss: -0.0025626271963119507\n",
      "      total_loss: 11.906448364257812\n",
      "      vf_explained_var: 0.9789934158325195\n",
      "      vf_loss: 11.909011840820312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10236315429210663\n",
      "      kl: 0.014910397119820118\n",
      "      policy_loss: 0.0003593072178773582\n",
      "      total_loss: 11.80501937866211\n",
      "      vf_explained_var: 0.9891056418418884\n",
      "      vf_loss: 11.804658889770508\n",
      "    sample_time_ms: 20208.311\n",
      "    update_time_ms: 6.422\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.37636735240957\n",
      "    rl_1: 105.62069101998858\n",
      "  time_since_restore: 9647.467920303345\n",
      "  time_this_iter_s: 22.69066858291626\n",
      "  time_total_s: 9647.467920303345\n",
      "  timestamp: 1550803068\n",
      "  timesteps_since_restore: 4050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4050000\n",
      "  training_iteration: 405\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9647 s, 405 iter, 4050000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-38-11\n",
      "  done: false\n",
      "  episode_len_mean: 94.4952380952381\n",
      "  episode_reward_max: 217.6954639190507\n",
      "  episode_reward_mean: 175.62424673369065\n",
      "  episode_reward_min: 9.646211503128944\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 41485\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3163.19\n",
      "    load_time_ms: 2.494\n",
      "    num_steps_sampled: 4060000\n",
      "    num_steps_trained: 4060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6483947038650513\n",
      "      kl: 0.012116651050746441\n",
      "      policy_loss: -0.005609814543277025\n",
      "      total_loss: 4.380216121673584\n",
      "      vf_explained_var: 0.9917974472045898\n",
      "      vf_loss: 4.3858256340026855\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10147055238485336\n",
      "      kl: 0.013297944329679012\n",
      "      policy_loss: -0.0008446549181826413\n",
      "      total_loss: 4.980353832244873\n",
      "      vf_explained_var: 0.9948881268501282\n",
      "      vf_loss: 4.981197834014893\n",
      "    sample_time_ms: 20185.689\n",
      "    update_time_ms: 6.516\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.65583428935187\n",
      "    rl_1: 103.96841244433874\n",
      "  time_since_restore: 9670.504813432693\n",
      "  time_this_iter_s: 23.036893129348755\n",
      "  time_total_s: 9670.504813432693\n",
      "  timestamp: 1550803091\n",
      "  timesteps_since_restore: 4060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4060000\n",
      "  training_iteration: 406\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9670 s, 406 iter, 4060000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-38-34\n",
      "  done: false\n",
      "  episode_len_mean: 91.24545454545455\n",
      "  episode_reward_max: 218.99549405326812\n",
      "  episode_reward_mean: 183.4090502363938\n",
      "  episode_reward_min: 136.57381183457076\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 41595\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3152.247\n",
      "    load_time_ms: 2.407\n",
      "    num_steps_sampled: 4070000\n",
      "    num_steps_trained: 4070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.62153160572052\n",
      "      kl: 0.00813346542418003\n",
      "      policy_loss: -0.002518263878300786\n",
      "      total_loss: 3.321956157684326\n",
      "      vf_explained_var: 0.9938984513282776\n",
      "      vf_loss: 3.324475049972534\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13514277338981628\n",
      "      kl: 0.009862711653113365\n",
      "      policy_loss: 0.000726329511962831\n",
      "      total_loss: 2.985989809036255\n",
      "      vf_explained_var: 0.997069239616394\n",
      "      vf_loss: 2.9852635860443115\n",
      "    sample_time_ms: 20105.17\n",
      "    update_time_ms: 6.685\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.59658797158693\n",
      "    rl_1: 107.81246226480685\n",
      "  time_since_restore: 9693.360368967056\n",
      "  time_this_iter_s: 22.855555534362793\n",
      "  time_total_s: 9693.360368967056\n",
      "  timestamp: 1550803114\n",
      "  timesteps_since_restore: 4070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4070000\n",
      "  training_iteration: 407\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9693 s, 407 iter, 4070000 ts, 183 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-38-57\n",
      "  done: false\n",
      "  episode_len_mean: 94.99047619047619\n",
      "  episode_reward_max: 220.2542290153346\n",
      "  episode_reward_mean: 176.20742845750237\n",
      "  episode_reward_min: 5.731405941129923\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 41700\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.68\n",
      "    load_time_ms: 2.432\n",
      "    num_steps_sampled: 4080000\n",
      "    num_steps_trained: 4080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6535976529121399\n",
      "      kl: 0.009022380225360394\n",
      "      policy_loss: -0.0027117503341287374\n",
      "      total_loss: 4.021921634674072\n",
      "      vf_explained_var: 0.9923178553581238\n",
      "      vf_loss: 4.024633407592773\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09100665152072906\n",
      "      kl: 0.007157925050705671\n",
      "      policy_loss: 0.0011473720660433173\n",
      "      total_loss: 4.93955135345459\n",
      "      vf_explained_var: 0.9948715567588806\n",
      "      vf_loss: 4.938403129577637\n",
      "    sample_time_ms: 20106.641\n",
      "    update_time_ms: 6.713\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.01135527566434\n",
      "    rl_1: 104.19607318183802\n",
      "  time_since_restore: 9716.675305366516\n",
      "  time_this_iter_s: 23.31493639945984\n",
      "  time_total_s: 9716.675305366516\n",
      "  timestamp: 1550803137\n",
      "  timesteps_since_restore: 4080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4080000\n",
      "  training_iteration: 408\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9716 s, 408 iter, 4080000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-39-21\n",
      "  done: false\n",
      "  episode_len_mean: 89.04464285714286\n",
      "  episode_reward_max: 215.8165404834254\n",
      "  episode_reward_mean: 172.2324702650868\n",
      "  episode_reward_min: -153.1667923943168\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 41812\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3165.645\n",
      "    load_time_ms: 2.384\n",
      "    num_steps_sampled: 4090000\n",
      "    num_steps_trained: 4090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6506869792938232\n",
      "      kl: 0.0141155906021595\n",
      "      policy_loss: -0.004541051108390093\n",
      "      total_loss: 55.68243408203125\n",
      "      vf_explained_var: 0.9109429717063904\n",
      "      vf_loss: 55.68696212768555\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12410707771778107\n",
      "      kl: 0.042977917939424515\n",
      "      policy_loss: 0.004643258173018694\n",
      "      total_loss: 77.7647933959961\n",
      "      vf_explained_var: 0.9277311563491821\n",
      "      vf_loss: 77.76014709472656\n",
      "    sample_time_ms: 20156.121\n",
      "    update_time_ms: 6.826\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.37646195812519\n",
      "    rl_1: 100.85600830696158\n",
      "  time_since_restore: 9740.212425470352\n",
      "  time_this_iter_s: 23.53712010383606\n",
      "  time_total_s: 9740.212425470352\n",
      "  timestamp: 1550803161\n",
      "  timesteps_since_restore: 4090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4090000\n",
      "  training_iteration: 409\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9740 s, 409 iter, 4090000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-39-44\n",
      "  done: false\n",
      "  episode_len_mean: 90.89090909090909\n",
      "  episode_reward_max: 213.14563648602265\n",
      "  episode_reward_mean: 178.7947572788275\n",
      "  episode_reward_min: 147.30713210760254\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 41922\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.448\n",
      "    load_time_ms: 2.357\n",
      "    num_steps_sampled: 4100000\n",
      "    num_steps_trained: 4100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.635381817817688\n",
      "      kl: 0.007579364813864231\n",
      "      policy_loss: -0.0031263886485248804\n",
      "      total_loss: 2.928557872772217\n",
      "      vf_explained_var: 0.9940578937530518\n",
      "      vf_loss: 2.931684732437134\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12774603068828583\n",
      "      kl: 0.012574334628880024\n",
      "      policy_loss: -0.0001734979887260124\n",
      "      total_loss: 2.4824843406677246\n",
      "      vf_explained_var: 0.997355043888092\n",
      "      vf_loss: 2.4826579093933105\n",
      "    sample_time_ms: 20075.572\n",
      "    update_time_ms: 6.691\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.02985736842618\n",
      "    rl_1: 105.76489991040128\n",
      "  time_since_restore: 9763.57443857193\n",
      "  time_this_iter_s: 23.36201310157776\n",
      "  time_total_s: 9763.57443857193\n",
      "  timestamp: 1550803184\n",
      "  timesteps_since_restore: 4100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4100000\n",
      "  training_iteration: 410\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9763 s, 410 iter, 4100000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-40-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.76363636363637\n",
      "  episode_reward_max: 215.8808270391747\n",
      "  episode_reward_mean: 180.8221728464839\n",
      "  episode_reward_min: 143.45779463881237\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 42032\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.756\n",
      "    load_time_ms: 2.362\n",
      "    num_steps_sampled: 4110000\n",
      "    num_steps_trained: 4110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6354461312294006\n",
      "      kl: 0.006106822285801172\n",
      "      policy_loss: -0.0026554246433079243\n",
      "      total_loss: 2.6531224250793457\n",
      "      vf_explained_var: 0.994737982749939\n",
      "      vf_loss: 2.6557776927948\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.15596486628055573\n",
      "      kl: 0.014275138266384602\n",
      "      policy_loss: 0.002194842556491494\n",
      "      total_loss: 2.3723137378692627\n",
      "      vf_explained_var: 0.997539222240448\n",
      "      vf_loss: 2.3701188564300537\n",
      "    sample_time_ms: 20026.529\n",
      "    update_time_ms: 6.706\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.6303011456068\n",
      "    rl_1: 106.1918717008771\n",
      "  time_since_restore: 9786.736060380936\n",
      "  time_this_iter_s: 23.161621809005737\n",
      "  time_total_s: 9786.736060380936\n",
      "  timestamp: 1550803208\n",
      "  timesteps_since_restore: 4110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4110000\n",
      "  training_iteration: 411\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9786 s, 411 iter, 4110000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-40-31\n",
      "  done: false\n",
      "  episode_len_mean: 90.33333333333333\n",
      "  episode_reward_max: 224.53234269003795\n",
      "  episode_reward_mean: 178.32430308964376\n",
      "  episode_reward_min: 146.11914411343076\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 42143\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.901\n",
      "    load_time_ms: 2.389\n",
      "    num_steps_sampled: 4120000\n",
      "    num_steps_trained: 4120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6308075189590454\n",
      "      kl: 0.008331196382641792\n",
      "      policy_loss: -0.0036643962375819683\n",
      "      total_loss: 2.4124951362609863\n",
      "      vf_explained_var: 0.9953356981277466\n",
      "      vf_loss: 2.4161598682403564\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.19300653040409088\n",
      "      kl: 0.011933434754610062\n",
      "      policy_loss: 0.0029648852068930864\n",
      "      total_loss: 2.696939468383789\n",
      "      vf_explained_var: 0.9971957802772522\n",
      "      vf_loss: 2.693974494934082\n",
      "    sample_time_ms: 20054.266\n",
      "    update_time_ms: 6.758\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.82233274870367\n",
      "    rl_1: 105.50197034094005\n",
      "  time_since_restore: 9810.102075099945\n",
      "  time_this_iter_s: 23.3660147190094\n",
      "  time_total_s: 9810.102075099945\n",
      "  timestamp: 1550803231\n",
      "  timesteps_since_restore: 4120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4120000\n",
      "  training_iteration: 412\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9810 s, 412 iter, 4120000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-40-54\n",
      "  done: false\n",
      "  episode_len_mean: 91.19090909090909\n",
      "  episode_reward_max: 218.7509536885655\n",
      "  episode_reward_mean: 169.11788789743468\n",
      "  episode_reward_min: -130.74426947241665\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 42253\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.711\n",
      "    load_time_ms: 2.447\n",
      "    num_steps_sampled: 4130000\n",
      "    num_steps_trained: 4130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6535571813583374\n",
      "      kl: 0.009348791092634201\n",
      "      policy_loss: -0.002176328795030713\n",
      "      total_loss: 92.09886932373047\n",
      "      vf_explained_var: 0.8580626249313354\n",
      "      vf_loss: 92.10104370117188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.16537483036518097\n",
      "      kl: 0.021221822127699852\n",
      "      policy_loss: 6.613278674194589e-05\n",
      "      total_loss: 116.48738098144531\n",
      "      vf_explained_var: 0.8956460356712341\n",
      "      vf_loss: 116.4873275756836\n",
      "    sample_time_ms: 20032.815\n",
      "    update_time_ms: 6.815\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.96711758878145\n",
      "    rl_1: 101.15077030865325\n",
      "  time_since_restore: 9833.479918956757\n",
      "  time_this_iter_s: 23.377843856811523\n",
      "  time_total_s: 9833.479918956757\n",
      "  timestamp: 1550803254\n",
      "  timesteps_since_restore: 4130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4130000\n",
      "  training_iteration: 413\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9833 s, 413 iter, 4130000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-41-18\n",
      "  done: false\n",
      "  episode_len_mean: 94.24528301886792\n",
      "  episode_reward_max: 218.45641369100943\n",
      "  episode_reward_mean: 168.082517816853\n",
      "  episode_reward_min: -164.60376302538006\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 42359\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.305\n",
      "    load_time_ms: 2.421\n",
      "    num_steps_sampled: 4140000\n",
      "    num_steps_trained: 4140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6867420077323914\n",
      "      kl: 0.016005560755729675\n",
      "      policy_loss: -0.007061583921313286\n",
      "      total_loss: 70.6437759399414\n",
      "      vf_explained_var: 0.8907435536384583\n",
      "      vf_loss: 70.65084838867188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.04491083323955536\n",
      "      kl: 0.016792353242635727\n",
      "      policy_loss: -0.0010755948023870587\n",
      "      total_loss: 93.42800903320312\n",
      "      vf_explained_var: 0.9119393825531006\n",
      "      vf_loss: 93.42910766601562\n",
      "    sample_time_ms: 20035.582\n",
      "    update_time_ms: 6.943\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.94901931569841\n",
      "    rl_1: 99.13349850115462\n",
      "  time_since_restore: 9856.732803344727\n",
      "  time_this_iter_s: 23.25288438796997\n",
      "  time_total_s: 9856.732803344727\n",
      "  timestamp: 1550803278\n",
      "  timesteps_since_restore: 4140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4140000\n",
      "  training_iteration: 414\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9856 s, 414 iter, 4140000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-41-41\n",
      "  done: false\n",
      "  episode_len_mean: 90.6\n",
      "  episode_reward_max: 219.26209639818205\n",
      "  episode_reward_mean: 178.2855143857363\n",
      "  episode_reward_min: -173.62786975626793\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 42469\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.734\n",
      "    load_time_ms: 2.343\n",
      "    num_steps_sampled: 4150000\n",
      "    num_steps_trained: 4150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6274426579475403\n",
      "      kl: 0.010119442827999592\n",
      "      policy_loss: -0.0033470934722572565\n",
      "      total_loss: 32.29561996459961\n",
      "      vf_explained_var: 0.9504463076591492\n",
      "      vf_loss: 32.2989616394043\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13771679997444153\n",
      "      kl: 0.00839727371931076\n",
      "      policy_loss: -0.002207177923992276\n",
      "      total_loss: 32.82014465332031\n",
      "      vf_explained_var: 0.9695820212364197\n",
      "      vf_loss: 32.82234573364258\n",
      "    sample_time_ms: 20104.309\n",
      "    update_time_ms: 6.954\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.61151590114298\n",
      "    rl_1: 104.67399848459327\n",
      "  time_since_restore: 9880.111468315125\n",
      "  time_this_iter_s: 23.37866497039795\n",
      "  time_total_s: 9880.111468315125\n",
      "  timestamp: 1550803301\n",
      "  timesteps_since_restore: 4150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4150000\n",
      "  training_iteration: 415\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9880 s, 415 iter, 4150000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-42-04\n",
      "  done: false\n",
      "  episode_len_mean: 90.53153153153153\n",
      "  episode_reward_max: 222.20652648388307\n",
      "  episode_reward_mean: 175.26979663955962\n",
      "  episode_reward_min: -159.4706698335057\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 42580\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.476\n",
      "    load_time_ms: 2.338\n",
      "    num_steps_sampled: 4160000\n",
      "    num_steps_trained: 4160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6471022963523865\n",
      "      kl: 0.00899781659245491\n",
      "      policy_loss: -0.0023863399401307106\n",
      "      total_loss: 24.394351959228516\n",
      "      vf_explained_var: 0.9609530568122864\n",
      "      vf_loss: 24.39674186706543\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11492643505334854\n",
      "      kl: 0.01526813954114914\n",
      "      policy_loss: -0.0025294360239058733\n",
      "      total_loss: 42.23797607421875\n",
      "      vf_explained_var: 0.9605110287666321\n",
      "      vf_loss: 42.24050521850586\n",
      "    sample_time_ms: 20118.958\n",
      "    update_time_ms: 6.869\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.18809865700396\n",
      "    rl_1: 103.08169798255571\n",
      "  time_since_restore: 9903.312694072723\n",
      "  time_this_iter_s: 23.201225757598877\n",
      "  time_total_s: 9903.312694072723\n",
      "  timestamp: 1550803324\n",
      "  timesteps_since_restore: 4160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4160000\n",
      "  training_iteration: 416\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9903 s, 416 iter, 4160000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-42-28\n",
      "  done: false\n",
      "  episode_len_mean: 91.03669724770643\n",
      "  episode_reward_max: 217.9282365502314\n",
      "  episode_reward_mean: 177.10767001362908\n",
      "  episode_reward_min: 137.79208016860585\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 42689\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.923\n",
      "    load_time_ms: 2.344\n",
      "    num_steps_sampled: 4170000\n",
      "    num_steps_trained: 4170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6597493886947632\n",
      "      kl: 0.011330142617225647\n",
      "      policy_loss: -0.004632249940186739\n",
      "      total_loss: 2.91871976852417\n",
      "      vf_explained_var: 0.9939649701118469\n",
      "      vf_loss: 2.9233522415161133\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09349093586206436\n",
      "      kl: 0.012746864929795265\n",
      "      policy_loss: -0.0004673798393923789\n",
      "      total_loss: 2.3648219108581543\n",
      "      vf_explained_var: 0.9974034428596497\n",
      "      vf_loss: 2.3652894496917725\n",
      "    sample_time_ms: 20167.565\n",
      "    update_time_ms: 6.8\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.35450022492738\n",
      "    rl_1: 104.75316978870167\n",
      "  time_since_restore: 9926.657527446747\n",
      "  time_this_iter_s: 23.344833374023438\n",
      "  time_total_s: 9926.657527446747\n",
      "  timestamp: 1550803348\n",
      "  timesteps_since_restore: 4170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4170000\n",
      "  training_iteration: 417\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9926 s, 417 iter, 4170000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-42-51\n",
      "  done: false\n",
      "  episode_len_mean: 91.69724770642202\n",
      "  episode_reward_max: 220.76582690363696\n",
      "  episode_reward_mean: 174.3161291845558\n",
      "  episode_reward_min: -71.72858456274506\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 42798\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3153.146\n",
      "    load_time_ms: 2.379\n",
      "    num_steps_sampled: 4180000\n",
      "    num_steps_trained: 4180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6657406091690063\n",
      "      kl: 0.006802965421229601\n",
      "      policy_loss: -0.002386712236329913\n",
      "      total_loss: 9.725401878356934\n",
      "      vf_explained_var: 0.9831825494766235\n",
      "      vf_loss: 9.727789878845215\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13136620819568634\n",
      "      kl: 0.01590188406407833\n",
      "      policy_loss: 0.002177987014874816\n",
      "      total_loss: 10.940214157104492\n",
      "      vf_explained_var: 0.9896130561828613\n",
      "      vf_loss: 10.938036918640137\n",
      "    sample_time_ms: 20168.962\n",
      "    update_time_ms: 6.811\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.28575006677087\n",
      "    rl_1: 104.03037911778489\n",
      "  time_since_restore: 9950.169785261154\n",
      "  time_this_iter_s: 23.51225781440735\n",
      "  time_total_s: 9950.169785261154\n",
      "  timestamp: 1550803371\n",
      "  timesteps_since_restore: 4180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4180000\n",
      "  training_iteration: 418\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9950 s, 418 iter, 4180000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-43-15\n",
      "  done: false\n",
      "  episode_len_mean: 91.61818181818182\n",
      "  episode_reward_max: 220.5118494869394\n",
      "  episode_reward_mean: 180.96825636041473\n",
      "  episode_reward_min: 121.57457249212793\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 42908\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.485\n",
      "    load_time_ms: 2.402\n",
      "    num_steps_sampled: 4190000\n",
      "    num_steps_trained: 4190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6195920705795288\n",
      "      kl: 0.008663268759846687\n",
      "      policy_loss: -0.005820404272526503\n",
      "      total_loss: 2.3936100006103516\n",
      "      vf_explained_var: 0.9954550266265869\n",
      "      vf_loss: 2.399430274963379\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.15192680060863495\n",
      "      kl: 0.009999876841902733\n",
      "      policy_loss: 0.0018360281828790903\n",
      "      total_loss: 2.1878364086151123\n",
      "      vf_explained_var: 0.9977636337280273\n",
      "      vf_loss: 2.186000108718872\n",
      "    sample_time_ms: 20155.802\n",
      "    update_time_ms: 6.724\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.34995772119554\n",
      "    rl_1: 106.61829863921918\n",
      "  time_since_restore: 9973.460770130157\n",
      "  time_this_iter_s: 23.290984869003296\n",
      "  time_total_s: 9973.460770130157\n",
      "  timestamp: 1550803395\n",
      "  timesteps_since_restore: 4190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4190000\n",
      "  training_iteration: 419\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9973 s, 419 iter, 4190000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-43-38\n",
      "  done: false\n",
      "  episode_len_mean: 91.31192660550458\n",
      "  episode_reward_max: 221.60790877066296\n",
      "  episode_reward_mean: 180.8749423761388\n",
      "  episode_reward_min: 138.56718121079928\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 43017\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.294\n",
      "    load_time_ms: 2.437\n",
      "    num_steps_sampled: 4200000\n",
      "    num_steps_trained: 4200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6292871236801147\n",
      "      kl: 0.006143894046545029\n",
      "      policy_loss: -0.0018451922805979848\n",
      "      total_loss: 2.4763622283935547\n",
      "      vf_explained_var: 0.9952134490013123\n",
      "      vf_loss: 2.47820782661438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.15657661855220795\n",
      "      kl: 0.011955258436501026\n",
      "      policy_loss: 0.0026453142054378986\n",
      "      total_loss: 2.0051870346069336\n",
      "      vf_explained_var: 0.9979158043861389\n",
      "      vf_loss: 2.0025413036346436\n",
      "    sample_time_ms: 20132.772\n",
      "    update_time_ms: 7.074\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.10668036565362\n",
      "    rl_1: 106.76826201048515\n",
      "  time_since_restore: 9996.604816913605\n",
      "  time_this_iter_s: 23.144046783447266\n",
      "  time_total_s: 9996.604816913605\n",
      "  timestamp: 1550803418\n",
      "  timesteps_since_restore: 4200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4200000\n",
      "  training_iteration: 420\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 9996 s, 420 iter, 4200000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-44-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.18018018018019\n",
      "  episode_reward_max: 219.48201804437306\n",
      "  episode_reward_mean: 162.242388269947\n",
      "  episode_reward_min: -168.32656144946876\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 43128\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.576\n",
      "    load_time_ms: 2.453\n",
      "    num_steps_sampled: 4210000\n",
      "    num_steps_trained: 4210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6578298211097717\n",
      "      kl: 0.008919436484575272\n",
      "      policy_loss: -0.004390362184494734\n",
      "      total_loss: 101.77863311767578\n",
      "      vf_explained_var: 0.8579988479614258\n",
      "      vf_loss: 101.78303527832031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12375461310148239\n",
      "      kl: 0.022877270355820656\n",
      "      policy_loss: 0.001881299540400505\n",
      "      total_loss: 153.3863067626953\n",
      "      vf_explained_var: 0.8757776021957397\n",
      "      vf_loss: 153.38438415527344\n",
      "    sample_time_ms: 20136.445\n",
      "    update_time_ms: 7.023\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.87740574400775\n",
      "    rl_1: 97.36498252593924\n",
      "  time_since_restore: 10019.836248636246\n",
      "  time_this_iter_s: 23.23143172264099\n",
      "  time_total_s: 10019.836248636246\n",
      "  timestamp: 1550803441\n",
      "  timesteps_since_restore: 4210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4210000\n",
      "  training_iteration: 421\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10019 s, 421 iter, 4210000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-44-24\n",
      "  done: false\n",
      "  episode_len_mean: 90.78181818181818\n",
      "  episode_reward_max: 221.8205078405748\n",
      "  episode_reward_mean: 176.39464662565652\n",
      "  episode_reward_min: -148.89784695206305\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 43238\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.043\n",
      "    load_time_ms: 2.429\n",
      "    num_steps_sampled: 4220000\n",
      "    num_steps_trained: 4220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6333562135696411\n",
      "      kl: 0.007833506911993027\n",
      "      policy_loss: -0.0038718783762305975\n",
      "      total_loss: 24.05113410949707\n",
      "      vf_explained_var: 0.9607068300247192\n",
      "      vf_loss: 24.055007934570312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12392544001340866\n",
      "      kl: 0.0070089721120893955\n",
      "      policy_loss: -0.0002317403705092147\n",
      "      total_loss: 37.64870071411133\n",
      "      vf_explained_var: 0.9629389643669128\n",
      "      vf_loss: 37.64892578125\n",
      "    sample_time_ms: 20102.573\n",
      "    update_time_ms: 6.929\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.54205515987974\n",
      "    rl_1: 103.85259146577674\n",
      "  time_since_restore: 10042.85708475113\n",
      "  time_this_iter_s: 23.020836114883423\n",
      "  time_total_s: 10042.85708475113\n",
      "  timestamp: 1550803464\n",
      "  timesteps_since_restore: 4220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4220000\n",
      "  training_iteration: 422\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10042 s, 422 iter, 4220000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-44-48\n",
      "  done: false\n",
      "  episode_len_mean: 89.77678571428571\n",
      "  episode_reward_max: 216.95259479536358\n",
      "  episode_reward_mean: 174.44225539618017\n",
      "  episode_reward_min: -120.27176796507476\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 43350\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.932\n",
      "    load_time_ms: 2.383\n",
      "    num_steps_sampled: 4230000\n",
      "    num_steps_trained: 4230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6388047337532043\n",
      "      kl: 0.015469757840037346\n",
      "      policy_loss: -0.005792135838419199\n",
      "      total_loss: 33.415340423583984\n",
      "      vf_explained_var: 0.9435551762580872\n",
      "      vf_loss: 33.42113494873047\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07205250859260559\n",
      "      kl: 0.023703688755631447\n",
      "      policy_loss: -0.0008992640650831163\n",
      "      total_loss: 41.63261032104492\n",
      "      vf_explained_var: 0.9599686861038208\n",
      "      vf_loss: 41.633506774902344\n",
      "    sample_time_ms: 20110.31\n",
      "    update_time_ms: 7.044\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.9769865373751\n",
      "    rl_1: 103.46526885880508\n",
      "  time_since_restore: 10066.331316232681\n",
      "  time_this_iter_s: 23.474231481552124\n",
      "  time_total_s: 10066.331316232681\n",
      "  timestamp: 1550803488\n",
      "  timesteps_since_restore: 4230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4230000\n",
      "  training_iteration: 423\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10066 s, 423 iter, 4230000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-45-11\n",
      "  done: false\n",
      "  episode_len_mean: 89.96363636363637\n",
      "  episode_reward_max: 219.9825842907492\n",
      "  episode_reward_mean: 177.02299646272797\n",
      "  episode_reward_min: -154.58963077559966\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 43460\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.013\n",
      "    load_time_ms: 2.353\n",
      "    num_steps_sampled: 4240000\n",
      "    num_steps_trained: 4240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6047295331954956\n",
      "      kl: 0.010049622505903244\n",
      "      policy_loss: -0.0029173458460718393\n",
      "      total_loss: 19.002559661865234\n",
      "      vf_explained_var: 0.9705667495727539\n",
      "      vf_loss: 19.00547981262207\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.04682006314396858\n",
      "      kl: 0.029652733355760574\n",
      "      policy_loss: 0.008492058143019676\n",
      "      total_loss: 33.87477493286133\n",
      "      vf_explained_var: 0.967259407043457\n",
      "      vf_loss: 33.866275787353516\n",
      "    sample_time_ms: 20106.708\n",
      "    update_time_ms: 7.373\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.07953372281236\n",
      "    rl_1: 103.94346273991559\n",
      "  time_since_restore: 10089.559384822845\n",
      "  time_this_iter_s: 23.228068590164185\n",
      "  time_total_s: 10089.559384822845\n",
      "  timestamp: 1550803511\n",
      "  timesteps_since_restore: 4240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4240000\n",
      "  training_iteration: 424\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10089 s, 424 iter, 4240000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-45-35\n",
      "  done: false\n",
      "  episode_len_mean: 91.28181818181818\n",
      "  episode_reward_max: 222.55450822279215\n",
      "  episode_reward_mean: 178.4300250488726\n",
      "  episode_reward_min: 143.4525074716325\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 43570\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.561\n",
      "    load_time_ms: 2.326\n",
      "    num_steps_sampled: 4250000\n",
      "    num_steps_trained: 4250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.628645122051239\n",
      "      kl: 0.013787872157990932\n",
      "      policy_loss: -0.0039597442373633385\n",
      "      total_loss: 3.2003259658813477\n",
      "      vf_explained_var: 0.9938137531280518\n",
      "      vf_loss: 3.2042858600616455\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0050159296952188015\n",
      "      kl: 0.012736931443214417\n",
      "      policy_loss: 0.0022928155958652496\n",
      "      total_loss: 2.429600238800049\n",
      "      vf_explained_var: 0.9973015189170837\n",
      "      vf_loss: 2.42730712890625\n",
      "    sample_time_ms: 20115.252\n",
      "    update_time_ms: 7.34\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.08806282568442\n",
      "    rl_1: 104.34196222318812\n",
      "  time_since_restore: 10113.029582500458\n",
      "  time_this_iter_s: 23.470197677612305\n",
      "  time_total_s: 10113.029582500458\n",
      "  timestamp: 1550803535\n",
      "  timesteps_since_restore: 4250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4250000\n",
      "  training_iteration: 425\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10113 s, 425 iter, 4250000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-45-58\n",
      "  done: false\n",
      "  episode_len_mean: 90.32432432432432\n",
      "  episode_reward_max: 219.169435954143\n",
      "  episode_reward_mean: 177.64102521980234\n",
      "  episode_reward_min: 139.70669970039665\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 43681\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.598\n",
      "    load_time_ms: 2.292\n",
      "    num_steps_sampled: 4260000\n",
      "    num_steps_trained: 4260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6112595796585083\n",
      "      kl: 0.00817821267992258\n",
      "      policy_loss: -0.0029903810936957598\n",
      "      total_loss: 2.808419704437256\n",
      "      vf_explained_var: 0.99418705701828\n",
      "      vf_loss: 2.811410665512085\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08375019580125809\n",
      "      kl: 0.009464641101658344\n",
      "      policy_loss: 0.0027064932510256767\n",
      "      total_loss: 2.756351947784424\n",
      "      vf_explained_var: 0.9970783591270447\n",
      "      vf_loss: 2.753645896911621\n",
      "    sample_time_ms: 20132.091\n",
      "    update_time_ms: 7.489\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.02001262433555\n",
      "    rl_1: 105.62101259546677\n",
      "  time_since_restore: 10136.397465705872\n",
      "  time_this_iter_s: 23.36788320541382\n",
      "  time_total_s: 10136.397465705872\n",
      "  timestamp: 1550803558\n",
      "  timesteps_since_restore: 4260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4260000\n",
      "  training_iteration: 426\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10136 s, 426 iter, 4260000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-46-22\n",
      "  done: false\n",
      "  episode_len_mean: 90.11711711711712\n",
      "  episode_reward_max: 219.40214150812963\n",
      "  episode_reward_mean: 175.23721831415693\n",
      "  episode_reward_min: -138.08571660881876\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 43792\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3164.718\n",
      "    load_time_ms: 2.268\n",
      "    num_steps_sampled: 4270000\n",
      "    num_steps_trained: 4270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6118059158325195\n",
      "      kl: 0.009923475794494152\n",
      "      policy_loss: -0.003065853612497449\n",
      "      total_loss: 33.5506477355957\n",
      "      vf_explained_var: 0.9430839419364929\n",
      "      vf_loss: 33.55371856689453\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11024272441864014\n",
      "      kl: 0.027632536366581917\n",
      "      policy_loss: 0.0014860141091048717\n",
      "      total_loss: 41.46474838256836\n",
      "      vf_explained_var: 0.9618025422096252\n",
      "      vf_loss: 41.46326446533203\n",
      "    sample_time_ms: 20166.113\n",
      "    update_time_ms: 7.546\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.49136801321453\n",
      "    rl_1: 104.74585030094237\n",
      "  time_since_restore: 10160.242888450623\n",
      "  time_this_iter_s: 23.845422744750977\n",
      "  time_total_s: 10160.242888450623\n",
      "  timestamp: 1550803582\n",
      "  timesteps_since_restore: 4270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4270000\n",
      "  training_iteration: 427\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10160 s, 427 iter, 4270000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-46-45\n",
      "  done: false\n",
      "  episode_len_mean: 91.46788990825688\n",
      "  episode_reward_max: 218.54757726591518\n",
      "  episode_reward_mean: 176.61933808428375\n",
      "  episode_reward_min: 132.57223334349422\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 43901\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.926\n",
      "    load_time_ms: 2.196\n",
      "    num_steps_sampled: 4280000\n",
      "    num_steps_trained: 4280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6228269934654236\n",
      "      kl: 0.0049331230111420155\n",
      "      policy_loss: -0.0028288725297898054\n",
      "      total_loss: 3.0855448246002197\n",
      "      vf_explained_var: 0.9935163855552673\n",
      "      vf_loss: 3.088373899459839\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1354065239429474\n",
      "      kl: 0.008895807899534702\n",
      "      policy_loss: 0.000601074134465307\n",
      "      total_loss: 2.5369975566864014\n",
      "      vf_explained_var: 0.9972912669181824\n",
      "      vf_loss: 2.5363965034484863\n",
      "    sample_time_ms: 20157.247\n",
      "    update_time_ms: 7.395\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.79708790890719\n",
      "    rl_1: 104.82225017537658\n",
      "  time_since_restore: 10183.496030569077\n",
      "  time_this_iter_s: 23.25314211845398\n",
      "  time_total_s: 10183.496030569077\n",
      "  timestamp: 1550803605\n",
      "  timesteps_since_restore: 4280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4280000\n",
      "  training_iteration: 428\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10183 s, 428 iter, 4280000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-47-08\n",
      "  done: false\n",
      "  episode_len_mean: 91.07272727272728\n",
      "  episode_reward_max: 219.64730188323568\n",
      "  episode_reward_mean: 166.45749484712786\n",
      "  episode_reward_min: -162.95876811267934\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 44011\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.953\n",
      "    load_time_ms: 2.123\n",
      "    num_steps_sampled: 4290000\n",
      "    num_steps_trained: 4290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6401451230049133\n",
      "      kl: 0.009080481715500355\n",
      "      policy_loss: -0.0017175754765048623\n",
      "      total_loss: 48.65504455566406\n",
      "      vf_explained_var: 0.9255506992340088\n",
      "      vf_loss: 48.65676498413086\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08185520023107529\n",
      "      kl: 0.016329601407051086\n",
      "      policy_loss: -0.00030139030423015356\n",
      "      total_loss: 54.811954498291016\n",
      "      vf_explained_var: 0.9493760466575623\n",
      "      vf_loss: 54.81226348876953\n",
      "    sample_time_ms: 20156.273\n",
      "    update_time_ms: 7.475\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.17841060363311\n",
      "    rl_1: 98.27908424349472\n",
      "  time_since_restore: 10206.726819515228\n",
      "  time_this_iter_s: 23.230788946151733\n",
      "  time_total_s: 10206.726819515228\n",
      "  timestamp: 1550803628\n",
      "  timesteps_since_restore: 4290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4290000\n",
      "  training_iteration: 429\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10206 s, 429 iter, 4290000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-47-32\n",
      "  done: false\n",
      "  episode_len_mean: 90.31531531531532\n",
      "  episode_reward_max: 219.9250820404108\n",
      "  episode_reward_mean: 173.4173858788434\n",
      "  episode_reward_min: -140.37599857721867\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 44122\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.086\n",
      "    load_time_ms: 2.096\n",
      "    num_steps_sampled: 4300000\n",
      "    num_steps_trained: 4300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6135630011558533\n",
      "      kl: 0.019288288429379463\n",
      "      policy_loss: -0.009510987438261509\n",
      "      total_loss: 37.60122299194336\n",
      "      vf_explained_var: 0.936936616897583\n",
      "      vf_loss: 37.610740661621094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06473232060670853\n",
      "      kl: 0.012731521390378475\n",
      "      policy_loss: -0.0019795356784015894\n",
      "      total_loss: 45.21064376831055\n",
      "      vf_explained_var: 0.9565461874008179\n",
      "      vf_loss: 45.212623596191406\n",
      "    sample_time_ms: 20176.549\n",
      "    update_time_ms: 7.084\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.96167331198171\n",
      "    rl_1: 102.4557125668617\n",
      "  time_since_restore: 10230.078793287277\n",
      "  time_this_iter_s: 23.35197377204895\n",
      "  time_total_s: 10230.078793287277\n",
      "  timestamp: 1550803652\n",
      "  timesteps_since_restore: 4300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4300000\n",
      "  training_iteration: 430\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10230 s, 430 iter, 4300000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-47-55\n",
      "  done: false\n",
      "  episode_len_mean: 90.70270270270271\n",
      "  episode_reward_max: 219.7408797921038\n",
      "  episode_reward_mean: 177.15030129709535\n",
      "  episode_reward_min: 141.35641901910043\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 44233\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.746\n",
      "    load_time_ms: 2.129\n",
      "    num_steps_sampled: 4310000\n",
      "    num_steps_trained: 4310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5785537958145142\n",
      "      kl: 0.008257218636572361\n",
      "      policy_loss: -0.0032219241838902235\n",
      "      total_loss: 2.7165367603302\n",
      "      vf_explained_var: 0.9943312406539917\n",
      "      vf_loss: 2.7197587490081787\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0691477432847023\n",
      "      kl: 0.009353904984891415\n",
      "      policy_loss: -0.0009027129271999002\n",
      "      total_loss: 2.4353370666503906\n",
      "      vf_explained_var: 0.9973759651184082\n",
      "      vf_loss: 2.436239719390869\n",
      "    sample_time_ms: 20187.153\n",
      "    update_time_ms: 7.217\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.59936696437342\n",
      "    rl_1: 104.55093433272191\n",
      "  time_since_restore: 10253.38442826271\n",
      "  time_this_iter_s: 23.30563497543335\n",
      "  time_total_s: 10253.38442826271\n",
      "  timestamp: 1550803675\n",
      "  timesteps_since_restore: 4310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4310000\n",
      "  training_iteration: 431\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10253 s, 431 iter, 4310000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-48-18\n",
      "  done: false\n",
      "  episode_len_mean: 90.85321100917432\n",
      "  episode_reward_max: 218.7216815075144\n",
      "  episode_reward_mean: 179.77271814877722\n",
      "  episode_reward_min: -32.47428638716673\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 44342\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.012\n",
      "    load_time_ms: 2.149\n",
      "    num_steps_sampled: 4320000\n",
      "    num_steps_trained: 4320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5660666823387146\n",
      "      kl: 0.010169248096644878\n",
      "      policy_loss: -0.0019708415493369102\n",
      "      total_loss: 12.418842315673828\n",
      "      vf_explained_var: 0.9788954257965088\n",
      "      vf_loss: 12.42081356048584\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.04169382527470589\n",
      "      kl: 0.009033613838255405\n",
      "      policy_loss: -0.00026067785802297294\n",
      "      total_loss: 12.074593544006348\n",
      "      vf_explained_var: 0.9889863729476929\n",
      "      vf_loss: 12.074853897094727\n",
      "    sample_time_ms: 20205.178\n",
      "    update_time_ms: 7.616\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.12752934155208\n",
      "    rl_1: 104.64518880722511\n",
      "  time_since_restore: 10276.59219956398\n",
      "  time_this_iter_s: 23.20777130126953\n",
      "  time_total_s: 10276.59219956398\n",
      "  timestamp: 1550803698\n",
      "  timesteps_since_restore: 4320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4320000\n",
      "  training_iteration: 432\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10276 s, 432 iter, 4320000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-48-42\n",
      "  done: false\n",
      "  episode_len_mean: 90.08035714285714\n",
      "  episode_reward_max: 218.9058168736566\n",
      "  episode_reward_mean: 168.02807597816943\n",
      "  episode_reward_min: -172.89465944163135\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 44454\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.362\n",
      "    load_time_ms: 2.174\n",
      "    num_steps_sampled: 4330000\n",
      "    num_steps_trained: 4330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5799704194068909\n",
      "      kl: 0.009323659352958202\n",
      "      policy_loss: -0.004669182002544403\n",
      "      total_loss: 98.0299301147461\n",
      "      vf_explained_var: 0.8591544032096863\n",
      "      vf_loss: 98.03460693359375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09652543812990189\n",
      "      kl: 0.0165938138961792\n",
      "      policy_loss: 0.0017174187814816833\n",
      "      total_loss: 130.92230224609375\n",
      "      vf_explained_var: 0.8879826068878174\n",
      "      vf_loss: 130.9205780029297\n",
      "    sample_time_ms: 20179.894\n",
      "    update_time_ms: 7.592\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.34623509229283\n",
      "    rl_1: 99.68184088587658\n",
      "  time_since_restore: 10299.815953493118\n",
      "  time_this_iter_s: 23.223753929138184\n",
      "  time_total_s: 10299.815953493118\n",
      "  timestamp: 1550803722\n",
      "  timesteps_since_restore: 4330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4330000\n",
      "  training_iteration: 433\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10299 s, 433 iter, 4330000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-49-05\n",
      "  done: false\n",
      "  episode_len_mean: 90.46363636363637\n",
      "  episode_reward_max: 223.67168092561366\n",
      "  episode_reward_mean: 166.74917460828902\n",
      "  episode_reward_min: -175.1787252164133\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 44564\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.464\n",
      "    load_time_ms: 2.208\n",
      "    num_steps_sampled: 4340000\n",
      "    num_steps_trained: 4340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5693139433860779\n",
      "      kl: 0.017752699553966522\n",
      "      policy_loss: -0.005981908179819584\n",
      "      total_loss: 92.5884017944336\n",
      "      vf_explained_var: 0.8613943457603455\n",
      "      vf_loss: 92.5943832397461\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11787521839141846\n",
      "      kl: 0.027949975803494453\n",
      "      policy_loss: 0.0019241032423451543\n",
      "      total_loss: 107.51091766357422\n",
      "      vf_explained_var: 0.9136006236076355\n",
      "      vf_loss: 107.50900268554688\n",
      "    sample_time_ms: 20187.129\n",
      "    update_time_ms: 7.091\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.62886652402739\n",
      "    rl_1: 101.12030808426164\n",
      "  time_since_restore: 10323.11239361763\n",
      "  time_this_iter_s: 23.29644012451172\n",
      "  time_total_s: 10323.11239361763\n",
      "  timestamp: 1550803745\n",
      "  timesteps_since_restore: 4340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4340000\n",
      "  training_iteration: 434\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10323 s, 434 iter, 4340000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-49-28\n",
      "  done: false\n",
      "  episode_len_mean: 90.68181818181819\n",
      "  episode_reward_max: 223.3286132780479\n",
      "  episode_reward_mean: 179.4034253780402\n",
      "  episode_reward_min: 139.77925798567276\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 44674\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.381\n",
      "    load_time_ms: 2.334\n",
      "    num_steps_sampled: 4350000\n",
      "    num_steps_trained: 4350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5208104252815247\n",
      "      kl: 0.011642680503427982\n",
      "      policy_loss: -0.006713186856359243\n",
      "      total_loss: 3.413396120071411\n",
      "      vf_explained_var: 0.993445873260498\n",
      "      vf_loss: 3.420109748840332\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13791972398757935\n",
      "      kl: 0.01349368505179882\n",
      "      policy_loss: 0.001360687892884016\n",
      "      total_loss: 2.4763526916503906\n",
      "      vf_explained_var: 0.9973911643028259\n",
      "      vf_loss: 2.474991798400879\n",
      "    sample_time_ms: 20156.568\n",
      "    update_time_ms: 7.02\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.85135581190286\n",
      "    rl_1: 105.55206956613732\n",
      "  time_since_restore: 10346.256091833115\n",
      "  time_this_iter_s: 23.14369821548462\n",
      "  time_total_s: 10346.256091833115\n",
      "  timestamp: 1550803768\n",
      "  timesteps_since_restore: 4350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4350000\n",
      "  training_iteration: 435\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10346 s, 435 iter, 4350000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-49-52\n",
      "  done: false\n",
      "  episode_len_mean: 90.41441441441441\n",
      "  episode_reward_max: 220.8565203571664\n",
      "  episode_reward_mean: 178.17421850525096\n",
      "  episode_reward_min: 142.37211575254773\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 44785\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3151.774\n",
      "    load_time_ms: 2.38\n",
      "    num_steps_sampled: 4360000\n",
      "    num_steps_trained: 4360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5468499660491943\n",
      "      kl: 0.006733749061822891\n",
      "      policy_loss: -0.001886395737528801\n",
      "      total_loss: 3.0253772735595703\n",
      "      vf_explained_var: 0.9939944744110107\n",
      "      vf_loss: 3.027263879776001\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09480750560760498\n",
      "      kl: 0.00844621192663908\n",
      "      policy_loss: 0.0007209161994978786\n",
      "      total_loss: 2.3574323654174805\n",
      "      vf_explained_var: 0.9974378943443298\n",
      "      vf_loss: 2.3567113876342773\n",
      "    sample_time_ms: 20174.714\n",
      "    update_time_ms: 7.562\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.96482590130528\n",
      "    rl_1: 104.20939260394572\n",
      "  time_since_restore: 10369.936043262482\n",
      "  time_this_iter_s: 23.679951429367065\n",
      "  time_total_s: 10369.936043262482\n",
      "  timestamp: 1550803792\n",
      "  timesteps_since_restore: 4360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4360000\n",
      "  training_iteration: 436\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10369 s, 436 iter, 4360000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-50-15\n",
      "  done: false\n",
      "  episode_len_mean: 90.9090909090909\n",
      "  episode_reward_max: 224.36537417928233\n",
      "  episode_reward_mean: 174.58010715377694\n",
      "  episode_reward_min: -147.17790476135667\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 44895\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.352\n",
      "    load_time_ms: 2.48\n",
      "    num_steps_sampled: 4370000\n",
      "    num_steps_trained: 4370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5518921613693237\n",
      "      kl: 0.016301237046718597\n",
      "      policy_loss: -0.008296860381960869\n",
      "      total_loss: 18.14300537109375\n",
      "      vf_explained_var: 0.9732338190078735\n",
      "      vf_loss: 18.151302337646484\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10189025849103928\n",
      "      kl: 0.007650142069905996\n",
      "      policy_loss: 0.00019539597269613296\n",
      "      total_loss: 18.518861770629883\n",
      "      vf_explained_var: 0.98333340883255\n",
      "      vf_loss: 18.518665313720703\n",
      "    sample_time_ms: 20111.109\n",
      "    update_time_ms: 7.471\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.71082114997138\n",
      "    rl_1: 101.86928600380551\n",
      "  time_since_restore: 10392.942747592926\n",
      "  time_this_iter_s: 23.006704330444336\n",
      "  time_total_s: 10392.942747592926\n",
      "  timestamp: 1550803815\n",
      "  timesteps_since_restore: 4370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4370000\n",
      "  training_iteration: 437\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10392 s, 437 iter, 4370000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-50-38\n",
      "  done: false\n",
      "  episode_len_mean: 91.24770642201835\n",
      "  episode_reward_max: 215.1711114453844\n",
      "  episode_reward_mean: 171.35472062451376\n",
      "  episode_reward_min: -161.0915232868698\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 45004\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.796\n",
      "    load_time_ms: 2.474\n",
      "    num_steps_sampled: 4380000\n",
      "    num_steps_trained: 4380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5506888628005981\n",
      "      kl: 0.009152580052614212\n",
      "      policy_loss: -0.0035101526882499456\n",
      "      total_loss: 36.891082763671875\n",
      "      vf_explained_var: 0.9431610107421875\n",
      "      vf_loss: 36.894596099853516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10450368374586105\n",
      "      kl: 0.017772937193512917\n",
      "      policy_loss: 0.0011761809000745416\n",
      "      total_loss: 52.67160415649414\n",
      "      vf_explained_var: 0.9523596167564392\n",
      "      vf_loss: 52.67043685913086\n",
      "    sample_time_ms: 20109.108\n",
      "    update_time_ms: 7.632\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.25033290097373\n",
      "    rl_1: 102.10438772353999\n",
      "  time_since_restore: 10416.22072315216\n",
      "  time_this_iter_s: 23.27797555923462\n",
      "  time_total_s: 10416.22072315216\n",
      "  timestamp: 1550803838\n",
      "  timesteps_since_restore: 4380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4380000\n",
      "  training_iteration: 438\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10416 s, 438 iter, 4380000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-51-02\n",
      "  done: false\n",
      "  episode_len_mean: 90.31818181818181\n",
      "  episode_reward_max: 225.23884264975996\n",
      "  episode_reward_mean: 176.650116598535\n",
      "  episode_reward_min: -155.22942169583126\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 45114\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.179\n",
      "    load_time_ms: 2.614\n",
      "    num_steps_sampled: 4390000\n",
      "    num_steps_trained: 4390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5203763842582703\n",
      "      kl: 0.00900063943117857\n",
      "      policy_loss: -0.005212623160332441\n",
      "      total_loss: 30.698326110839844\n",
      "      vf_explained_var: 0.9519584774971008\n",
      "      vf_loss: 30.703540802001953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1059047281742096\n",
      "      kl: 0.02346808649599552\n",
      "      policy_loss: 0.004611077252775431\n",
      "      total_loss: 44.196014404296875\n",
      "      vf_explained_var: 0.9584743976593018\n",
      "      vf_loss: 44.19139862060547\n",
      "    sample_time_ms: 20119.44\n",
      "    update_time_ms: 7.707\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.4163806433044\n",
      "    rl_1: 103.23373595523056\n",
      "  time_since_restore: 10439.580353498459\n",
      "  time_this_iter_s: 23.359630346298218\n",
      "  time_total_s: 10439.580353498459\n",
      "  timestamp: 1550803862\n",
      "  timesteps_since_restore: 4390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4390000\n",
      "  training_iteration: 439\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10439 s, 439 iter, 4390000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-51-25\n",
      "  done: false\n",
      "  episode_len_mean: 91.26363636363637\n",
      "  episode_reward_max: 221.2574467006446\n",
      "  episode_reward_mean: 171.8776794081289\n",
      "  episode_reward_min: -146.96018693101047\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 45224\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.081\n",
      "    load_time_ms: 2.625\n",
      "    num_steps_sampled: 4400000\n",
      "    num_steps_trained: 4400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5391667485237122\n",
      "      kl: 0.010651148855686188\n",
      "      policy_loss: -0.0014424179680645466\n",
      "      total_loss: 67.93031311035156\n",
      "      vf_explained_var: 0.8909093141555786\n",
      "      vf_loss: 67.9317626953125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10257535427808762\n",
      "      kl: 0.08602698892354965\n",
      "      policy_loss: 0.0011056860676035285\n",
      "      total_loss: 83.41642761230469\n",
      "      vf_explained_var: 0.9236538410186768\n",
      "      vf_loss: 83.41532135009766\n",
      "    sample_time_ms: 20087.701\n",
      "    update_time_ms: 7.733\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.53991359668306\n",
      "    rl_1: 103.33776581144585\n",
      "  time_since_restore: 10462.605954647064\n",
      "  time_this_iter_s: 23.025601148605347\n",
      "  time_total_s: 10462.605954647064\n",
      "  timestamp: 1550803885\n",
      "  timesteps_since_restore: 4400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4400000\n",
      "  training_iteration: 440\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10462 s, 440 iter, 4400000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-51-48\n",
      "  done: false\n",
      "  episode_len_mean: 91.64545454545454\n",
      "  episode_reward_max: 219.28552365227495\n",
      "  episode_reward_mean: 173.65242751141773\n",
      "  episode_reward_min: -119.31623739042521\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 45334\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.14\n",
      "    load_time_ms: 2.582\n",
      "    num_steps_sampled: 4410000\n",
      "    num_steps_trained: 4410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5607214570045471\n",
      "      kl: 0.007591395638883114\n",
      "      policy_loss: -0.003616400994360447\n",
      "      total_loss: 39.108062744140625\n",
      "      vf_explained_var: 0.93670654296875\n",
      "      vf_loss: 39.11167526245117\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.05101112648844719\n",
      "      kl: 0.015224027447402477\n",
      "      policy_loss: 0.0019172922475263476\n",
      "      total_loss: 47.79436492919922\n",
      "      vf_explained_var: 0.9553646445274353\n",
      "      vf_loss: 47.79243850708008\n",
      "    sample_time_ms: 20072.613\n",
      "    update_time_ms: 7.718\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.95981451398819\n",
      "    rl_1: 102.69261299742955\n",
      "  time_since_restore: 10485.740008592606\n",
      "  time_this_iter_s: 23.134053945541382\n",
      "  time_total_s: 10485.740008592606\n",
      "  timestamp: 1550803908\n",
      "  timesteps_since_restore: 4410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4410000\n",
      "  training_iteration: 441\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10485 s, 441 iter, 4410000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-52-12\n",
      "  done: false\n",
      "  episode_len_mean: 91.12844036697248\n",
      "  episode_reward_max: 216.975098043155\n",
      "  episode_reward_mean: 181.8929492490929\n",
      "  episode_reward_min: 147.6492490499086\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 45443\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.861\n",
      "    load_time_ms: 2.647\n",
      "    num_steps_sampled: 4420000\n",
      "    num_steps_trained: 4420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5019117593765259\n",
      "      kl: 0.008993489667773247\n",
      "      policy_loss: -0.0035661058500409126\n",
      "      total_loss: 3.8741321563720703\n",
      "      vf_explained_var: 0.9925628304481506\n",
      "      vf_loss: 3.877699136734009\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0734945610165596\n",
      "      kl: 0.009067184291779995\n",
      "      policy_loss: 0.0002849770535249263\n",
      "      total_loss: 3.363652467727661\n",
      "      vf_explained_var: 0.9964771866798401\n",
      "      vf_loss: 3.363368034362793\n",
      "    sample_time_ms: 20108.273\n",
      "    update_time_ms: 7.316\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.8350153005534\n",
      "    rl_1: 107.05793394853949\n",
      "  time_since_restore: 10509.28902721405\n",
      "  time_this_iter_s: 23.549018621444702\n",
      "  time_total_s: 10509.28902721405\n",
      "  timestamp: 1550803932\n",
      "  timesteps_since_restore: 4420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4420000\n",
      "  training_iteration: 442\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10509 s, 442 iter, 4420000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-52-34\n",
      "  done: false\n",
      "  episode_len_mean: 90.57272727272728\n",
      "  episode_reward_max: 222.12978174893084\n",
      "  episode_reward_mean: 170.86972691927267\n",
      "  episode_reward_min: -124.6679517734909\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 45553\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.664\n",
      "    load_time_ms: 2.638\n",
      "    num_steps_sampled: 4430000\n",
      "    num_steps_trained: 4430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5127769708633423\n",
      "      kl: 0.010850224643945694\n",
      "      policy_loss: -0.0016035275766626\n",
      "      total_loss: 77.8394546508789\n",
      "      vf_explained_var: 0.8853944540023804\n",
      "      vf_loss: 77.84105682373047\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11833988130092621\n",
      "      kl: 0.01153051108121872\n",
      "      policy_loss: 0.00045008340384811163\n",
      "      total_loss: 100.9597396850586\n",
      "      vf_explained_var: 0.909222424030304\n",
      "      vf_loss: 100.95928955078125\n",
      "    sample_time_ms: 20053.548\n",
      "    update_time_ms: 7.234\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.8321777347684\n",
      "    rl_1: 102.03754918450424\n",
      "  time_since_restore: 10531.915661096573\n",
      "  time_this_iter_s: 22.626633882522583\n",
      "  time_total_s: 10531.915661096573\n",
      "  timestamp: 1550803954\n",
      "  timesteps_since_restore: 4430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4430000\n",
      "  training_iteration: 443\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10531 s, 443 iter, 4430000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-52-57\n",
      "  done: false\n",
      "  episode_len_mean: 91.2909090909091\n",
      "  episode_reward_max: 219.13991588627323\n",
      "  episode_reward_mean: 179.10605408896387\n",
      "  episode_reward_min: 144.34372061300851\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 45663\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.286\n",
      "    load_time_ms: 2.602\n",
      "    num_steps_sampled: 4440000\n",
      "    num_steps_trained: 4440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5106496214866638\n",
      "      kl: 0.008315698243677616\n",
      "      policy_loss: -0.0029997541569173336\n",
      "      total_loss: 3.5457825660705566\n",
      "      vf_explained_var: 0.9931315779685974\n",
      "      vf_loss: 3.5487828254699707\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08974616229534149\n",
      "      kl: 0.014168853871524334\n",
      "      policy_loss: 0.0011202992172911763\n",
      "      total_loss: 2.859382152557373\n",
      "      vf_explained_var: 0.9969357252120972\n",
      "      vf_loss: 2.8582613468170166\n",
      "    sample_time_ms: 20028.964\n",
      "    update_time_ms: 7.307\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.71360103551207\n",
      "    rl_1: 106.39245305345186\n",
      "  time_since_restore: 10554.951857089996\n",
      "  time_this_iter_s: 23.036195993423462\n",
      "  time_total_s: 10554.951857089996\n",
      "  timestamp: 1550803977\n",
      "  timesteps_since_restore: 4440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4440000\n",
      "  training_iteration: 444\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10554 s, 444 iter, 4440000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-53-21\n",
      "  done: false\n",
      "  episode_len_mean: 90.97272727272727\n",
      "  episode_reward_max: 222.46650373422838\n",
      "  episode_reward_mean: 176.91903558594868\n",
      "  episode_reward_min: 139.31766787947478\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 45773\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.039\n",
      "    load_time_ms: 2.515\n",
      "    num_steps_sampled: 4450000\n",
      "    num_steps_trained: 4450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5186630487442017\n",
      "      kl: 0.00947515107691288\n",
      "      policy_loss: -0.003928683698177338\n",
      "      total_loss: 3.035250425338745\n",
      "      vf_explained_var: 0.9939457774162292\n",
      "      vf_loss: 3.0391788482666016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.044532306492328644\n",
      "      kl: 0.009110800921916962\n",
      "      policy_loss: 0.0020763473585247993\n",
      "      total_loss: 2.4357247352600098\n",
      "      vf_explained_var: 0.9972919821739197\n",
      "      vf_loss: 2.4336485862731934\n",
      "    sample_time_ms: 20039.1\n",
      "    update_time_ms: 7.529\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.61719951851681\n",
      "    rl_1: 104.30183606743182\n",
      "  time_since_restore: 10578.35596704483\n",
      "  time_this_iter_s: 23.404109954833984\n",
      "  time_total_s: 10578.35596704483\n",
      "  timestamp: 1550804001\n",
      "  timesteps_since_restore: 4450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4450000\n",
      "  training_iteration: 445\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10578 s, 445 iter, 4450000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-53-45\n",
      "  done: false\n",
      "  episode_len_mean: 91.30909090909091\n",
      "  episode_reward_max: 222.71142519556108\n",
      "  episode_reward_mean: 174.24030049343335\n",
      "  episode_reward_min: -122.57288012343288\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 45883\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.491\n",
      "    load_time_ms: 2.434\n",
      "    num_steps_sampled: 4460000\n",
      "    num_steps_trained: 4460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5295519232749939\n",
      "      kl: 0.011708476580679417\n",
      "      policy_loss: -0.004335800651460886\n",
      "      total_loss: 36.96370315551758\n",
      "      vf_explained_var: 0.9434480667114258\n",
      "      vf_loss: 36.96804428100586\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.03362663462758064\n",
      "      kl: 0.010559448041021824\n",
      "      policy_loss: -0.0018872746732085943\n",
      "      total_loss: 46.67831039428711\n",
      "      vf_explained_var: 0.959466278553009\n",
      "      vf_loss: 46.68019104003906\n",
      "    sample_time_ms: 20100.966\n",
      "    update_time_ms: 6.827\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.23533770703625\n",
      "    rl_1: 104.00496278639706\n",
      "  time_since_restore: 10602.520465373993\n",
      "  time_this_iter_s: 24.164498329162598\n",
      "  time_total_s: 10602.520465373993\n",
      "  timestamp: 1550804025\n",
      "  timesteps_since_restore: 4460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4460000\n",
      "  training_iteration: 446\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10602 s, 446 iter, 4460000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-54-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.38738738738739\n",
      "  episode_reward_max: 219.4012370543684\n",
      "  episode_reward_mean: 174.91497815646025\n",
      "  episode_reward_min: 143.10468645720655\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 45994\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.046\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 4470000\n",
      "    num_steps_trained: 4470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5124049782752991\n",
      "      kl: 0.008648551069200039\n",
      "      policy_loss: -0.003925054334104061\n",
      "      total_loss: 4.013801574707031\n",
      "      vf_explained_var: 0.9922134280204773\n",
      "      vf_loss: 4.017726421356201\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07120852172374725\n",
      "      kl: 0.012964353896677494\n",
      "      policy_loss: 0.002829405013471842\n",
      "      total_loss: 3.559365749359131\n",
      "      vf_explained_var: 0.9962859749794006\n",
      "      vf_loss: 3.5565364360809326\n",
      "    sample_time_ms: 20136.106\n",
      "    update_time_ms: 7.042\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.30057975741684\n",
      "    rl_1: 103.61439839904342\n",
      "  time_since_restore: 10625.88438463211\n",
      "  time_this_iter_s: 23.363919258117676\n",
      "  time_total_s: 10625.88438463211\n",
      "  timestamp: 1550804048\n",
      "  timesteps_since_restore: 4470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4470000\n",
      "  training_iteration: 447\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10625 s, 447 iter, 4470000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-54-32\n",
      "  done: false\n",
      "  episode_len_mean: 95.76923076923077\n",
      "  episode_reward_max: 218.84175718187254\n",
      "  episode_reward_mean: 175.7758720813323\n",
      "  episode_reward_min: 11.97609510557711\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 46098\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.518\n",
      "    load_time_ms: 2.393\n",
      "    num_steps_sampled: 4480000\n",
      "    num_steps_trained: 4480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5685598254203796\n",
      "      kl: 0.007430850062519312\n",
      "      policy_loss: -0.0022930221166461706\n",
      "      total_loss: 3.9420552253723145\n",
      "      vf_explained_var: 0.9929162263870239\n",
      "      vf_loss: 3.9443485736846924\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.04030966758728027\n",
      "      kl: 0.0077445367351174355\n",
      "      policy_loss: 0.00048373572644777596\n",
      "      total_loss: 3.870283365249634\n",
      "      vf_explained_var: 0.9961488246917725\n",
      "      vf_loss: 3.8697991371154785\n",
      "    sample_time_ms: 20144.655\n",
      "    update_time_ms: 6.901\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.37273513800478\n",
      "    rl_1: 103.4031369433275\n",
      "  time_since_restore: 10649.1804728508\n",
      "  time_this_iter_s: 23.296088218688965\n",
      "  time_total_s: 10649.1804728508\n",
      "  timestamp: 1550804072\n",
      "  timesteps_since_restore: 4480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4480000\n",
      "  training_iteration: 448\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10649 s, 448 iter, 4480000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-54-56\n",
      "  done: false\n",
      "  episode_len_mean: 90.51818181818182\n",
      "  episode_reward_max: 218.07581549703679\n",
      "  episode_reward_mean: 172.45777596337194\n",
      "  episode_reward_min: -157.2850496258205\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 46208\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.897\n",
      "    load_time_ms: 2.328\n",
      "    num_steps_sampled: 4490000\n",
      "    num_steps_trained: 4490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.49671173095703125\n",
      "      kl: 0.008936187252402306\n",
      "      policy_loss: -0.0010199014795944095\n",
      "      total_loss: 71.8592529296875\n",
      "      vf_explained_var: 0.8937065005302429\n",
      "      vf_loss: 71.86026763916016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.061069734394550323\n",
      "      kl: 0.02920825593173504\n",
      "      policy_loss: 0.0032501087989658117\n",
      "      total_loss: 89.3900375366211\n",
      "      vf_explained_var: 0.921585202217102\n",
      "      vf_loss: 89.38679504394531\n",
      "    sample_time_ms: 20174.084\n",
      "    update_time_ms: 6.752\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.53170040389193\n",
      "    rl_1: 100.92607555947995\n",
      "  time_since_restore: 10672.817198514938\n",
      "  time_this_iter_s: 23.636725664138794\n",
      "  time_total_s: 10672.817198514938\n",
      "  timestamp: 1550804096\n",
      "  timesteps_since_restore: 4490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4490000\n",
      "  training_iteration: 449\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10672 s, 449 iter, 4490000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-55-19\n",
      "  done: false\n",
      "  episode_len_mean: 91.68807339449542\n",
      "  episode_reward_max: 218.43286792645083\n",
      "  episode_reward_mean: 178.74197775216228\n",
      "  episode_reward_min: 146.7490329595033\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 46317\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.069\n",
      "    load_time_ms: 2.315\n",
      "    num_steps_sampled: 4500000\n",
      "    num_steps_trained: 4500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5102900266647339\n",
      "      kl: 0.008670409210026264\n",
      "      policy_loss: -0.00462861405685544\n",
      "      total_loss: 3.0795178413391113\n",
      "      vf_explained_var: 0.9939712882041931\n",
      "      vf_loss: 3.08414626121521\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.04572603851556778\n",
      "      kl: 0.01830485463142395\n",
      "      policy_loss: 0.0025968763511627913\n",
      "      total_loss: 2.857151746749878\n",
      "      vf_explained_var: 0.9969772100448608\n",
      "      vf_loss: 2.854555130004883\n",
      "    sample_time_ms: 20238.723\n",
      "    update_time_ms: 7.177\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.16970959033634\n",
      "    rl_1: 105.57226816182599\n",
      "  time_since_restore: 10696.655726194382\n",
      "  time_this_iter_s: 23.83852767944336\n",
      "  time_total_s: 10696.655726194382\n",
      "  timestamp: 1550804119\n",
      "  timesteps_since_restore: 4500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4500000\n",
      "  training_iteration: 450\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10696 s, 450 iter, 4500000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-55-43\n",
      "  done: false\n",
      "  episode_len_mean: 90.49549549549549\n",
      "  episode_reward_max: 220.49788032262515\n",
      "  episode_reward_mean: 178.37595638496637\n",
      "  episode_reward_min: -154.9810389870667\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 46428\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.683\n",
      "    load_time_ms: 2.364\n",
      "    num_steps_sampled: 4510000\n",
      "    num_steps_trained: 4510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4671800434589386\n",
      "      kl: 0.009247909300029278\n",
      "      policy_loss: -0.0010852295672520995\n",
      "      total_loss: 32.99357604980469\n",
      "      vf_explained_var: 0.9453893899917603\n",
      "      vf_loss: 32.99466323852539\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1075226217508316\n",
      "      kl: 0.05136407911777496\n",
      "      policy_loss: 0.006000797729939222\n",
      "      total_loss: 40.92234420776367\n",
      "      vf_explained_var: 0.9617229104042053\n",
      "      vf_loss: 40.91634750366211\n",
      "    sample_time_ms: 20260.076\n",
      "    update_time_ms: 6.998\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.44054287682766\n",
      "    rl_1: 104.93541350813864\n",
      "  time_since_restore: 10720.020033836365\n",
      "  time_this_iter_s: 23.364307641983032\n",
      "  time_total_s: 10720.020033836365\n",
      "  timestamp: 1550804143\n",
      "  timesteps_since_restore: 4510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4510000\n",
      "  training_iteration: 451\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10720 s, 451 iter, 4510000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-56-07\n",
      "  done: false\n",
      "  episode_len_mean: 90.6036036036036\n",
      "  episode_reward_max: 216.05542801556\n",
      "  episode_reward_mean: 177.58034375734152\n",
      "  episode_reward_min: 142.1367841825191\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 46539\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.189\n",
      "    load_time_ms: 2.342\n",
      "    num_steps_sampled: 4520000\n",
      "    num_steps_trained: 4520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.49564558267593384\n",
      "      kl: 0.008362552151083946\n",
      "      policy_loss: -0.004035904537886381\n",
      "      total_loss: 2.7875096797943115\n",
      "      vf_explained_var: 0.9943598508834839\n",
      "      vf_loss: 2.7915456295013428\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12381408363580704\n",
      "      kl: 0.015922192484140396\n",
      "      policy_loss: 0.003836637130007148\n",
      "      total_loss: 2.353306770324707\n",
      "      vf_explained_var: 0.9974455237388611\n",
      "      vf_loss: 2.3494701385498047\n",
      "    sample_time_ms: 20277.967\n",
      "    update_time_ms: 7.19\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.68254512114257\n",
      "    rl_1: 103.89779863619898\n",
      "  time_since_restore: 10743.762424945831\n",
      "  time_this_iter_s: 23.742391109466553\n",
      "  time_total_s: 10743.762424945831\n",
      "  timestamp: 1550804167\n",
      "  timesteps_since_restore: 4520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4520000\n",
      "  training_iteration: 452\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10743 s, 452 iter, 4520000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-56-30\n",
      "  done: false\n",
      "  episode_len_mean: 89.84684684684684\n",
      "  episode_reward_max: 225.8439209925981\n",
      "  episode_reward_mean: 166.350105622263\n",
      "  episode_reward_min: -165.3535335479648\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 46650\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.268\n",
      "    load_time_ms: 2.338\n",
      "    num_steps_sampled: 4530000\n",
      "    num_steps_trained: 4530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.49131500720977783\n",
      "      kl: 0.011498615145683289\n",
      "      policy_loss: -0.004269598517566919\n",
      "      total_loss: 88.92046356201172\n",
      "      vf_explained_var: 0.873792290687561\n",
      "      vf_loss: 88.92474365234375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.16353629529476166\n",
      "      kl: 0.02389845810830593\n",
      "      policy_loss: -0.00029413722222670913\n",
      "      total_loss: 127.33502197265625\n",
      "      vf_explained_var: 0.891240119934082\n",
      "      vf_loss: 127.3353271484375\n",
      "    sample_time_ms: 20341.785\n",
      "    update_time_ms: 7.205\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.22696138179899\n",
      "    rl_1: 99.12314424046397\n",
      "  time_since_restore: 10767.056544303894\n",
      "  time_this_iter_s: 23.294119358062744\n",
      "  time_total_s: 10767.056544303894\n",
      "  timestamp: 1550804190\n",
      "  timesteps_since_restore: 4530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4530000\n",
      "  training_iteration: 453\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10767 s, 453 iter, 4530000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-56-53\n",
      "  done: false\n",
      "  episode_len_mean: 91.59633027522936\n",
      "  episode_reward_max: 223.7032284066964\n",
      "  episode_reward_mean: 178.66883340177068\n",
      "  episode_reward_min: 139.70123868547867\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 46759\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.847\n",
      "    load_time_ms: 2.377\n",
      "    num_steps_sampled: 4540000\n",
      "    num_steps_trained: 4540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4884328544139862\n",
      "      kl: 0.009525751695036888\n",
      "      policy_loss: -0.004452335648238659\n",
      "      total_loss: 3.472198486328125\n",
      "      vf_explained_var: 0.9931330680847168\n",
      "      vf_loss: 3.4766504764556885\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1118774265050888\n",
      "      kl: 0.01088201068341732\n",
      "      policy_loss: 0.0007209159666672349\n",
      "      total_loss: 2.278315544128418\n",
      "      vf_explained_var: 0.9974650144577026\n",
      "      vf_loss: 2.2775940895080566\n",
      "    sample_time_ms: 20342.651\n",
      "    update_time_ms: 7.18\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.20647594583829\n",
      "    rl_1: 104.46235745593242\n",
      "  time_since_restore: 10790.110072851181\n",
      "  time_this_iter_s: 23.053528547286987\n",
      "  time_total_s: 10790.110072851181\n",
      "  timestamp: 1550804213\n",
      "  timesteps_since_restore: 4540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4540000\n",
      "  training_iteration: 454\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10790 s, 454 iter, 4540000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-57-16\n",
      "  done: false\n",
      "  episode_len_mean: 90.28181818181818\n",
      "  episode_reward_max: 219.09193339896592\n",
      "  episode_reward_mean: 178.24756724911154\n",
      "  episode_reward_min: -119.006356785115\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 46869\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.151\n",
      "    load_time_ms: 2.334\n",
      "    num_steps_sampled: 4550000\n",
      "    num_steps_trained: 4550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.43363332748413086\n",
      "      kl: 0.011432794854044914\n",
      "      policy_loss: -0.00195028237067163\n",
      "      total_loss: 31.446430206298828\n",
      "      vf_explained_var: 0.9494279026985168\n",
      "      vf_loss: 31.448379516601562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06687775999307632\n",
      "      kl: 0.024622706696391106\n",
      "      policy_loss: -0.0036834205966442823\n",
      "      total_loss: 40.1087760925293\n",
      "      vf_explained_var: 0.9625433683395386\n",
      "      vf_loss: 40.11246871948242\n",
      "    sample_time_ms: 20262.936\n",
      "    update_time_ms: 7.024\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.75460474781043\n",
      "    rl_1: 104.49296250130102\n",
      "  time_since_restore: 10812.558874368668\n",
      "  time_this_iter_s: 22.448801517486572\n",
      "  time_total_s: 10812.558874368668\n",
      "  timestamp: 1550804236\n",
      "  timesteps_since_restore: 4550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4550000\n",
      "  training_iteration: 455\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10812 s, 455 iter, 4550000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-57-39\n",
      "  done: false\n",
      "  episode_len_mean: 89.85714285714286\n",
      "  episode_reward_max: 219.6863995815887\n",
      "  episode_reward_mean: 165.70330256914045\n",
      "  episode_reward_min: -146.93967601030704\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 46981\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.328\n",
      "    load_time_ms: 2.331\n",
      "    num_steps_sampled: 4560000\n",
      "    num_steps_trained: 4560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.45301154255867004\n",
      "      kl: 0.008772874251008034\n",
      "      policy_loss: -0.0028417441062629223\n",
      "      total_loss: 107.07758331298828\n",
      "      vf_explained_var: 0.8443495035171509\n",
      "      vf_loss: 107.08042907714844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11826253682374954\n",
      "      kl: 0.014703182503581047\n",
      "      policy_loss: -0.00021720334189012647\n",
      "      total_loss: 135.12071228027344\n",
      "      vf_explained_var: 0.8873904347419739\n",
      "      vf_loss: 135.1209259033203\n",
      "    sample_time_ms: 20142.368\n",
      "    update_time_ms: 7.189\n",
      "  iterations_since_restore: 456\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.45417326655429\n",
      "    rl_1: 98.24912930258617\n",
      "  time_since_restore: 10835.532388925552\n",
      "  time_this_iter_s: 22.973514556884766\n",
      "  time_total_s: 10835.532388925552\n",
      "  timestamp: 1550804259\n",
      "  timesteps_since_restore: 4560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4560000\n",
      "  training_iteration: 456\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10835 s, 456 iter, 4560000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-58-02\n",
      "  done: false\n",
      "  episode_len_mean: 89.25225225225225\n",
      "  episode_reward_max: 218.52720150570002\n",
      "  episode_reward_mean: 165.38878026343053\n",
      "  episode_reward_min: -155.12146738339604\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 47092\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.766\n",
      "    load_time_ms: 2.33\n",
      "    num_steps_sampled: 4570000\n",
      "    num_steps_trained: 4570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4136051833629608\n",
      "      kl: 0.011762021109461784\n",
      "      policy_loss: -0.0048093548975884914\n",
      "      total_loss: 119.54351806640625\n",
      "      vf_explained_var: 0.8251116871833801\n",
      "      vf_loss: 119.54832458496094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06313247978687286\n",
      "      kl: 0.01421318482607603\n",
      "      policy_loss: -0.0014358059270307422\n",
      "      total_loss: 147.045166015625\n",
      "      vf_explained_var: 0.8675162196159363\n",
      "      vf_loss: 147.0465545654297\n",
      "    sample_time_ms: 20117.024\n",
      "    update_time_ms: 6.985\n",
      "  iterations_since_restore: 457\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.27349059586454\n",
      "    rl_1: 96.11528966756595\n",
      "  time_since_restore: 10858.677918434143\n",
      "  time_this_iter_s: 23.1455295085907\n",
      "  time_total_s: 10858.677918434143\n",
      "  timestamp: 1550804282\n",
      "  timesteps_since_restore: 4570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4570000\n",
      "  training_iteration: 457\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10858 s, 457 iter, 4570000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-58-25\n",
      "  done: false\n",
      "  episode_len_mean: 90.68468468468468\n",
      "  episode_reward_max: 220.56492782034974\n",
      "  episode_reward_mean: 177.94398720327357\n",
      "  episode_reward_min: 140.25363965216974\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 47203\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.054\n",
      "    load_time_ms: 2.426\n",
      "    num_steps_sampled: 4580000\n",
      "    num_steps_trained: 4580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.41886481642723083\n",
      "      kl: 0.009766270406544209\n",
      "      policy_loss: -0.004145260900259018\n",
      "      total_loss: 4.486664772033691\n",
      "      vf_explained_var: 0.9908356070518494\n",
      "      vf_loss: 4.490809440612793\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06586351245641708\n",
      "      kl: 0.009591232053935528\n",
      "      policy_loss: 0.0013376785209402442\n",
      "      total_loss: 3.852262020111084\n",
      "      vf_explained_var: 0.9956018328666687\n",
      "      vf_loss: 3.850924253463745\n",
      "    sample_time_ms: 20101.932\n",
      "    update_time_ms: 7.201\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.22903008591109\n",
      "    rl_1: 103.7149571173625\n",
      "  time_since_restore: 10881.831659555435\n",
      "  time_this_iter_s: 23.153741121292114\n",
      "  time_total_s: 10881.831659555435\n",
      "  timestamp: 1550804305\n",
      "  timesteps_since_restore: 4580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4580000\n",
      "  training_iteration: 458\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10881 s, 458 iter, 4580000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-58-48\n",
      "  done: false\n",
      "  episode_len_mean: 91.65137614678899\n",
      "  episode_reward_max: 217.22034204219744\n",
      "  episode_reward_mean: 177.49012478408562\n",
      "  episode_reward_min: 141.96616203289673\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 47312\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.671\n",
      "    load_time_ms: 2.448\n",
      "    num_steps_sampled: 4590000\n",
      "    num_steps_trained: 4590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3995981812477112\n",
      "      kl: 0.012060707435011864\n",
      "      policy_loss: -0.007950721308588982\n",
      "      total_loss: 4.300543785095215\n",
      "      vf_explained_var: 0.9912592768669128\n",
      "      vf_loss: 4.308495044708252\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13702180981636047\n",
      "      kl: 0.011455529369413853\n",
      "      policy_loss: 0.0010899696499109268\n",
      "      total_loss: 3.683192253112793\n",
      "      vf_explained_var: 0.9960172772407532\n",
      "      vf_loss: 3.6821019649505615\n",
      "    sample_time_ms: 20053.582\n",
      "    update_time_ms: 7.234\n",
      "  iterations_since_restore: 459\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.92880957761304\n",
      "    rl_1: 105.56131520647261\n",
      "  time_since_restore: 10904.969887971878\n",
      "  time_this_iter_s: 23.13822841644287\n",
      "  time_total_s: 10904.969887971878\n",
      "  timestamp: 1550804328\n",
      "  timesteps_since_restore: 4590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4590000\n",
      "  training_iteration: 459\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10904 s, 459 iter, 4590000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-59-11\n",
      "  done: false\n",
      "  episode_len_mean: 90.9\n",
      "  episode_reward_max: 219.0674262546999\n",
      "  episode_reward_mean: 178.2266232751606\n",
      "  episode_reward_min: 144.18236331311374\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 47422\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3116.306\n",
      "    load_time_ms: 2.611\n",
      "    num_steps_sampled: 4600000\n",
      "    num_steps_trained: 4600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39389869570732117\n",
      "      kl: 0.006300134584307671\n",
      "      policy_loss: -0.0023860507644712925\n",
      "      total_loss: 3.292689323425293\n",
      "      vf_explained_var: 0.9934850931167603\n",
      "      vf_loss: 3.295074939727783\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09943664073944092\n",
      "      kl: 0.012324458919465542\n",
      "      policy_loss: 0.006967362482100725\n",
      "      total_loss: 2.8444337844848633\n",
      "      vf_explained_var: 0.9969711303710938\n",
      "      vf_loss: 2.837466239929199\n",
      "    sample_time_ms: 20015.666\n",
      "    update_time_ms: 6.884\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.40536681237826\n",
      "    rl_1: 104.82125646278229\n",
      "  time_since_restore: 10928.252646684647\n",
      "  time_this_iter_s: 23.282758712768555\n",
      "  time_total_s: 10928.252646684647\n",
      "  timestamp: 1550804351\n",
      "  timesteps_since_restore: 4600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4600000\n",
      "  training_iteration: 460\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10928 s, 460 iter, 4600000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-59-35\n",
      "  done: false\n",
      "  episode_len_mean: 91.7090909090909\n",
      "  episode_reward_max: 220.32826661516904\n",
      "  episode_reward_mean: 179.44426031348098\n",
      "  episode_reward_min: 136.84565095670285\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 47532\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3115.477\n",
      "    load_time_ms: 2.608\n",
      "    num_steps_sampled: 4610000\n",
      "    num_steps_trained: 4610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4058885872364044\n",
      "      kl: 0.007647489197552204\n",
      "      policy_loss: -0.004295299295336008\n",
      "      total_loss: 2.96909761428833\n",
      "      vf_explained_var: 0.99411541223526\n",
      "      vf_loss: 2.9733927249908447\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07490465044975281\n",
      "      kl: 0.010302293114364147\n",
      "      policy_loss: -0.0001983109541470185\n",
      "      total_loss: 2.4588241577148438\n",
      "      vf_explained_var: 0.9974592328071594\n",
      "      vf_loss: 2.4590227603912354\n",
      "    sample_time_ms: 20017.692\n",
      "    update_time_ms: 7.133\n",
      "  iterations_since_restore: 461\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.65918880500489\n",
      "    rl_1: 105.78507150847608\n",
      "  time_since_restore: 10951.628866672516\n",
      "  time_this_iter_s: 23.376219987869263\n",
      "  time_total_s: 10951.628866672516\n",
      "  timestamp: 1550804375\n",
      "  timesteps_since_restore: 4610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4610000\n",
      "  training_iteration: 461\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10951 s, 461 iter, 4610000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_03-59-58\n",
      "  done: false\n",
      "  episode_len_mean: 90.76363636363637\n",
      "  episode_reward_max: 220.29495336920812\n",
      "  episode_reward_mean: 179.4790794632008\n",
      "  episode_reward_min: -55.57884472724729\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 47642\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3112.368\n",
      "    load_time_ms: 2.544\n",
      "    num_steps_sampled: 4620000\n",
      "    num_steps_trained: 4620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3782722055912018\n",
      "      kl: 0.0054143983870744705\n",
      "      policy_loss: -0.001891166321001947\n",
      "      total_loss: 12.792057991027832\n",
      "      vf_explained_var: 0.9808962941169739\n",
      "      vf_loss: 12.793947219848633\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13018332421779633\n",
      "      kl: 0.00796114094555378\n",
      "      policy_loss: -0.0008992343791760504\n",
      "      total_loss: 12.804436683654785\n",
      "      vf_explained_var: 0.9887588620185852\n",
      "      vf_loss: 12.80533504486084\n",
      "    sample_time_ms: 19949.02\n",
      "    update_time_ms: 7.073\n",
      "  iterations_since_restore: 462\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.04509711944785\n",
      "    rl_1: 105.4339823437529\n",
      "  time_since_restore: 10974.650851488113\n",
      "  time_this_iter_s: 23.021984815597534\n",
      "  time_total_s: 10974.650851488113\n",
      "  timestamp: 1550804398\n",
      "  timesteps_since_restore: 4620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4620000\n",
      "  training_iteration: 462\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10974 s, 462 iter, 4620000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-00-22\n",
      "  done: false\n",
      "  episode_len_mean: 91.02752293577981\n",
      "  episode_reward_max: 216.1452310870592\n",
      "  episode_reward_mean: 178.837209698336\n",
      "  episode_reward_min: 140.86171818451425\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 47751\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.053\n",
      "    load_time_ms: 2.536\n",
      "    num_steps_sampled: 4630000\n",
      "    num_steps_trained: 4630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.38773924112319946\n",
      "      kl: 0.010123367421329021\n",
      "      policy_loss: -0.0035445804242044687\n",
      "      total_loss: 2.180546760559082\n",
      "      vf_explained_var: 0.995705246925354\n",
      "      vf_loss: 2.184091329574585\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13118720054626465\n",
      "      kl: 0.010062620043754578\n",
      "      policy_loss: 0.00038762035546824336\n",
      "      total_loss: 2.424755334854126\n",
      "      vf_explained_var: 0.9974526762962341\n",
      "      vf_loss: 2.424367904663086\n",
      "    sample_time_ms: 19963.108\n",
      "    update_time_ms: 6.948\n",
      "  iterations_since_restore: 463\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.35918927745524\n",
      "    rl_1: 105.47802042088075\n",
      "  time_since_restore: 10998.251561164856\n",
      "  time_this_iter_s: 23.600709676742554\n",
      "  time_total_s: 10998.251561164856\n",
      "  timestamp: 1550804422\n",
      "  timesteps_since_restore: 4630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4630000\n",
      "  training_iteration: 463\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 10998 s, 463 iter, 4630000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-00-45\n",
      "  done: false\n",
      "  episode_len_mean: 90.50450450450451\n",
      "  episode_reward_max: 225.47250019200698\n",
      "  episode_reward_mean: 176.90376887570486\n",
      "  episode_reward_min: 136.1523902643695\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 47862\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.266\n",
      "    load_time_ms: 2.522\n",
      "    num_steps_sampled: 4640000\n",
      "    num_steps_trained: 4640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.36195266246795654\n",
      "      kl: 0.010210282169282436\n",
      "      policy_loss: -0.0002791230217553675\n",
      "      total_loss: 1.8914377689361572\n",
      "      vf_explained_var: 0.9962318539619446\n",
      "      vf_loss: 1.8917165994644165\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11194407939910889\n",
      "      kl: 0.008834083564579487\n",
      "      policy_loss: 0.0017753717256709933\n",
      "      total_loss: 1.9816782474517822\n",
      "      vf_explained_var: 0.9978538155555725\n",
      "      vf_loss: 1.9799028635025024\n",
      "    sample_time_ms: 20019.092\n",
      "    update_time_ms: 7.09\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.88834516673381\n",
      "    rl_1: 103.01542370897103\n",
      "  time_since_restore: 11021.878502368927\n",
      "  time_this_iter_s: 23.626941204071045\n",
      "  time_total_s: 11021.878502368927\n",
      "  timestamp: 1550804445\n",
      "  timesteps_since_restore: 4640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4640000\n",
      "  training_iteration: 464\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11021 s, 464 iter, 4640000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-01-09\n",
      "  done: false\n",
      "  episode_len_mean: 95.05714285714286\n",
      "  episode_reward_max: 219.72722297313334\n",
      "  episode_reward_mean: 172.69976848297387\n",
      "  episode_reward_min: -126.15883867096544\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 47967\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.902\n",
      "    load_time_ms: 2.583\n",
      "    num_steps_sampled: 4650000\n",
      "    num_steps_trained: 4650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.48916366696357727\n",
      "      kl: 0.01490121427923441\n",
      "      policy_loss: -0.005439485423266888\n",
      "      total_loss: 42.89194869995117\n",
      "      vf_explained_var: 0.942189633846283\n",
      "      vf_loss: 42.89739227294922\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.01706322841346264\n",
      "      kl: 0.01784958876669407\n",
      "      policy_loss: -0.00039603523327969015\n",
      "      total_loss: 50.70510482788086\n",
      "      vf_explained_var: 0.962049126625061\n",
      "      vf_loss: 50.70549011230469\n",
      "    sample_time_ms: 20112.11\n",
      "    update_time_ms: 7.253\n",
      "  iterations_since_restore: 465\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.36651836540965\n",
      "    rl_1: 101.33325011756418\n",
      "  time_since_restore: 11045.285499095917\n",
      "  time_this_iter_s: 23.406996726989746\n",
      "  time_total_s: 11045.285499095917\n",
      "  timestamp: 1550804469\n",
      "  timesteps_since_restore: 4650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4650000\n",
      "  training_iteration: 465\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11045 s, 465 iter, 4650000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-01-32\n",
      "  done: false\n",
      "  episode_len_mean: 90.02702702702703\n",
      "  episode_reward_max: 219.74632940377313\n",
      "  episode_reward_mean: 177.72781514188796\n",
      "  episode_reward_min: -147.17201608380307\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 48078\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.93\n",
      "    load_time_ms: 2.591\n",
      "    num_steps_sampled: 4660000\n",
      "    num_steps_trained: 4660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.38997676968574524\n",
      "      kl: 0.008432607166469097\n",
      "      policy_loss: -0.002148502040654421\n",
      "      total_loss: 34.59104919433594\n",
      "      vf_explained_var: 0.9435539245605469\n",
      "      vf_loss: 34.59320068359375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1659071445465088\n",
      "      kl: 0.05694439634680748\n",
      "      policy_loss: 0.0024842817801982164\n",
      "      total_loss: 45.47024154663086\n",
      "      vf_explained_var: 0.9561676383018494\n",
      "      vf_loss: 45.46775436401367\n",
      "    sample_time_ms: 20176.071\n",
      "    update_time_ms: 7.149\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.279898787136\n",
      "    rl_1: 104.44791635475192\n",
      "  time_since_restore: 11068.937890768051\n",
      "  time_this_iter_s: 23.6523916721344\n",
      "  time_total_s: 11068.937890768051\n",
      "  timestamp: 1550804492\n",
      "  timesteps_since_restore: 4660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4660000\n",
      "  training_iteration: 466\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11068 s, 466 iter, 4660000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-01-56\n",
      "  done: false\n",
      "  episode_len_mean: 90.41441441441441\n",
      "  episode_reward_max: 220.56730607796084\n",
      "  episode_reward_mean: 181.46666739715712\n",
      "  episode_reward_min: 139.2274069113812\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 48189\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.203\n",
      "    load_time_ms: 2.56\n",
      "    num_steps_sampled: 4670000\n",
      "    num_steps_trained: 4670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.38730236887931824\n",
      "      kl: 0.00973094068467617\n",
      "      policy_loss: -0.002925876760855317\n",
      "      total_loss: 2.1824254989624023\n",
      "      vf_explained_var: 0.9959143400192261\n",
      "      vf_loss: 2.185351848602295\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0994311198592186\n",
      "      kl: 0.013327036052942276\n",
      "      policy_loss: 0.003346699057146907\n",
      "      total_loss: 2.240973949432373\n",
      "      vf_explained_var: 0.9976035952568054\n",
      "      vf_loss: 2.2376270294189453\n",
      "    sample_time_ms: 20208.444\n",
      "    update_time_ms: 7.145\n",
      "  iterations_since_restore: 467\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.5383556940057\n",
      "    rl_1: 104.92831170315145\n",
      "  time_since_restore: 11092.387323617935\n",
      "  time_this_iter_s: 23.449432849884033\n",
      "  time_total_s: 11092.387323617935\n",
      "  timestamp: 1550804516\n",
      "  timesteps_since_restore: 4670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4670000\n",
      "  training_iteration: 467\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11092 s, 467 iter, 4670000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-02-19\n",
      "  done: false\n",
      "  episode_len_mean: 90.65454545454546\n",
      "  episode_reward_max: 220.90329892584592\n",
      "  episode_reward_mean: 176.84874140475728\n",
      "  episode_reward_min: -156.07181118614304\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 48299\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3154.141\n",
      "    load_time_ms: 2.462\n",
      "    num_steps_sampled: 4680000\n",
      "    num_steps_trained: 4680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.40789860486984253\n",
      "      kl: 0.008427726104855537\n",
      "      policy_loss: -0.0027864782605320215\n",
      "      total_loss: 28.837636947631836\n",
      "      vf_explained_var: 0.9547809958457947\n",
      "      vf_loss: 28.840425491333008\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1332244873046875\n",
      "      kl: 0.007165810093283653\n",
      "      policy_loss: -0.0018919626018032432\n",
      "      total_loss: 44.42238998413086\n",
      "      vf_explained_var: 0.9580709934234619\n",
      "      vf_loss: 44.424278259277344\n",
      "    sample_time_ms: 20204.117\n",
      "    update_time_ms: 7.536\n",
      "  iterations_since_restore: 468\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.98468734656906\n",
      "    rl_1: 103.86405405818822\n",
      "  time_since_restore: 11115.696055173874\n",
      "  time_this_iter_s: 23.30873155593872\n",
      "  time_total_s: 11115.696055173874\n",
      "  timestamp: 1550804539\n",
      "  timesteps_since_restore: 4680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4680000\n",
      "  training_iteration: 468\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11115 s, 468 iter, 4680000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-02-43\n",
      "  done: false\n",
      "  episode_len_mean: 91.80555555555556\n",
      "  episode_reward_max: 220.48169474421388\n",
      "  episode_reward_mean: 177.09547688079618\n",
      "  episode_reward_min: -56.32306799357619\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 48407\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3160.16\n",
      "    load_time_ms: 2.361\n",
      "    num_steps_sampled: 4690000\n",
      "    num_steps_trained: 4690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.40950489044189453\n",
      "      kl: 0.010933546349406242\n",
      "      policy_loss: -0.0027822793927043676\n",
      "      total_loss: 10.675506591796875\n",
      "      vf_explained_var: 0.9825001358985901\n",
      "      vf_loss: 10.678289413452148\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.16349069774150848\n",
      "      kl: 0.01040448434650898\n",
      "      policy_loss: -0.0002075096854241565\n",
      "      total_loss: 11.469767570495605\n",
      "      vf_explained_var: 0.9895468354225159\n",
      "      vf_loss: 11.469975471496582\n",
      "    sample_time_ms: 20206.958\n",
      "    update_time_ms: 7.643\n",
      "  iterations_since_restore: 469\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.04859790688485\n",
      "    rl_1: 105.04687897391135\n",
      "  time_since_restore: 11138.922232151031\n",
      "  time_this_iter_s: 23.226176977157593\n",
      "  time_total_s: 11138.922232151031\n",
      "  timestamp: 1550804563\n",
      "  timesteps_since_restore: 4690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4690000\n",
      "  training_iteration: 469\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11138 s, 469 iter, 4690000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 90.32142857142857\n",
      "  episode_reward_max: 216.07087294985146\n",
      "  episode_reward_mean: 177.80827797339455\n",
      "  episode_reward_min: 142.34112943292132\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 48519\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3159.989\n",
      "    load_time_ms: 2.189\n",
      "    num_steps_sampled: 4700000\n",
      "    num_steps_trained: 4700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4256884753704071\n",
      "      kl: 0.006373842246830463\n",
      "      policy_loss: -0.003073599189519882\n",
      "      total_loss: 2.568274736404419\n",
      "      vf_explained_var: 0.9949055910110474\n",
      "      vf_loss: 2.5713484287261963\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.15534156560897827\n",
      "      kl: 0.014508672058582306\n",
      "      policy_loss: 0.00019486338715068996\n",
      "      total_loss: 2.6168079376220703\n",
      "      vf_explained_var: 0.997203528881073\n",
      "      vf_loss: 2.6166136264801025\n",
      "    sample_time_ms: 20178.334\n",
      "    update_time_ms: 7.906\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.1515505339073\n",
      "    rl_1: 103.65672743948724\n",
      "  time_since_restore: 11161.917350769043\n",
      "  time_this_iter_s: 22.995118618011475\n",
      "  time_total_s: 11161.917350769043\n",
      "  timestamp: 1550804586\n",
      "  timesteps_since_restore: 4700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4700000\n",
      "  training_iteration: 470\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11161 s, 470 iter, 4700000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-03-29\n",
      "  done: false\n",
      "  episode_len_mean: 90.73636363636363\n",
      "  episode_reward_max: 221.05935216418658\n",
      "  episode_reward_mean: 173.39632707636395\n",
      "  episode_reward_min: -147.62415471043823\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 48629\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3161.25\n",
      "    load_time_ms: 2.185\n",
      "    num_steps_sampled: 4710000\n",
      "    num_steps_trained: 4710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.393014520406723\n",
      "      kl: 0.006067652255296707\n",
      "      policy_loss: -0.001310613821260631\n",
      "      total_loss: 39.08546447753906\n",
      "      vf_explained_var: 0.9386346340179443\n",
      "      vf_loss: 39.08677673339844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1597316414117813\n",
      "      kl: 0.23470145463943481\n",
      "      policy_loss: 0.02779480069875717\n",
      "      total_loss: 48.24315643310547\n",
      "      vf_explained_var: 0.9577247500419617\n",
      "      vf_loss: 48.21535873413086\n",
      "    sample_time_ms: 20180.294\n",
      "    update_time_ms: 7.717\n",
      "  iterations_since_restore: 471\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.85382020738054\n",
      "    rl_1: 102.54250686898337\n",
      "  time_since_restore: 11185.325653791428\n",
      "  time_this_iter_s: 23.408303022384644\n",
      "  time_total_s: 11185.325653791428\n",
      "  timestamp: 1550804609\n",
      "  timesteps_since_restore: 4710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4710000\n",
      "  training_iteration: 471\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11185 s, 471 iter, 4710000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-03-52\n",
      "  done: false\n",
      "  episode_len_mean: 90.75454545454545\n",
      "  episode_reward_max: 224.23684741523712\n",
      "  episode_reward_mean: 179.58182259661717\n",
      "  episode_reward_min: -43.44332030149258\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 48739\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3177.551\n",
      "    load_time_ms: 2.218\n",
      "    num_steps_sampled: 4720000\n",
      "    num_steps_trained: 4720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.43093469738960266\n",
      "      kl: 0.011324743740260601\n",
      "      policy_loss: -0.0021215968299657106\n",
      "      total_loss: 8.549762725830078\n",
      "      vf_explained_var: 0.9870399236679077\n",
      "      vf_loss: 8.551886558532715\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.14212052524089813\n",
      "      kl: 0.006861809175461531\n",
      "      policy_loss: -0.001086717238649726\n",
      "      total_loss: 9.070707321166992\n",
      "      vf_explained_var: 0.9914804100990295\n",
      "      vf_loss: 9.071794509887695\n",
      "    sample_time_ms: 20169.381\n",
      "    update_time_ms: 7.586\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.2948756516764\n",
      "    rl_1: 104.28694694494074\n",
      "  time_since_restore: 11208.401809930801\n",
      "  time_this_iter_s: 23.07615613937378\n",
      "  time_total_s: 11208.401809930801\n",
      "  timestamp: 1550804632\n",
      "  timesteps_since_restore: 4720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4720000\n",
      "  training_iteration: 472\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11208 s, 472 iter, 4720000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-04-15\n",
      "  done: false\n",
      "  episode_len_mean: 91.01818181818182\n",
      "  episode_reward_max: 223.88100908550854\n",
      "  episode_reward_mean: 177.87617557915797\n",
      "  episode_reward_min: 147.29072445955416\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 48849\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3164.817\n",
      "    load_time_ms: 2.209\n",
      "    num_steps_sampled: 4730000\n",
      "    num_steps_trained: 4730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.40138691663742065\n",
      "      kl: 0.011053803376853466\n",
      "      policy_loss: -0.005219775717705488\n",
      "      total_loss: 2.782944917678833\n",
      "      vf_explained_var: 0.9944310188293457\n",
      "      vf_loss: 2.7881646156311035\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.20873180031776428\n",
      "      kl: 0.008854635059833527\n",
      "      policy_loss: 0.0009388381149619818\n",
      "      total_loss: 2.6941006183624268\n",
      "      vf_explained_var: 0.997216522693634\n",
      "      vf_loss: 2.693161964416504\n",
      "    sample_time_ms: 20129.831\n",
      "    update_time_ms: 7.84\n",
      "  iterations_since_restore: 473\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.18742193066312\n",
      "    rl_1: 105.68875364849481\n",
      "  time_since_restore: 11231.481307506561\n",
      "  time_this_iter_s: 23.079497575759888\n",
      "  time_total_s: 11231.481307506561\n",
      "  timestamp: 1550804655\n",
      "  timesteps_since_restore: 4730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4730000\n",
      "  training_iteration: 473\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11231 s, 473 iter, 4730000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-04-39\n",
      "  done: false\n",
      "  episode_len_mean: 91.01834862385321\n",
      "  episode_reward_max: 221.31542957938348\n",
      "  episode_reward_mean: 173.93407375093372\n",
      "  episode_reward_min: -140.89753922052464\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 48958\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3170.801\n",
      "    load_time_ms: 2.264\n",
      "    num_steps_sampled: 4740000\n",
      "    num_steps_trained: 4740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.37919744849205017\n",
      "      kl: 0.012108663097023964\n",
      "      policy_loss: -0.006260000169277191\n",
      "      total_loss: 8.928567886352539\n",
      "      vf_explained_var: 0.9859022498130798\n",
      "      vf_loss: 8.93482780456543\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21976324915885925\n",
      "      kl: 0.012154307216405869\n",
      "      policy_loss: 0.0010744398459792137\n",
      "      total_loss: 8.965461730957031\n",
      "      vf_explained_var: 0.991899847984314\n",
      "      vf_loss: 8.964387893676758\n",
      "    sample_time_ms: 20092.901\n",
      "    update_time_ms: 7.894\n",
      "  iterations_since_restore: 474\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.95701216932328\n",
      "    rl_1: 102.97706158161037\n",
      "  time_since_restore: 11254.802598953247\n",
      "  time_this_iter_s: 23.32129144668579\n",
      "  time_total_s: 11254.802598953247\n",
      "  timestamp: 1550804679\n",
      "  timesteps_since_restore: 4740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4740000\n",
      "  training_iteration: 474\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11254 s, 474 iter, 4740000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-05-02\n",
      "  done: false\n",
      "  episode_len_mean: 90.84684684684684\n",
      "  episode_reward_max: 220.57471937238435\n",
      "  episode_reward_mean: 176.5974557161089\n",
      "  episode_reward_min: 142.41877863501088\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 49069\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3169.97\n",
      "    load_time_ms: 2.224\n",
      "    num_steps_sampled: 4750000\n",
      "    num_steps_trained: 4750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3751617968082428\n",
      "      kl: 0.009506159462034702\n",
      "      policy_loss: -0.0024755308404564857\n",
      "      total_loss: 2.1046509742736816\n",
      "      vf_explained_var: 0.9959837794303894\n",
      "      vf_loss: 2.1071267127990723\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.17399927973747253\n",
      "      kl: 0.018087545409798622\n",
      "      policy_loss: 0.005207275506108999\n",
      "      total_loss: 2.4263031482696533\n",
      "      vf_explained_var: 0.9973892569541931\n",
      "      vf_loss: 2.421096086502075\n",
      "    sample_time_ms: 20043.306\n",
      "    update_time_ms: 7.644\n",
      "  iterations_since_restore: 475\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.7392830440454\n",
      "    rl_1: 103.85817267206343\n",
      "  time_since_restore: 11277.701583385468\n",
      "  time_this_iter_s: 22.89898443222046\n",
      "  time_total_s: 11277.701583385468\n",
      "  timestamp: 1550804702\n",
      "  timesteps_since_restore: 4750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4750000\n",
      "  training_iteration: 475\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11277 s, 475 iter, 4750000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-05-25\n",
      "  done: false\n",
      "  episode_len_mean: 91.27522935779817\n",
      "  episode_reward_max: 216.86886527627985\n",
      "  episode_reward_mean: 176.077593105657\n",
      "  episode_reward_min: 140.39102752626297\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 49178\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3164.517\n",
      "    load_time_ms: 2.22\n",
      "    num_steps_sampled: 4760000\n",
      "    num_steps_trained: 4760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4116266071796417\n",
      "      kl: 0.009667239151895046\n",
      "      policy_loss: -0.003087047254666686\n",
      "      total_loss: 2.3894169330596924\n",
      "      vf_explained_var: 0.9952282309532166\n",
      "      vf_loss: 2.3925037384033203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.14119157195091248\n",
      "      kl: 0.014588994905352592\n",
      "      policy_loss: 0.0013533971505239606\n",
      "      total_loss: 2.490325689315796\n",
      "      vf_explained_var: 0.9972119331359863\n",
      "      vf_loss: 2.4889721870422363\n",
      "    sample_time_ms: 19996.753\n",
      "    update_time_ms: 7.637\n",
      "  iterations_since_restore: 476\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.49638052912618\n",
      "    rl_1: 102.58121257653082\n",
      "  time_since_restore: 11300.83258342743\n",
      "  time_this_iter_s: 23.13100004196167\n",
      "  time_total_s: 11300.83258342743\n",
      "  timestamp: 1550804725\n",
      "  timesteps_since_restore: 4760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4760000\n",
      "  training_iteration: 476\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11300 s, 476 iter, 4760000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-05-48\n",
      "  done: false\n",
      "  episode_len_mean: 93.48148148148148\n",
      "  episode_reward_max: 214.90640714887377\n",
      "  episode_reward_mean: 175.37845601214215\n",
      "  episode_reward_min: 18.01074993804856\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 49286\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3163.209\n",
      "    load_time_ms: 2.318\n",
      "    num_steps_sampled: 4770000\n",
      "    num_steps_trained: 4770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4381858706474304\n",
      "      kl: 0.009815583936870098\n",
      "      policy_loss: -0.0025063715875148773\n",
      "      total_loss: 3.1777687072753906\n",
      "      vf_explained_var: 0.9946658611297607\n",
      "      vf_loss: 3.1802749633789062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0848020687699318\n",
      "      kl: 0.014990353025496006\n",
      "      policy_loss: 0.0030772765167057514\n",
      "      total_loss: 2.9380850791931152\n",
      "      vf_explained_var: 0.9972150921821594\n",
      "      vf_loss: 2.9350078105926514\n",
      "    sample_time_ms: 19994.078\n",
      "    update_time_ms: 7.787\n",
      "  iterations_since_restore: 477\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.80403333124057\n",
      "    rl_1: 101.5744226809016\n",
      "  time_since_restore: 11324.247151136398\n",
      "  time_this_iter_s: 23.414567708969116\n",
      "  time_total_s: 11324.247151136398\n",
      "  timestamp: 1550804748\n",
      "  timesteps_since_restore: 4770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4770000\n",
      "  training_iteration: 477\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11324 s, 477 iter, 4770000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-06-11\n",
      "  done: false\n",
      "  episode_len_mean: 91.1559633027523\n",
      "  episode_reward_max: 221.42918025945903\n",
      "  episode_reward_mean: 182.71545516937232\n",
      "  episode_reward_min: 138.51639580509564\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 49395\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.455\n",
      "    load_time_ms: 2.384\n",
      "    num_steps_sampled: 4780000\n",
      "    num_steps_trained: 4780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3422165513038635\n",
      "      kl: 0.007892658933997154\n",
      "      policy_loss: -0.0018613612046465278\n",
      "      total_loss: 1.7339874505996704\n",
      "      vf_explained_var: 0.9967889189720154\n",
      "      vf_loss: 1.7358487844467163\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1795530915260315\n",
      "      kl: 0.014425726607441902\n",
      "      policy_loss: 0.001461838255636394\n",
      "      total_loss: 2.10823392868042\n",
      "      vf_explained_var: 0.9978440999984741\n",
      "      vf_loss: 2.1067724227905273\n",
      "    sample_time_ms: 19991.937\n",
      "    update_time_ms: 7.307\n",
      "  iterations_since_restore: 478\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.04418375523069\n",
      "    rl_1: 106.67127141414167\n",
      "  time_since_restore: 11347.307434558868\n",
      "  time_this_iter_s: 23.060283422470093\n",
      "  time_total_s: 11347.307434558868\n",
      "  timestamp: 1550804771\n",
      "  timesteps_since_restore: 4780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4780000\n",
      "  training_iteration: 478\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11347 s, 478 iter, 4780000 ts, 183 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-06-34\n",
      "  done: false\n",
      "  episode_len_mean: 91.0\n",
      "  episode_reward_max: 222.38364628543084\n",
      "  episode_reward_mean: 176.88700171494932\n",
      "  episode_reward_min: -141.0720148562643\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 49505\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.593\n",
      "    load_time_ms: 2.423\n",
      "    num_steps_sampled: 4790000\n",
      "    num_steps_trained: 4790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39930295944213867\n",
      "      kl: 0.010794536210596561\n",
      "      policy_loss: -0.003790112677961588\n",
      "      total_loss: 11.502217292785645\n",
      "      vf_explained_var: 0.981683611869812\n",
      "      vf_loss: 11.50600528717041\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1995050311088562\n",
      "      kl: 0.010259103961288929\n",
      "      policy_loss: -0.0010510921711102128\n",
      "      total_loss: 11.963501930236816\n",
      "      vf_explained_var: 0.9895879030227661\n",
      "      vf_loss: 11.964553833007812\n",
      "    sample_time_ms: 19971.023\n",
      "    update_time_ms: 7.127\n",
      "  iterations_since_restore: 479\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.11489730609273\n",
      "    rl_1: 103.77210440885656\n",
      "  time_since_restore: 11370.278454780579\n",
      "  time_this_iter_s: 22.971020221710205\n",
      "  time_total_s: 11370.278454780579\n",
      "  timestamp: 1550804794\n",
      "  timesteps_since_restore: 4790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4790000\n",
      "  training_iteration: 479\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11370 s, 479 iter, 4790000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-06-58\n",
      "  done: false\n",
      "  episode_len_mean: 91.68807339449542\n",
      "  episode_reward_max: 224.65807048759692\n",
      "  episode_reward_mean: 178.92851113460037\n",
      "  episode_reward_min: 142.17216071652004\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 49614\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.914\n",
      "    load_time_ms: 2.416\n",
      "    num_steps_sampled: 4800000\n",
      "    num_steps_trained: 4800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39998120069503784\n",
      "      kl: 0.007498788647353649\n",
      "      policy_loss: -0.0026596705429255962\n",
      "      total_loss: 2.1424338817596436\n",
      "      vf_explained_var: 0.9958379864692688\n",
      "      vf_loss: 2.1450939178466797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21726249158382416\n",
      "      kl: 0.01480246614664793\n",
      "      policy_loss: 0.0005949355545453727\n",
      "      total_loss: 2.173478126525879\n",
      "      vf_explained_var: 0.9977132081985474\n",
      "      vf_loss: 2.1728832721710205\n",
      "    sample_time_ms: 20011.917\n",
      "    update_time_ms: 6.838\n",
      "  iterations_since_restore: 480\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.22292885734606\n",
      "    rl_1: 105.70558227725431\n",
      "  time_since_restore: 11393.68250823021\n",
      "  time_this_iter_s: 23.404053449630737\n",
      "  time_total_s: 11393.68250823021\n",
      "  timestamp: 1550804818\n",
      "  timesteps_since_restore: 4800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4800000\n",
      "  training_iteration: 480\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11393 s, 480 iter, 4800000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-07-21\n",
      "  done: false\n",
      "  episode_len_mean: 91.62385321100918\n",
      "  episode_reward_max: 219.56248277356758\n",
      "  episode_reward_mean: 178.7196596877476\n",
      "  episode_reward_min: 144.37484191581441\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 49723\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3154.995\n",
      "    load_time_ms: 2.373\n",
      "    num_steps_sampled: 4810000\n",
      "    num_steps_trained: 4810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.40560683608055115\n",
      "      kl: 0.01122476439923048\n",
      "      policy_loss: -0.0033000020775943995\n",
      "      total_loss: 2.2608485221862793\n",
      "      vf_explained_var: 0.9956620335578918\n",
      "      vf_loss: 2.264148473739624\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1719367653131485\n",
      "      kl: 0.01174356509000063\n",
      "      policy_loss: 0.0014903700212016702\n",
      "      total_loss: 2.360870122909546\n",
      "      vf_explained_var: 0.9975020289421082\n",
      "      vf_loss: 2.359380006790161\n",
      "    sample_time_ms: 19960.549\n",
      "    update_time_ms: 7.023\n",
      "  iterations_since_restore: 481\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.44871907252812\n",
      "    rl_1: 105.27094061521947\n",
      "  time_since_restore: 11416.759610891342\n",
      "  time_this_iter_s: 23.077102661132812\n",
      "  time_total_s: 11416.759610891342\n",
      "  timestamp: 1550804841\n",
      "  timesteps_since_restore: 4810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4810000\n",
      "  training_iteration: 481\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11416 s, 481 iter, 4810000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-07-44\n",
      "  done: false\n",
      "  episode_len_mean: 91.44545454545455\n",
      "  episode_reward_max: 219.9208500523943\n",
      "  episode_reward_mean: 173.05380086796097\n",
      "  episode_reward_min: -157.441281680619\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 49833\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.665\n",
      "    load_time_ms: 2.402\n",
      "    num_steps_sampled: 4820000\n",
      "    num_steps_trained: 4820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4465242028236389\n",
      "      kl: 0.006863137241452932\n",
      "      policy_loss: -0.002167737577110529\n",
      "      total_loss: 19.553741455078125\n",
      "      vf_explained_var: 0.9726366996765137\n",
      "      vf_loss: 19.555910110473633\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.16375158727169037\n",
      "      kl: 0.005774541292339563\n",
      "      policy_loss: -0.00028534329612739384\n",
      "      total_loss: 20.252038955688477\n",
      "      vf_explained_var: 0.9830619692802429\n",
      "      vf_loss: 20.2523250579834\n",
      "    sample_time_ms: 19992.294\n",
      "    update_time_ms: 7.106\n",
      "  iterations_since_restore: 482\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.6409412395448\n",
      "    rl_1: 101.41285962841619\n",
      "  time_since_restore: 11440.044885635376\n",
      "  time_this_iter_s: 23.285274744033813\n",
      "  time_total_s: 11440.044885635376\n",
      "  timestamp: 1550804864\n",
      "  timesteps_since_restore: 4820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4820000\n",
      "  training_iteration: 482\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11440 s, 482 iter, 4820000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-08-08\n",
      "  done: false\n",
      "  episode_len_mean: 89.8018018018018\n",
      "  episode_reward_max: 219.44117916023026\n",
      "  episode_reward_mean: 170.98182795262028\n",
      "  episode_reward_min: -165.15776894059996\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 49944\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.042\n",
      "    load_time_ms: 2.416\n",
      "    num_steps_sampled: 4830000\n",
      "    num_steps_trained: 4830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4020162522792816\n",
      "      kl: 0.015643887221813202\n",
      "      policy_loss: -0.0022960554342716932\n",
      "      total_loss: 80.91178131103516\n",
      "      vf_explained_var: 0.8830456733703613\n",
      "      vf_loss: 80.91407775878906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.172951802611351\n",
      "      kl: 0.03029693104326725\n",
      "      policy_loss: 0.0006356211379170418\n",
      "      total_loss: 100.24720764160156\n",
      "      vf_explained_var: 0.9161661863327026\n",
      "      vf_loss: 100.24656677246094\n",
      "    sample_time_ms: 20006.553\n",
      "    update_time_ms: 6.818\n",
      "  iterations_since_restore: 483\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.08017880135881\n",
      "    rl_1: 99.90164915126147\n",
      "  time_since_restore: 11463.218257188797\n",
      "  time_this_iter_s: 23.17337155342102\n",
      "  time_total_s: 11463.218257188797\n",
      "  timestamp: 1550804888\n",
      "  timesteps_since_restore: 4830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4830000\n",
      "  training_iteration: 483\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11463 s, 483 iter, 4830000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-08-31\n",
      "  done: false\n",
      "  episode_len_mean: 92.19266055045871\n",
      "  episode_reward_max: 220.77175410053817\n",
      "  episode_reward_mean: 174.57006179942948\n",
      "  episode_reward_min: -175.64925998693872\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 50053\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.965\n",
      "    load_time_ms: 2.331\n",
      "    num_steps_sampled: 4840000\n",
      "    num_steps_trained: 4840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4002038836479187\n",
      "      kl: 0.00902504287660122\n",
      "      policy_loss: -0.003473358228802681\n",
      "      total_loss: 37.48065185546875\n",
      "      vf_explained_var: 0.9435930252075195\n",
      "      vf_loss: 37.48411560058594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.20048850774765015\n",
      "      kl: 14.359667778015137\n",
      "      policy_loss: 0.06044707074761391\n",
      "      total_loss: 48.88824462890625\n",
      "      vf_explained_var: 0.9571095705032349\n",
      "      vf_loss: 48.827796936035156\n",
      "    sample_time_ms: 20017.151\n",
      "    update_time_ms: 6.634\n",
      "  iterations_since_restore: 484\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.4800840282104\n",
      "    rl_1: 104.08997777121904\n",
      "  time_since_restore: 11486.607941627502\n",
      "  time_this_iter_s: 23.389684438705444\n",
      "  time_total_s: 11486.607941627502\n",
      "  timestamp: 1550804911\n",
      "  timesteps_since_restore: 4840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4840000\n",
      "  training_iteration: 484\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11486 s, 484 iter, 4840000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-08-55\n",
      "  done: false\n",
      "  episode_len_mean: 90.2\n",
      "  episode_reward_max: 216.83283177808386\n",
      "  episode_reward_mean: 162.85802391730985\n",
      "  episode_reward_min: -173.7708405562255\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 50163\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.697\n",
      "    load_time_ms: 2.32\n",
      "    num_steps_sampled: 4850000\n",
      "    num_steps_trained: 4850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3960415720939636\n",
      "      kl: 0.009415820240974426\n",
      "      policy_loss: -0.0012629283592104912\n",
      "      total_loss: 174.6447296142578\n",
      "      vf_explained_var: 0.771302342414856\n",
      "      vf_loss: 174.64602661132812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21442249417304993\n",
      "      kl: 0.013938882388174534\n",
      "      policy_loss: -0.0009574946598149836\n",
      "      total_loss: 207.48342895507812\n",
      "      vf_explained_var: 0.8222395181655884\n",
      "      vf_loss: 207.484375\n",
      "    sample_time_ms: 20082.632\n",
      "    update_time_ms: 6.655\n",
      "  iterations_since_restore: 485\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.18406881429927\n",
      "    rl_1: 95.67395510301058\n",
      "  time_since_restore: 11510.130118608475\n",
      "  time_this_iter_s: 23.52217698097229\n",
      "  time_total_s: 11510.130118608475\n",
      "  timestamp: 1550804935\n",
      "  timesteps_since_restore: 4850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4850000\n",
      "  training_iteration: 485\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11510 s, 485 iter, 4850000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-09-18\n",
      "  done: false\n",
      "  episode_len_mean: 90.38738738738739\n",
      "  episode_reward_max: 225.23171253413915\n",
      "  episode_reward_mean: 162.7775973055977\n",
      "  episode_reward_min: -168.6772762685675\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 50274\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.15\n",
      "    load_time_ms: 2.316\n",
      "    num_steps_sampled: 4860000\n",
      "    num_steps_trained: 4860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3988243639469147\n",
      "      kl: 0.01116927806288004\n",
      "      policy_loss: -0.0025809332728385925\n",
      "      total_loss: 167.23495483398438\n",
      "      vf_explained_var: 0.7811813950538635\n",
      "      vf_loss: 167.23753356933594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.20726382732391357\n",
      "      kl: 0.037813570350408554\n",
      "      policy_loss: 0.00451701832935214\n",
      "      total_loss: 205.35452270507812\n",
      "      vf_explained_var: 0.8236099481582642\n",
      "      vf_loss: 205.34999084472656\n",
      "    sample_time_ms: 20080.456\n",
      "    update_time_ms: 6.84\n",
      "  iterations_since_restore: 486\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.75264912022186\n",
      "    rl_1: 96.02494818537579\n",
      "  time_since_restore: 11533.226272583008\n",
      "  time_this_iter_s: 23.09615397453308\n",
      "  time_total_s: 11533.226272583008\n",
      "  timestamp: 1550804958\n",
      "  timesteps_since_restore: 4860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4860000\n",
      "  training_iteration: 486\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11533 s, 486 iter, 4860000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-09-41\n",
      "  done: false\n",
      "  episode_len_mean: 88.41071428571429\n",
      "  episode_reward_max: 225.08841096253514\n",
      "  episode_reward_mean: 151.47230018102104\n",
      "  episode_reward_min: -152.36478249613583\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 50386\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.283\n",
      "    load_time_ms: 2.313\n",
      "    num_steps_sampled: 4870000\n",
      "    num_steps_trained: 4870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.40770846605300903\n",
      "      kl: 0.00919153168797493\n",
      "      policy_loss: -0.002189376624301076\n",
      "      total_loss: 238.80921936035156\n",
      "      vf_explained_var: 0.7101160287857056\n",
      "      vf_loss: 238.81137084960938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1899222880601883\n",
      "      kl: 0.013804561458528042\n",
      "      policy_loss: 0.0016215264331549406\n",
      "      total_loss: 284.921630859375\n",
      "      vf_explained_var: 0.7572788000106812\n",
      "      vf_loss: 284.91998291015625\n",
      "    sample_time_ms: 20029.572\n",
      "    update_time_ms: 6.851\n",
      "  iterations_since_restore: 487\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.037549121198346\n",
      "    rl_1: 88.43475105982272\n",
      "  time_since_restore: 11556.152693033218\n",
      "  time_this_iter_s: 22.92642045021057\n",
      "  time_total_s: 11556.152693033218\n",
      "  timestamp: 1550804981\n",
      "  timesteps_since_restore: 4870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4870000\n",
      "  training_iteration: 487\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11556 s, 487 iter, 4870000 ts, 151 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-10-04\n",
      "  done: false\n",
      "  episode_len_mean: 88.63716814159292\n",
      "  episode_reward_max: 222.09884983289882\n",
      "  episode_reward_mean: 149.13885216823667\n",
      "  episode_reward_min: -176.37345704745897\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 50499\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.773\n",
      "    load_time_ms: 2.298\n",
      "    num_steps_sampled: 4880000\n",
      "    num_steps_trained: 4880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3563784956932068\n",
      "      kl: 0.00952050369232893\n",
      "      policy_loss: -0.0030175186693668365\n",
      "      total_loss: 248.7499237060547\n",
      "      vf_explained_var: 0.7089422941207886\n",
      "      vf_loss: 248.7529754638672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.19013233482837677\n",
      "      kl: 0.02458803541958332\n",
      "      policy_loss: 0.003885293146595359\n",
      "      total_loss: 307.2625732421875\n",
      "      vf_explained_var: 0.7408519387245178\n",
      "      vf_loss: 307.2586975097656\n",
      "    sample_time_ms: 20075.728\n",
      "    update_time_ms: 6.766\n",
      "  iterations_since_restore: 488\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.66455720704066\n",
      "    rl_1: 87.47429496119604\n",
      "  time_since_restore: 11579.688504457474\n",
      "  time_this_iter_s: 23.53581142425537\n",
      "  time_total_s: 11579.688504457474\n",
      "  timestamp: 1550805004\n",
      "  timesteps_since_restore: 4880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4880000\n",
      "  training_iteration: 488\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11579 s, 488 iter, 4880000 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-10-28\n",
      "  done: false\n",
      "  episode_len_mean: 90.67567567567568\n",
      "  episode_reward_max: 227.2039863230343\n",
      "  episode_reward_mean: 157.4070472099417\n",
      "  episode_reward_min: -155.31935796199213\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 50610\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.471\n",
      "    load_time_ms: 2.266\n",
      "    num_steps_sampled: 4890000\n",
      "    num_steps_trained: 4890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39343827962875366\n",
      "      kl: 0.007221621461212635\n",
      "      policy_loss: -0.0021265894174575806\n",
      "      total_loss: 211.75390625\n",
      "      vf_explained_var: 0.7339730858802795\n",
      "      vf_loss: 211.7559814453125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1806047409772873\n",
      "      kl: 0.009239559061825275\n",
      "      policy_loss: 0.0002313644072273746\n",
      "      total_loss: 265.8932189941406\n",
      "      vf_explained_var: 0.7613692283630371\n",
      "      vf_loss: 265.89300537109375\n",
      "    sample_time_ms: 20115.695\n",
      "    update_time_ms: 6.958\n",
      "  iterations_since_restore: 489\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.85353928500374\n",
      "    rl_1: 93.55350792493796\n",
      "  time_since_restore: 11603.06423830986\n",
      "  time_this_iter_s: 23.375733852386475\n",
      "  time_total_s: 11603.06423830986\n",
      "  timestamp: 1550805028\n",
      "  timesteps_since_restore: 4890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4890000\n",
      "  training_iteration: 489\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11603 s, 489 iter, 4890000 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-10-51\n",
      "  done: false\n",
      "  episode_len_mean: 91.21818181818182\n",
      "  episode_reward_max: 222.51537865987376\n",
      "  episode_reward_mean: 168.04975832559978\n",
      "  episode_reward_min: -155.81379901106288\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 50720\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3152.313\n",
      "    load_time_ms: 2.322\n",
      "    num_steps_sampled: 4900000\n",
      "    num_steps_trained: 4900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3576447069644928\n",
      "      kl: 0.009567847475409508\n",
      "      policy_loss: -0.0006522359326481819\n",
      "      total_loss: 108.03842163085938\n",
      "      vf_explained_var: 0.844518780708313\n",
      "      vf_loss: 108.0390853881836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.20950505137443542\n",
      "      kl: 0.009687180630862713\n",
      "      policy_loss: -0.0026401057839393616\n",
      "      total_loss: 128.74119567871094\n",
      "      vf_explained_var: 0.8790974617004395\n",
      "      vf_loss: 128.7438507080078\n",
      "    sample_time_ms: 20103.396\n",
      "    update_time_ms: 7.032\n",
      "  iterations_since_restore: 490\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.58241784418125\n",
      "    rl_1: 99.46734048141863\n",
      "  time_since_restore: 11626.514955759048\n",
      "  time_this_iter_s: 23.450717449188232\n",
      "  time_total_s: 11626.514955759048\n",
      "  timestamp: 1550805051\n",
      "  timesteps_since_restore: 4900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4900000\n",
      "  training_iteration: 490\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11626 s, 490 iter, 4900000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-11-14\n",
      "  done: false\n",
      "  episode_len_mean: 92.67592592592592\n",
      "  episode_reward_max: 222.88197287825685\n",
      "  episode_reward_mean: 171.54807593786387\n",
      "  episode_reward_min: -156.4079857671672\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 50828\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.238\n",
      "    load_time_ms: 2.386\n",
      "    num_steps_sampled: 4910000\n",
      "    num_steps_trained: 4910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.33828532695770264\n",
      "      kl: 0.008701883256435394\n",
      "      policy_loss: -0.003144079353660345\n",
      "      total_loss: 71.68460845947266\n",
      "      vf_explained_var: 0.885396420955658\n",
      "      vf_loss: 71.68775177001953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.17101871967315674\n",
      "      kl: 0.011317668482661247\n",
      "      policy_loss: 0.001825301325879991\n",
      "      total_loss: 88.94551849365234\n",
      "      vf_explained_var: 0.9057579040527344\n",
      "      vf_loss: 88.9437026977539\n",
      "    sample_time_ms: 20092.972\n",
      "    update_time_ms: 6.911\n",
      "  iterations_since_restore: 491\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.277033026964\n",
      "    rl_1: 100.27104291089982\n",
      "  time_since_restore: 11649.254970550537\n",
      "  time_this_iter_s: 22.740014791488647\n",
      "  time_total_s: 11649.254970550537\n",
      "  timestamp: 1550805074\n",
      "  timesteps_since_restore: 4910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4910000\n",
      "  training_iteration: 491\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11649 s, 491 iter, 4910000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-11-37\n",
      "  done: false\n",
      "  episode_len_mean: 92.38888888888889\n",
      "  episode_reward_max: 222.9474818007999\n",
      "  episode_reward_mean: 176.93173808485503\n",
      "  episode_reward_min: -137.05987527873754\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 50936\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.211\n",
      "    load_time_ms: 2.333\n",
      "    num_steps_sampled: 4920000\n",
      "    num_steps_trained: 4920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32555875182151794\n",
      "      kl: 0.009545504115521908\n",
      "      policy_loss: -0.002293065655976534\n",
      "      total_loss: 39.99089431762695\n",
      "      vf_explained_var: 0.9309515357017517\n",
      "      vf_loss: 39.993186950683594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25618264079093933\n",
      "      kl: 0.026134390383958817\n",
      "      policy_loss: 0.008486835286021233\n",
      "      total_loss: 47.96460723876953\n",
      "      vf_explained_var: 0.948815643787384\n",
      "      vf_loss: 47.95612716674805\n",
      "    sample_time_ms: 20104.834\n",
      "    update_time_ms: 6.897\n",
      "  iterations_since_restore: 492\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.0598260704983\n",
      "    rl_1: 103.87191201435672\n",
      "  time_since_restore: 11672.616666793823\n",
      "  time_this_iter_s: 23.361696243286133\n",
      "  time_total_s: 11672.616666793823\n",
      "  timestamp: 1550805097\n",
      "  timesteps_since_restore: 4920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4920000\n",
      "  training_iteration: 492\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11672 s, 492 iter, 4920000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-12-01\n",
      "  done: false\n",
      "  episode_len_mean: 91.19090909090909\n",
      "  episode_reward_max: 226.25405049903216\n",
      "  episode_reward_mean: 175.53259592332947\n",
      "  episode_reward_min: -144.64358861106277\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 51046\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.415\n",
      "    load_time_ms: 2.392\n",
      "    num_steps_sampled: 4930000\n",
      "    num_steps_trained: 4930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.35609006881713867\n",
      "      kl: 0.008645979687571526\n",
      "      policy_loss: -0.0016110875876620412\n",
      "      total_loss: 62.186065673828125\n",
      "      vf_explained_var: 0.9046086072921753\n",
      "      vf_loss: 62.18768310546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21258491277694702\n",
      "      kl: 0.024848734959959984\n",
      "      policy_loss: 0.0017631598748266697\n",
      "      total_loss: 72.09806060791016\n",
      "      vf_explained_var: 0.9290248155593872\n",
      "      vf_loss: 72.09629821777344\n",
      "    sample_time_ms: 20129.323\n",
      "    update_time_ms: 7.22\n",
      "  iterations_since_restore: 493\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.65257277667902\n",
      "    rl_1: 101.88002314665046\n",
      "  time_since_restore: 11696.04270529747\n",
      "  time_this_iter_s: 23.42603850364685\n",
      "  time_total_s: 11696.04270529747\n",
      "  timestamp: 1550805121\n",
      "  timesteps_since_restore: 4930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4930000\n",
      "  training_iteration: 493\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11696 s, 493 iter, 4930000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-12-24\n",
      "  done: false\n",
      "  episode_len_mean: 92.29629629629629\n",
      "  episode_reward_max: 225.9084524834662\n",
      "  episode_reward_mean: 182.32840716466453\n",
      "  episode_reward_min: 140.63995373035507\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 51154\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3121.062\n",
      "    load_time_ms: 2.425\n",
      "    num_steps_sampled: 4940000\n",
      "    num_steps_trained: 4940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32481077313423157\n",
      "      kl: 0.010899107903242111\n",
      "      policy_loss: -0.0021719958167523146\n",
      "      total_loss: 8.659542083740234\n",
      "      vf_explained_var: 0.9843501448631287\n",
      "      vf_loss: 8.661714553833008\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26193612813949585\n",
      "      kl: 0.010378909297287464\n",
      "      policy_loss: 0.0017240154556930065\n",
      "      total_loss: 8.138863563537598\n",
      "      vf_explained_var: 0.9918338060379028\n",
      "      vf_loss: 8.137139320373535\n",
      "    sample_time_ms: 20108.871\n",
      "    update_time_ms: 7.336\n",
      "  iterations_since_restore: 494\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.9017267923484\n",
      "    rl_1: 106.42668037231608\n",
      "  time_since_restore: 11719.188251495361\n",
      "  time_this_iter_s: 23.145546197891235\n",
      "  time_total_s: 11719.188251495361\n",
      "  timestamp: 1550805144\n",
      "  timesteps_since_restore: 4940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4940000\n",
      "  training_iteration: 494\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11719 s, 494 iter, 4940000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-12-47\n",
      "  done: false\n",
      "  episode_len_mean: 92.60185185185185\n",
      "  episode_reward_max: 225.01905508842415\n",
      "  episode_reward_mean: 175.73068556685973\n",
      "  episode_reward_min: -149.965587135356\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 51262\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3123.246\n",
      "    load_time_ms: 2.47\n",
      "    num_steps_sampled: 4950000\n",
      "    num_steps_trained: 4950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.345965176820755\n",
      "      kl: 0.011131174862384796\n",
      "      policy_loss: -0.002360375365242362\n",
      "      total_loss: 34.57021713256836\n",
      "      vf_explained_var: 0.9434690475463867\n",
      "      vf_loss: 34.572574615478516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21176470816135406\n",
      "      kl: 0.01290904451161623\n",
      "      policy_loss: 0.0005095350788906217\n",
      "      total_loss: 41.65727996826172\n",
      "      vf_explained_var: 0.9602513313293457\n",
      "      vf_loss: 41.65676498413086\n",
      "    sample_time_ms: 20091.586\n",
      "    update_time_ms: 7.78\n",
      "  iterations_since_restore: 495\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.8823038332723\n",
      "    rl_1: 103.84838173358746\n",
      "  time_since_restore: 11742.56382894516\n",
      "  time_this_iter_s: 23.375577449798584\n",
      "  time_total_s: 11742.56382894516\n",
      "  timestamp: 1550805167\n",
      "  timesteps_since_restore: 4950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4950000\n",
      "  training_iteration: 495\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11742 s, 495 iter, 4950000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-13-11\n",
      "  done: false\n",
      "  episode_len_mean: 91.42201834862385\n",
      "  episode_reward_max: 218.31919129507713\n",
      "  episode_reward_mean: 170.03056646697374\n",
      "  episode_reward_min: -158.44299006256097\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 51371\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.213\n",
      "    load_time_ms: 2.467\n",
      "    num_steps_sampled: 4960000\n",
      "    num_steps_trained: 4960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.31734853982925415\n",
      "      kl: 0.009367658756673336\n",
      "      policy_loss: -0.0016191995237022638\n",
      "      total_loss: 71.87517547607422\n",
      "      vf_explained_var: 0.8842312693595886\n",
      "      vf_loss: 71.87678527832031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.17807388305664062\n",
      "      kl: 0.02889064885675907\n",
      "      policy_loss: 0.0026690096128731966\n",
      "      total_loss: 85.16004943847656\n",
      "      vf_explained_var: 0.9187260270118713\n",
      "      vf_loss: 85.15738677978516\n",
      "    sample_time_ms: 20093.78\n",
      "    update_time_ms: 7.584\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.42121579686851\n",
      "    rl_1: 98.60935067010519\n",
      "  time_since_restore: 11765.69932961464\n",
      "  time_this_iter_s: 23.13550066947937\n",
      "  time_total_s: 11765.69932961464\n",
      "  timestamp: 1550805191\n",
      "  timesteps_since_restore: 4960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4960000\n",
      "  training_iteration: 496\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11765 s, 496 iter, 4960000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-13-34\n",
      "  done: false\n",
      "  episode_len_mean: 92.39814814814815\n",
      "  episode_reward_max: 223.04272305578112\n",
      "  episode_reward_mean: 174.19311806216695\n",
      "  episode_reward_min: -144.56460557809112\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 51479\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.94\n",
      "    load_time_ms: 2.403\n",
      "    num_steps_sampled: 4970000\n",
      "    num_steps_trained: 4970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3435952663421631\n",
      "      kl: 0.007183232344686985\n",
      "      policy_loss: -0.003007087856531143\n",
      "      total_loss: 36.477848052978516\n",
      "      vf_explained_var: 0.9413436651229858\n",
      "      vf_loss: 36.480857849121094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2099718451499939\n",
      "      kl: 0.03884999826550484\n",
      "      policy_loss: 0.005616955924779177\n",
      "      total_loss: 43.937191009521484\n",
      "      vf_explained_var: 0.9559425711631775\n",
      "      vf_loss: 43.93157196044922\n",
      "    sample_time_ms: 20158.527\n",
      "    update_time_ms: 7.49\n",
      "  iterations_since_restore: 497\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.17231374240599\n",
      "    rl_1: 102.02080431976096\n",
      "  time_since_restore: 11789.319027662277\n",
      "  time_this_iter_s: 23.61969804763794\n",
      "  time_total_s: 11789.319027662277\n",
      "  timestamp: 1550805214\n",
      "  timesteps_since_restore: 4970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4970000\n",
      "  training_iteration: 497\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11789 s, 497 iter, 4970000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-13-58\n",
      "  done: false\n",
      "  episode_len_mean: 92.30275229357798\n",
      "  episode_reward_max: 219.33404159250858\n",
      "  episode_reward_mean: 178.30394472071868\n",
      "  episode_reward_min: -91.38976621479033\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 51588\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.751\n",
      "    load_time_ms: 2.419\n",
      "    num_steps_sampled: 4980000\n",
      "    num_steps_trained: 4980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.29619070887565613\n",
      "      kl: 0.014381496235728264\n",
      "      policy_loss: -0.002325982553884387\n",
      "      total_loss: 9.988862991333008\n",
      "      vf_explained_var: 0.9843209981918335\n",
      "      vf_loss: 9.991189002990723\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22160498797893524\n",
      "      kl: 0.007089271210134029\n",
      "      policy_loss: -0.0007710257195867598\n",
      "      total_loss: 10.645112991333008\n",
      "      vf_explained_var: 0.9900941848754883\n",
      "      vf_loss: 10.64588451385498\n",
      "    sample_time_ms: 20150.3\n",
      "    update_time_ms: 7.574\n",
      "  iterations_since_restore: 498\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.16161652104607\n",
      "    rl_1: 104.14232819967265\n",
      "  time_since_restore: 11812.791092634201\n",
      "  time_this_iter_s: 23.472064971923828\n",
      "  time_total_s: 11812.791092634201\n",
      "  timestamp: 1550805238\n",
      "  timesteps_since_restore: 4980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4980000\n",
      "  training_iteration: 498\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11812 s, 498 iter, 4980000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-14-22\n",
      "  done: false\n",
      "  episode_len_mean: 92.69158878504673\n",
      "  episode_reward_max: 219.1286534608536\n",
      "  episode_reward_mean: 174.85497856491136\n",
      "  episode_reward_min: -127.66249796247651\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 51695\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3155.477\n",
      "    load_time_ms: 2.425\n",
      "    num_steps_sampled: 4990000\n",
      "    num_steps_trained: 4990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.302152156829834\n",
      "      kl: 0.008021647110581398\n",
      "      policy_loss: -0.003916803281754255\n",
      "      total_loss: 11.645095825195312\n",
      "      vf_explained_var: 0.9808641076087952\n",
      "      vf_loss: 11.649012565612793\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2066408097743988\n",
      "      kl: 0.01318720355629921\n",
      "      policy_loss: 0.0005899463430978358\n",
      "      total_loss: 11.727425575256348\n",
      "      vf_explained_var: 0.9887710213661194\n",
      "      vf_loss: 11.726835250854492\n",
      "    sample_time_ms: 20192.087\n",
      "    update_time_ms: 8.159\n",
      "  iterations_since_restore: 499\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.39663228531698\n",
      "    rl_1: 103.45834627959442\n",
      "  time_since_restore: 11836.827817678452\n",
      "  time_this_iter_s: 24.03672504425049\n",
      "  time_total_s: 11836.827817678452\n",
      "  timestamp: 1550805262\n",
      "  timesteps_since_restore: 4990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 4990000\n",
      "  training_iteration: 499\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11836 s, 499 iter, 4990000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-14-45\n",
      "  done: false\n",
      "  episode_len_mean: 91.65137614678899\n",
      "  episode_reward_max: 220.63935278925138\n",
      "  episode_reward_mean: 178.65604678951576\n",
      "  episode_reward_min: -171.71827823057066\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 51804\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.416\n",
      "    load_time_ms: 2.403\n",
      "    num_steps_sampled: 5000000\n",
      "    num_steps_trained: 5000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2800767719745636\n",
      "      kl: 0.012391932308673859\n",
      "      policy_loss: -0.0036662076599895954\n",
      "      total_loss: 32.47400665283203\n",
      "      vf_explained_var: 0.9486196637153625\n",
      "      vf_loss: 32.4776725769043\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.15383926033973694\n",
      "      kl: 0.04033347964286804\n",
      "      policy_loss: 0.00033714136225171387\n",
      "      total_loss: 42.79023361206055\n",
      "      vf_explained_var: 0.9602011442184448\n",
      "      vf_loss: 42.789894104003906\n",
      "    sample_time_ms: 20193.04\n",
      "    update_time_ms: 8.334\n",
      "  iterations_since_restore: 500\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.5974179666346\n",
      "    rl_1: 104.05862882288113\n",
      "  time_since_restore: 11860.121653079987\n",
      "  time_this_iter_s: 23.293835401535034\n",
      "  time_total_s: 11860.121653079987\n",
      "  timestamp: 1550805285\n",
      "  timesteps_since_restore: 5000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5000000\n",
      "  training_iteration: 500\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11860 s, 500 iter, 5000000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-15-09\n",
      "  done: false\n",
      "  episode_len_mean: 93.29629629629629\n",
      "  episode_reward_max: 224.03335565326\n",
      "  episode_reward_mean: 179.525050209738\n",
      "  episode_reward_min: -100.91194879060501\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 51912\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.695\n",
      "    load_time_ms: 2.328\n",
      "    num_steps_sampled: 5010000\n",
      "    num_steps_trained: 5010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2661938965320587\n",
      "      kl: 0.010311402380466461\n",
      "      policy_loss: -0.0023097347002476454\n",
      "      total_loss: 14.473224639892578\n",
      "      vf_explained_var: 0.9768792390823364\n",
      "      vf_loss: 14.475534439086914\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.14478649199008942\n",
      "      kl: 0.011239858344197273\n",
      "      policy_loss: -8.169327338691801e-05\n",
      "      total_loss: 15.447345733642578\n",
      "      vf_explained_var: 0.9858916997909546\n",
      "      vf_loss: 15.447426795959473\n",
      "    sample_time_ms: 20267.051\n",
      "    update_time_ms: 8.257\n",
      "  iterations_since_restore: 501\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.16354340158856\n",
      "    rl_1: 104.36150680814939\n",
      "  time_since_restore: 11883.65314912796\n",
      "  time_this_iter_s: 23.531496047973633\n",
      "  time_total_s: 11883.65314912796\n",
      "  timestamp: 1550805309\n",
      "  timesteps_since_restore: 5010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5010000\n",
      "  training_iteration: 501\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11883 s, 501 iter, 5010000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-15-32\n",
      "  done: false\n",
      "  episode_len_mean: 92.96296296296296\n",
      "  episode_reward_max: 219.27330209515688\n",
      "  episode_reward_mean: 178.71379785131305\n",
      "  episode_reward_min: 142.08960368791745\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 52020\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.542\n",
      "    load_time_ms: 2.31\n",
      "    num_steps_sampled: 5020000\n",
      "    num_steps_trained: 5020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2712615728378296\n",
      "      kl: 0.01155413780361414\n",
      "      policy_loss: -0.0020707903895527124\n",
      "      total_loss: 3.148735284805298\n",
      "      vf_explained_var: 0.9940556287765503\n",
      "      vf_loss: 3.150806188583374\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1799301654100418\n",
      "      kl: 0.011950830928981304\n",
      "      policy_loss: 0.0024004909209907055\n",
      "      total_loss: 3.2118542194366455\n",
      "      vf_explained_var: 0.9966711401939392\n",
      "      vf_loss: 3.209453582763672\n",
      "    sample_time_ms: 20205.753\n",
      "    update_time_ms: 8.276\n",
      "  iterations_since_restore: 502\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.26174490226005\n",
      "    rl_1: 105.452052949053\n",
      "  time_since_restore: 11906.428263664246\n",
      "  time_this_iter_s: 22.7751145362854\n",
      "  time_total_s: 11906.428263664246\n",
      "  timestamp: 1550805332\n",
      "  timesteps_since_restore: 5020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5020000\n",
      "  training_iteration: 502\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11906 s, 502 iter, 5020000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-15-55\n",
      "  done: false\n",
      "  episode_len_mean: 92.21495327102804\n",
      "  episode_reward_max: 221.2620392576665\n",
      "  episode_reward_mean: 179.96562602341152\n",
      "  episode_reward_min: 138.84601770245956\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 52127\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.237\n",
      "    load_time_ms: 2.248\n",
      "    num_steps_sampled: 5030000\n",
      "    num_steps_trained: 5030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.21735017001628876\n",
      "      kl: 0.01076011173427105\n",
      "      policy_loss: -0.0027889327611774206\n",
      "      total_loss: 2.4837772846221924\n",
      "      vf_explained_var: 0.995081901550293\n",
      "      vf_loss: 2.4865663051605225\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.184865802526474\n",
      "      kl: 0.010003849864006042\n",
      "      policy_loss: 0.0024270813446491957\n",
      "      total_loss: 2.6674046516418457\n",
      "      vf_explained_var: 0.9971246123313904\n",
      "      vf_loss: 2.664978265762329\n",
      "    sample_time_ms: 20244.482\n",
      "    update_time_ms: 8.003\n",
      "  iterations_since_restore: 503\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.2920863033396\n",
      "    rl_1: 103.67353972007191\n",
      "  time_since_restore: 11930.232985258102\n",
      "  time_this_iter_s: 23.80472159385681\n",
      "  time_total_s: 11930.232985258102\n",
      "  timestamp: 1550805355\n",
      "  timesteps_since_restore: 5030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5030000\n",
      "  training_iteration: 503\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11930 s, 503 iter, 5030000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-16-19\n",
      "  done: false\n",
      "  episode_len_mean: 92.3211009174312\n",
      "  episode_reward_max: 221.81719780506194\n",
      "  episode_reward_mean: 180.48843894660772\n",
      "  episode_reward_min: 145.72725168262158\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 52236\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.582\n",
      "    load_time_ms: 2.225\n",
      "    num_steps_sampled: 5040000\n",
      "    num_steps_trained: 5040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22173409163951874\n",
      "      kl: 0.01152599137276411\n",
      "      policy_loss: -0.0029054407496005297\n",
      "      total_loss: 2.5750038623809814\n",
      "      vf_explained_var: 0.9951643347740173\n",
      "      vf_loss: 2.5779097080230713\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.19550345838069916\n",
      "      kl: 0.013190297409892082\n",
      "      policy_loss: 0.0031111317221075296\n",
      "      total_loss: 3.1219565868377686\n",
      "      vf_explained_var: 0.9967083930969238\n",
      "      vf_loss: 3.1188457012176514\n",
      "    sample_time_ms: 20270.702\n",
      "    update_time_ms: 7.904\n",
      "  iterations_since_restore: 504\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.9882154605698\n",
      "    rl_1: 105.50022348603785\n",
      "  time_since_restore: 11953.686943531036\n",
      "  time_this_iter_s: 23.45395827293396\n",
      "  time_total_s: 11953.686943531036\n",
      "  timestamp: 1550805379\n",
      "  timesteps_since_restore: 5040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5040000\n",
      "  training_iteration: 504\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11953 s, 504 iter, 5040000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-16-42\n",
      "  done: false\n",
      "  episode_len_mean: 91.56880733944953\n",
      "  episode_reward_max: 224.97899293332648\n",
      "  episode_reward_mean: 180.85416850227188\n",
      "  episode_reward_min: 145.40970876378734\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 52345\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3153.082\n",
      "    load_time_ms: 2.256\n",
      "    num_steps_sampled: 5050000\n",
      "    num_steps_trained: 5050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.25259774923324585\n",
      "      kl: 0.008018746972084045\n",
      "      policy_loss: -0.0015420118579640985\n",
      "      total_loss: 2.492643356323242\n",
      "      vf_explained_var: 0.9954460263252258\n",
      "      vf_loss: 2.494185447692871\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.20547455549240112\n",
      "      kl: 0.007577385753393173\n",
      "      policy_loss: -0.0011866807471960783\n",
      "      total_loss: 3.3144524097442627\n",
      "      vf_explained_var: 0.9965742230415344\n",
      "      vf_loss: 3.3156394958496094\n",
      "    sample_time_ms: 20264.2\n",
      "    update_time_ms: 8.28\n",
      "  iterations_since_restore: 505\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.68340982513055\n",
      "    rl_1: 105.17075867714138\n",
      "  time_since_restore: 11977.027434110641\n",
      "  time_this_iter_s: 23.340490579605103\n",
      "  time_total_s: 11977.027434110641\n",
      "  timestamp: 1550805402\n",
      "  timesteps_since_restore: 5050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5050000\n",
      "  training_iteration: 505\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 11977 s, 505 iter, 5050000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-17-06\n",
      "  done: false\n",
      "  episode_len_mean: 94.24528301886792\n",
      "  episode_reward_max: 224.07466672583593\n",
      "  episode_reward_mean: 177.4892694914062\n",
      "  episode_reward_min: -76.93850729680209\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 52451\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3152.084\n",
      "    load_time_ms: 2.264\n",
      "    num_steps_sampled: 5060000\n",
      "    num_steps_trained: 5060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.21488110721111298\n",
      "      kl: 0.007788350339978933\n",
      "      policy_loss: -0.0018603394273668528\n",
      "      total_loss: 7.132737159729004\n",
      "      vf_explained_var: 0.9889258146286011\n",
      "      vf_loss: 7.134597301483154\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.19800624251365662\n",
      "      kl: 0.008372640237212181\n",
      "      policy_loss: -6.845727330073714e-05\n",
      "      total_loss: 8.198004722595215\n",
      "      vf_explained_var: 0.992652416229248\n",
      "      vf_loss: 8.198074340820312\n",
      "    sample_time_ms: 20266.028\n",
      "    update_time_ms: 8.325\n",
      "  iterations_since_restore: 506\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.08277377025428\n",
      "    rl_1: 104.40649572115194\n",
      "  time_since_restore: 12000.171312570572\n",
      "  time_this_iter_s: 23.14387845993042\n",
      "  time_total_s: 12000.171312570572\n",
      "  timestamp: 1550805426\n",
      "  timesteps_since_restore: 5060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5060000\n",
      "  training_iteration: 506\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12000 s, 506 iter, 5060000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-17-28\n",
      "  done: false\n",
      "  episode_len_mean: 92.56481481481481\n",
      "  episode_reward_max: 225.52144758990457\n",
      "  episode_reward_mean: 181.3852057176621\n",
      "  episode_reward_min: 142.705298154789\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 52559\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.559\n",
      "    load_time_ms: 2.233\n",
      "    num_steps_sampled: 5070000\n",
      "    num_steps_trained: 5070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.20584498345851898\n",
      "      kl: 0.009146067313849926\n",
      "      policy_loss: -0.0005503222346305847\n",
      "      total_loss: 1.984993815422058\n",
      "      vf_explained_var: 0.9961503744125366\n",
      "      vf_loss: 1.9855438470840454\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21898171305656433\n",
      "      kl: 0.024757176637649536\n",
      "      policy_loss: 0.00852289143949747\n",
      "      total_loss: 2.5192360877990723\n",
      "      vf_explained_var: 0.9974172711372375\n",
      "      vf_loss: 2.5107133388519287\n",
      "    sample_time_ms: 20190.916\n",
      "    update_time_ms: 8.186\n",
      "  iterations_since_restore: 507\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.95204647673755\n",
      "    rl_1: 106.43315924092455\n",
      "  time_since_restore: 12022.989562034607\n",
      "  time_this_iter_s: 22.818249464035034\n",
      "  time_total_s: 12022.989562034607\n",
      "  timestamp: 1550805448\n",
      "  timesteps_since_restore: 5070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5070000\n",
      "  training_iteration: 507\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12022 s, 507 iter, 5070000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-17-51\n",
      "  done: false\n",
      "  episode_len_mean: 92.77777777777777\n",
      "  episode_reward_max: 224.43381280551563\n",
      "  episode_reward_mean: 177.7346420426185\n",
      "  episode_reward_min: -64.04664538265305\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 52667\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3166.17\n",
      "    load_time_ms: 2.17\n",
      "    num_steps_sampled: 5080000\n",
      "    num_steps_trained: 5080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22383053600788116\n",
      "      kl: 0.006428887601941824\n",
      "      policy_loss: -0.0008062075357884169\n",
      "      total_loss: 8.556318283081055\n",
      "      vf_explained_var: 0.9857481718063354\n",
      "      vf_loss: 8.557124137878418\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1893947422504425\n",
      "      kl: 0.008993172086775303\n",
      "      policy_loss: -0.0011516576632857323\n",
      "      total_loss: 10.376165390014648\n",
      "      vf_explained_var: 0.9902766346931458\n",
      "      vf_loss: 10.377315521240234\n",
      "    sample_time_ms: 20117.032\n",
      "    update_time_ms: 8.476\n",
      "  iterations_since_restore: 508\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.97062817241341\n",
      "    rl_1: 103.76401387020512\n",
      "  time_since_restore: 12045.909555912018\n",
      "  time_this_iter_s: 22.91999387741089\n",
      "  time_total_s: 12045.909555912018\n",
      "  timestamp: 1550805471\n",
      "  timesteps_since_restore: 5080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5080000\n",
      "  training_iteration: 508\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12045 s, 508 iter, 5080000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-18-14\n",
      "  done: false\n",
      "  episode_len_mean: 92.99074074074075\n",
      "  episode_reward_max: 219.95080674481184\n",
      "  episode_reward_mean: 178.4331587408308\n",
      "  episode_reward_min: 146.28926335832764\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 52775\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.951\n",
      "    load_time_ms: 2.166\n",
      "    num_steps_sampled: 5090000\n",
      "    num_steps_trained: 5090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24005654454231262\n",
      "      kl: 0.007158344145864248\n",
      "      policy_loss: -0.001231638714671135\n",
      "      total_loss: 2.3044095039367676\n",
      "      vf_explained_var: 0.9954692125320435\n",
      "      vf_loss: 2.305640935897827\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.24333351850509644\n",
      "      kl: 0.011939356103539467\n",
      "      policy_loss: -0.0013101693475618958\n",
      "      total_loss: 2.7571206092834473\n",
      "      vf_explained_var: 0.997106671333313\n",
      "      vf_loss: 2.7584309577941895\n",
      "    sample_time_ms: 20029.337\n",
      "    update_time_ms: 7.875\n",
      "  iterations_since_restore: 509\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.33847774291442\n",
      "    rl_1: 105.0946809979164\n",
      "  time_since_restore: 12068.845160484314\n",
      "  time_this_iter_s: 22.935604572296143\n",
      "  time_total_s: 12068.845160484314\n",
      "  timestamp: 1550805494\n",
      "  timesteps_since_restore: 5090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5090000\n",
      "  training_iteration: 509\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12068 s, 509 iter, 5090000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-18-38\n",
      "  done: false\n",
      "  episode_len_mean: 92.32407407407408\n",
      "  episode_reward_max: 222.89462874671543\n",
      "  episode_reward_mean: 182.82765083376222\n",
      "  episode_reward_min: 143.8233608131533\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 52883\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.734\n",
      "    load_time_ms: 2.124\n",
      "    num_steps_sampled: 5100000\n",
      "    num_steps_trained: 5100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.17913872003555298\n",
      "      kl: 0.008171743713319302\n",
      "      policy_loss: -0.002312474185600877\n",
      "      total_loss: 2.069894552230835\n",
      "      vf_explained_var: 0.9961955547332764\n",
      "      vf_loss: 2.0722062587738037\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26455157995224\n",
      "      kl: 0.014325754716992378\n",
      "      policy_loss: 0.003588498802855611\n",
      "      total_loss: 2.3950867652893066\n",
      "      vf_explained_var: 0.9974987506866455\n",
      "      vf_loss: 2.391498327255249\n",
      "    sample_time_ms: 20023.634\n",
      "    update_time_ms: 7.633\n",
      "  iterations_since_restore: 510\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.152277526163\n",
      "    rl_1: 106.67537330759924\n",
      "  time_since_restore: 12092.144341945648\n",
      "  time_this_iter_s: 23.29918146133423\n",
      "  time_total_s: 12092.144341945648\n",
      "  timestamp: 1550805518\n",
      "  timesteps_since_restore: 5100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5100000\n",
      "  training_iteration: 510\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12092 s, 510 iter, 5100000 ts, 183 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-19-01\n",
      "  done: false\n",
      "  episode_len_mean: 92.62037037037037\n",
      "  episode_reward_max: 221.7861166538338\n",
      "  episode_reward_mean: 179.63784825545974\n",
      "  episode_reward_min: 142.6021507192933\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 52991\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.904\n",
      "    load_time_ms: 2.296\n",
      "    num_steps_sampled: 5110000\n",
      "    num_steps_trained: 5110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.21888026595115662\n",
      "      kl: 0.00941595621407032\n",
      "      policy_loss: -0.002749233739450574\n",
      "      total_loss: 2.119723081588745\n",
      "      vf_explained_var: 0.9958462119102478\n",
      "      vf_loss: 2.1224727630615234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2629545032978058\n",
      "      kl: 0.012153537012636662\n",
      "      policy_loss: 0.0003752076008822769\n",
      "      total_loss: 2.648362874984741\n",
      "      vf_explained_var: 0.9971925020217896\n",
      "      vf_loss: 2.6479878425598145\n",
      "    sample_time_ms: 19946.796\n",
      "    update_time_ms: 7.556\n",
      "  iterations_since_restore: 511\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.47087095046464\n",
      "    rl_1: 105.16697730499507\n",
      "  time_since_restore: 12114.911545038223\n",
      "  time_this_iter_s: 22.767203092575073\n",
      "  time_total_s: 12114.911545038223\n",
      "  timestamp: 1550805541\n",
      "  timesteps_since_restore: 5110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5110000\n",
      "  training_iteration: 511\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12114 s, 511 iter, 5110000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-19-24\n",
      "  done: false\n",
      "  episode_len_mean: 91.9908256880734\n",
      "  episode_reward_max: 223.76161147661205\n",
      "  episode_reward_mean: 178.42365521582917\n",
      "  episode_reward_min: -153.36065341690815\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 53100\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.27\n",
      "    load_time_ms: 2.298\n",
      "    num_steps_sampled: 5120000\n",
      "    num_steps_trained: 5120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.19037531316280365\n",
      "      kl: 0.009946119971573353\n",
      "      policy_loss: -0.00535234110429883\n",
      "      total_loss: 24.460676193237305\n",
      "      vf_explained_var: 0.9636980891227722\n",
      "      vf_loss: 24.466032028198242\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2042599320411682\n",
      "      kl: 0.10579686611890793\n",
      "      policy_loss: -0.012890017591416836\n",
      "      total_loss: 42.365867614746094\n",
      "      vf_explained_var: 0.9610981345176697\n",
      "      vf_loss: 42.378746032714844\n",
      "    sample_time_ms: 19965.512\n",
      "    update_time_ms: 7.591\n",
      "  iterations_since_restore: 512\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.01379925359268\n",
      "    rl_1: 104.40985596223645\n",
      "  time_since_restore: 12137.867567777634\n",
      "  time_this_iter_s: 22.9560227394104\n",
      "  time_total_s: 12137.867567777634\n",
      "  timestamp: 1550805564\n",
      "  timesteps_since_restore: 5120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5120000\n",
      "  training_iteration: 512\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12137 s, 512 iter, 5120000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-19-47\n",
      "  done: false\n",
      "  episode_len_mean: 93.01869158878505\n",
      "  episode_reward_max: 218.97684344541508\n",
      "  episode_reward_mean: 170.0885487825926\n",
      "  episode_reward_min: -148.78697725770894\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 53207\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.748\n",
      "    load_time_ms: 2.311\n",
      "    num_steps_sampled: 5130000\n",
      "    num_steps_trained: 5130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22308491170406342\n",
      "      kl: 0.007531516253948212\n",
      "      policy_loss: -0.002185805467888713\n",
      "      total_loss: 31.42625617980957\n",
      "      vf_explained_var: 0.9618853330612183\n",
      "      vf_loss: 31.428438186645508\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23434340953826904\n",
      "      kl: 0.013117073103785515\n",
      "      policy_loss: -0.0038764518685638905\n",
      "      total_loss: 29.64048194885254\n",
      "      vf_explained_var: 0.9782501459121704\n",
      "      vf_loss: 29.64435577392578\n",
      "    sample_time_ms: 19886.33\n",
      "    update_time_ms: 7.637\n",
      "  iterations_since_restore: 513\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.91102755356135\n",
      "    rl_1: 99.17752122903124\n",
      "  time_since_restore: 12160.867067813873\n",
      "  time_this_iter_s: 22.999500036239624\n",
      "  time_total_s: 12160.867067813873\n",
      "  timestamp: 1550805587\n",
      "  timesteps_since_restore: 5130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5130000\n",
      "  training_iteration: 513\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12160 s, 513 iter, 5130000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-20-10\n",
      "  done: false\n",
      "  episode_len_mean: 92.75\n",
      "  episode_reward_max: 215.01044333795022\n",
      "  episode_reward_mean: 179.33616673004832\n",
      "  episode_reward_min: 138.92279571738305\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 53315\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.551\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 5140000\n",
      "    num_steps_trained: 5140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1980493813753128\n",
      "      kl: 0.013025541789829731\n",
      "      policy_loss: -0.002563638146966696\n",
      "      total_loss: 2.374091625213623\n",
      "      vf_explained_var: 0.9953047037124634\n",
      "      vf_loss: 2.376655101776123\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25066763162612915\n",
      "      kl: 0.012542527168989182\n",
      "      policy_loss: 0.006090180482715368\n",
      "      total_loss: 2.4874355792999268\n",
      "      vf_explained_var: 0.9973856806755066\n",
      "      vf_loss: 2.4813451766967773\n",
      "    sample_time_ms: 19850.937\n",
      "    update_time_ms: 7.671\n",
      "  iterations_since_restore: 514\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.30699552046825\n",
      "    rl_1: 105.02917120958003\n",
      "  time_since_restore: 12183.932897567749\n",
      "  time_this_iter_s: 23.065829753875732\n",
      "  time_total_s: 12183.932897567749\n",
      "  timestamp: 1550805610\n",
      "  timesteps_since_restore: 5140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5140000\n",
      "  training_iteration: 514\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12183 s, 514 iter, 5140000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-20-33\n",
      "  done: false\n",
      "  episode_len_mean: 93.79439252336448\n",
      "  episode_reward_max: 221.3147731264678\n",
      "  episode_reward_mean: 175.7681950265926\n",
      "  episode_reward_min: -153.9789495979148\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 53422\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.433\n",
      "    load_time_ms: 2.327\n",
      "    num_steps_sampled: 5150000\n",
      "    num_steps_trained: 5150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.22660742700099945\n",
      "      kl: 0.012144886888563633\n",
      "      policy_loss: -0.0018563541816547513\n",
      "      total_loss: 12.723835945129395\n",
      "      vf_explained_var: 0.9794804453849792\n",
      "      vf_loss: 12.725691795349121\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.24110408127307892\n",
      "      kl: 0.010631720535457134\n",
      "      policy_loss: 7.81375783844851e-05\n",
      "      total_loss: 12.841089248657227\n",
      "      vf_explained_var: 0.9885396957397461\n",
      "      vf_loss: 12.841011047363281\n",
      "    sample_time_ms: 19855.536\n",
      "    update_time_ms: 7.088\n",
      "  iterations_since_restore: 515\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.268505718172\n",
      "    rl_1: 104.4996893084206\n",
      "  time_since_restore: 12207.26937842369\n",
      "  time_this_iter_s: 23.336480855941772\n",
      "  time_total_s: 12207.26937842369\n",
      "  timestamp: 1550805633\n",
      "  timesteps_since_restore: 5150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5150000\n",
      "  training_iteration: 515\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12207 s, 515 iter, 5150000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-20-56\n",
      "  done: false\n",
      "  episode_len_mean: 92.17592592592592\n",
      "  episode_reward_max: 218.29221280913612\n",
      "  episode_reward_mean: 179.85849101931925\n",
      "  episode_reward_min: 144.13238259377698\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 53530\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.614\n",
      "    load_time_ms: 2.372\n",
      "    num_steps_sampled: 5160000\n",
      "    num_steps_trained: 5160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.17031176388263702\n",
      "      kl: 0.014196091331541538\n",
      "      policy_loss: 0.00017472061153966933\n",
      "      total_loss: 2.2725272178649902\n",
      "      vf_explained_var: 0.9957141280174255\n",
      "      vf_loss: 2.272352457046509\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22448714077472687\n",
      "      kl: 0.013930990360677242\n",
      "      policy_loss: 0.0008069719187915325\n",
      "      total_loss: 2.549525260925293\n",
      "      vf_explained_var: 0.9972509741783142\n",
      "      vf_loss: 2.548718214035034\n",
      "    sample_time_ms: 19837.682\n",
      "    update_time_ms: 7.35\n",
      "  iterations_since_restore: 516\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.83844209380355\n",
      "    rl_1: 104.02004892551575\n",
      "  time_since_restore: 12230.231169223785\n",
      "  time_this_iter_s: 22.961790800094604\n",
      "  time_total_s: 12230.231169223785\n",
      "  timestamp: 1550805656\n",
      "  timesteps_since_restore: 5160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5160000\n",
      "  training_iteration: 516\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12230 s, 516 iter, 5160000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-21-19\n",
      "  done: false\n",
      "  episode_len_mean: 91.3\n",
      "  episode_reward_max: 221.25229804058964\n",
      "  episode_reward_mean: 181.1982565451176\n",
      "  episode_reward_min: 144.8450300249738\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 53640\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3155.712\n",
      "    load_time_ms: 2.381\n",
      "    num_steps_sampled: 5170000\n",
      "    num_steps_trained: 5170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.14837150275707245\n",
      "      kl: 0.011629750020802021\n",
      "      policy_loss: -0.0017882329411804676\n",
      "      total_loss: 2.0404582023620605\n",
      "      vf_explained_var: 0.996321439743042\n",
      "      vf_loss: 2.042246103286743\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22334903478622437\n",
      "      kl: 0.014459073543548584\n",
      "      policy_loss: 0.0007434126455336809\n",
      "      total_loss: 2.3216793537139893\n",
      "      vf_explained_var: 0.9974977374076843\n",
      "      vf_loss: 2.3209362030029297\n",
      "    sample_time_ms: 19852.032\n",
      "    update_time_ms: 7.359\n",
      "  iterations_since_restore: 517\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.74449322619277\n",
      "    rl_1: 104.45376331892473\n",
      "  time_since_restore: 12253.350734472275\n",
      "  time_this_iter_s: 23.11956524848938\n",
      "  time_total_s: 12253.350734472275\n",
      "  timestamp: 1550805679\n",
      "  timesteps_since_restore: 5170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5170000\n",
      "  training_iteration: 517\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12253 s, 517 iter, 5170000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-21-43\n",
      "  done: false\n",
      "  episode_len_mean: 92.91588785046729\n",
      "  episode_reward_max: 223.90611480560264\n",
      "  episode_reward_mean: 179.68966084425747\n",
      "  episode_reward_min: -114.1134213570765\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 53747\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.543\n",
      "    load_time_ms: 2.431\n",
      "    num_steps_sampled: 5180000\n",
      "    num_steps_trained: 5180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1522490531206131\n",
      "      kl: 0.010232198052108288\n",
      "      policy_loss: -0.0017625883920118213\n",
      "      total_loss: 12.989787101745605\n",
      "      vf_explained_var: 0.9811779856681824\n",
      "      vf_loss: 12.991547584533691\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2866925597190857\n",
      "      kl: 0.01671813242137432\n",
      "      policy_loss: -0.002685908693820238\n",
      "      total_loss: 13.820013999938965\n",
      "      vf_explained_var: 0.9881318211555481\n",
      "      vf_loss: 13.822699546813965\n",
      "    sample_time_ms: 19922.224\n",
      "    update_time_ms: 7.145\n",
      "  iterations_since_restore: 518\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.26536331189753\n",
      "    rl_1: 105.42429753235996\n",
      "  time_since_restore: 12276.77018070221\n",
      "  time_this_iter_s: 23.419446229934692\n",
      "  time_total_s: 12276.77018070221\n",
      "  timestamp: 1550805703\n",
      "  timesteps_since_restore: 5180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5180000\n",
      "  training_iteration: 518\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12276 s, 518 iter, 5180000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-22-06\n",
      "  done: false\n",
      "  episode_len_mean: 92.71296296296296\n",
      "  episode_reward_max: 221.81481022549195\n",
      "  episode_reward_mean: 174.8966977550516\n",
      "  episode_reward_min: -138.64537735775514\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 53855\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.427\n",
      "    load_time_ms: 2.5\n",
      "    num_steps_sampled: 5190000\n",
      "    num_steps_trained: 5190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.12461414188146591\n",
      "      kl: 0.011638877913355827\n",
      "      policy_loss: -0.0021105408668518066\n",
      "      total_loss: 14.902121543884277\n",
      "      vf_explained_var: 0.9789066910743713\n",
      "      vf_loss: 14.9042329788208\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2671717405319214\n",
      "      kl: 0.01219451054930687\n",
      "      policy_loss: -0.00032190457568503916\n",
      "      total_loss: 13.981711387634277\n",
      "      vf_explained_var: 0.9888639450073242\n",
      "      vf_loss: 13.98203182220459\n",
      "    sample_time_ms: 19912.192\n",
      "    update_time_ms: 7.191\n",
      "  iterations_since_restore: 519\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.9586316932235\n",
      "    rl_1: 102.93806606182815\n",
      "  time_since_restore: 12299.592935323715\n",
      "  time_this_iter_s: 22.822754621505737\n",
      "  time_total_s: 12299.592935323715\n",
      "  timestamp: 1550805726\n",
      "  timesteps_since_restore: 5190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5190000\n",
      "  training_iteration: 519\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12299 s, 519 iter, 5190000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-22-29\n",
      "  done: false\n",
      "  episode_len_mean: 92.94444444444444\n",
      "  episode_reward_max: 220.1956973374762\n",
      "  episode_reward_mean: 176.82903981953334\n",
      "  episode_reward_min: -105.95159625984266\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 53963\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.589\n",
      "    load_time_ms: 2.533\n",
      "    num_steps_sampled: 5200000\n",
      "    num_steps_trained: 5200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1418997049331665\n",
      "      kl: 0.01181873306632042\n",
      "      policy_loss: -0.001393117825500667\n",
      "      total_loss: 22.01140022277832\n",
      "      vf_explained_var: 0.967886209487915\n",
      "      vf_loss: 22.012794494628906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.27727749943733215\n",
      "      kl: 0.018020551651716232\n",
      "      policy_loss: 0.0002170568041037768\n",
      "      total_loss: 23.523365020751953\n",
      "      vf_explained_var: 0.980085015296936\n",
      "      vf_loss: 23.523149490356445\n",
      "    sample_time_ms: 19962.88\n",
      "    update_time_ms: 7.143\n",
      "  iterations_since_restore: 520\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.26311063522432\n",
      "    rl_1: 103.565929184309\n",
      "  time_since_restore: 12323.332893133163\n",
      "  time_this_iter_s: 23.739957809448242\n",
      "  time_total_s: 12323.332893133163\n",
      "  timestamp: 1550805749\n",
      "  timesteps_since_restore: 5200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5200000\n",
      "  training_iteration: 520\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12323 s, 520 iter, 5200000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-22-53\n",
      "  done: false\n",
      "  episode_len_mean: 92.77777777777777\n",
      "  episode_reward_max: 222.34928757916748\n",
      "  episode_reward_mean: 182.09005230071327\n",
      "  episode_reward_min: 143.53721998627407\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 54071\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.812\n",
      "    load_time_ms: 2.384\n",
      "    num_steps_sampled: 5210000\n",
      "    num_steps_trained: 5210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.10562457889318466\n",
      "      kl: 0.010795683600008488\n",
      "      policy_loss: -0.002463785232976079\n",
      "      total_loss: 2.063319206237793\n",
      "      vf_explained_var: 0.9962170124053955\n",
      "      vf_loss: 2.0657827854156494\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3116982877254486\n",
      "      kl: 0.01947234943509102\n",
      "      policy_loss: 0.0038410066626966\n",
      "      total_loss: 2.314976692199707\n",
      "      vf_explained_var: 0.9976025819778442\n",
      "      vf_loss: 2.311135768890381\n",
      "    sample_time_ms: 20020.981\n",
      "    update_time_ms: 7.31\n",
      "  iterations_since_restore: 521\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.24652289608454\n",
      "    rl_1: 105.8435294046287\n",
      "  time_since_restore: 12346.683291912079\n",
      "  time_this_iter_s: 23.350398778915405\n",
      "  time_total_s: 12346.683291912079\n",
      "  timestamp: 1550805773\n",
      "  timesteps_since_restore: 5210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5210000\n",
      "  training_iteration: 521\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12346 s, 521 iter, 5210000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-23-16\n",
      "  done: false\n",
      "  episode_len_mean: 91.66055045871559\n",
      "  episode_reward_max: 221.0107234885192\n",
      "  episode_reward_mean: 174.65292625500408\n",
      "  episode_reward_min: -173.4549453043\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 54180\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.103\n",
      "    load_time_ms: 2.439\n",
      "    num_steps_sampled: 5220000\n",
      "    num_steps_trained: 5220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.09755497425794601\n",
      "      kl: 0.010430925525724888\n",
      "      policy_loss: -0.0006258542998693883\n",
      "      total_loss: 34.082542419433594\n",
      "      vf_explained_var: 0.9507501125335693\n",
      "      vf_loss: 34.08317184448242\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3068961501121521\n",
      "      kl: 0.012150784023106098\n",
      "      policy_loss: -0.004028133116662502\n",
      "      total_loss: 38.91759490966797\n",
      "      vf_explained_var: 0.9663238525390625\n",
      "      vf_loss: 38.92162322998047\n",
      "    sample_time_ms: 20051.039\n",
      "    update_time_ms: 7.185\n",
      "  iterations_since_restore: 522\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.48841497345107\n",
      "    rl_1: 102.1645112815529\n",
      "  time_since_restore: 12369.932472467422\n",
      "  time_this_iter_s: 23.249180555343628\n",
      "  time_total_s: 12369.932472467422\n",
      "  timestamp: 1550805796\n",
      "  timesteps_since_restore: 5220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5220000\n",
      "  training_iteration: 522\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12369 s, 522 iter, 5220000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-23-39\n",
      "  done: false\n",
      "  episode_len_mean: 92.49074074074075\n",
      "  episode_reward_max: 218.72117264205005\n",
      "  episode_reward_mean: 177.40409069220044\n",
      "  episode_reward_min: -176.85874797758612\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 54288\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.307\n",
      "    load_time_ms: 2.414\n",
      "    num_steps_sampled: 5230000\n",
      "    num_steps_trained: 5230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.13142459094524384\n",
      "      kl: 0.010773678310215473\n",
      "      policy_loss: -0.001314002089202404\n",
      "      total_loss: 17.53964614868164\n",
      "      vf_explained_var: 0.9722961187362671\n",
      "      vf_loss: 17.540958404541016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2892274856567383\n",
      "      kl: 0.014017379842698574\n",
      "      policy_loss: 0.0012283684918656945\n",
      "      total_loss: 17.060359954833984\n",
      "      vf_explained_var: 0.9850059151649475\n",
      "      vf_loss: 17.05912971496582\n",
      "    sample_time_ms: 20034.661\n",
      "    update_time_ms: 7.187\n",
      "  iterations_since_restore: 523\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.60996932831223\n",
      "    rl_1: 104.79412136388821\n",
      "  time_since_restore: 12392.78978395462\n",
      "  time_this_iter_s: 22.857311487197876\n",
      "  time_total_s: 12392.78978395462\n",
      "  timestamp: 1550805819\n",
      "  timesteps_since_restore: 5230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5230000\n",
      "  training_iteration: 523\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12392 s, 523 iter, 5230000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-24-03\n",
      "  done: false\n",
      "  episode_len_mean: 92.43518518518519\n",
      "  episode_reward_max: 226.82293295482398\n",
      "  episode_reward_mean: 177.72074805257802\n",
      "  episode_reward_min: -178.71704165727866\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 54396\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.189\n",
      "    load_time_ms: 2.348\n",
      "    num_steps_sampled: 5240000\n",
      "    num_steps_trained: 5240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1338350921869278\n",
      "      kl: 0.015258836559951305\n",
      "      policy_loss: -0.004398559685796499\n",
      "      total_loss: 13.94708251953125\n",
      "      vf_explained_var: 0.9787017703056335\n",
      "      vf_loss: 13.951486587524414\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25795337557792664\n",
      "      kl: 0.0382096990942955\n",
      "      policy_loss: 0.0024063598830252886\n",
      "      total_loss: 14.510327339172363\n",
      "      vf_explained_var: 0.9867428541183472\n",
      "      vf_loss: 14.507920265197754\n",
      "    sample_time_ms: 20110.411\n",
      "    update_time_ms: 7.25\n",
      "  iterations_since_restore: 524\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.09349062103688\n",
      "    rl_1: 103.62725743154114\n",
      "  time_since_restore: 12416.610310792923\n",
      "  time_this_iter_s: 23.820526838302612\n",
      "  time_total_s: 12416.610310792923\n",
      "  timestamp: 1550805843\n",
      "  timesteps_since_restore: 5240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5240000\n",
      "  training_iteration: 524\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12416 s, 524 iter, 5240000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-24-26\n",
      "  done: false\n",
      "  episode_len_mean: 93.3177570093458\n",
      "  episode_reward_max: 219.57587030049078\n",
      "  episode_reward_mean: 172.9899404286537\n",
      "  episode_reward_min: -56.35198996873956\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 54503\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.337\n",
      "    load_time_ms: 2.341\n",
      "    num_steps_sampled: 5250000\n",
      "    num_steps_trained: 5250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1507939100265503\n",
      "      kl: 0.01577414944767952\n",
      "      policy_loss: -0.0004926433903165162\n",
      "      total_loss: 18.844697952270508\n",
      "      vf_explained_var: 0.9704954624176025\n",
      "      vf_loss: 18.84519386291504\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31641125679016113\n",
      "      kl: 0.02596288174390793\n",
      "      policy_loss: 0.003343207761645317\n",
      "      total_loss: 22.27865219116211\n",
      "      vf_explained_var: 0.9805404543876648\n",
      "      vf_loss: 22.27530860900879\n",
      "    sample_time_ms: 20061.393\n",
      "    update_time_ms: 7.413\n",
      "  iterations_since_restore: 525\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.41557143928009\n",
      "    rl_1: 102.57436898937364\n",
      "  time_since_restore: 12439.489709615707\n",
      "  time_this_iter_s: 22.879398822784424\n",
      "  time_total_s: 12439.489709615707\n",
      "  timestamp: 1550805866\n",
      "  timesteps_since_restore: 5250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5250000\n",
      "  training_iteration: 525\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12439 s, 525 iter, 5250000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-24-49\n",
      "  done: false\n",
      "  episode_len_mean: 94.72641509433963\n",
      "  episode_reward_max: 222.27812896732019\n",
      "  episode_reward_mean: 176.24712244514816\n",
      "  episode_reward_min: -171.97891578493648\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 54609\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.176\n",
      "    load_time_ms: 2.297\n",
      "    num_steps_sampled: 5260000\n",
      "    num_steps_trained: 5260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.13500502705574036\n",
      "      kl: 0.011996682733297348\n",
      "      policy_loss: -0.0027600592002272606\n",
      "      total_loss: 11.06017017364502\n",
      "      vf_explained_var: 0.983485996723175\n",
      "      vf_loss: 11.0629301071167\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.33397409319877625\n",
      "      kl: 0.017177054658532143\n",
      "      policy_loss: 0.00013088698324281722\n",
      "      total_loss: 11.311089515686035\n",
      "      vf_explained_var: 0.9901406764984131\n",
      "      vf_loss: 11.310958862304688\n",
      "    sample_time_ms: 20056.161\n",
      "    update_time_ms: 7.097\n",
      "  iterations_since_restore: 526\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.65158877083029\n",
      "    rl_1: 103.59553367431785\n",
      "  time_since_restore: 12462.43575000763\n",
      "  time_this_iter_s: 22.946040391921997\n",
      "  time_total_s: 12462.43575000763\n",
      "  timestamp: 1550805889\n",
      "  timesteps_since_restore: 5260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5260000\n",
      "  training_iteration: 526\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12462 s, 526 iter, 5260000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-25-12\n",
      "  done: false\n",
      "  episode_len_mean: 91.54128440366972\n",
      "  episode_reward_max: 224.22031060099607\n",
      "  episode_reward_mean: 181.64694513735344\n",
      "  episode_reward_min: 148.34447673225733\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 54718\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3117.288\n",
      "    load_time_ms: 2.327\n",
      "    num_steps_sampled: 5270000\n",
      "    num_steps_trained: 5270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.06804651021957397\n",
      "      kl: 0.013491244986653328\n",
      "      policy_loss: -0.005866717081516981\n",
      "      total_loss: 2.0767788887023926\n",
      "      vf_explained_var: 0.9961777925491333\n",
      "      vf_loss: 2.082646131515503\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.39921537041664124\n",
      "      kl: 0.044720981270074844\n",
      "      policy_loss: 0.014123843051493168\n",
      "      total_loss: 2.553363800048828\n",
      "      vf_explained_var: 0.9973431825637817\n",
      "      vf_loss: 2.5392396450042725\n",
      "    sample_time_ms: 20110.561\n",
      "    update_time_ms: 7.194\n",
      "  iterations_since_restore: 527\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.94120873424231\n",
      "    rl_1: 105.70573640311113\n",
      "  time_since_restore: 12485.9059112072\n",
      "  time_this_iter_s: 23.470161199569702\n",
      "  time_total_s: 12485.9059112072\n",
      "  timestamp: 1550805912\n",
      "  timesteps_since_restore: 5270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5270000\n",
      "  training_iteration: 527\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12485 s, 527 iter, 5270000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-25-35\n",
      "  done: false\n",
      "  episode_len_mean: 92.4074074074074\n",
      "  episode_reward_max: 226.49030668215048\n",
      "  episode_reward_mean: 179.72673744763816\n",
      "  episode_reward_min: 145.89672116193887\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 54826\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3121.382\n",
      "    load_time_ms: 2.282\n",
      "    num_steps_sampled: 5280000\n",
      "    num_steps_trained: 5280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.09469670802354813\n",
      "      kl: 0.013279263861477375\n",
      "      policy_loss: -0.003886179765686393\n",
      "      total_loss: 2.9244914054870605\n",
      "      vf_explained_var: 0.9947456121444702\n",
      "      vf_loss: 2.928377389907837\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.30476149916648865\n",
      "      kl: 0.021896274760365486\n",
      "      policy_loss: 0.0016849720850586891\n",
      "      total_loss: 3.8759398460388184\n",
      "      vf_explained_var: 0.9960934519767761\n",
      "      vf_loss: 3.8742547035217285\n",
      "    sample_time_ms: 20070.49\n",
      "    update_time_ms: 7.0\n",
      "  iterations_since_restore: 528\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.6193217899852\n",
      "    rl_1: 104.107415657653\n",
      "  time_since_restore: 12508.963546037674\n",
      "  time_this_iter_s: 23.057634830474854\n",
      "  time_total_s: 12508.963546037674\n",
      "  timestamp: 1550805935\n",
      "  timesteps_since_restore: 5280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5280000\n",
      "  training_iteration: 528\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12508 s, 528 iter, 5280000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-25-59\n",
      "  done: false\n",
      "  episode_len_mean: 93.55140186915888\n",
      "  episode_reward_max: 221.82729517834954\n",
      "  episode_reward_mean: 178.11389797502943\n",
      "  episode_reward_min: 143.8926107399307\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 54933\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3121.733\n",
      "    load_time_ms: 2.251\n",
      "    num_steps_sampled: 5290000\n",
      "    num_steps_trained: 5290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.08005256205797195\n",
      "      kl: 0.013320724479854107\n",
      "      policy_loss: -0.0003841454454232007\n",
      "      total_loss: 1.9541903734207153\n",
      "      vf_explained_var: 0.9959825277328491\n",
      "      vf_loss: 1.9545743465423584\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35606518387794495\n",
      "      kl: 0.012347856536507607\n",
      "      policy_loss: 0.0005557452095672488\n",
      "      total_loss: 2.3809361457824707\n",
      "      vf_explained_var: 0.9975167512893677\n",
      "      vf_loss: 2.380380153656006\n",
      "    sample_time_ms: 20099.485\n",
      "    update_time_ms: 7.013\n",
      "  iterations_since_restore: 529\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.18140343117604\n",
      "    rl_1: 105.93249454385338\n",
      "  time_since_restore: 12532.078061580658\n",
      "  time_this_iter_s: 23.11451554298401\n",
      "  time_total_s: 12532.078061580658\n",
      "  timestamp: 1550805959\n",
      "  timesteps_since_restore: 5290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5290000\n",
      "  training_iteration: 529\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12532 s, 529 iter, 5290000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-26-22\n",
      "  done: false\n",
      "  episode_len_mean: 92.18348623853211\n",
      "  episode_reward_max: 222.69679777874552\n",
      "  episode_reward_mean: 177.54087574463313\n",
      "  episode_reward_min: -164.6488133486579\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 55042\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.792\n",
      "    load_time_ms: 2.311\n",
      "    num_steps_sampled: 5300000\n",
      "    num_steps_trained: 5300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.039727065712213516\n",
      "      kl: 0.01507138554006815\n",
      "      policy_loss: -0.0061575621366500854\n",
      "      total_loss: 12.125645637512207\n",
      "      vf_explained_var: 0.982132077217102\n",
      "      vf_loss: 12.131803512573242\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.354326456785202\n",
      "      kl: 0.016580680385231972\n",
      "      policy_loss: 0.0009757744846865535\n",
      "      total_loss: 12.473073959350586\n",
      "      vf_explained_var: 0.9887521266937256\n",
      "      vf_loss: 12.472098350524902\n",
      "    sample_time_ms: 20070.884\n",
      "    update_time_ms: 7.176\n",
      "  iterations_since_restore: 530\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.49655926634628\n",
      "    rl_1: 103.04431647828686\n",
      "  time_since_restore: 12555.643447637558\n",
      "  time_this_iter_s: 23.565386056900024\n",
      "  time_total_s: 12555.643447637558\n",
      "  timestamp: 1550805982\n",
      "  timesteps_since_restore: 5300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5300000\n",
      "  training_iteration: 530\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12555 s, 530 iter, 5300000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-26-45\n",
      "  done: false\n",
      "  episode_len_mean: 93.18691588785046\n",
      "  episode_reward_max: 221.31753119548017\n",
      "  episode_reward_mean: 176.07213821301676\n",
      "  episode_reward_min: -51.304315891045974\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 55149\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.071\n",
      "    load_time_ms: 2.306\n",
      "    num_steps_sampled: 5310000\n",
      "    num_steps_trained: 5310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.09352698922157288\n",
      "      kl: 0.008542078547179699\n",
      "      policy_loss: -0.001762984087690711\n",
      "      total_loss: 13.218515396118164\n",
      "      vf_explained_var: 0.9806827902793884\n",
      "      vf_loss: 13.22027587890625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3548073172569275\n",
      "      kl: 0.01957932487130165\n",
      "      policy_loss: -0.0018928274512290955\n",
      "      total_loss: 13.732988357543945\n",
      "      vf_explained_var: 0.9880805611610413\n",
      "      vf_loss: 13.734878540039062\n",
      "    sample_time_ms: 20042.685\n",
      "    update_time_ms: 7.357\n",
      "  iterations_since_restore: 531\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.14946975899868\n",
      "    rl_1: 102.92266845401808\n",
      "  time_since_restore: 12578.693260192871\n",
      "  time_this_iter_s: 23.04981255531311\n",
      "  time_total_s: 12578.693260192871\n",
      "  timestamp: 1550806005\n",
      "  timesteps_since_restore: 5310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5310000\n",
      "  training_iteration: 531\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12578 s, 531 iter, 5310000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-27-09\n",
      "  done: false\n",
      "  episode_len_mean: 92.34862385321101\n",
      "  episode_reward_max: 223.89373921395756\n",
      "  episode_reward_mean: 180.563989846709\n",
      "  episode_reward_min: 143.7677326841309\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 55258\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.877\n",
      "    load_time_ms: 2.293\n",
      "    num_steps_sampled: 5320000\n",
      "    num_steps_trained: 5320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.021892957389354706\n",
      "      kl: 0.015076044015586376\n",
      "      policy_loss: -0.002029287861660123\n",
      "      total_loss: 1.9186444282531738\n",
      "      vf_explained_var: 0.9964341521263123\n",
      "      vf_loss: 1.9206740856170654\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3965742588043213\n",
      "      kl: 0.030041517689824104\n",
      "      policy_loss: 0.004863788839429617\n",
      "      total_loss: 2.0982937812805176\n",
      "      vf_explained_var: 0.9977684020996094\n",
      "      vf_loss: 2.0934300422668457\n",
      "    sample_time_ms: 20081.694\n",
      "    update_time_ms: 7.536\n",
      "  iterations_since_restore: 532\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.1518989057119\n",
      "    rl_1: 105.41209094099712\n",
      "  time_since_restore: 12602.34203672409\n",
      "  time_this_iter_s: 23.648776531219482\n",
      "  time_total_s: 12602.34203672409\n",
      "  timestamp: 1550806029\n",
      "  timesteps_since_restore: 5320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5320000\n",
      "  training_iteration: 532\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12602 s, 532 iter, 5320000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-27-33\n",
      "  done: false\n",
      "  episode_len_mean: 93.01869158878505\n",
      "  episode_reward_max: 222.37700755794413\n",
      "  episode_reward_mean: 176.92189787582208\n",
      "  episode_reward_min: 144.39367058332414\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 55365\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.294\n",
      "    load_time_ms: 2.336\n",
      "    num_steps_sampled: 5330000\n",
      "    num_steps_trained: 5330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.0795314684510231\n",
      "      kl: 0.011296487413346767\n",
      "      policy_loss: -0.0020063340198248625\n",
      "      total_loss: 2.0173139572143555\n",
      "      vf_explained_var: 0.9958041310310364\n",
      "      vf_loss: 2.0193209648132324\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3395315110683441\n",
      "      kl: 0.020511917769908905\n",
      "      policy_loss: 0.007471362594515085\n",
      "      total_loss: 2.4821858406066895\n",
      "      vf_explained_var: 0.9973466396331787\n",
      "      vf_loss: 2.474714517593384\n",
      "    sample_time_ms: 20155.448\n",
      "    update_time_ms: 7.499\n",
      "  iterations_since_restore: 533\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.39933268355146\n",
      "    rl_1: 104.52256519227063\n",
      "  time_since_restore: 12625.972450971603\n",
      "  time_this_iter_s: 23.630414247512817\n",
      "  time_total_s: 12625.972450971603\n",
      "  timestamp: 1550806053\n",
      "  timesteps_since_restore: 5330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5330000\n",
      "  training_iteration: 533\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12625 s, 533 iter, 5330000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-27-56\n",
      "  done: false\n",
      "  episode_len_mean: 91.71296296296296\n",
      "  episode_reward_max: 224.10255086706243\n",
      "  episode_reward_mean: 180.00164168172466\n",
      "  episode_reward_min: -174.1760755918978\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 55473\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.142\n",
      "    load_time_ms: 2.319\n",
      "    num_steps_sampled: 5340000\n",
      "    num_steps_trained: 5340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.055996187031269073\n",
      "      kl: 0.009985108859837055\n",
      "      policy_loss: -0.007288606371730566\n",
      "      total_loss: 29.490991592407227\n",
      "      vf_explained_var: 0.9543865919113159\n",
      "      vf_loss: 29.498281478881836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3876578211784363\n",
      "      kl: 0.017733696848154068\n",
      "      policy_loss: -0.0015518513973802328\n",
      "      total_loss: 31.79424285888672\n",
      "      vf_explained_var: 0.9715754389762878\n",
      "      vf_loss: 31.795791625976562\n",
      "    sample_time_ms: 20107.005\n",
      "    update_time_ms: 7.567\n",
      "  iterations_since_restore: 534\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.32540483506097\n",
      "    rl_1: 104.6762368466637\n",
      "  time_since_restore: 12649.314835071564\n",
      "  time_this_iter_s: 23.342384099960327\n",
      "  time_total_s: 12649.314835071564\n",
      "  timestamp: 1550806076\n",
      "  timesteps_since_restore: 5340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5340000\n",
      "  training_iteration: 534\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12649 s, 534 iter, 5340000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-28-20\n",
      "  done: false\n",
      "  episode_len_mean: 92.74074074074075\n",
      "  episode_reward_max: 218.01183543278822\n",
      "  episode_reward_mean: 178.88036809325607\n",
      "  episode_reward_min: 146.1065232014531\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 55581\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3151.917\n",
      "    load_time_ms: 2.349\n",
      "    num_steps_sampled: 5350000\n",
      "    num_steps_trained: 5350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.06989459693431854\n",
      "      kl: 0.016688993200659752\n",
      "      policy_loss: 0.0006168138352222741\n",
      "      total_loss: 2.547893524169922\n",
      "      vf_explained_var: 0.9951017498970032\n",
      "      vf_loss: 2.547276496887207\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.41302919387817383\n",
      "      kl: 0.022018438205122948\n",
      "      policy_loss: 0.0017497852677479386\n",
      "      total_loss: 2.9204697608947754\n",
      "      vf_explained_var: 0.996934175491333\n",
      "      vf_loss: 2.918720006942749\n",
      "    sample_time_ms: 20164.432\n",
      "    update_time_ms: 7.465\n",
      "  iterations_since_restore: 535\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.19924589789787\n",
      "    rl_1: 104.68112219535821\n",
      "  time_since_restore: 12672.926303863525\n",
      "  time_this_iter_s: 23.61146879196167\n",
      "  time_total_s: 12672.926303863525\n",
      "  timestamp: 1550806100\n",
      "  timesteps_since_restore: 5350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5350000\n",
      "  training_iteration: 535\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12672 s, 535 iter, 5350000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-28-43\n",
      "  done: false\n",
      "  episode_len_mean: 92.71296296296296\n",
      "  episode_reward_max: 223.55713990236998\n",
      "  episode_reward_mean: 178.7037183052969\n",
      "  episode_reward_min: 140.56753505897078\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 55689\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.8\n",
      "    load_time_ms: 2.373\n",
      "    num_steps_sampled: 5360000\n",
      "    num_steps_trained: 5360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.01440573763102293\n",
      "      kl: 0.01842251420021057\n",
      "      policy_loss: -0.0008890792960301042\n",
      "      total_loss: 1.918634057044983\n",
      "      vf_explained_var: 0.9962998628616333\n",
      "      vf_loss: 1.9195232391357422\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4127732217311859\n",
      "      kl: 0.017090298235416412\n",
      "      policy_loss: 0.004205808974802494\n",
      "      total_loss: 2.370142698287964\n",
      "      vf_explained_var: 0.9975351095199585\n",
      "      vf_loss: 2.3659374713897705\n",
      "    sample_time_ms: 20201.598\n",
      "    update_time_ms: 7.511\n",
      "  iterations_since_restore: 536\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.98032003211263\n",
      "    rl_1: 105.72339827318423\n",
      "  time_since_restore: 12696.233505249023\n",
      "  time_this_iter_s: 23.307201385498047\n",
      "  time_total_s: 12696.233505249023\n",
      "  timestamp: 1550806123\n",
      "  timesteps_since_restore: 5360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5360000\n",
      "  training_iteration: 536\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12696 s, 536 iter, 5360000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-29-06\n",
      "  done: false\n",
      "  episode_len_mean: 92.88888888888889\n",
      "  episode_reward_max: 218.652751007757\n",
      "  episode_reward_mean: 176.40929870023123\n",
      "  episode_reward_min: -155.01922590514727\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 55797\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3151.664\n",
      "    load_time_ms: 2.336\n",
      "    num_steps_sampled: 5370000\n",
      "    num_steps_trained: 5370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.0400308221578598\n",
      "      kl: 0.011496367864310741\n",
      "      policy_loss: -0.0019273388898000121\n",
      "      total_loss: 5.721927165985107\n",
      "      vf_explained_var: 0.9912531971931458\n",
      "      vf_loss: 5.723853588104248\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.43719711899757385\n",
      "      kl: 0.009430802427232265\n",
      "      policy_loss: -0.0005271258996799588\n",
      "      total_loss: 5.09423303604126\n",
      "      vf_explained_var: 0.9955595135688782\n",
      "      vf_loss: 5.094760417938232\n",
      "    sample_time_ms: 20151.227\n",
      "    update_time_ms: 7.639\n",
      "  iterations_since_restore: 537\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.35187693450115\n",
      "    rl_1: 104.0574217657301\n",
      "  time_since_restore: 12719.20975446701\n",
      "  time_this_iter_s: 22.97624921798706\n",
      "  time_total_s: 12719.20975446701\n",
      "  timestamp: 1550806146\n",
      "  timesteps_since_restore: 5370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5370000\n",
      "  training_iteration: 537\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12719 s, 537 iter, 5370000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-29-29\n",
      "  done: false\n",
      "  episode_len_mean: 92.83333333333333\n",
      "  episode_reward_max: 220.10796437828097\n",
      "  episode_reward_mean: 178.14619764662328\n",
      "  episode_reward_min: 144.9804181885198\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 55905\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.805\n",
      "    load_time_ms: 2.339\n",
      "    num_steps_sampled: 5380000\n",
      "    num_steps_trained: 5380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.06976328045129776\n",
      "      kl: 0.01564699038863182\n",
      "      policy_loss: -0.003334114095196128\n",
      "      total_loss: 2.049229145050049\n",
      "      vf_explained_var: 0.9959610104560852\n",
      "      vf_loss: 2.052563428878784\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3252675533294678\n",
      "      kl: 0.019073890522122383\n",
      "      policy_loss: 0.0007105095428414643\n",
      "      total_loss: 2.3394200801849365\n",
      "      vf_explained_var: 0.9974645972251892\n",
      "      vf_loss: 2.338709592819214\n",
      "    sample_time_ms: 20162.966\n",
      "    update_time_ms: 7.797\n",
      "  iterations_since_restore: 538\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.14853071837625\n",
      "    rl_1: 103.99766692824703\n",
      "  time_since_restore: 12742.356580734253\n",
      "  time_this_iter_s: 23.14682626724243\n",
      "  time_total_s: 12742.356580734253\n",
      "  timestamp: 1550806169\n",
      "  timesteps_since_restore: 5380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5380000\n",
      "  training_iteration: 538\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12742 s, 538 iter, 5380000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-29-53\n",
      "  done: false\n",
      "  episode_len_mean: 92.5\n",
      "  episode_reward_max: 226.06356176016277\n",
      "  episode_reward_mean: 179.2010497400986\n",
      "  episode_reward_min: 145.0585052716807\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 56013\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3149.784\n",
      "    load_time_ms: 2.358\n",
      "    num_steps_sampled: 5390000\n",
      "    num_steps_trained: 5390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.06898870319128036\n",
      "      kl: 0.01759222522377968\n",
      "      policy_loss: -0.000969147658906877\n",
      "      total_loss: 1.8301281929016113\n",
      "      vf_explained_var: 0.9963696599006653\n",
      "      vf_loss: 1.8310974836349487\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3699767589569092\n",
      "      kl: 0.022567370906472206\n",
      "      policy_loss: 0.005199449602514505\n",
      "      total_loss: 1.9988735914230347\n",
      "      vf_explained_var: 0.9978865385055542\n",
      "      vf_loss: 1.9936742782592773\n",
      "    sample_time_ms: 20199.853\n",
      "    update_time_ms: 7.545\n",
      "  iterations_since_restore: 539\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.86751468346641\n",
      "    rl_1: 104.33353505663216\n",
      "  time_since_restore: 12765.84769821167\n",
      "  time_this_iter_s: 23.491117477416992\n",
      "  time_total_s: 12765.84769821167\n",
      "  timestamp: 1550806193\n",
      "  timesteps_since_restore: 5390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5390000\n",
      "  training_iteration: 539\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12765 s, 539 iter, 5390000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-30-16\n",
      "  done: false\n",
      "  episode_len_mean: 92.26851851851852\n",
      "  episode_reward_max: 222.35898900454706\n",
      "  episode_reward_mean: 184.41676116861996\n",
      "  episode_reward_min: 148.96996532210585\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 56121\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.421\n",
      "    load_time_ms: 2.352\n",
      "    num_steps_sampled: 5400000\n",
      "    num_steps_trained: 5400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.02027786895632744\n",
      "      kl: 0.014234197326004505\n",
      "      policy_loss: -0.0027849699836224318\n",
      "      total_loss: 1.5075253248214722\n",
      "      vf_explained_var: 0.9972040057182312\n",
      "      vf_loss: 1.5103102922439575\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.37868475914001465\n",
      "      kl: 0.018470030277967453\n",
      "      policy_loss: 0.0024525178596377373\n",
      "      total_loss: 1.642068862915039\n",
      "      vf_explained_var: 0.9982638955116272\n",
      "      vf_loss: 1.6396164894104004\n",
      "    sample_time_ms: 20134.047\n",
      "    update_time_ms: 7.464\n",
      "  iterations_since_restore: 540\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 77.86565260221244\n",
      "    rl_1: 106.55110856640752\n",
      "  time_since_restore: 12788.642165899277\n",
      "  time_this_iter_s: 22.79446768760681\n",
      "  time_total_s: 12788.642165899277\n",
      "  timestamp: 1550806216\n",
      "  timesteps_since_restore: 5400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5400000\n",
      "  training_iteration: 540\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12788 s, 540 iter, 5400000 ts, 184 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-30-39\n",
      "  done: false\n",
      "  episode_len_mean: 92.63888888888889\n",
      "  episode_reward_max: 225.58902255659274\n",
      "  episode_reward_mean: 178.3196817686432\n",
      "  episode_reward_min: -83.01780444181598\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 56229\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.53\n",
      "    load_time_ms: 2.426\n",
      "    num_steps_sampled: 5410000\n",
      "    num_steps_trained: 5410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.024388689547777176\n",
      "      kl: 0.012569029815495014\n",
      "      policy_loss: -0.0027653758879750967\n",
      "      total_loss: 9.270642280578613\n",
      "      vf_explained_var: 0.9856842160224915\n",
      "      vf_loss: 9.273408889770508\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.41345563530921936\n",
      "      kl: 0.02012518234550953\n",
      "      policy_loss: 0.00020010367734357715\n",
      "      total_loss: 12.02889633178711\n",
      "      vf_explained_var: 0.9889265894889832\n",
      "      vf_loss: 12.028696060180664\n",
      "    sample_time_ms: 20173.035\n",
      "    update_time_ms: 7.23\n",
      "  iterations_since_restore: 541\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.6117714246909\n",
      "    rl_1: 103.70791034395228\n",
      "  time_since_restore: 12812.205776691437\n",
      "  time_this_iter_s: 23.563610792160034\n",
      "  time_total_s: 12812.205776691437\n",
      "  timestamp: 1550806239\n",
      "  timesteps_since_restore: 5410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5410000\n",
      "  training_iteration: 541\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12812 s, 541 iter, 5410000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-31-03\n",
      "  done: false\n",
      "  episode_len_mean: 93.10185185185185\n",
      "  episode_reward_max: 221.945304372298\n",
      "  episode_reward_mean: 176.50024879058677\n",
      "  episode_reward_min: -49.133740818585835\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 56337\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3149.994\n",
      "    load_time_ms: 2.383\n",
      "    num_steps_sampled: 5420000\n",
      "    num_steps_trained: 5420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.04379776865243912\n",
      "      kl: 0.0072446358390152454\n",
      "      policy_loss: -0.0012965945061296225\n",
      "      total_loss: 17.26336669921875\n",
      "      vf_explained_var: 0.976119339466095\n",
      "      vf_loss: 17.264663696289062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.39531785249710083\n",
      "      kl: 0.010692395269870758\n",
      "      policy_loss: -0.0009649806888774037\n",
      "      total_loss: 17.225215911865234\n",
      "      vf_explained_var: 0.986801028251648\n",
      "      vf_loss: 17.22618293762207\n",
      "    sample_time_ms: 20157.765\n",
      "    update_time_ms: 7.075\n",
      "  iterations_since_restore: 542\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.49077981764219\n",
      "    rl_1: 103.00946897294462\n",
      "  time_since_restore: 12835.695470809937\n",
      "  time_this_iter_s: 23.489694118499756\n",
      "  time_total_s: 12835.695470809937\n",
      "  timestamp: 1550806263\n",
      "  timesteps_since_restore: 5420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5420000\n",
      "  training_iteration: 542\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12835 s, 542 iter, 5420000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-31-26\n",
      "  done: false\n",
      "  episode_len_mean: 92.43518518518519\n",
      "  episode_reward_max: 221.92302731681474\n",
      "  episode_reward_mean: 180.22739310534035\n",
      "  episode_reward_min: -53.121320166259096\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 56445\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.255\n",
      "    load_time_ms: 2.361\n",
      "    num_steps_sampled: 5430000\n",
      "    num_steps_trained: 5430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.013683400116860867\n",
      "      kl: 0.018855644389986992\n",
      "      policy_loss: -0.0035007596015930176\n",
      "      total_loss: 5.979555130004883\n",
      "      vf_explained_var: 0.990590512752533\n",
      "      vf_loss: 5.983055114746094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.45090460777282715\n",
      "      kl: 0.014464805833995342\n",
      "      policy_loss: -0.0007127278950065374\n",
      "      total_loss: 7.960201740264893\n",
      "      vf_explained_var: 0.9930926561355591\n",
      "      vf_loss: 7.960911750793457\n",
      "    sample_time_ms: 20142.337\n",
      "    update_time_ms: 7.289\n",
      "  iterations_since_restore: 543\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.61364698038767\n",
      "    rl_1: 104.61374612495267\n",
      "  time_since_restore: 12859.115280151367\n",
      "  time_this_iter_s: 23.419809341430664\n",
      "  time_total_s: 12859.115280151367\n",
      "  timestamp: 1550806286\n",
      "  timesteps_since_restore: 5430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5430000\n",
      "  training_iteration: 543\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12859 s, 543 iter, 5430000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-31-50\n",
      "  done: false\n",
      "  episode_len_mean: 91.75925925925925\n",
      "  episode_reward_max: 221.9240688735447\n",
      "  episode_reward_mean: 181.14691276670274\n",
      "  episode_reward_min: 143.63028288492612\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 56553\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3159.448\n",
      "    load_time_ms: 2.378\n",
      "    num_steps_sampled: 5440000\n",
      "    num_steps_trained: 5440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.01580032706260681\n",
      "      kl: 0.022401727735996246\n",
      "      policy_loss: -0.001478937454521656\n",
      "      total_loss: 2.2003369331359863\n",
      "      vf_explained_var: 0.9960211515426636\n",
      "      vf_loss: 2.2018163204193115\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.45282644033432007\n",
      "      kl: 0.020436104387044907\n",
      "      policy_loss: 0.0027859967667609453\n",
      "      total_loss: 2.5794830322265625\n",
      "      vf_explained_var: 0.9974011182785034\n",
      "      vf_loss: 2.57669734954834\n",
      "    sample_time_ms: 20152.925\n",
      "    update_time_ms: 7.107\n",
      "  iterations_since_restore: 544\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.78260358381597\n",
      "    rl_1: 104.36430918288676\n",
      "  time_since_restore: 12882.715286016464\n",
      "  time_this_iter_s: 23.600005865097046\n",
      "  time_total_s: 12882.715286016464\n",
      "  timestamp: 1550806310\n",
      "  timesteps_since_restore: 5440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5440000\n",
      "  training_iteration: 544\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12882 s, 544 iter, 5440000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-32-14\n",
      "  done: false\n",
      "  episode_len_mean: 92.4770642201835\n",
      "  episode_reward_max: 222.57803347910883\n",
      "  episode_reward_mean: 179.88918082654368\n",
      "  episode_reward_min: 146.34618583810777\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 56662\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.313\n",
      "    load_time_ms: 2.386\n",
      "    num_steps_sampled: 5450000\n",
      "    num_steps_trained: 5450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.01893557608127594\n",
      "      kl: 0.014733193442225456\n",
      "      policy_loss: -0.0020073633641004562\n",
      "      total_loss: 1.90941321849823\n",
      "      vf_explained_var: 0.9964204430580139\n",
      "      vf_loss: 1.9114209413528442\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4227433204650879\n",
      "      kl: 0.03384583443403244\n",
      "      policy_loss: 0.004701054655015469\n",
      "      total_loss: 2.233809232711792\n",
      "      vf_explained_var: 0.9976069927215576\n",
      "      vf_loss: 2.2291080951690674\n",
      "    sample_time_ms: 20186.204\n",
      "    update_time_ms: 6.827\n",
      "  iterations_since_restore: 545\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.58128134468318\n",
      "    rl_1: 104.3078994818605\n",
      "  time_since_restore: 12906.49801492691\n",
      "  time_this_iter_s: 23.782728910446167\n",
      "  time_total_s: 12906.49801492691\n",
      "  timestamp: 1550806334\n",
      "  timesteps_since_restore: 5450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5450000\n",
      "  training_iteration: 545\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12906 s, 545 iter, 5450000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-32-37\n",
      "  done: false\n",
      "  episode_len_mean: 92.54629629629629\n",
      "  episode_reward_max: 226.58865216076896\n",
      "  episode_reward_mean: 179.88689160423098\n",
      "  episode_reward_min: 146.65351975185288\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 56770\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.411\n",
      "    load_time_ms: 2.347\n",
      "    num_steps_sampled: 5460000\n",
      "    num_steps_trained: 5460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0033862232230603695\n",
      "      kl: 0.017885562032461166\n",
      "      policy_loss: 0.0012475663097575307\n",
      "      total_loss: 1.7797415256500244\n",
      "      vf_explained_var: 0.9966710209846497\n",
      "      vf_loss: 1.7784940004348755\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4556833803653717\n",
      "      kl: 0.06561757624149323\n",
      "      policy_loss: 0.021466931328177452\n",
      "      total_loss: 1.664646863937378\n",
      "      vf_explained_var: 0.9982035756111145\n",
      "      vf_loss: 1.643180251121521\n",
      "    sample_time_ms: 20215.624\n",
      "    update_time_ms: 6.799\n",
      "  iterations_since_restore: 546\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.85062719586386\n",
      "    rl_1: 104.03626440836715\n",
      "  time_since_restore: 12930.078578710556\n",
      "  time_this_iter_s: 23.58056378364563\n",
      "  time_total_s: 12930.078578710556\n",
      "  timestamp: 1550806357\n",
      "  timesteps_since_restore: 5460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5460000\n",
      "  training_iteration: 546\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12930 s, 546 iter, 5460000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-33-00\n",
      "  done: false\n",
      "  episode_len_mean: 92.35185185185185\n",
      "  episode_reward_max: 224.03051347441743\n",
      "  episode_reward_mean: 183.19886957455932\n",
      "  episode_reward_min: 145.1315688981462\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 56878\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.611\n",
      "    load_time_ms: 2.329\n",
      "    num_steps_sampled: 5470000\n",
      "    num_steps_trained: 5470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.02363685704767704\n",
      "      kl: 0.012734443880617619\n",
      "      policy_loss: -0.0011066397419199347\n",
      "      total_loss: 1.7169848680496216\n",
      "      vf_explained_var: 0.9969291687011719\n",
      "      vf_loss: 1.7180919647216797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5085309147834778\n",
      "      kl: 0.019839199259877205\n",
      "      policy_loss: 0.006334308069199324\n",
      "      total_loss: 1.86473548412323\n",
      "      vf_explained_var: 0.9980563521385193\n",
      "      vf_loss: 1.8584014177322388\n",
      "    sample_time_ms: 20212.838\n",
      "    update_time_ms: 6.597\n",
      "  iterations_since_restore: 547\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 77.17526615209745\n",
      "    rl_1: 106.0236034224619\n",
      "  time_since_restore: 12953.056318759918\n",
      "  time_this_iter_s: 22.977740049362183\n",
      "  time_total_s: 12953.056318759918\n",
      "  timestamp: 1550806380\n",
      "  timesteps_since_restore: 5470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5470000\n",
      "  training_iteration: 547\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12953 s, 547 iter, 5470000 ts, 183 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-33-24\n",
      "  done: false\n",
      "  episode_len_mean: 93.32710280373831\n",
      "  episode_reward_max: 221.12862212197376\n",
      "  episode_reward_mean: 182.03230466663916\n",
      "  episode_reward_min: 143.67349699015904\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 56985\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.457\n",
      "    load_time_ms: 2.314\n",
      "    num_steps_sampled: 5480000\n",
      "    num_steps_trained: 5480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.010521995835006237\n",
      "      kl: 0.016314547508955002\n",
      "      policy_loss: -0.0015258544590324163\n",
      "      total_loss: 1.8266359567642212\n",
      "      vf_explained_var: 0.9964912533760071\n",
      "      vf_loss: 1.8281618356704712\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.49768224358558655\n",
      "      kl: 0.029048562049865723\n",
      "      policy_loss: 0.007398214656859636\n",
      "      total_loss: 1.8895232677459717\n",
      "      vf_explained_var: 0.9980251789093018\n",
      "      vf_loss: 1.882124900817871\n",
      "    sample_time_ms: 20242.915\n",
      "    update_time_ms: 6.453\n",
      "  iterations_since_restore: 548\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.75598245516939\n",
      "    rl_1: 106.27632221146976\n",
      "  time_since_restore: 12976.468868732452\n",
      "  time_this_iter_s: 23.41254997253418\n",
      "  time_total_s: 12976.468868732452\n",
      "  timestamp: 1550806404\n",
      "  timesteps_since_restore: 5480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5480000\n",
      "  training_iteration: 548\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12976 s, 548 iter, 5480000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-33-47\n",
      "  done: false\n",
      "  episode_len_mean: 92.1743119266055\n",
      "  episode_reward_max: 222.40979574945788\n",
      "  episode_reward_mean: 183.72819830657906\n",
      "  episode_reward_min: 149.6519128798561\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 57094\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.428\n",
      "    load_time_ms: 2.316\n",
      "    num_steps_sampled: 5490000\n",
      "    num_steps_trained: 5490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.03223741054534912\n",
      "      kl: 0.017607301473617554\n",
      "      policy_loss: -0.003030196065083146\n",
      "      total_loss: 1.644068956375122\n",
      "      vf_explained_var: 0.9969249367713928\n",
      "      vf_loss: 1.6470990180969238\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.47271406650543213\n",
      "      kl: 0.0163169763982296\n",
      "      policy_loss: 0.006895042955875397\n",
      "      total_loss: 1.928833246231079\n",
      "      vf_explained_var: 0.998013436794281\n",
      "      vf_loss: 1.921938180923462\n",
      "    sample_time_ms: 20237.437\n",
      "    update_time_ms: 6.685\n",
      "  iterations_since_restore: 549\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.7609007635352\n",
      "    rl_1: 106.96729754304381\n",
      "  time_since_restore: 12999.900778770447\n",
      "  time_this_iter_s: 23.431910037994385\n",
      "  time_total_s: 12999.900778770447\n",
      "  timestamp: 1550806427\n",
      "  timesteps_since_restore: 5490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5490000\n",
      "  training_iteration: 549\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 12999 s, 549 iter, 5490000 ts, 184 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-34-10\n",
      "  done: false\n",
      "  episode_len_mean: 91.72222222222223\n",
      "  episode_reward_max: 217.26638170210174\n",
      "  episode_reward_mean: 178.6132681165067\n",
      "  episode_reward_min: 147.05676017953306\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 57202\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.284\n",
      "    load_time_ms: 2.268\n",
      "    num_steps_sampled: 5500000\n",
      "    num_steps_trained: 5500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.02869790606200695\n",
      "      kl: 0.015352091751992702\n",
      "      policy_loss: 0.00020632702216971666\n",
      "      total_loss: 1.781625747680664\n",
      "      vf_explained_var: 0.9965299963951111\n",
      "      vf_loss: 1.7814191579818726\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4403410255908966\n",
      "      kl: 0.03388519957661629\n",
      "      policy_loss: 0.011390267871320248\n",
      "      total_loss: 2.0348758697509766\n",
      "      vf_explained_var: 0.9978182911872864\n",
      "      vf_loss: 2.0234858989715576\n",
      "    sample_time_ms: 20262.869\n",
      "    update_time_ms: 6.651\n",
      "  iterations_since_restore: 550\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.61487163220245\n",
      "    rl_1: 103.99839648430424\n",
      "  time_since_restore: 13022.987465143204\n",
      "  time_this_iter_s: 23.086686372756958\n",
      "  time_total_s: 13022.987465143204\n",
      "  timestamp: 1550806450\n",
      "  timesteps_since_restore: 5500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5500000\n",
      "  training_iteration: 550\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13022 s, 550 iter, 5500000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-34-34\n",
      "  done: false\n",
      "  episode_len_mean: 92.64220183486239\n",
      "  episode_reward_max: 225.74315817910707\n",
      "  episode_reward_mean: 181.97668171745389\n",
      "  episode_reward_min: 143.84932341496042\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 57311\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.881\n",
      "    load_time_ms: 2.177\n",
      "    num_steps_sampled: 5510000\n",
      "    num_steps_trained: 5510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.011343911290168762\n",
      "      kl: 0.014195439405739307\n",
      "      policy_loss: -0.000327725283568725\n",
      "      total_loss: 1.553485631942749\n",
      "      vf_explained_var: 0.9971290826797485\n",
      "      vf_loss: 1.5538133382797241\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.42559656500816345\n",
      "      kl: 0.022357361391186714\n",
      "      policy_loss: 0.009114949032664299\n",
      "      total_loss: 1.6054109334945679\n",
      "      vf_explained_var: 0.9982852339744568\n",
      "      vf_loss: 1.5962960720062256\n",
      "    sample_time_ms: 20247.944\n",
      "    update_time_ms: 7.076\n",
      "  iterations_since_restore: 551\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 77.11940712022756\n",
      "    rl_1: 104.85727459722632\n",
      "  time_since_restore: 13046.319697141647\n",
      "  time_this_iter_s: 23.332231998443604\n",
      "  time_total_s: 13046.319697141647\n",
      "  timestamp: 1550806474\n",
      "  timesteps_since_restore: 5510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5510000\n",
      "  training_iteration: 551\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13046 s, 551 iter, 5510000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-34-58\n",
      "  done: false\n",
      "  episode_len_mean: 92.39814814814815\n",
      "  episode_reward_max: 218.58035719745152\n",
      "  episode_reward_mean: 177.018229864059\n",
      "  episode_reward_min: -84.07200490907323\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 57419\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.685\n",
      "    load_time_ms: 2.176\n",
      "    num_steps_sampled: 5520000\n",
      "    num_steps_trained: 5520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.017940936610102654\n",
      "      kl: 0.006648033857345581\n",
      "      policy_loss: -0.001074204221367836\n",
      "      total_loss: 15.625236511230469\n",
      "      vf_explained_var: 0.9784938097000122\n",
      "      vf_loss: 15.626310348510742\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5037330985069275\n",
      "      kl: 0.02352115698158741\n",
      "      policy_loss: -0.0025996582116931677\n",
      "      total_loss: 16.08139991760254\n",
      "      vf_explained_var: 0.985733687877655\n",
      "      vf_loss: 16.08399772644043\n",
      "    sample_time_ms: 20303.455\n",
      "    update_time_ms: 7.088\n",
      "  iterations_since_restore: 552\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.87625773712458\n",
      "    rl_1: 103.14197212693446\n",
      "  time_since_restore: 13070.353218793869\n",
      "  time_this_iter_s: 24.03352165222168\n",
      "  time_total_s: 13070.353218793869\n",
      "  timestamp: 1550806498\n",
      "  timesteps_since_restore: 5520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5520000\n",
      "  training_iteration: 552\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13070 s, 552 iter, 5520000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-35-21\n",
      "  done: false\n",
      "  episode_len_mean: 92.48148148148148\n",
      "  episode_reward_max: 226.18522302989567\n",
      "  episode_reward_mean: 181.81628630898672\n",
      "  episode_reward_min: -150.0325597502785\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 57527\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3156.128\n",
      "    load_time_ms: 2.152\n",
      "    num_steps_sampled: 5530000\n",
      "    num_steps_trained: 5530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09007970243692398\n",
      "      kl: 0.0201979111880064\n",
      "      policy_loss: -0.0038680576253682375\n",
      "      total_loss: 25.211637496948242\n",
      "      vf_explained_var: 0.9636237025260925\n",
      "      vf_loss: 25.215505599975586\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4974980354309082\n",
      "      kl: 0.02999270334839821\n",
      "      policy_loss: -0.0030715474858880043\n",
      "      total_loss: 41.87114715576172\n",
      "      vf_explained_var: 0.9636602997779846\n",
      "      vf_loss: 41.87422180175781\n",
      "    sample_time_ms: 20288.127\n",
      "    update_time_ms: 6.995\n",
      "  iterations_since_restore: 553\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.8940472410434\n",
      "    rl_1: 106.92223906794331\n",
      "  time_since_restore: 13093.829479455948\n",
      "  time_this_iter_s: 23.476260662078857\n",
      "  time_total_s: 13093.829479455948\n",
      "  timestamp: 1550806521\n",
      "  timesteps_since_restore: 5530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5530000\n",
      "  training_iteration: 553\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13093 s, 553 iter, 5530000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-35-45\n",
      "  done: false\n",
      "  episode_len_mean: 91.5137614678899\n",
      "  episode_reward_max: 224.690831545091\n",
      "  episode_reward_mean: 179.12053015963355\n",
      "  episode_reward_min: -156.46440072991285\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 57636\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.349\n",
      "    load_time_ms: 2.148\n",
      "    num_steps_sampled: 5540000\n",
      "    num_steps_trained: 5540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.05931415408849716\n",
      "      kl: 0.013170668855309486\n",
      "      policy_loss: -0.004132432863116264\n",
      "      total_loss: 26.88970184326172\n",
      "      vf_explained_var: 0.959257960319519\n",
      "      vf_loss: 26.89383316040039\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4146857261657715\n",
      "      kl: 10.490218162536621\n",
      "      policy_loss: 0.021231865510344505\n",
      "      total_loss: 42.18241500854492\n",
      "      vf_explained_var: 0.9586398601531982\n",
      "      vf_loss: 42.16118240356445\n",
      "    sample_time_ms: 20309.309\n",
      "    update_time_ms: 7.666\n",
      "  iterations_since_restore: 554\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.84387225953212\n",
      "    rl_1: 104.2766579001014\n",
      "  time_since_restore: 13117.51013970375\n",
      "  time_this_iter_s: 23.680660247802734\n",
      "  time_total_s: 13117.51013970375\n",
      "  timestamp: 1550806545\n",
      "  timesteps_since_restore: 5540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5540000\n",
      "  training_iteration: 554\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13117 s, 554 iter, 5540000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-36-08\n",
      "  done: false\n",
      "  episode_len_mean: 92.99074074074075\n",
      "  episode_reward_max: 220.0591182042873\n",
      "  episode_reward_mean: 176.0974154531276\n",
      "  episode_reward_min: -151.79706103113227\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 57744\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.434\n",
      "    load_time_ms: 2.084\n",
      "    num_steps_sampled: 5550000\n",
      "    num_steps_trained: 5550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.02935134805738926\n",
      "      kl: 0.024639127776026726\n",
      "      policy_loss: -0.0061862231232225895\n",
      "      total_loss: 28.7691650390625\n",
      "      vf_explained_var: 0.9553466439247131\n",
      "      vf_loss: 28.77535057067871\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.40286970138549805\n",
      "      kl: 5.629371166229248\n",
      "      policy_loss: 0.010906210169196129\n",
      "      total_loss: 45.230499267578125\n",
      "      vf_explained_var: 0.9565101265907288\n",
      "      vf_loss: 45.21958923339844\n",
      "    sample_time_ms: 20242.223\n",
      "    update_time_ms: 7.676\n",
      "  iterations_since_restore: 555\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.18130719862225\n",
      "    rl_1: 103.91610825450535\n",
      "  time_since_restore: 13140.63225531578\n",
      "  time_this_iter_s: 23.12211561203003\n",
      "  time_total_s: 13140.63225531578\n",
      "  timestamp: 1550806568\n",
      "  timesteps_since_restore: 5550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5550000\n",
      "  training_iteration: 555\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13140 s, 555 iter, 5550000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-36-31\n",
      "  done: false\n",
      "  episode_len_mean: 92.8425925925926\n",
      "  episode_reward_max: 228.2618117212416\n",
      "  episode_reward_mean: 164.48278055418322\n",
      "  episode_reward_min: -165.80800154131398\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 57852\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.608\n",
      "    load_time_ms: 2.095\n",
      "    num_steps_sampled: 5560000\n",
      "    num_steps_trained: 5560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.033894848078489304\n",
      "      kl: 0.02061339281499386\n",
      "      policy_loss: -0.0032400803174823523\n",
      "      total_loss: 147.60020446777344\n",
      "      vf_explained_var: 0.8015753030776978\n",
      "      vf_loss: 147.60345458984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.39593321084976196\n",
      "      kl: 0.07678715139627457\n",
      "      policy_loss: 0.005997577682137489\n",
      "      total_loss: 177.51129150390625\n",
      "      vf_explained_var: 0.8581538200378418\n",
      "      vf_loss: 177.5052947998047\n",
      "    sample_time_ms: 20145.888\n",
      "    update_time_ms: 7.697\n",
      "  iterations_since_restore: 556\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.96375558373705\n",
      "    rl_1: 96.51902497044618\n",
      "  time_since_restore: 13163.261891841888\n",
      "  time_this_iter_s: 22.629636526107788\n",
      "  time_total_s: 13163.261891841888\n",
      "  timestamp: 1550806591\n",
      "  timesteps_since_restore: 5560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5560000\n",
      "  training_iteration: 556\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13163 s, 556 iter, 5560000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-36-55\n",
      "  done: false\n",
      "  episode_len_mean: 93.29906542056075\n",
      "  episode_reward_max: 222.83489988160864\n",
      "  episode_reward_mean: 184.9198028618794\n",
      "  episode_reward_min: 147.76523788697835\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 57959\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.218\n",
      "    load_time_ms: 2.165\n",
      "    num_steps_sampled: 5570000\n",
      "    num_steps_trained: 5570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06608861684799194\n",
      "      kl: 0.024103516712784767\n",
      "      policy_loss: 0.0005768424598500133\n",
      "      total_loss: 2.3160831928253174\n",
      "      vf_explained_var: 0.9955676198005676\n",
      "      vf_loss: 2.3155064582824707\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.39166709780693054\n",
      "      kl: 0.016704348847270012\n",
      "      policy_loss: 0.002652805531397462\n",
      "      total_loss: 2.2570271492004395\n",
      "      vf_explained_var: 0.9975603818893433\n",
      "      vf_loss: 2.2543742656707764\n",
      "    sample_time_ms: 20233.193\n",
      "    update_time_ms: 7.747\n",
      "  iterations_since_restore: 557\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 78.16623840267977\n",
      "    rl_1: 106.75356445919964\n",
      "  time_since_restore: 13187.089757204056\n",
      "  time_this_iter_s: 23.82786536216736\n",
      "  time_total_s: 13187.089757204056\n",
      "  timestamp: 1550806615\n",
      "  timesteps_since_restore: 5570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5570000\n",
      "  training_iteration: 557\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13187 s, 557 iter, 5570000 ts, 185 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-37-18\n",
      "  done: false\n",
      "  episode_len_mean: 93.7196261682243\n",
      "  episode_reward_max: 220.11865020172257\n",
      "  episode_reward_mean: 170.85825792059782\n",
      "  episode_reward_min: -161.35856645991862\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 58066\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.615\n",
      "    load_time_ms: 2.18\n",
      "    num_steps_sampled: 5580000\n",
      "    num_steps_trained: 5580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.030117686837911606\n",
      "      kl: 0.013188776560127735\n",
      "      policy_loss: -0.0025937871541827917\n",
      "      total_loss: 63.7348518371582\n",
      "      vf_explained_var: 0.9192048907279968\n",
      "      vf_loss: 63.737449645996094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3566286861896515\n",
      "      kl: 0.013148837722837925\n",
      "      policy_loss: -0.002848575823009014\n",
      "      total_loss: 70.22209930419922\n",
      "      vf_explained_var: 0.9447030425071716\n",
      "      vf_loss: 70.22493743896484\n",
      "    sample_time_ms: 20191.252\n",
      "    update_time_ms: 7.766\n",
      "  iterations_since_restore: 558\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.7432147076886\n",
      "    rl_1: 100.1150432129092\n",
      "  time_since_restore: 13210.148254871368\n",
      "  time_this_iter_s: 23.058497667312622\n",
      "  time_total_s: 13210.148254871368\n",
      "  timestamp: 1550806638\n",
      "  timesteps_since_restore: 5580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5580000\n",
      "  training_iteration: 558\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13210 s, 558 iter, 5580000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-37-41\n",
      "  done: false\n",
      "  episode_len_mean: 94.01886792452831\n",
      "  episode_reward_max: 219.03531229835718\n",
      "  episode_reward_mean: 180.23015322836142\n",
      "  episode_reward_min: 143.26640111027774\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 58172\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.859\n",
      "    load_time_ms: 2.105\n",
      "    num_steps_sampled: 5590000\n",
      "    num_steps_trained: 5590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.0017440114170312881\n",
      "      kl: 0.014415256679058075\n",
      "      policy_loss: -0.0036423890851438046\n",
      "      total_loss: 2.31083083152771\n",
      "      vf_explained_var: 0.9955082535743713\n",
      "      vf_loss: 2.3144733905792236\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3390007019042969\n",
      "      kl: 0.020182665437459946\n",
      "      policy_loss: 0.000593976816162467\n",
      "      total_loss: 2.249178647994995\n",
      "      vf_explained_var: 0.9975149631500244\n",
      "      vf_loss: 2.248584747314453\n",
      "    sample_time_ms: 20179.847\n",
      "    update_time_ms: 7.575\n",
      "  iterations_since_restore: 559\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.56569095944471\n",
      "    rl_1: 103.6644622689167\n",
      "  time_since_restore: 13233.442489385605\n",
      "  time_this_iter_s: 23.29423451423645\n",
      "  time_total_s: 13233.442489385605\n",
      "  timestamp: 1550806661\n",
      "  timesteps_since_restore: 5590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5590000\n",
      "  training_iteration: 559\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13233 s, 559 iter, 5590000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-38-05\n",
      "  done: false\n",
      "  episode_len_mean: 92.66666666666667\n",
      "  episode_reward_max: 226.86630944059053\n",
      "  episode_reward_mean: 177.48794211357037\n",
      "  episode_reward_min: -124.7164676195646\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 58280\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.432\n",
      "    load_time_ms: 2.178\n",
      "    num_steps_sampled: 5600000\n",
      "    num_steps_trained: 5600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.017728056758642197\n",
      "      kl: 0.02481766603887081\n",
      "      policy_loss: -0.004247876815497875\n",
      "      total_loss: 36.877933502197266\n",
      "      vf_explained_var: 0.9416093826293945\n",
      "      vf_loss: 36.882179260253906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35752958059310913\n",
      "      kl: 0.03000098094344139\n",
      "      policy_loss: -0.0028621272649616003\n",
      "      total_loss: 45.15326690673828\n",
      "      vf_explained_var: 0.9561379551887512\n",
      "      vf_loss: 45.156131744384766\n",
      "    sample_time_ms: 20209.312\n",
      "    update_time_ms: 7.558\n",
      "  iterations_since_restore: 560\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.59297898461071\n",
      "    rl_1: 102.89496312895963\n",
      "  time_since_restore: 13256.821707248688\n",
      "  time_this_iter_s: 23.379217863082886\n",
      "  time_total_s: 13256.821707248688\n",
      "  timestamp: 1550806685\n",
      "  timesteps_since_restore: 5600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5600000\n",
      "  training_iteration: 560\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13256 s, 560 iter, 5600000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-38-28\n",
      "  done: false\n",
      "  episode_len_mean: 92.10185185185185\n",
      "  episode_reward_max: 224.9850092509703\n",
      "  episode_reward_mean: 175.45735398143637\n",
      "  episode_reward_min: -169.1574991490101\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 58388\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.494\n",
      "    load_time_ms: 2.189\n",
      "    num_steps_sampled: 5610000\n",
      "    num_steps_trained: 5610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.03344949707388878\n",
      "      kl: 0.012988674454391003\n",
      "      policy_loss: -0.009619930759072304\n",
      "      total_loss: 66.67887115478516\n",
      "      vf_explained_var: 0.9071179628372192\n",
      "      vf_loss: 66.68851470947266\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3091689646244049\n",
      "      kl: 0.0257223229855299\n",
      "      policy_loss: -0.00021340043167583644\n",
      "      total_loss: 89.8420181274414\n",
      "      vf_explained_var: 0.9204404950141907\n",
      "      vf_loss: 89.84222412109375\n",
      "    sample_time_ms: 20199.222\n",
      "    update_time_ms: 7.072\n",
      "  iterations_since_restore: 561\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.64046441009172\n",
      "    rl_1: 100.81688957134457\n",
      "  time_since_restore: 13280.0294713974\n",
      "  time_this_iter_s: 23.207764148712158\n",
      "  time_total_s: 13280.0294713974\n",
      "  timestamp: 1550806708\n",
      "  timesteps_since_restore: 5610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5610000\n",
      "  training_iteration: 561\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13280 s, 561 iter, 5610000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-38-52\n",
      "  done: false\n",
      "  episode_len_mean: 93.41121495327103\n",
      "  episode_reward_max: 224.66760920464267\n",
      "  episode_reward_mean: 177.79722099974003\n",
      "  episode_reward_min: -78.4651072376863\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 58495\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3167.646\n",
      "    load_time_ms: 2.197\n",
      "    num_steps_sampled: 5620000\n",
      "    num_steps_trained: 5620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.011053704656660557\n",
      "      kl: 0.011171570047736168\n",
      "      policy_loss: -0.0007412752602249384\n",
      "      total_loss: 11.806864738464355\n",
      "      vf_explained_var: 0.9805008172988892\n",
      "      vf_loss: 11.80760669708252\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3299175798892975\n",
      "      kl: 0.019385704770684242\n",
      "      policy_loss: -0.0024245276581496\n",
      "      total_loss: 11.53093147277832\n",
      "      vf_explained_var: 0.9892609715461731\n",
      "      vf_loss: 11.533357620239258\n",
      "    sample_time_ms: 20116.499\n",
      "    update_time_ms: 6.974\n",
      "  iterations_since_restore: 562\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.44968011839532\n",
      "    rl_1: 104.34754088134476\n",
      "  time_since_restore: 13303.464150428772\n",
      "  time_this_iter_s: 23.43467903137207\n",
      "  time_total_s: 13303.464150428772\n",
      "  timestamp: 1550806732\n",
      "  timesteps_since_restore: 5620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5620000\n",
      "  training_iteration: 562\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13303 s, 562 iter, 5620000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-39-15\n",
      "  done: false\n",
      "  episode_len_mean: 94.54716981132076\n",
      "  episode_reward_max: 224.7254110465377\n",
      "  episode_reward_mean: 181.28160106845925\n",
      "  episode_reward_min: 137.4873836668034\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 58601\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.837\n",
      "    load_time_ms: 2.213\n",
      "    num_steps_sampled: 5630000\n",
      "    num_steps_trained: 5630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.02366117388010025\n",
      "      kl: 0.012192302383482456\n",
      "      policy_loss: -0.0016901139169931412\n",
      "      total_loss: 2.1327760219573975\n",
      "      vf_explained_var: 0.9959849119186401\n",
      "      vf_loss: 2.1344664096832275\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3309762477874756\n",
      "      kl: 0.01849229447543621\n",
      "      policy_loss: 0.003290492342785001\n",
      "      total_loss: 1.9444119930267334\n",
      "      vf_explained_var: 0.9979164004325867\n",
      "      vf_loss: 1.9411218166351318\n",
      "    sample_time_ms: 20108.027\n",
      "    update_time_ms: 6.865\n",
      "  iterations_since_restore: 563\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.57677563945305\n",
      "    rl_1: 105.7048254290062\n",
      "  time_since_restore: 13326.670780658722\n",
      "  time_this_iter_s: 23.20663022994995\n",
      "  time_total_s: 13326.670780658722\n",
      "  timestamp: 1550806755\n",
      "  timesteps_since_restore: 5630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5630000\n",
      "  training_iteration: 563\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13326 s, 563 iter, 5630000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-39-38\n",
      "  done: false\n",
      "  episode_len_mean: 93.32710280373831\n",
      "  episode_reward_max: 219.28830066039342\n",
      "  episode_reward_mean: 182.50967741555488\n",
      "  episode_reward_min: 145.56862695295968\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 58708\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.952\n",
      "    load_time_ms: 2.244\n",
      "    num_steps_sampled: 5640000\n",
      "    num_steps_trained: 5640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.02911369875073433\n",
      "      kl: 0.013696323148906231\n",
      "      policy_loss: -0.0015668153064325452\n",
      "      total_loss: 1.8482329845428467\n",
      "      vf_explained_var: 0.9966816306114197\n",
      "      vf_loss: 1.8497997522354126\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31019437313079834\n",
      "      kl: 0.013230440206825733\n",
      "      policy_loss: -0.0017577415565028787\n",
      "      total_loss: 1.7972229719161987\n",
      "      vf_explained_var: 0.9980596899986267\n",
      "      vf_loss: 1.798980951309204\n",
      "    sample_time_ms: 20041.403\n",
      "    update_time_ms: 6.352\n",
      "  iterations_since_restore: 564\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 77.60358904659022\n",
      "    rl_1: 104.90608836896465\n",
      "  time_since_restore: 13349.68286037445\n",
      "  time_this_iter_s: 23.01207971572876\n",
      "  time_total_s: 13349.68286037445\n",
      "  timestamp: 1550806778\n",
      "  timesteps_since_restore: 5640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5640000\n",
      "  training_iteration: 564\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13349 s, 564 iter, 5640000 ts, 183 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-40-01\n",
      "  done: false\n",
      "  episode_len_mean: 93.25\n",
      "  episode_reward_max: 222.51739039306247\n",
      "  episode_reward_mean: 181.76729031927553\n",
      "  episode_reward_min: -136.2034229449707\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 58816\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.356\n",
      "    load_time_ms: 2.312\n",
      "    num_steps_sampled: 5650000\n",
      "    num_steps_trained: 5650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.05126745626330376\n",
      "      kl: 0.01309160701930523\n",
      "      policy_loss: -0.00029808320687152445\n",
      "      total_loss: 9.730304718017578\n",
      "      vf_explained_var: 0.9857478737831116\n",
      "      vf_loss: 9.73060417175293\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.34244194626808167\n",
      "      kl: 0.01249250303953886\n",
      "      policy_loss: -0.0010692498181015253\n",
      "      total_loss: 9.698461532592773\n",
      "      vf_explained_var: 0.9911843538284302\n",
      "      vf_loss: 9.699530601501465\n",
      "    sample_time_ms: 20027.55\n",
      "    update_time_ms: 6.414\n",
      "  iterations_since_restore: 565\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 77.43998628030987\n",
      "    rl_1: 104.32730403896568\n",
      "  time_since_restore: 13372.661269903183\n",
      "  time_this_iter_s: 22.9784095287323\n",
      "  time_total_s: 13372.661269903183\n",
      "  timestamp: 1550806801\n",
      "  timesteps_since_restore: 5650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5650000\n",
      "  training_iteration: 565\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13372 s, 565 iter, 5650000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-40-24\n",
      "  done: false\n",
      "  episode_len_mean: 95.32692307692308\n",
      "  episode_reward_max: 222.7422255411143\n",
      "  episode_reward_mean: 177.33375356326619\n",
      "  episode_reward_min: 147.73919663469937\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 58920\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.391\n",
      "    load_time_ms: 2.397\n",
      "    num_steps_sampled: 5660000\n",
      "    num_steps_trained: 5660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.02599535509943962\n",
      "      kl: 0.011399181559681892\n",
      "      policy_loss: -0.0007048675324767828\n",
      "      total_loss: 2.46439266204834\n",
      "      vf_explained_var: 0.9951843619346619\n",
      "      vf_loss: 2.465097427368164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.34186771512031555\n",
      "      kl: 0.02032306045293808\n",
      "      policy_loss: -0.00019976783369202167\n",
      "      total_loss: 2.895390748977661\n",
      "      vf_explained_var: 0.9970976710319519\n",
      "      vf_loss: 2.8955905437469482\n",
      "    sample_time_ms: 20118.959\n",
      "    update_time_ms: 6.483\n",
      "  iterations_since_restore: 566\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.79401259920112\n",
      "    rl_1: 105.53974096406503\n",
      "  time_since_restore: 13396.195837974548\n",
      "  time_this_iter_s: 23.534568071365356\n",
      "  time_total_s: 13396.195837974548\n",
      "  timestamp: 1550806824\n",
      "  timesteps_since_restore: 5660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5660000\n",
      "  training_iteration: 566\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13396 s, 566 iter, 5660000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-40-48\n",
      "  done: false\n",
      "  episode_len_mean: 94.69811320754717\n",
      "  episode_reward_max: 226.14014062550925\n",
      "  episode_reward_mean: 176.88750629685896\n",
      "  episode_reward_min: -153.25254985807746\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 59026\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.3\n",
      "    load_time_ms: 2.404\n",
      "    num_steps_sampled: 5670000\n",
      "    num_steps_trained: 5670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.04399356618523598\n",
      "      kl: 0.015137565322220325\n",
      "      policy_loss: -0.0034783673472702503\n",
      "      total_loss: 41.69906997680664\n",
      "      vf_explained_var: 0.9380003213882446\n",
      "      vf_loss: 41.702545166015625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3308113217353821\n",
      "      kl: 0.02610524743795395\n",
      "      policy_loss: -0.0006582823698408902\n",
      "      total_loss: 55.16876220703125\n",
      "      vf_explained_var: 0.9508529305458069\n",
      "      vf_loss: 55.16942596435547\n",
      "    sample_time_ms: 20083.852\n",
      "    update_time_ms: 6.639\n",
      "  iterations_since_restore: 567\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.23539460203973\n",
      "    rl_1: 104.65211169481917\n",
      "  time_since_restore: 13419.685945034027\n",
      "  time_this_iter_s: 23.49010705947876\n",
      "  time_total_s: 13419.685945034027\n",
      "  timestamp: 1550806848\n",
      "  timesteps_since_restore: 5670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5670000\n",
      "  training_iteration: 567\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13419 s, 567 iter, 5670000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-41-12\n",
      "  done: false\n",
      "  episode_len_mean: 92.35185185185185\n",
      "  episode_reward_max: 225.6711509754031\n",
      "  episode_reward_mean: 179.06895338560363\n",
      "  episode_reward_min: -152.5897476527924\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 59134\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.551\n",
      "    load_time_ms: 2.441\n",
      "    num_steps_sampled: 5680000\n",
      "    num_steps_trained: 5680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.04123943671584129\n",
      "      kl: 0.015118768438696861\n",
      "      policy_loss: -0.006233471445739269\n",
      "      total_loss: 23.367103576660156\n",
      "      vf_explained_var: 0.9654198884963989\n",
      "      vf_loss: 23.373336791992188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.36864617466926575\n",
      "      kl: 0.022040661424398422\n",
      "      policy_loss: -0.0005072488565929234\n",
      "      total_loss: 38.72761917114258\n",
      "      vf_explained_var: 0.9624879360198975\n",
      "      vf_loss: 38.728118896484375\n",
      "    sample_time_ms: 20152.277\n",
      "    update_time_ms: 6.622\n",
      "  iterations_since_restore: 568\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.89018603489636\n",
      "    rl_1: 103.17876735070723\n",
      "  time_since_restore: 13443.394041538239\n",
      "  time_this_iter_s: 23.708096504211426\n",
      "  time_total_s: 13443.394041538239\n",
      "  timestamp: 1550806872\n",
      "  timesteps_since_restore: 5680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5680000\n",
      "  training_iteration: 568\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13443 s, 568 iter, 5680000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-41-35\n",
      "  done: false\n",
      "  episode_len_mean: 92.86111111111111\n",
      "  episode_reward_max: 224.7223318001024\n",
      "  episode_reward_mean: 178.8304984915107\n",
      "  episode_reward_min: -148.68706771284928\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 59242\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.506\n",
      "    load_time_ms: 2.454\n",
      "    num_steps_sampled: 5690000\n",
      "    num_steps_trained: 5690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07128749787807465\n",
      "      kl: 0.017088955268263817\n",
      "      policy_loss: -0.00224894005805254\n",
      "      total_loss: 5.200013637542725\n",
      "      vf_explained_var: 0.9915481805801392\n",
      "      vf_loss: 5.202262878417969\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32700952887535095\n",
      "      kl: 0.01594315655529499\n",
      "      policy_loss: -0.0002428148000035435\n",
      "      total_loss: 4.9387946128845215\n",
      "      vf_explained_var: 0.9951075911521912\n",
      "      vf_loss: 4.939037799835205\n",
      "    sample_time_ms: 20136.641\n",
      "    update_time_ms: 6.731\n",
      "  iterations_since_restore: 569\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.12757751822335\n",
      "    rl_1: 102.70292097328736\n",
      "  time_since_restore: 13466.592426538467\n",
      "  time_this_iter_s: 23.198385000228882\n",
      "  time_total_s: 13466.592426538467\n",
      "  timestamp: 1550806895\n",
      "  timesteps_since_restore: 5690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5690000\n",
      "  training_iteration: 569\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13466 s, 569 iter, 5690000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-41-58\n",
      "  done: false\n",
      "  episode_len_mean: 96.08653846153847\n",
      "  episode_reward_max: 224.01393971412676\n",
      "  episode_reward_mean: 175.4821660244137\n",
      "  episode_reward_min: -157.6275353043024\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 59346\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.454\n",
      "    load_time_ms: 2.404\n",
      "    num_steps_sampled: 5700000\n",
      "    num_steps_trained: 5700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06363976001739502\n",
      "      kl: 0.01781683973968029\n",
      "      policy_loss: -0.0034934182185679674\n",
      "      total_loss: 9.487702369689941\n",
      "      vf_explained_var: 0.9852319359779358\n",
      "      vf_loss: 9.491196632385254\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4158762991428375\n",
      "      kl: 0.011919289827346802\n",
      "      policy_loss: -6.131541340437252e-06\n",
      "      total_loss: 9.185869216918945\n",
      "      vf_explained_var: 0.9918451309204102\n",
      "      vf_loss: 9.18587589263916\n",
      "    sample_time_ms: 20117.434\n",
      "    update_time_ms: 7.191\n",
      "  iterations_since_restore: 570\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.87913766709089\n",
      "    rl_1: 104.60302835732276\n",
      "  time_since_restore: 13489.7819378376\n",
      "  time_this_iter_s: 23.1895112991333\n",
      "  time_total_s: 13489.7819378376\n",
      "  timestamp: 1550806918\n",
      "  timesteps_since_restore: 5700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5700000\n",
      "  training_iteration: 570\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13489 s, 570 iter, 5700000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-42-22\n",
      "  done: false\n",
      "  episode_len_mean: 92.3425925925926\n",
      "  episode_reward_max: 221.23218661742146\n",
      "  episode_reward_mean: 177.31902054152872\n",
      "  episode_reward_min: -154.10054133946093\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 59454\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3169.492\n",
      "    load_time_ms: 2.438\n",
      "    num_steps_sampled: 5710000\n",
      "    num_steps_trained: 5710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12459146976470947\n",
      "      kl: 0.023052353411912918\n",
      "      policy_loss: 0.001359240268357098\n",
      "      total_loss: 42.02477264404297\n",
      "      vf_explained_var: 0.9372674226760864\n",
      "      vf_loss: 42.02341079711914\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3683483302593231\n",
      "      kl: 14.759601593017578\n",
      "      policy_loss: 0.03870556876063347\n",
      "      total_loss: 50.50423049926758\n",
      "      vf_explained_var: 0.9530631899833679\n",
      "      vf_loss: 50.46553039550781\n",
      "    sample_time_ms: 20151.913\n",
      "    update_time_ms: 7.297\n",
      "  iterations_since_restore: 571\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.6436037842106\n",
      "    rl_1: 102.67541675731806\n",
      "  time_since_restore: 13513.523369312286\n",
      "  time_this_iter_s: 23.74143147468567\n",
      "  time_total_s: 13513.523369312286\n",
      "  timestamp: 1550806942\n",
      "  timesteps_since_restore: 5710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5710000\n",
      "  training_iteration: 571\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13513 s, 571 iter, 5710000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-42-45\n",
      "  done: false\n",
      "  episode_len_mean: 92.72222222222223\n",
      "  episode_reward_max: 218.6832522397042\n",
      "  episode_reward_mean: 174.38111104889882\n",
      "  episode_reward_min: -94.60769916546505\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 59562\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.353\n",
      "    load_time_ms: 2.447\n",
      "    num_steps_sampled: 5720000\n",
      "    num_steps_trained: 5720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.01663387566804886\n",
      "      kl: 0.015642507001757622\n",
      "      policy_loss: -0.00018705088587012142\n",
      "      total_loss: 6.94398832321167\n",
      "      vf_explained_var: 0.9888148307800293\n",
      "      vf_loss: 6.944175720214844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2874579429626465\n",
      "      kl: 0.01443906594067812\n",
      "      policy_loss: -0.002913131145760417\n",
      "      total_loss: 8.50667667388916\n",
      "      vf_explained_var: 0.9919462203979492\n",
      "      vf_loss: 8.509591102600098\n",
      "    sample_time_ms: 20142.65\n",
      "    update_time_ms: 7.596\n",
      "  iterations_since_restore: 572\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.87016538451878\n",
      "    rl_1: 101.51094566438003\n",
      "  time_since_restore: 13536.648519992828\n",
      "  time_this_iter_s: 23.125150680541992\n",
      "  time_total_s: 13536.648519992828\n",
      "  timestamp: 1550806965\n",
      "  timesteps_since_restore: 5720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5720000\n",
      "  training_iteration: 572\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13536 s, 572 iter, 5720000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-43-09\n",
      "  done: false\n",
      "  episode_len_mean: 97.43137254901961\n",
      "  episode_reward_max: 222.42133061720924\n",
      "  episode_reward_mean: 171.3105800987979\n",
      "  episode_reward_min: -155.5545859212862\n",
      "  episodes_this_iter: 102\n",
      "  episodes_total: 59664\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.639\n",
      "    load_time_ms: 2.431\n",
      "    num_steps_sampled: 5730000\n",
      "    num_steps_trained: 5730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.03144891932606697\n",
      "      kl: 0.015976261347532272\n",
      "      policy_loss: -0.0052319965325295925\n",
      "      total_loss: 17.80751609802246\n",
      "      vf_explained_var: 0.9824157953262329\n",
      "      vf_loss: 17.8127498626709\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2486189305782318\n",
      "      kl: 0.012732569128274918\n",
      "      policy_loss: -0.0009109364473260939\n",
      "      total_loss: 17.284448623657227\n",
      "      vf_explained_var: 0.9878193736076355\n",
      "      vf_loss: 17.285356521606445\n",
      "    sample_time_ms: 20178.458\n",
      "    update_time_ms: 7.649\n",
      "  iterations_since_restore: 573\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.15392425288675\n",
      "    rl_1: 99.15665584591115\n",
      "  time_since_restore: 13560.202542304993\n",
      "  time_this_iter_s: 23.554022312164307\n",
      "  time_total_s: 13560.202542304993\n",
      "  timestamp: 1550806989\n",
      "  timesteps_since_restore: 5730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5730000\n",
      "  training_iteration: 573\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13560 s, 573 iter, 5730000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-43-32\n",
      "  done: false\n",
      "  episode_len_mean: 93.14814814814815\n",
      "  episode_reward_max: 219.89920717383282\n",
      "  episode_reward_mean: 175.2836571811475\n",
      "  episode_reward_min: -155.53612971566065\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 59772\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.131\n",
      "    load_time_ms: 2.498\n",
      "    num_steps_sampled: 5740000\n",
      "    num_steps_trained: 5740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07591681182384491\n",
      "      kl: 0.014275590889155865\n",
      "      policy_loss: -0.002522221766412258\n",
      "      total_loss: 18.75278663635254\n",
      "      vf_explained_var: 0.9725877642631531\n",
      "      vf_loss: 18.755308151245117\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3749609589576721\n",
      "      kl: 0.031192034482955933\n",
      "      policy_loss: -0.003680469933897257\n",
      "      total_loss: 25.74938201904297\n",
      "      vf_explained_var: 0.9783552885055542\n",
      "      vf_loss: 25.75306510925293\n",
      "    sample_time_ms: 20214.626\n",
      "    update_time_ms: 7.547\n",
      "  iterations_since_restore: 574\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.4039293697913\n",
      "    rl_1: 102.87972781135622\n",
      "  time_since_restore: 13583.563322782516\n",
      "  time_this_iter_s: 23.360780477523804\n",
      "  time_total_s: 13583.563322782516\n",
      "  timestamp: 1550807012\n",
      "  timesteps_since_restore: 5740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5740000\n",
      "  training_iteration: 574\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13583 s, 574 iter, 5740000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-43-56\n",
      "  done: false\n",
      "  episode_len_mean: 92.63888888888889\n",
      "  episode_reward_max: 221.29599655242956\n",
      "  episode_reward_mean: 175.8987769629947\n",
      "  episode_reward_min: -153.24465291500084\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 59880\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.58\n",
      "    load_time_ms: 2.439\n",
      "    num_steps_sampled: 5750000\n",
      "    num_steps_trained: 5750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09735173732042313\n",
      "      kl: 0.01065945066511631\n",
      "      policy_loss: -0.0020944576244801283\n",
      "      total_loss: 22.533363342285156\n",
      "      vf_explained_var: 0.9634032845497131\n",
      "      vf_loss: 22.535457611083984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3972315788269043\n",
      "      kl: 0.019093262031674385\n",
      "      policy_loss: -0.001463641063310206\n",
      "      total_loss: 37.413516998291016\n",
      "      vf_explained_var: 0.9646517634391785\n",
      "      vf_loss: 37.414974212646484\n",
      "    sample_time_ms: 20282.719\n",
      "    update_time_ms: 7.677\n",
      "  iterations_since_restore: 575\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.01474992867382\n",
      "    rl_1: 103.88402703432087\n",
      "  time_since_restore: 13607.19703745842\n",
      "  time_this_iter_s: 23.63371467590332\n",
      "  time_total_s: 13607.19703745842\n",
      "  timestamp: 1550807036\n",
      "  timesteps_since_restore: 5750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5750000\n",
      "  training_iteration: 575\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13607 s, 575 iter, 5750000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-44-19\n",
      "  done: false\n",
      "  episode_len_mean: 92.5229357798165\n",
      "  episode_reward_max: 220.42169927743538\n",
      "  episode_reward_mean: 180.96204118395923\n",
      "  episode_reward_min: 140.18864627846796\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 59989\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.159\n",
      "    load_time_ms: 2.36\n",
      "    num_steps_sampled: 5760000\n",
      "    num_steps_trained: 5760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10197040438652039\n",
      "      kl: 0.022352317348122597\n",
      "      policy_loss: -0.0022447986993938684\n",
      "      total_loss: 2.5916190147399902\n",
      "      vf_explained_var: 0.9950029253959656\n",
      "      vf_loss: 2.5938639640808105\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32407447695732117\n",
      "      kl: 8.650205612182617\n",
      "      policy_loss: 0.03212883323431015\n",
      "      total_loss: 2.2816457748413086\n",
      "      vf_explained_var: 0.997552216053009\n",
      "      vf_loss: 2.2495169639587402\n",
      "    sample_time_ms: 20223.395\n",
      "    update_time_ms: 7.541\n",
      "  iterations_since_restore: 576\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.20232825918403\n",
      "    rl_1: 104.75971292477524\n",
      "  time_since_restore: 13630.14619922638\n",
      "  time_this_iter_s: 22.949161767959595\n",
      "  time_total_s: 13630.14619922638\n",
      "  timestamp: 1550807059\n",
      "  timesteps_since_restore: 5760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5760000\n",
      "  training_iteration: 576\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13630 s, 576 iter, 5760000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-44-42\n",
      "  done: false\n",
      "  episode_len_mean: 93.67924528301887\n",
      "  episode_reward_max: 221.56652292012743\n",
      "  episode_reward_mean: 177.44169078635946\n",
      "  episode_reward_min: -147.8927887196803\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 60095\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.497\n",
      "    load_time_ms: 2.363\n",
      "    num_steps_sampled: 5770000\n",
      "    num_steps_trained: 5770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09369045495986938\n",
      "      kl: 0.045947447419166565\n",
      "      policy_loss: 0.001152588753029704\n",
      "      total_loss: 37.83817672729492\n",
      "      vf_explained_var: 0.9387150406837463\n",
      "      vf_loss: 37.83702850341797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3282468020915985\n",
      "      kl: 0.19774244725704193\n",
      "      policy_loss: 7.532361632911488e-05\n",
      "      total_loss: 45.057159423828125\n",
      "      vf_explained_var: 0.9566256403923035\n",
      "      vf_loss: 45.05708312988281\n",
      "    sample_time_ms: 20222.475\n",
      "    update_time_ms: 7.392\n",
      "  iterations_since_restore: 577\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.55956868707737\n",
      "    rl_1: 102.88212209928207\n",
      "  time_since_restore: 13653.678093910217\n",
      "  time_this_iter_s: 23.53189468383789\n",
      "  time_total_s: 13653.678093910217\n",
      "  timestamp: 1550807082\n",
      "  timesteps_since_restore: 5770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5770000\n",
      "  training_iteration: 577\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13653 s, 577 iter, 5770000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-45-06\n",
      "  done: false\n",
      "  episode_len_mean: 92.71296296296296\n",
      "  episode_reward_max: 222.99190099610692\n",
      "  episode_reward_mean: 168.71010566230038\n",
      "  episode_reward_min: -153.45953335058599\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 60203\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.74\n",
      "    load_time_ms: 2.308\n",
      "    num_steps_sampled: 5780000\n",
      "    num_steps_trained: 5780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07713907957077026\n",
      "      kl: 0.022638333961367607\n",
      "      policy_loss: -0.003269571578130126\n",
      "      total_loss: 110.17293548583984\n",
      "      vf_explained_var: 0.846673309803009\n",
      "      vf_loss: 110.17620849609375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25945958495140076\n",
      "      kl: 0.028559012338519096\n",
      "      policy_loss: 0.00509497057646513\n",
      "      total_loss: 154.37600708007812\n",
      "      vf_explained_var: 0.8543329238891602\n",
      "      vf_loss: 154.37091064453125\n",
      "    sample_time_ms: 20196.263\n",
      "    update_time_ms: 7.572\n",
      "  iterations_since_restore: 578\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.30701212266979\n",
      "    rl_1: 97.40309353963058\n",
      "  time_since_restore: 13677.097378969193\n",
      "  time_this_iter_s: 23.41928505897522\n",
      "  time_total_s: 13677.097378969193\n",
      "  timestamp: 1550807106\n",
      "  timesteps_since_restore: 5780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5780000\n",
      "  training_iteration: 578\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13677 s, 578 iter, 5780000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-45-29\n",
      "  done: false\n",
      "  episode_len_mean: 92.30555555555556\n",
      "  episode_reward_max: 224.6605592496329\n",
      "  episode_reward_mean: 162.30441246458304\n",
      "  episode_reward_min: -158.58783535382062\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 60311\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.155\n",
      "    load_time_ms: 2.327\n",
      "    num_steps_sampled: 5790000\n",
      "    num_steps_trained: 5790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.03651026636362076\n",
      "      kl: 0.023596426472067833\n",
      "      policy_loss: -0.003041053656488657\n",
      "      total_loss: 145.32186889648438\n",
      "      vf_explained_var: 0.7925812005996704\n",
      "      vf_loss: 145.32492065429688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2650315463542938\n",
      "      kl: 0.017896069213747978\n",
      "      policy_loss: -0.0016238819807767868\n",
      "      total_loss: 194.47970581054688\n",
      "      vf_explained_var: 0.819246232509613\n",
      "      vf_loss: 194.48133850097656\n",
      "    sample_time_ms: 20162.214\n",
      "    update_time_ms: 8.02\n",
      "  iterations_since_restore: 579\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.6904819299332\n",
      "    rl_1: 94.61393053464984\n",
      "  time_since_restore: 13699.874195814133\n",
      "  time_this_iter_s: 22.776816844940186\n",
      "  time_total_s: 13699.874195814133\n",
      "  timestamp: 1550807129\n",
      "  timesteps_since_restore: 5790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5790000\n",
      "  training_iteration: 579\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13699 s, 579 iter, 5790000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-45-52\n",
      "  done: false\n",
      "  episode_len_mean: 91.44954128440367\n",
      "  episode_reward_max: 221.1320585541779\n",
      "  episode_reward_mean: 144.18186906584975\n",
      "  episode_reward_min: -159.21936536168775\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 60420\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3152.098\n",
      "    load_time_ms: 2.356\n",
      "    num_steps_sampled: 5800000\n",
      "    num_steps_trained: 5800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07664763927459717\n",
      "      kl: 0.017159758135676384\n",
      "      policy_loss: -0.0007446068921126425\n",
      "      total_loss: 294.3515930175781\n",
      "      vf_explained_var: 0.7026011347770691\n",
      "      vf_loss: 294.3523254394531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2706693410873413\n",
      "      kl: 0.016683122143149376\n",
      "      policy_loss: 1.160762258223258e-05\n",
      "      total_loss: 387.27264404296875\n",
      "      vf_explained_var: 0.699434757232666\n",
      "      vf_loss: 387.27264404296875\n",
      "    sample_time_ms: 20166.132\n",
      "    update_time_ms: 7.594\n",
      "  iterations_since_restore: 580\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.45880642063883\n",
      "    rl_1: 85.72306264521092\n",
      "  time_since_restore: 13723.24585723877\n",
      "  time_this_iter_s: 23.37166142463684\n",
      "  time_total_s: 13723.24585723877\n",
      "  timestamp: 1550807152\n",
      "  timesteps_since_restore: 5800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5800000\n",
      "  training_iteration: 580\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13723 s, 580 iter, 5800000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-46-15\n",
      "  done: false\n",
      "  episode_len_mean: 93.44444444444444\n",
      "  episode_reward_max: 220.538221599002\n",
      "  episode_reward_mean: 153.5669760327168\n",
      "  episode_reward_min: -159.00900194880032\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 60528\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.147\n",
      "    load_time_ms: 2.316\n",
      "    num_steps_sampled: 5810000\n",
      "    num_steps_trained: 5810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.022485442459583282\n",
      "      kl: 0.01952441595494747\n",
      "      policy_loss: -0.0007572670583613217\n",
      "      total_loss: 174.8017120361328\n",
      "      vf_explained_var: 0.7931010723114014\n",
      "      vf_loss: 174.80247497558594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2274906486272812\n",
      "      kl: 0.03154689818620682\n",
      "      policy_loss: 0.006636431440711021\n",
      "      total_loss: 211.951171875\n",
      "      vf_explained_var: 0.8198418021202087\n",
      "      vf_loss: 211.9445037841797\n",
      "    sample_time_ms: 20101.713\n",
      "    update_time_ms: 7.619\n",
      "  iterations_since_restore: 581\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.82365679286081\n",
      "    rl_1: 90.74331923985595\n",
      "  time_since_restore: 13746.104092359543\n",
      "  time_this_iter_s: 22.858235120773315\n",
      "  time_total_s: 13746.104092359543\n",
      "  timestamp: 1550807175\n",
      "  timesteps_since_restore: 5810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5810000\n",
      "  training_iteration: 581\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13746 s, 581 iter, 5810000 ts, 154 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-46-38\n",
      "  done: false\n",
      "  episode_len_mean: 93.0754716981132\n",
      "  episode_reward_max: 217.99306642142272\n",
      "  episode_reward_mean: 166.1851611358487\n",
      "  episode_reward_min: -161.7364746694825\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 60634\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.254\n",
      "    load_time_ms: 2.357\n",
      "    num_steps_sampled: 5820000\n",
      "    num_steps_trained: 5820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.05982045456767082\n",
      "      kl: 0.024822017177939415\n",
      "      policy_loss: -0.001925820717588067\n",
      "      total_loss: 128.47335815429688\n",
      "      vf_explained_var: 0.8062176704406738\n",
      "      vf_loss: 128.47528076171875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3048683702945709\n",
      "      kl: 0.03394778445363045\n",
      "      policy_loss: 0.0029003454837948084\n",
      "      total_loss: 162.40673828125\n",
      "      vf_explained_var: 0.8359872698783875\n",
      "      vf_loss: 162.40383911132812\n",
      "    sample_time_ms: 20053.923\n",
      "    update_time_ms: 7.362\n",
      "  iterations_since_restore: 582\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.20657619435107\n",
      "    rl_1: 97.9785849414976\n",
      "  time_since_restore: 13768.792485237122\n",
      "  time_this_iter_s: 22.688392877578735\n",
      "  time_total_s: 13768.792485237122\n",
      "  timestamp: 1550807198\n",
      "  timesteps_since_restore: 5820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5820000\n",
      "  training_iteration: 582\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13768 s, 582 iter, 5820000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-47-01\n",
      "  done: false\n",
      "  episode_len_mean: 94.83018867924528\n",
      "  episode_reward_max: 221.13329585927548\n",
      "  episode_reward_mean: 163.3849908469359\n",
      "  episode_reward_min: -143.2621790918672\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 60740\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.585\n",
      "    load_time_ms: 2.355\n",
      "    num_steps_sampled: 5830000\n",
      "    num_steps_trained: 5830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.02539851702749729\n",
      "      kl: 0.013438941910862923\n",
      "      policy_loss: -0.005380893591791391\n",
      "      total_loss: 132.42929077148438\n",
      "      vf_explained_var: 0.8170448541641235\n",
      "      vf_loss: 132.43467712402344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22767814993858337\n",
      "      kl: 0.019790329039096832\n",
      "      policy_loss: 0.0019551182631403208\n",
      "      total_loss: 156.31394958496094\n",
      "      vf_explained_var: 0.8522903323173523\n",
      "      vf_loss: 156.31199645996094\n",
      "    sample_time_ms: 20018.229\n",
      "    update_time_ms: 7.576\n",
      "  iterations_since_restore: 583\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.93395116175297\n",
      "    rl_1: 96.45103968518295\n",
      "  time_since_restore: 13791.987337827682\n",
      "  time_this_iter_s: 23.194852590560913\n",
      "  time_total_s: 13791.987337827682\n",
      "  timestamp: 1550807221\n",
      "  timesteps_since_restore: 5830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5830000\n",
      "  training_iteration: 583\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13791 s, 583 iter, 5830000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-47-24\n",
      "  done: false\n",
      "  episode_len_mean: 91.1891891891892\n",
      "  episode_reward_max: 225.12504587618446\n",
      "  episode_reward_mean: 152.00668886473454\n",
      "  episode_reward_min: -155.44004512185222\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 60851\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.352\n",
      "    load_time_ms: 2.272\n",
      "    num_steps_sampled: 5840000\n",
      "    num_steps_trained: 5840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07161351293325424\n",
      "      kl: 0.01833595149219036\n",
      "      policy_loss: -0.0031832156237214804\n",
      "      total_loss: 222.76573181152344\n",
      "      vf_explained_var: 0.7361643314361572\n",
      "      vf_loss: 222.76895141601562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2892419993877411\n",
      "      kl: 0.01426111999899149\n",
      "      policy_loss: -0.0015729254810139537\n",
      "      total_loss: 272.5569152832031\n",
      "      vf_explained_var: 0.7719579935073853\n",
      "      vf_loss: 272.55853271484375\n",
      "    sample_time_ms: 19985.536\n",
      "    update_time_ms: 7.518\n",
      "  iterations_since_restore: 584\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.69752073593844\n",
      "    rl_1: 89.30916812879612\n",
      "  time_since_restore: 13814.993225336075\n",
      "  time_this_iter_s: 23.005887508392334\n",
      "  time_total_s: 13814.993225336075\n",
      "  timestamp: 1550807244\n",
      "  timesteps_since_restore: 5840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5840000\n",
      "  training_iteration: 584\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13814 s, 584 iter, 5840000 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-47-47\n",
      "  done: false\n",
      "  episode_len_mean: 90.1891891891892\n",
      "  episode_reward_max: 224.88768520841566\n",
      "  episode_reward_mean: 141.65608534083515\n",
      "  episode_reward_min: -159.2481724993712\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 60962\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.968\n",
      "    load_time_ms: 2.265\n",
      "    num_steps_sampled: 5850000\n",
      "    num_steps_trained: 5850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.03229796886444092\n",
      "      kl: 0.01633453741669655\n",
      "      policy_loss: -0.0035540079697966576\n",
      "      total_loss: 262.3460388183594\n",
      "      vf_explained_var: 0.7399086952209473\n",
      "      vf_loss: 262.34954833984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2578312158584595\n",
      "      kl: 0.02853347174823284\n",
      "      policy_loss: 0.007085426710546017\n",
      "      total_loss: 333.9808044433594\n",
      "      vf_explained_var: 0.7505870461463928\n",
      "      vf_loss: 333.9737243652344\n",
      "    sample_time_ms: 19915.717\n",
      "    update_time_ms: 7.458\n",
      "  iterations_since_restore: 585\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.268389497305186\n",
      "    rl_1: 84.38769584352991\n",
      "  time_since_restore: 13837.924634218216\n",
      "  time_this_iter_s: 22.931408882141113\n",
      "  time_total_s: 13837.924634218216\n",
      "  timestamp: 1550807267\n",
      "  timesteps_since_restore: 5850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5850000\n",
      "  training_iteration: 585\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13837 s, 585 iter, 5850000 ts, 142 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-48-10\n",
      "  done: false\n",
      "  episode_len_mean: 89.82727272727273\n",
      "  episode_reward_max: 225.90409302415225\n",
      "  episode_reward_mean: 140.30836369354466\n",
      "  episode_reward_min: -162.36589242385006\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 61072\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.481\n",
      "    load_time_ms: 2.252\n",
      "    num_steps_sampled: 5860000\n",
      "    num_steps_trained: 5860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.03715099021792412\n",
      "      kl: 0.014896678738296032\n",
      "      policy_loss: -0.0015631562564522028\n",
      "      total_loss: 310.4560852050781\n",
      "      vf_explained_var: 0.7182536125183105\n",
      "      vf_loss: 310.4576110839844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23978175222873688\n",
      "      kl: 0.011276556178927422\n",
      "      policy_loss: -0.0015740141971036792\n",
      "      total_loss: 391.85064697265625\n",
      "      vf_explained_var: 0.7093594670295715\n",
      "      vf_loss: 391.8522033691406\n",
      "    sample_time_ms: 19950.555\n",
      "    update_time_ms: 7.502\n",
      "  iterations_since_restore: 586\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.79120769367076\n",
      "    rl_1: 82.51715599987395\n",
      "  time_since_restore: 13861.211282730103\n",
      "  time_this_iter_s: 23.286648511886597\n",
      "  time_total_s: 13861.211282730103\n",
      "  timestamp: 1550807290\n",
      "  timesteps_since_restore: 5860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5860000\n",
      "  training_iteration: 586\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13861 s, 586 iter, 5860000 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-48-34\n",
      "  done: false\n",
      "  episode_len_mean: 87.66666666666667\n",
      "  episode_reward_max: 224.12524533121527\n",
      "  episode_reward_mean: 124.88985943060072\n",
      "  episode_reward_min: -168.3947103982246\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 61186\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.378\n",
      "    load_time_ms: 2.233\n",
      "    num_steps_sampled: 5870000\n",
      "    num_steps_trained: 5870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08188828825950623\n",
      "      kl: 0.018936021253466606\n",
      "      policy_loss: -0.0015713740140199661\n",
      "      total_loss: 302.9602966308594\n",
      "      vf_explained_var: 0.7739728093147278\n",
      "      vf_loss: 302.96185302734375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2721274495124817\n",
      "      kl: 0.013281933963298798\n",
      "      policy_loss: 0.001490107853896916\n",
      "      total_loss: 395.75189208984375\n",
      "      vf_explained_var: 0.745559573173523\n",
      "      vf_loss: 395.7503662109375\n",
      "    sample_time_ms: 19980.167\n",
      "    update_time_ms: 7.412\n",
      "  iterations_since_restore: 587\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.00404152136989\n",
      "    rl_1: 74.88581790923084\n",
      "  time_since_restore: 13885.006017923355\n",
      "  time_this_iter_s: 23.794735193252563\n",
      "  time_total_s: 13885.006017923355\n",
      "  timestamp: 1550807314\n",
      "  timesteps_since_restore: 5870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5870000\n",
      "  training_iteration: 587\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13885 s, 587 iter, 5870000 ts, 125 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-48-58\n",
      "  done: false\n",
      "  episode_len_mean: 90.36036036036036\n",
      "  episode_reward_max: 221.7389478132002\n",
      "  episode_reward_mean: 144.1427638388858\n",
      "  episode_reward_min: -162.20998553437352\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 61297\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.47\n",
      "    load_time_ms: 2.242\n",
      "    num_steps_sampled: 5880000\n",
      "    num_steps_trained: 5880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08424363285303116\n",
      "      kl: 0.009677626192569733\n",
      "      policy_loss: -0.0017716250149533153\n",
      "      total_loss: 240.8863983154297\n",
      "      vf_explained_var: 0.7847657203674316\n",
      "      vf_loss: 240.88815307617188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21500712633132935\n",
      "      kl: 0.012688000686466694\n",
      "      policy_loss: 0.0015107918297871947\n",
      "      total_loss: 296.56329345703125\n",
      "      vf_explained_var: 0.7718247771263123\n",
      "      vf_loss: 296.561767578125\n",
      "    sample_time_ms: 20006.146\n",
      "    update_time_ms: 7.322\n",
      "  iterations_since_restore: 588\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.88408232172693\n",
      "    rl_1: 83.2586815171589\n",
      "  time_since_restore: 13908.673022508621\n",
      "  time_this_iter_s: 23.667004585266113\n",
      "  time_total_s: 13908.673022508621\n",
      "  timestamp: 1550807338\n",
      "  timesteps_since_restore: 5880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5880000\n",
      "  training_iteration: 588\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13908 s, 588 iter, 5880000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-49-22\n",
      "  done: false\n",
      "  episode_len_mean: 90.30630630630631\n",
      "  episode_reward_max: 222.97390345387814\n",
      "  episode_reward_mean: 150.4973565605184\n",
      "  episode_reward_min: -158.28674523681906\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 61408\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.02\n",
      "    load_time_ms: 2.324\n",
      "    num_steps_sampled: 5890000\n",
      "    num_steps_trained: 5890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.04620549455285072\n",
      "      kl: 0.011759386397898197\n",
      "      policy_loss: -0.0005041863187216222\n",
      "      total_loss: 218.73866271972656\n",
      "      vf_explained_var: 0.7909049391746521\n",
      "      vf_loss: 218.73915100097656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.213134303689003\n",
      "      kl: 0.01710142381489277\n",
      "      policy_loss: 0.005026749800890684\n",
      "      total_loss: 267.548583984375\n",
      "      vf_explained_var: 0.7854219675064087\n",
      "      vf_loss: 267.54351806640625\n",
      "    sample_time_ms: 20080.687\n",
      "    update_time_ms: 6.929\n",
      "  iterations_since_restore: 589\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.440530243852685\n",
      "    rl_1: 88.05682631666569\n",
      "  time_since_restore: 13932.388165473938\n",
      "  time_this_iter_s: 23.715142965316772\n",
      "  time_total_s: 13932.388165473938\n",
      "  timestamp: 1550807362\n",
      "  timesteps_since_restore: 5890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5890000\n",
      "  training_iteration: 589\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13932 s, 589 iter, 5890000 ts, 150 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-49-45\n",
      "  done: false\n",
      "  episode_len_mean: 91.31192660550458\n",
      "  episode_reward_max: 221.28527641887183\n",
      "  episode_reward_mean: 163.13907935003252\n",
      "  episode_reward_min: -164.02893392293822\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 61517\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.378\n",
      "    load_time_ms: 2.317\n",
      "    num_steps_sampled: 5900000\n",
      "    num_steps_trained: 5900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07809716463088989\n",
      "      kl: 0.014943043701350689\n",
      "      policy_loss: -0.0020793976727873087\n",
      "      total_loss: 172.74026489257812\n",
      "      vf_explained_var: 0.7961028218269348\n",
      "      vf_loss: 172.7423553466797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.24242356419563293\n",
      "      kl: 0.015498449094593525\n",
      "      policy_loss: -0.0008307775715366006\n",
      "      total_loss: 226.08848571777344\n",
      "      vf_explained_var: 0.7953284382820129\n",
      "      vf_loss: 226.08932495117188\n",
      "    sample_time_ms: 20094.012\n",
      "    update_time_ms: 6.788\n",
      "  iterations_since_restore: 590\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.52239582388945\n",
      "    rl_1: 95.61668352614308\n",
      "  time_since_restore: 13955.72567820549\n",
      "  time_this_iter_s: 23.337512731552124\n",
      "  time_total_s: 13955.72567820549\n",
      "  timestamp: 1550807385\n",
      "  timesteps_since_restore: 5900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5900000\n",
      "  training_iteration: 590\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13955 s, 590 iter, 5900000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-50-08\n",
      "  done: false\n",
      "  episode_len_mean: 93.71296296296296\n",
      "  episode_reward_max: 223.47118716206188\n",
      "  episode_reward_mean: 141.06283157014693\n",
      "  episode_reward_min: -156.60500618938738\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 61625\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.624\n",
      "    load_time_ms: 2.36\n",
      "    num_steps_sampled: 5910000\n",
      "    num_steps_trained: 5910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.013466138392686844\n",
      "      kl: 0.016386184841394424\n",
      "      policy_loss: -0.0012566009536385536\n",
      "      total_loss: 246.88877868652344\n",
      "      vf_explained_var: 0.7771725058555603\n",
      "      vf_loss: 246.89002990722656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23663292825222015\n",
      "      kl: 0.01193259283900261\n",
      "      policy_loss: 0.001812245580367744\n",
      "      total_loss: 291.4731140136719\n",
      "      vf_explained_var: 0.7703853249549866\n",
      "      vf_loss: 291.4712829589844\n",
      "    sample_time_ms: 20130.237\n",
      "    update_time_ms: 6.632\n",
      "  iterations_since_restore: 591\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.74953210667367\n",
      "    rl_1: 83.31329946347329\n",
      "  time_since_restore: 13978.977857112885\n",
      "  time_this_iter_s: 23.25217890739441\n",
      "  time_total_s: 13978.977857112885\n",
      "  timestamp: 1550807408\n",
      "  timesteps_since_restore: 5910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5910000\n",
      "  training_iteration: 591\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 13978 s, 591 iter, 5910000 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-50-32\n",
      "  done: false\n",
      "  episode_len_mean: 92.75700934579439\n",
      "  episode_reward_max: 223.04769001915108\n",
      "  episode_reward_mean: 161.22454578676715\n",
      "  episode_reward_min: -161.07324813785257\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 61732\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3123.942\n",
      "    load_time_ms: 2.327\n",
      "    num_steps_sampled: 5920000\n",
      "    num_steps_trained: 5920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.040603481233119965\n",
      "      kl: 0.011965406127274036\n",
      "      policy_loss: -0.0024533099494874477\n",
      "      total_loss: 152.41891479492188\n",
      "      vf_explained_var: 0.8080030083656311\n",
      "      vf_loss: 152.42140197753906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2823932468891144\n",
      "      kl: 0.017284957692027092\n",
      "      policy_loss: 0.0006399693666025996\n",
      "      total_loss: 199.7928466796875\n",
      "      vf_explained_var: 0.7901586890220642\n",
      "      vf_loss: 199.7921905517578\n",
      "    sample_time_ms: 20200.448\n",
      "    update_time_ms: 6.744\n",
      "  iterations_since_restore: 592\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.80337529789398\n",
      "    rl_1: 94.42117048887317\n",
      "  time_since_restore: 14002.300878763199\n",
      "  time_this_iter_s: 23.32302165031433\n",
      "  time_total_s: 14002.300878763199\n",
      "  timestamp: 1550807432\n",
      "  timesteps_since_restore: 5920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5920000\n",
      "  training_iteration: 592\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14002 s, 592 iter, 5920000 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-50-55\n",
      "  done: false\n",
      "  episode_len_mean: 92.83177570093459\n",
      "  episode_reward_max: 223.4402155567061\n",
      "  episode_reward_mean: 163.24794849212307\n",
      "  episode_reward_min: -161.48771356782345\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 61839\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.317\n",
      "    load_time_ms: 2.396\n",
      "    num_steps_sampled: 5930000\n",
      "    num_steps_trained: 5930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06987428665161133\n",
      "      kl: 0.01386716403067112\n",
      "      policy_loss: -0.00043817213736474514\n",
      "      total_loss: 151.1372833251953\n",
      "      vf_explained_var: 0.8130232691764832\n",
      "      vf_loss: 151.13772583007812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.27307918667793274\n",
      "      kl: 0.017607418820261955\n",
      "      policy_loss: 0.0005010248278267682\n",
      "      total_loss: 182.07020568847656\n",
      "      vf_explained_var: 0.8333540558815002\n",
      "      vf_loss: 182.06971740722656\n",
      "    sample_time_ms: 20212.941\n",
      "    update_time_ms: 6.849\n",
      "  iterations_since_restore: 593\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.3112936454577\n",
      "    rl_1: 94.93665484666539\n",
      "  time_since_restore: 14025.646448373795\n",
      "  time_this_iter_s: 23.345569610595703\n",
      "  time_total_s: 14025.646448373795\n",
      "  timestamp: 1550807455\n",
      "  timesteps_since_restore: 5930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5930000\n",
      "  training_iteration: 593\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14025 s, 593 iter, 5930000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-51-19\n",
      "  done: false\n",
      "  episode_len_mean: 92.34862385321101\n",
      "  episode_reward_max: 224.52798038683946\n",
      "  episode_reward_mean: 155.5522367541228\n",
      "  episode_reward_min: -159.66704090525684\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 61948\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.265\n",
      "    load_time_ms: 2.372\n",
      "    num_steps_sampled: 5940000\n",
      "    num_steps_trained: 5940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08924904465675354\n",
      "      kl: 0.01919489912688732\n",
      "      policy_loss: -0.0007619612151756883\n",
      "      total_loss: 169.31167602539062\n",
      "      vf_explained_var: 0.8093323111534119\n",
      "      vf_loss: 169.3124542236328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3222339451313019\n",
      "      kl: 0.014024618081748486\n",
      "      policy_loss: 0.0023786064703017473\n",
      "      total_loss: 220.6747589111328\n",
      "      vf_explained_var: 0.8004536032676697\n",
      "      vf_loss: 220.67237854003906\n",
      "    sample_time_ms: 20252.562\n",
      "    update_time_ms: 7.128\n",
      "  iterations_since_restore: 594\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.69341722759319\n",
      "    rl_1: 91.85881952652964\n",
      "  time_since_restore: 14049.059783697128\n",
      "  time_this_iter_s: 23.41333532333374\n",
      "  time_total_s: 14049.059783697128\n",
      "  timestamp: 1550807479\n",
      "  timesteps_since_restore: 5940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5940000\n",
      "  training_iteration: 594\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14049 s, 594 iter, 5940000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-51-42\n",
      "  done: false\n",
      "  episode_len_mean: 92.03669724770643\n",
      "  episode_reward_max: 220.452450690874\n",
      "  episode_reward_mean: 148.9573740026162\n",
      "  episode_reward_min: -161.9398654777641\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 62057\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.476\n",
      "    load_time_ms: 2.372\n",
      "    num_steps_sampled: 5950000\n",
      "    num_steps_trained: 5950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.012886254116892815\n",
      "      kl: 0.017995843663811684\n",
      "      policy_loss: -0.0025999583303928375\n",
      "      total_loss: 218.45242309570312\n",
      "      vf_explained_var: 0.7734289765357971\n",
      "      vf_loss: 218.45504760742188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.29102036356925964\n",
      "      kl: 0.026343749836087227\n",
      "      policy_loss: 0.005565221421420574\n",
      "      total_loss: 280.6105651855469\n",
      "      vf_explained_var: 0.7531306743621826\n",
      "      vf_loss: 280.6050109863281\n",
      "    sample_time_ms: 20278.311\n",
      "    update_time_ms: 6.922\n",
      "  iterations_since_restore: 595\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.79238135382015\n",
      "    rl_1: 88.164992648796\n",
      "  time_since_restore: 14072.24753189087\n",
      "  time_this_iter_s: 23.187748193740845\n",
      "  time_total_s: 14072.24753189087\n",
      "  timestamp: 1550807502\n",
      "  timesteps_since_restore: 5950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5950000\n",
      "  training_iteration: 595\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14072 s, 595 iter, 5950000 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-52-06\n",
      "  done: false\n",
      "  episode_len_mean: 92.16666666666667\n",
      "  episode_reward_max: 224.48166742255773\n",
      "  episode_reward_mean: 158.53021128433204\n",
      "  episode_reward_min: -156.92766427508425\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 62165\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.174\n",
      "    load_time_ms: 2.373\n",
      "    num_steps_sampled: 5960000\n",
      "    num_steps_trained: 5960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.03762657940387726\n",
      "      kl: 0.0353432297706604\n",
      "      policy_loss: 0.0028731476049870253\n",
      "      total_loss: 198.629150390625\n",
      "      vf_explained_var: 0.7814878225326538\n",
      "      vf_loss: 198.62625122070312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.34042447805404663\n",
      "      kl: 0.01466790959239006\n",
      "      policy_loss: 0.0029304756317287683\n",
      "      total_loss: 257.2271728515625\n",
      "      vf_explained_var: 0.7577484846115112\n",
      "      vf_loss: 257.2242431640625\n",
      "    sample_time_ms: 20311.956\n",
      "    update_time_ms: 6.944\n",
      "  iterations_since_restore: 596\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.43129526650888\n",
      "    rl_1: 93.09891601782319\n",
      "  time_since_restore: 14095.898214817047\n",
      "  time_this_iter_s: 23.65068292617798\n",
      "  time_total_s: 14095.898214817047\n",
      "  timestamp: 1550807526\n",
      "  timesteps_since_restore: 5960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5960000\n",
      "  training_iteration: 596\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14095 s, 596 iter, 5960000 ts, 159 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-52-28\n",
      "  done: false\n",
      "  episode_len_mean: 93.68867924528301\n",
      "  episode_reward_max: 224.6022973820853\n",
      "  episode_reward_mean: 182.80174513450018\n",
      "  episode_reward_min: -126.59982129893292\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 62271\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.722\n",
      "    load_time_ms: 2.424\n",
      "    num_steps_sampled: 5970000\n",
      "    num_steps_trained: 5970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0842992514371872\n",
      "      kl: 0.02103075198829174\n",
      "      policy_loss: -0.0011396176414564252\n",
      "      total_loss: 50.1903076171875\n",
      "      vf_explained_var: 0.9236839413642883\n",
      "      vf_loss: 50.19145584106445\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.33162224292755127\n",
      "      kl: 0.026483478024601936\n",
      "      policy_loss: 0.00817572046071291\n",
      "      total_loss: 62.486427307128906\n",
      "      vf_explained_var: 0.9334611892700195\n",
      "      vf_loss: 62.47824478149414\n",
      "    sample_time_ms: 20207.981\n",
      "    update_time_ms: 6.986\n",
      "  iterations_since_restore: 597\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.75267446237322\n",
      "    rl_1: 106.04907067212693\n",
      "  time_since_restore: 14118.630428314209\n",
      "  time_this_iter_s: 22.732213497161865\n",
      "  time_total_s: 14118.630428314209\n",
      "  timestamp: 1550807548\n",
      "  timesteps_since_restore: 5970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5970000\n",
      "  training_iteration: 597\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14118 s, 597 iter, 5970000 ts, 183 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-52-52\n",
      "  done: false\n",
      "  episode_len_mean: 92.55963302752293\n",
      "  episode_reward_max: 223.19175581733455\n",
      "  episode_reward_mean: 171.11411073112424\n",
      "  episode_reward_min: -165.63157721324214\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 62380\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3151.572\n",
      "    load_time_ms: 2.425\n",
      "    num_steps_sampled: 5980000\n",
      "    num_steps_trained: 5980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.04519841447472572\n",
      "      kl: 0.021109839901328087\n",
      "      policy_loss: -0.0037043828051537275\n",
      "      total_loss: 113.85643005371094\n",
      "      vf_explained_var: 0.84613037109375\n",
      "      vf_loss: 113.86017608642578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3478100001811981\n",
      "      kl: 0.014595560729503632\n",
      "      policy_loss: 0.0017650444060564041\n",
      "      total_loss: 146.95860290527344\n",
      "      vf_explained_var: 0.8555428981781006\n",
      "      vf_loss: 146.95684814453125\n",
      "    sample_time_ms: 20172.18\n",
      "    update_time_ms: 7.106\n",
      "  iterations_since_restore: 598\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.83753431141847\n",
      "    rl_1: 99.27657641970578\n",
      "  time_since_restore: 14142.178864479065\n",
      "  time_this_iter_s: 23.548436164855957\n",
      "  time_total_s: 14142.178864479065\n",
      "  timestamp: 1550807572\n",
      "  timesteps_since_restore: 5980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5980000\n",
      "  training_iteration: 598\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14142 s, 598 iter, 5980000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-53-15\n",
      "  done: false\n",
      "  episode_len_mean: 90.66363636363636\n",
      "  episode_reward_max: 227.5544257529836\n",
      "  episode_reward_mean: 156.364110148111\n",
      "  episode_reward_min: -160.81248050658758\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 62490\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.942\n",
      "    load_time_ms: 2.408\n",
      "    num_steps_sampled: 5990000\n",
      "    num_steps_trained: 5990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09744624048471451\n",
      "      kl: 0.02508525922894478\n",
      "      policy_loss: -0.0014024345437064767\n",
      "      total_loss: 190.3029022216797\n",
      "      vf_explained_var: 0.7981729507446289\n",
      "      vf_loss: 190.30430603027344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3416433036327362\n",
      "      kl: 0.029017742723226547\n",
      "      policy_loss: 0.006519452668726444\n",
      "      total_loss: 233.43788146972656\n",
      "      vf_explained_var: 0.8182873725891113\n",
      "      vf_loss: 233.43133544921875\n",
      "    sample_time_ms: 20143.574\n",
      "    update_time_ms: 7.09\n",
      "  iterations_since_restore: 599\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.8987047117276\n",
      "    rl_1: 91.46540543638336\n",
      "  time_since_restore: 14165.444018363953\n",
      "  time_this_iter_s: 23.265153884887695\n",
      "  time_total_s: 14165.444018363953\n",
      "  timestamp: 1550807595\n",
      "  timesteps_since_restore: 5990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 5990000\n",
      "  training_iteration: 599\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14165 s, 599 iter, 5990000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-53-39\n",
      "  done: false\n",
      "  episode_len_mean: 92.89814814814815\n",
      "  episode_reward_max: 218.68428649193328\n",
      "  episode_reward_mean: 167.65138427723144\n",
      "  episode_reward_min: -169.63324871454842\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 62598\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.617\n",
      "    load_time_ms: 2.362\n",
      "    num_steps_sampled: 6000000\n",
      "    num_steps_trained: 6000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07871878147125244\n",
      "      kl: 0.016901804134249687\n",
      "      policy_loss: -0.004777256399393082\n",
      "      total_loss: 106.66046905517578\n",
      "      vf_explained_var: 0.8739640712738037\n",
      "      vf_loss: 106.66525268554688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.295134574174881\n",
      "      kl: 0.013552269898355007\n",
      "      policy_loss: 0.0006436980911530554\n",
      "      total_loss: 140.5245819091797\n",
      "      vf_explained_var: 0.8820584416389465\n",
      "      vf_loss: 140.52392578125\n",
      "    sample_time_ms: 20128.753\n",
      "    update_time_ms: 7.614\n",
      "  iterations_since_restore: 600\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.81628956461275\n",
      "    rl_1: 97.8350947126187\n",
      "  time_since_restore: 14188.646154642105\n",
      "  time_this_iter_s: 23.202136278152466\n",
      "  time_total_s: 14188.646154642105\n",
      "  timestamp: 1550807619\n",
      "  timesteps_since_restore: 6000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6000000\n",
      "  training_iteration: 600\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14188 s, 600 iter, 6000000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-54-03\n",
      "  done: false\n",
      "  episode_len_mean: 91.13761467889908\n",
      "  episode_reward_max: 226.19841848970094\n",
      "  episode_reward_mean: 152.44786984955982\n",
      "  episode_reward_min: -151.88900499434408\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 62707\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.227\n",
      "    load_time_ms: 2.436\n",
      "    num_steps_sampled: 6010000\n",
      "    num_steps_trained: 6010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.042308658361434937\n",
      "      kl: 0.020253198221325874\n",
      "      policy_loss: 8.634310506749898e-05\n",
      "      total_loss: 211.6054229736328\n",
      "      vf_explained_var: 0.7864735722541809\n",
      "      vf_loss: 211.6053466796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3299495279788971\n",
      "      kl: 0.01859922520816326\n",
      "      policy_loss: 0.0033422857522964478\n",
      "      total_loss: 255.27194213867188\n",
      "      vf_explained_var: 0.7941322922706604\n",
      "      vf_loss: 255.2686004638672\n",
      "    sample_time_ms: 20224.124\n",
      "    update_time_ms: 7.631\n",
      "  iterations_since_restore: 601\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.97349678735632\n",
      "    rl_1: 89.4743730622035\n",
      "  time_since_restore: 14212.899374008179\n",
      "  time_this_iter_s: 24.25321936607361\n",
      "  time_total_s: 14212.899374008179\n",
      "  timestamp: 1550807643\n",
      "  timesteps_since_restore: 6010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6010000\n",
      "  training_iteration: 601\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14212 s, 601 iter, 6010000 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-54-26\n",
      "  done: false\n",
      "  episode_len_mean: 94.00934579439253\n",
      "  episode_reward_max: 223.99399816540668\n",
      "  episode_reward_mean: 176.01417131529044\n",
      "  episode_reward_min: -125.45979595696566\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 62814\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.31\n",
      "    load_time_ms: 2.411\n",
      "    num_steps_sampled: 6020000\n",
      "    num_steps_trained: 6020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07437357306480408\n",
      "      kl: 0.015359397046267986\n",
      "      policy_loss: -0.0016729094786569476\n",
      "      total_loss: 76.59275817871094\n",
      "      vf_explained_var: 0.8862431049346924\n",
      "      vf_loss: 76.59442901611328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3891565501689911\n",
      "      kl: 0.028890499845147133\n",
      "      policy_loss: 0.006953980773687363\n",
      "      total_loss: 92.022216796875\n",
      "      vf_explained_var: 0.9039196968078613\n",
      "      vf_loss: 92.0152587890625\n",
      "    sample_time_ms: 20231.324\n",
      "    update_time_ms: 7.493\n",
      "  iterations_since_restore: 602\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.9210437731155\n",
      "    rl_1: 102.09312754217493\n",
      "  time_since_restore: 14236.323514699936\n",
      "  time_this_iter_s: 23.424140691757202\n",
      "  time_total_s: 14236.323514699936\n",
      "  timestamp: 1550807666\n",
      "  timesteps_since_restore: 6020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6020000\n",
      "  training_iteration: 602\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14236 s, 602 iter, 6020000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-54-50\n",
      "  done: false\n",
      "  episode_len_mean: 91.29357798165138\n",
      "  episode_reward_max: 223.71212513709008\n",
      "  episode_reward_mean: 164.84041300409308\n",
      "  episode_reward_min: -150.63398736704517\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 62923\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3159.46\n",
      "    load_time_ms: 2.366\n",
      "    num_steps_sampled: 6030000\n",
      "    num_steps_trained: 6030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07575546205043793\n",
      "      kl: 0.017724180594086647\n",
      "      policy_loss: -0.0024404586292803288\n",
      "      total_loss: 112.27478790283203\n",
      "      vf_explained_var: 0.8629670739173889\n",
      "      vf_loss: 112.27720642089844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3094615340232849\n",
      "      kl: 3.742652654647827\n",
      "      policy_loss: 0.06749017536640167\n",
      "      total_loss: 140.6129608154297\n",
      "      vf_explained_var: 0.874781608581543\n",
      "      vf_loss: 140.54551696777344\n",
      "    sample_time_ms: 20288.243\n",
      "    update_time_ms: 7.419\n",
      "  iterations_since_restore: 603\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.00779650341379\n",
      "    rl_1: 94.8326165006793\n",
      "  time_since_restore: 14260.396746635437\n",
      "  time_this_iter_s: 24.0732319355011\n",
      "  time_total_s: 14260.396746635437\n",
      "  timestamp: 1550807690\n",
      "  timesteps_since_restore: 6030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6030000\n",
      "  training_iteration: 603\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14260 s, 603 iter, 6030000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-55-13\n",
      "  done: false\n",
      "  episode_len_mean: 92.18348623853211\n",
      "  episode_reward_max: 223.34682173967894\n",
      "  episode_reward_mean: 182.74682690807387\n",
      "  episode_reward_min: -164.6910852638106\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 63032\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3161.114\n",
      "    load_time_ms: 2.372\n",
      "    num_steps_sampled: 6040000\n",
      "    num_steps_trained: 6040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13312935829162598\n",
      "      kl: 0.02300223521888256\n",
      "      policy_loss: -0.0025075930170714855\n",
      "      total_loss: 41.87773132324219\n",
      "      vf_explained_var: 0.9389630556106567\n",
      "      vf_loss: 41.88024139404297\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.347116619348526\n",
      "      kl: 0.01807563751935959\n",
      "      policy_loss: 0.0008996698306873441\n",
      "      total_loss: 55.49890899658203\n",
      "      vf_explained_var: 0.9424803256988525\n",
      "      vf_loss: 55.498008728027344\n",
      "    sample_time_ms: 20244.392\n",
      "    update_time_ms: 7.274\n",
      "  iterations_since_restore: 604\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 78.30241372443699\n",
      "    rl_1: 104.4444131836369\n",
      "  time_since_restore: 14283.388417482376\n",
      "  time_this_iter_s: 22.991670846939087\n",
      "  time_total_s: 14283.388417482376\n",
      "  timestamp: 1550807713\n",
      "  timesteps_since_restore: 6040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6040000\n",
      "  training_iteration: 604\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14283 s, 604 iter, 6040000 ts, 183 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-55-37\n",
      "  done: false\n",
      "  episode_len_mean: 93.14018691588785\n",
      "  episode_reward_max: 219.36221550036313\n",
      "  episode_reward_mean: 175.30356234474132\n",
      "  episode_reward_min: -150.94484291721275\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 63139\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3162.18\n",
      "    load_time_ms: 2.399\n",
      "    num_steps_sampled: 6050000\n",
      "    num_steps_trained: 6050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10584156215190887\n",
      "      kl: 0.023823311552405357\n",
      "      policy_loss: 0.00046599635970778763\n",
      "      total_loss: 40.77996826171875\n",
      "      vf_explained_var: 0.9388801455497742\n",
      "      vf_loss: 40.77949905395508\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.30916935205459595\n",
      "      kl: 0.014947650954127312\n",
      "      policy_loss: 0.003206270281225443\n",
      "      total_loss: 51.259788513183594\n",
      "      vf_explained_var: 0.9499214291572571\n",
      "      vf_loss: 51.2565803527832\n",
      "    sample_time_ms: 20230.48\n",
      "    update_time_ms: 7.549\n",
      "  iterations_since_restore: 605\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.18785231272443\n",
      "    rl_1: 102.11571003201685\n",
      "  time_since_restore: 14306.452796697617\n",
      "  time_this_iter_s: 23.06437921524048\n",
      "  time_total_s: 14306.452796697617\n",
      "  timestamp: 1550807737\n",
      "  timesteps_since_restore: 6050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6050000\n",
      "  training_iteration: 605\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14306 s, 605 iter, 6050000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-56-00\n",
      "  done: false\n",
      "  episode_len_mean: 92.99074074074075\n",
      "  episode_reward_max: 217.90513657054296\n",
      "  episode_reward_mean: 181.9045796779042\n",
      "  episode_reward_min: 142.86019456509544\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 63247\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3159.232\n",
      "    load_time_ms: 2.41\n",
      "    num_steps_sampled: 6060000\n",
      "    num_steps_trained: 6060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12881317734718323\n",
      "      kl: 0.019512107595801353\n",
      "      policy_loss: -0.0002488130994606763\n",
      "      total_loss: 5.897965431213379\n",
      "      vf_explained_var: 0.9888818264007568\n",
      "      vf_loss: 5.898213863372803\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3560241460800171\n",
      "      kl: 0.015365996398031712\n",
      "      policy_loss: 0.002926359884440899\n",
      "      total_loss: 5.042355060577393\n",
      "      vf_explained_var: 0.9945490956306458\n",
      "      vf_loss: 5.039429187774658\n",
      "    sample_time_ms: 20214.954\n",
      "    update_time_ms: 7.511\n",
      "  iterations_since_restore: 606\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.23596777197882\n",
      "    rl_1: 105.66861190592536\n",
      "  time_since_restore: 14329.919643640518\n",
      "  time_this_iter_s: 23.46684694290161\n",
      "  time_total_s: 14329.919643640518\n",
      "  timestamp: 1550807760\n",
      "  timesteps_since_restore: 6060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6060000\n",
      "  training_iteration: 606\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14329 s, 606 iter, 6060000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 93.73831775700934\n",
      "  episode_reward_max: 222.6240168046249\n",
      "  episode_reward_mean: 175.18491357927928\n",
      "  episode_reward_min: -156.8459425769866\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 63354\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3157.97\n",
      "    load_time_ms: 2.351\n",
      "    num_steps_sampled: 6070000\n",
      "    num_steps_trained: 6070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0821584016084671\n",
      "      kl: 0.014966463670134544\n",
      "      policy_loss: -0.002574040787294507\n",
      "      total_loss: 39.565155029296875\n",
      "      vf_explained_var: 0.9456354975700378\n",
      "      vf_loss: 39.56772232055664\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35237422585487366\n",
      "      kl: 0.014856955967843533\n",
      "      policy_loss: 0.003842985024675727\n",
      "      total_loss: 48.16696548461914\n",
      "      vf_explained_var: 0.9568649530410767\n",
      "      vf_loss: 48.16312026977539\n",
      "    sample_time_ms: 20265.151\n",
      "    update_time_ms: 7.681\n",
      "  iterations_since_restore: 607\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.92530773769468\n",
      "    rl_1: 102.2596058415846\n",
      "  time_since_restore: 14353.140731334686\n",
      "  time_this_iter_s: 23.22108769416809\n",
      "  time_total_s: 14353.140731334686\n",
      "  timestamp: 1550807783\n",
      "  timesteps_since_restore: 6070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6070000\n",
      "  training_iteration: 607\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14353 s, 607 iter, 6070000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-56-47\n",
      "  done: false\n",
      "  episode_len_mean: 93.61320754716981\n",
      "  episode_reward_max: 222.6289570686859\n",
      "  episode_reward_mean: 178.12051413696466\n",
      "  episode_reward_min: -80.64315420204679\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 63460\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.258\n",
      "    load_time_ms: 2.342\n",
      "    num_steps_sampled: 6080000\n",
      "    num_steps_trained: 6080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09803734719753265\n",
      "      kl: 0.029270969331264496\n",
      "      policy_loss: 0.001320850569754839\n",
      "      total_loss: 9.879095077514648\n",
      "      vf_explained_var: 0.9831280708312988\n",
      "      vf_loss: 9.87777328491211\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.360562801361084\n",
      "      kl: 0.016620393842458725\n",
      "      policy_loss: 0.0011373707093298435\n",
      "      total_loss: 10.099991798400879\n",
      "      vf_explained_var: 0.9903883337974548\n",
      "      vf_loss: 10.098855018615723\n",
      "    sample_time_ms: 20269.513\n",
      "    update_time_ms: 7.484\n",
      "  iterations_since_restore: 608\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.49236579824687\n",
      "    rl_1: 103.62814833871776\n",
      "  time_since_restore: 14376.53437614441\n",
      "  time_this_iter_s: 23.3936448097229\n",
      "  time_total_s: 14376.53437614441\n",
      "  timestamp: 1550807807\n",
      "  timesteps_since_restore: 6080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6080000\n",
      "  training_iteration: 608\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14376 s, 608 iter, 6080000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-57-10\n",
      "  done: false\n",
      "  episode_len_mean: 93.27102803738318\n",
      "  episode_reward_max: 219.02688530377642\n",
      "  episode_reward_mean: 171.97595346949393\n",
      "  episode_reward_min: -162.8591100808877\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 63567\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.308\n",
      "    load_time_ms: 2.251\n",
      "    num_steps_sampled: 6090000\n",
      "    num_steps_trained: 6090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12357675284147263\n",
      "      kl: 0.011349980719387531\n",
      "      policy_loss: -0.0009762108093127608\n",
      "      total_loss: 64.04875183105469\n",
      "      vf_explained_var: 0.9152147173881531\n",
      "      vf_loss: 64.04972839355469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.34721094369888306\n",
      "      kl: 0.6944698691368103\n",
      "      policy_loss: 0.000947645865380764\n",
      "      total_loss: 97.8403549194336\n",
      "      vf_explained_var: 0.9077067375183105\n",
      "      vf_loss: 97.83940887451172\n",
      "    sample_time_ms: 20225.515\n",
      "    update_time_ms: 7.354\n",
      "  iterations_since_restore: 609\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.04895766484341\n",
      "    rl_1: 98.92699580465055\n",
      "  time_since_restore: 14399.365014791489\n",
      "  time_this_iter_s: 22.830638647079468\n",
      "  time_total_s: 14399.365014791489\n",
      "  timestamp: 1550807830\n",
      "  timesteps_since_restore: 6090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6090000\n",
      "  training_iteration: 609\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14399 s, 609 iter, 6090000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-57-33\n",
      "  done: false\n",
      "  episode_len_mean: 93.12037037037037\n",
      "  episode_reward_max: 221.8800957647205\n",
      "  episode_reward_mean: 179.54241663953314\n",
      "  episode_reward_min: 147.36755345171846\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 63675\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.613\n",
      "    load_time_ms: 2.225\n",
      "    num_steps_sampled: 6100000\n",
      "    num_steps_trained: 6100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.046770721673965454\n",
      "      kl: 0.018999213352799416\n",
      "      policy_loss: 0.001376982661895454\n",
      "      total_loss: 4.617800712585449\n",
      "      vf_explained_var: 0.9915809631347656\n",
      "      vf_loss: 4.6164231300354\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3101555407047272\n",
      "      kl: 0.01938539557158947\n",
      "      policy_loss: 0.0036778044886887074\n",
      "      total_loss: 3.297942638397217\n",
      "      vf_explained_var: 0.9962045550346375\n",
      "      vf_loss: 3.294264554977417\n",
      "    sample_time_ms: 20257.676\n",
      "    update_time_ms: 6.947\n",
      "  iterations_since_restore: 610\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 77.05832378830559\n",
      "    rl_1: 102.48409285122752\n",
      "  time_since_restore: 14422.894580125809\n",
      "  time_this_iter_s: 23.52956533432007\n",
      "  time_total_s: 14422.894580125809\n",
      "  timestamp: 1550807853\n",
      "  timesteps_since_restore: 6100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6100000\n",
      "  training_iteration: 610\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14422 s, 610 iter, 6100000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-57-57\n",
      "  done: false\n",
      "  episode_len_mean: 93.09345794392523\n",
      "  episode_reward_max: 226.0105838237144\n",
      "  episode_reward_mean: 178.8328836583937\n",
      "  episode_reward_min: -152.96249790550723\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 63782\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.698\n",
      "    load_time_ms: 2.191\n",
      "    num_steps_sampled: 6110000\n",
      "    num_steps_trained: 6110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09638630598783493\n",
      "      kl: 0.008631435222923756\n",
      "      policy_loss: -0.0013047140091657639\n",
      "      total_loss: 26.209381103515625\n",
      "      vf_explained_var: 0.961008608341217\n",
      "      vf_loss: 26.210683822631836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.34905993938446045\n",
      "      kl: 0.006392884999513626\n",
      "      policy_loss: -0.0015781469410285354\n",
      "      total_loss: 39.62685775756836\n",
      "      vf_explained_var: 0.9615955948829651\n",
      "      vf_loss: 39.62843322753906\n",
      "    sample_time_ms: 20187.821\n",
      "    update_time_ms: 7.033\n",
      "  iterations_since_restore: 611\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.7516580184526\n",
      "    rl_1: 103.08122563994107\n",
      "  time_since_restore: 14446.391567468643\n",
      "  time_this_iter_s: 23.496987342834473\n",
      "  time_total_s: 14446.391567468643\n",
      "  timestamp: 1550807877\n",
      "  timesteps_since_restore: 6110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6110000\n",
      "  training_iteration: 611\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14446 s, 611 iter, 6110000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-58-20\n",
      "  done: false\n",
      "  episode_len_mean: 92.28440366972477\n",
      "  episode_reward_max: 220.35581507096612\n",
      "  episode_reward_mean: 177.61038139987141\n",
      "  episode_reward_min: -164.36531862837484\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 63891\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3157.593\n",
      "    load_time_ms: 2.194\n",
      "    num_steps_sampled: 6120000\n",
      "    num_steps_trained: 6120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.14925070106983185\n",
      "      kl: 0.01537582091987133\n",
      "      policy_loss: -0.0015422981232404709\n",
      "      total_loss: 42.65754318237305\n",
      "      vf_explained_var: 0.9403732419013977\n",
      "      vf_loss: 42.65908432006836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3157420754432678\n",
      "      kl: 0.03407111391425133\n",
      "      policy_loss: 0.00463161850348115\n",
      "      total_loss: 65.93848419189453\n",
      "      vf_explained_var: 0.940977931022644\n",
      "      vf_loss: 65.9338607788086\n",
      "    sample_time_ms: 20172.357\n",
      "    update_time_ms: 7.355\n",
      "  iterations_since_restore: 612\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.32623547256873\n",
      "    rl_1: 103.28414592730272\n",
      "  time_since_restore: 14469.890137910843\n",
      "  time_this_iter_s: 23.498570442199707\n",
      "  time_total_s: 14469.890137910843\n",
      "  timestamp: 1550807900\n",
      "  timesteps_since_restore: 6120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6120000\n",
      "  training_iteration: 612\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14469 s, 612 iter, 6120000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-58-43\n",
      "  done: false\n",
      "  episode_len_mean: 93.23584905660377\n",
      "  episode_reward_max: 224.18982244613764\n",
      "  episode_reward_mean: 179.15621024405658\n",
      "  episode_reward_min: 144.57317567882993\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 63997\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.504\n",
      "    load_time_ms: 2.243\n",
      "    num_steps_sampled: 6130000\n",
      "    num_steps_trained: 6130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0956893190741539\n",
      "      kl: 0.03832411766052246\n",
      "      policy_loss: 0.006693054456263781\n",
      "      total_loss: 3.255918025970459\n",
      "      vf_explained_var: 0.9938043355941772\n",
      "      vf_loss: 3.249224901199341\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28914448618888855\n",
      "      kl: 0.012793236412107944\n",
      "      policy_loss: 0.004092483781278133\n",
      "      total_loss: 2.681046485900879\n",
      "      vf_explained_var: 0.9969897866249084\n",
      "      vf_loss: 2.676954507827759\n",
      "    sample_time_ms: 20076.092\n",
      "    update_time_ms: 7.219\n",
      "  iterations_since_restore: 613\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.37494575401284\n",
      "    rl_1: 103.7812644900437\n",
      "  time_since_restore: 14492.809617757797\n",
      "  time_this_iter_s: 22.919479846954346\n",
      "  time_total_s: 14492.809617757797\n",
      "  timestamp: 1550807923\n",
      "  timesteps_since_restore: 6130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6130000\n",
      "  training_iteration: 613\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14492 s, 613 iter, 6130000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-59-07\n",
      "  done: false\n",
      "  episode_len_mean: 93.01851851851852\n",
      "  episode_reward_max: 217.8922653306524\n",
      "  episode_reward_mean: 174.84643979111752\n",
      "  episode_reward_min: -149.19207891134013\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 64105\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.802\n",
      "    load_time_ms: 2.379\n",
      "    num_steps_sampled: 6140000\n",
      "    num_steps_trained: 6140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12264837324619293\n",
      "      kl: 0.017851730808615685\n",
      "      policy_loss: -0.0011949165491387248\n",
      "      total_loss: 7.158875465393066\n",
      "      vf_explained_var: 0.9889631271362305\n",
      "      vf_loss: 7.160069942474365\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32321324944496155\n",
      "      kl: 0.01224199403077364\n",
      "      policy_loss: -0.0006200726493261755\n",
      "      total_loss: 7.343967437744141\n",
      "      vf_explained_var: 0.9933143258094788\n",
      "      vf_loss: 7.344588279724121\n",
      "    sample_time_ms: 20104.767\n",
      "    update_time_ms: 7.076\n",
      "  iterations_since_restore: 614\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.77789796276824\n",
      "    rl_1: 102.06854182834928\n",
      "  time_since_restore: 14516.072127342224\n",
      "  time_this_iter_s: 23.26250958442688\n",
      "  time_total_s: 14516.072127342224\n",
      "  timestamp: 1550807947\n",
      "  timesteps_since_restore: 6140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6140000\n",
      "  training_iteration: 614\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14516 s, 614 iter, 6140000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-59-30\n",
      "  done: false\n",
      "  episode_len_mean: 98.03921568627452\n",
      "  episode_reward_max: 220.56776752323324\n",
      "  episode_reward_mean: 178.11066594756892\n",
      "  episode_reward_min: 5.357862552018303\n",
      "  episodes_this_iter: 102\n",
      "  episodes_total: 64207\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.393\n",
      "    load_time_ms: 2.426\n",
      "    num_steps_sampled: 6150000\n",
      "    num_steps_trained: 6150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09896938502788544\n",
      "      kl: 0.012421480379998684\n",
      "      policy_loss: -0.0016649439930915833\n",
      "      total_loss: 4.3778157234191895\n",
      "      vf_explained_var: 0.9940611124038696\n",
      "      vf_loss: 4.379479885101318\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26348641514778137\n",
      "      kl: 0.012755832634866238\n",
      "      policy_loss: 0.004365613684058189\n",
      "      total_loss: 2.9893639087677\n",
      "      vf_explained_var: 0.9973279237747192\n",
      "      vf_loss: 2.9849984645843506\n",
      "    sample_time_ms: 20105.491\n",
      "    update_time_ms: 6.993\n",
      "  iterations_since_restore: 615\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.94156200905087\n",
      "    rl_1: 104.16910393851806\n",
      "  time_since_restore: 14539.098223686218\n",
      "  time_this_iter_s: 23.02609634399414\n",
      "  time_total_s: 14539.098223686218\n",
      "  timestamp: 1550807970\n",
      "  timesteps_since_restore: 6150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6150000\n",
      "  training_iteration: 615\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14539 s, 615 iter, 6150000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_04-59-53\n",
      "  done: false\n",
      "  episode_len_mean: 93.00925925925925\n",
      "  episode_reward_max: 225.20245110126675\n",
      "  episode_reward_mean: 173.20170240228157\n",
      "  episode_reward_min: -150.20777367975845\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 64315\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.758\n",
      "    load_time_ms: 2.413\n",
      "    num_steps_sampled: 6160000\n",
      "    num_steps_trained: 6160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1307443529367447\n",
      "      kl: 0.012037619948387146\n",
      "      policy_loss: -0.005757852923125029\n",
      "      total_loss: 34.092342376708984\n",
      "      vf_explained_var: 0.9537739753723145\n",
      "      vf_loss: 34.09809875488281\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.29749056696891785\n",
      "      kl: 0.015374191105365753\n",
      "      policy_loss: -0.005193415097892284\n",
      "      total_loss: 51.489288330078125\n",
      "      vf_explained_var: 0.9556465148925781\n",
      "      vf_loss: 51.49448013305664\n",
      "    sample_time_ms: 20072.299\n",
      "    update_time_ms: 6.992\n",
      "  iterations_since_restore: 616\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.59221135929758\n",
      "    rl_1: 100.60949104298403\n",
      "  time_since_restore: 14562.217873334885\n",
      "  time_this_iter_s: 23.119649648666382\n",
      "  time_total_s: 14562.217873334885\n",
      "  timestamp: 1550807993\n",
      "  timesteps_since_restore: 6160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6160000\n",
      "  training_iteration: 616\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14562 s, 616 iter, 6160000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-00-16\n",
      "  done: false\n",
      "  episode_len_mean: 95.00952380952381\n",
      "  episode_reward_max: 218.63155620850657\n",
      "  episode_reward_mean: 179.83275498106\n",
      "  episode_reward_min: 133.66810372625963\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 64420\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.793\n",
      "    load_time_ms: 2.378\n",
      "    num_steps_sampled: 6170000\n",
      "    num_steps_trained: 6170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08426006883382797\n",
      "      kl: 0.026086801663041115\n",
      "      policy_loss: 0.001277509843930602\n",
      "      total_loss: 3.2057530879974365\n",
      "      vf_explained_var: 0.9937333464622498\n",
      "      vf_loss: 3.2044758796691895\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.30127042531967163\n",
      "      kl: 0.021328536793589592\n",
      "      policy_loss: 0.0017564785666763783\n",
      "      total_loss: 3.1323330402374268\n",
      "      vf_explained_var: 0.9966729283332825\n",
      "      vf_loss: 3.1305766105651855\n",
      "    sample_time_ms: 20043.671\n",
      "    update_time_ms: 7.059\n",
      "  iterations_since_restore: 617\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.42488776560364\n",
      "    rl_1: 105.40786721545633\n",
      "  time_since_restore: 14585.184520483017\n",
      "  time_this_iter_s: 22.966647148132324\n",
      "  time_total_s: 14585.184520483017\n",
      "  timestamp: 1550808016\n",
      "  timesteps_since_restore: 6170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6170000\n",
      "  training_iteration: 617\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14585 s, 617 iter, 6170000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-00-40\n",
      "  done: false\n",
      "  episode_len_mean: 93.60747663551402\n",
      "  episode_reward_max: 223.03377099948835\n",
      "  episode_reward_mean: 182.12669762918907\n",
      "  episode_reward_min: 150.00451300882187\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 64527\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.026\n",
      "    load_time_ms: 2.386\n",
      "    num_steps_sampled: 6180000\n",
      "    num_steps_trained: 6180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.15963634848594666\n",
      "      kl: 0.024447493255138397\n",
      "      policy_loss: 0.004043616354465485\n",
      "      total_loss: 2.427699089050293\n",
      "      vf_explained_var: 0.995537281036377\n",
      "      vf_loss: 2.4236557483673096\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.36280396580696106\n",
      "      kl: 0.01430972758680582\n",
      "      policy_loss: 0.0011771550634875894\n",
      "      total_loss: 2.600003480911255\n",
      "      vf_explained_var: 0.9972507953643799\n",
      "      vf_loss: 2.5988264083862305\n",
      "    sample_time_ms: 20103.488\n",
      "    update_time_ms: 7.134\n",
      "  iterations_since_restore: 618\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.12889516934854\n",
      "    rl_1: 105.9978024598405\n",
      "  time_since_restore: 14609.258720874786\n",
      "  time_this_iter_s: 24.07420039176941\n",
      "  time_total_s: 14609.258720874786\n",
      "  timestamp: 1550808040\n",
      "  timesteps_since_restore: 6180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6180000\n",
      "  training_iteration: 618\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14609 s, 618 iter, 6180000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-01-03\n",
      "  done: false\n",
      "  episode_len_mean: 93.1574074074074\n",
      "  episode_reward_max: 222.40474819261047\n",
      "  episode_reward_mean: 175.1317589371896\n",
      "  episode_reward_min: -156.70180071178783\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 64635\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.241\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 6190000\n",
      "    num_steps_trained: 6190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1095096543431282\n",
      "      kl: 0.014941956847906113\n",
      "      policy_loss: -0.0035407941322773695\n",
      "      total_loss: 42.81357192993164\n",
      "      vf_explained_var: 0.9385592937469482\n",
      "      vf_loss: 42.817108154296875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32740259170532227\n",
      "      kl: 32.69357681274414\n",
      "      policy_loss: 0.03445909544825554\n",
      "      total_loss: 72.70309448242188\n",
      "      vf_explained_var: 0.9365820288658142\n",
      "      vf_loss: 72.66864776611328\n",
      "    sample_time_ms: 20135.115\n",
      "    update_time_ms: 7.139\n",
      "  iterations_since_restore: 619\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.03602995567718\n",
      "    rl_1: 103.09572898151238\n",
      "  time_since_restore: 14632.406872749329\n",
      "  time_this_iter_s: 23.148151874542236\n",
      "  time_total_s: 14632.406872749329\n",
      "  timestamp: 1550808063\n",
      "  timesteps_since_restore: 6190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6190000\n",
      "  training_iteration: 619\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14632 s, 619 iter, 6190000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-01-26\n",
      "  done: false\n",
      "  episode_len_mean: 91.66666666666667\n",
      "  episode_reward_max: 222.250995521105\n",
      "  episode_reward_mean: 180.6705334858827\n",
      "  episode_reward_min: 141.27274115645872\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 64743\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.482\n",
      "    load_time_ms: 2.405\n",
      "    num_steps_sampled: 6200000\n",
      "    num_steps_trained: 6200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1773919314146042\n",
      "      kl: 11.454299926757812\n",
      "      policy_loss: 0.08509930223226547\n",
      "      total_loss: 2.459563732147217\n",
      "      vf_explained_var: 0.995552659034729\n",
      "      vf_loss: 2.374464273452759\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.33474382758140564\n",
      "      kl: 0.009913894347846508\n",
      "      policy_loss: -0.0031527518294751644\n",
      "      total_loss: 2.503049373626709\n",
      "      vf_explained_var: 0.9973170161247253\n",
      "      vf_loss: 2.506201982498169\n",
      "    sample_time_ms: 20103.453\n",
      "    update_time_ms: 7.303\n",
      "  iterations_since_restore: 620\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.34458133813463\n",
      "    rl_1: 105.32595214774803\n",
      "  time_since_restore: 14655.596061229706\n",
      "  time_this_iter_s: 23.189188480377197\n",
      "  time_total_s: 14655.596061229706\n",
      "  timestamp: 1550808086\n",
      "  timesteps_since_restore: 6200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6200000\n",
      "  training_iteration: 620\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14655 s, 620 iter, 6200000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-01-49\n",
      "  done: false\n",
      "  episode_len_mean: 87.8859649122807\n",
      "  episode_reward_max: 221.7686604417893\n",
      "  episode_reward_mean: 146.40955655895317\n",
      "  episode_reward_min: -178.1043235004343\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 64857\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3163.643\n",
      "    load_time_ms: 2.444\n",
      "    num_steps_sampled: 6210000\n",
      "    num_steps_trained: 6210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11427175998687744\n",
      "      kl: 0.01753055676817894\n",
      "      policy_loss: -0.0012988070957362652\n",
      "      total_loss: 314.0119934082031\n",
      "      vf_explained_var: 0.6651270985603333\n",
      "      vf_loss: 314.0132751464844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3115071952342987\n",
      "      kl: 0.024853091686964035\n",
      "      policy_loss: 0.0005165297188796103\n",
      "      total_loss: 415.4043273925781\n",
      "      vf_explained_var: 0.6916974186897278\n",
      "      vf_loss: 415.40380859375\n",
      "    sample_time_ms: 20024.482\n",
      "    update_time_ms: 7.36\n",
      "  iterations_since_restore: 621\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.09507202659025\n",
      "    rl_1: 86.31448453236291\n",
      "  time_since_restore: 14678.545025587082\n",
      "  time_this_iter_s: 22.9489643573761\n",
      "  time_total_s: 14678.545025587082\n",
      "  timestamp: 1550808109\n",
      "  timesteps_since_restore: 6210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6210000\n",
      "  training_iteration: 621\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14678 s, 621 iter, 6210000 ts, 146 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-02-13\n",
      "  done: false\n",
      "  episode_len_mean: 87.2695652173913\n",
      "  episode_reward_max: 224.72248270908182\n",
      "  episode_reward_mean: 141.25806768677884\n",
      "  episode_reward_min: -169.8284410828545\n",
      "  episodes_this_iter: 115\n",
      "  episodes_total: 64972\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.385\n",
      "    load_time_ms: 2.445\n",
      "    num_steps_sampled: 6220000\n",
      "    num_steps_trained: 6220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1505005657672882\n",
      "      kl: 0.01246584765613079\n",
      "      policy_loss: -0.0030601222533732653\n",
      "      total_loss: 372.07965087890625\n",
      "      vf_explained_var: 0.6429017186164856\n",
      "      vf_loss: 372.08270263671875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31544554233551025\n",
      "      kl: 0.018518002703785896\n",
      "      policy_loss: -0.002304863417521119\n",
      "      total_loss: 495.8721923828125\n",
      "      vf_explained_var: 0.6371162533760071\n",
      "      vf_loss: 495.87451171875\n",
      "    sample_time_ms: 20012.849\n",
      "    update_time_ms: 7.039\n",
      "  iterations_since_restore: 622\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.26057743397044\n",
      "    rl_1: 82.99749025280838\n",
      "  time_since_restore: 14701.675159692764\n",
      "  time_this_iter_s: 23.130134105682373\n",
      "  time_total_s: 14701.675159692764\n",
      "  timestamp: 1550808133\n",
      "  timesteps_since_restore: 6220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6220000\n",
      "  training_iteration: 622\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14701 s, 622 iter, 6220000 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-02-36\n",
      "  done: false\n",
      "  episode_len_mean: 86.77586206896552\n",
      "  episode_reward_max: 220.00486187672203\n",
      "  episode_reward_mean: 148.35884266438117\n",
      "  episode_reward_min: -174.02232035445675\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 65088\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.583\n",
      "    load_time_ms: 2.375\n",
      "    num_steps_sampled: 6230000\n",
      "    num_steps_trained: 6230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.18716385960578918\n",
      "      kl: 0.021391846239566803\n",
      "      policy_loss: -1.2157341188867576e-05\n",
      "      total_loss: 285.403076171875\n",
      "      vf_explained_var: 0.7230395674705505\n",
      "      vf_loss: 285.4031066894531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.37226641178131104\n",
      "      kl: 0.017105627804994583\n",
      "      policy_loss: 0.001679767039604485\n",
      "      total_loss: 393.7086486816406\n",
      "      vf_explained_var: 0.6921993494033813\n",
      "      vf_loss: 393.70697021484375\n",
      "    sample_time_ms: 20025.218\n",
      "    update_time_ms: 6.892\n",
      "  iterations_since_restore: 623\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.63376122354995\n",
      "    rl_1: 86.7250814408312\n",
      "  time_since_restore: 14724.731018304825\n",
      "  time_this_iter_s: 23.055858612060547\n",
      "  time_total_s: 14724.731018304825\n",
      "  timestamp: 1550808156\n",
      "  timesteps_since_restore: 6230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6230000\n",
      "  training_iteration: 623\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14724 s, 623 iter, 6230000 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-02-59\n",
      "  done: false\n",
      "  episode_len_mean: 87.32456140350877\n",
      "  episode_reward_max: 222.59730723992993\n",
      "  episode_reward_mean: 154.10388691392214\n",
      "  episode_reward_min: -176.57713857683012\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 65202\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.687\n",
      "    load_time_ms: 2.293\n",
      "    num_steps_sampled: 6240000\n",
      "    num_steps_trained: 6240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.15581680834293365\n",
      "      kl: 0.022529512643814087\n",
      "      policy_loss: -0.0034341709688305855\n",
      "      total_loss: 236.79922485351562\n",
      "      vf_explained_var: 0.7358633875846863\n",
      "      vf_loss: 236.80270385742188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3539985418319702\n",
      "      kl: 0.02749665267765522\n",
      "      policy_loss: 0.0005823700339533389\n",
      "      total_loss: 309.84637451171875\n",
      "      vf_explained_var: 0.7321652173995972\n",
      "      vf_loss: 309.84588623046875\n",
      "    sample_time_ms: 20037.2\n",
      "    update_time_ms: 7.247\n",
      "  iterations_since_restore: 624\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.76485648561503\n",
      "    rl_1: 89.33903042830708\n",
      "  time_since_restore: 14748.124009370804\n",
      "  time_this_iter_s: 23.392991065979004\n",
      "  time_total_s: 14748.124009370804\n",
      "  timestamp: 1550808179\n",
      "  timesteps_since_restore: 6240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6240000\n",
      "  training_iteration: 624\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14748 s, 624 iter, 6240000 ts, 154 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-03-22\n",
      "  done: false\n",
      "  episode_len_mean: 87.7280701754386\n",
      "  episode_reward_max: 225.19078616071252\n",
      "  episode_reward_mean: 153.30463847290744\n",
      "  episode_reward_min: -168.04343249442377\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 65316\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.681\n",
      "    load_time_ms: 2.28\n",
      "    num_steps_sampled: 6250000\n",
      "    num_steps_trained: 6250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.14347708225250244\n",
      "      kl: 0.03446187078952789\n",
      "      policy_loss: 0.0036578660365194082\n",
      "      total_loss: 217.72789001464844\n",
      "      vf_explained_var: 0.7598517537117004\n",
      "      vf_loss: 217.72421264648438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3183114230632782\n",
      "      kl: 0.02545740082859993\n",
      "      policy_loss: -0.0012921264860779047\n",
      "      total_loss: 287.7679443359375\n",
      "      vf_explained_var: 0.7434878349304199\n",
      "      vf_loss: 287.7692565917969\n",
      "    sample_time_ms: 20053.498\n",
      "    update_time_ms: 7.017\n",
      "  iterations_since_restore: 625\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.56794555072092\n",
      "    rl_1: 88.73669292218648\n",
      "  time_since_restore: 14771.340811729431\n",
      "  time_this_iter_s: 23.21680235862732\n",
      "  time_total_s: 14771.340811729431\n",
      "  timestamp: 1550808202\n",
      "  timesteps_since_restore: 6250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6250000\n",
      "  training_iteration: 625\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14771 s, 625 iter, 6250000 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-03-45\n",
      "  done: false\n",
      "  episode_len_mean: 89.8108108108108\n",
      "  episode_reward_max: 223.64481769972218\n",
      "  episode_reward_mean: 160.46295964666663\n",
      "  episode_reward_min: -173.09643451319369\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 65427\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.718\n",
      "    load_time_ms: 2.303\n",
      "    num_steps_sampled: 6260000\n",
      "    num_steps_trained: 6260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.15560343861579895\n",
      "      kl: 0.044008854776620865\n",
      "      policy_loss: 0.001907406491227448\n",
      "      total_loss: 203.57240295410156\n",
      "      vf_explained_var: 0.7776116132736206\n",
      "      vf_loss: 203.5705108642578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3805093765258789\n",
      "      kl: 0.022145910188555717\n",
      "      policy_loss: 0.004283101297914982\n",
      "      total_loss: 271.3984375\n",
      "      vf_explained_var: 0.7499766945838928\n",
      "      vf_loss: 271.3941345214844\n",
      "    sample_time_ms: 19987.945\n",
      "    update_time_ms: 7.058\n",
      "  iterations_since_restore: 626\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.88669411712898\n",
      "    rl_1: 94.57626552953765\n",
      "  time_since_restore: 14793.794400930405\n",
      "  time_this_iter_s: 22.45358920097351\n",
      "  time_total_s: 14793.794400930405\n",
      "  timestamp: 1550808225\n",
      "  timesteps_since_restore: 6260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6260000\n",
      "  training_iteration: 626\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14793 s, 626 iter, 6260000 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-04-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.11711711711712\n",
      "  episode_reward_max: 223.15255571807486\n",
      "  episode_reward_mean: 169.5000114353915\n",
      "  episode_reward_min: -171.77573221340555\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 65538\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.437\n",
      "    load_time_ms: 2.418\n",
      "    num_steps_sampled: 6270000\n",
      "    num_steps_trained: 6270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1327318549156189\n",
      "      kl: 0.01645936630666256\n",
      "      policy_loss: -0.00017478904919698834\n",
      "      total_loss: 100.08325958251953\n",
      "      vf_explained_var: 0.8617815971374512\n",
      "      vf_loss: 100.08344268798828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.29906538128852844\n",
      "      kl: 0.013737084344029427\n",
      "      policy_loss: -0.0008664657361805439\n",
      "      total_loss: 127.16329956054688\n",
      "      vf_explained_var: 0.8638516068458557\n",
      "      vf_loss: 127.16417694091797\n",
      "    sample_time_ms: 20016.933\n",
      "    update_time_ms: 6.804\n",
      "  iterations_since_restore: 627\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.43762785262814\n",
      "    rl_1: 97.06238358276339\n",
      "  time_since_restore: 14817.061811208725\n",
      "  time_this_iter_s: 23.267410278320312\n",
      "  time_total_s: 14817.061811208725\n",
      "  timestamp: 1550808248\n",
      "  timesteps_since_restore: 6270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6270000\n",
      "  training_iteration: 627\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14817 s, 627 iter, 6270000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-04-32\n",
      "  done: false\n",
      "  episode_len_mean: 89.48648648648648\n",
      "  episode_reward_max: 224.6618148161257\n",
      "  episode_reward_mean: 165.83872773391494\n",
      "  episode_reward_min: -169.42414449381397\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 65649\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.164\n",
      "    load_time_ms: 2.42\n",
      "    num_steps_sampled: 6280000\n",
      "    num_steps_trained: 6280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.14645083248615265\n",
      "      kl: 0.02117280662059784\n",
      "      policy_loss: -0.00043982782517559826\n",
      "      total_loss: 132.23416137695312\n",
      "      vf_explained_var: 0.8367851376533508\n",
      "      vf_loss: 132.234619140625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31109580397605896\n",
      "      kl: 0.025444453582167625\n",
      "      policy_loss: 0.0064137778244912624\n",
      "      total_loss: 168.73707580566406\n",
      "      vf_explained_var: 0.8455384373664856\n",
      "      vf_loss: 168.73068237304688\n",
      "    sample_time_ms: 19970.59\n",
      "    update_time_ms: 6.857\n",
      "  iterations_since_restore: 628\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.54876971951109\n",
      "    rl_1: 96.28995801440387\n",
      "  time_since_restore: 14840.581045389175\n",
      "  time_this_iter_s: 23.51923418045044\n",
      "  time_total_s: 14840.581045389175\n",
      "  timestamp: 1550808272\n",
      "  timesteps_since_restore: 6280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6280000\n",
      "  training_iteration: 628\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14840 s, 628 iter, 6280000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-04-55\n",
      "  done: false\n",
      "  episode_len_mean: 89.57522123893806\n",
      "  episode_reward_max: 217.26210793985788\n",
      "  episode_reward_mean: 158.4919763198019\n",
      "  episode_reward_min: -167.53356449997779\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 65762\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.31\n",
      "    load_time_ms: 2.432\n",
      "    num_steps_sampled: 6290000\n",
      "    num_steps_trained: 6290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0904015302658081\n",
      "      kl: 0.01924063265323639\n",
      "      policy_loss: -0.00022798709687776864\n",
      "      total_loss: 149.92344665527344\n",
      "      vf_explained_var: 0.8103561997413635\n",
      "      vf_loss: 149.9237060546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28514373302459717\n",
      "      kl: 0.022429944947361946\n",
      "      policy_loss: 0.003667814191430807\n",
      "      total_loss: 207.56199645996094\n",
      "      vf_explained_var: 0.8041405081748962\n",
      "      vf_loss: 207.55831909179688\n",
      "    sample_time_ms: 19976.56\n",
      "    update_time_ms: 6.784\n",
      "  iterations_since_restore: 629\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.74105913674747\n",
      "    rl_1: 93.7509171830544\n",
      "  time_since_restore: 14863.790118932724\n",
      "  time_this_iter_s: 23.209073543548584\n",
      "  time_total_s: 14863.790118932724\n",
      "  timestamp: 1550808295\n",
      "  timesteps_since_restore: 6290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6290000\n",
      "  training_iteration: 629\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14863 s, 629 iter, 6290000 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-05-18\n",
      "  done: false\n",
      "  episode_len_mean: 89.99090909090908\n",
      "  episode_reward_max: 222.97657865405847\n",
      "  episode_reward_mean: 167.537524037891\n",
      "  episode_reward_min: -162.34235554014\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 65872\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.459\n",
      "    load_time_ms: 2.423\n",
      "    num_steps_sampled: 6300000\n",
      "    num_steps_trained: 6300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13680624961853027\n",
      "      kl: 0.03506672382354736\n",
      "      policy_loss: 0.001170328469015658\n",
      "      total_loss: 124.4832992553711\n",
      "      vf_explained_var: 0.851803719997406\n",
      "      vf_loss: 124.48210906982422\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3352612257003784\n",
      "      kl: 0.021173065528273582\n",
      "      policy_loss: 0.003284502075985074\n",
      "      total_loss: 162.8551483154297\n",
      "      vf_explained_var: 0.843912661075592\n",
      "      vf_loss: 162.8518829345703\n",
      "    sample_time_ms: 19926.836\n",
      "    update_time_ms: 6.81\n",
      "  iterations_since_restore: 630\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.76979938806213\n",
      "    rl_1: 97.76772464982892\n",
      "  time_since_restore: 14886.48467540741\n",
      "  time_this_iter_s: 22.69455647468567\n",
      "  time_total_s: 14886.48467540741\n",
      "  timestamp: 1550808318\n",
      "  timesteps_since_restore: 6300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6300000\n",
      "  training_iteration: 630\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14886 s, 630 iter, 6300000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-05-41\n",
      "  done: false\n",
      "  episode_len_mean: 89.1875\n",
      "  episode_reward_max: 222.27271497078132\n",
      "  episode_reward_mean: 164.2590747621362\n",
      "  episode_reward_min: -135.31072054350017\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 65984\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3112.287\n",
      "    load_time_ms: 2.35\n",
      "    num_steps_sampled: 6310000\n",
      "    num_steps_trained: 6310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13007305562496185\n",
      "      kl: 0.02466776967048645\n",
      "      policy_loss: -8.703304047230631e-05\n",
      "      total_loss: 117.56039428710938\n",
      "      vf_explained_var: 0.8385125994682312\n",
      "      vf_loss: 117.56048583984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32927998900413513\n",
      "      kl: 0.02760828100144863\n",
      "      policy_loss: 0.006065962836146355\n",
      "      total_loss: 144.7193145751953\n",
      "      vf_explained_var: 0.8550502061843872\n",
      "      vf_loss: 144.7132568359375\n",
      "    sample_time_ms: 20004.194\n",
      "    update_time_ms: 6.762\n",
      "  iterations_since_restore: 631\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.51641124051869\n",
      "    rl_1: 95.74266352161749\n",
      "  time_since_restore: 14909.985032558441\n",
      "  time_this_iter_s: 23.500357151031494\n",
      "  time_total_s: 14909.985032558441\n",
      "  timestamp: 1550808341\n",
      "  timesteps_since_restore: 6310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6310000\n",
      "  training_iteration: 631\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14909 s, 631 iter, 6310000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-06-05\n",
      "  done: false\n",
      "  episode_len_mean: 88.42477876106194\n",
      "  episode_reward_max: 224.22093907753236\n",
      "  episode_reward_mean: 158.55612671323055\n",
      "  episode_reward_min: -151.7084061965174\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 66097\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3114.393\n",
      "    load_time_ms: 2.369\n",
      "    num_steps_sampled: 6320000\n",
      "    num_steps_trained: 6320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.151570662856102\n",
      "      kl: 0.027384718880057335\n",
      "      policy_loss: -0.0020270764362066984\n",
      "      total_loss: 187.016845703125\n",
      "      vf_explained_var: 0.785987913608551\n",
      "      vf_loss: 187.01889038085938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.34391501545906067\n",
      "      kl: 0.021440740674734116\n",
      "      policy_loss: 0.0038387118838727474\n",
      "      total_loss: 236.85899353027344\n",
      "      vf_explained_var: 0.7924409508705139\n",
      "      vf_loss: 236.8551483154297\n",
      "    sample_time_ms: 20041.57\n",
      "    update_time_ms: 6.929\n",
      "  iterations_since_restore: 632\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.1976015627124\n",
      "    rl_1: 93.35852515051818\n",
      "  time_since_restore: 14933.51099038124\n",
      "  time_this_iter_s: 23.525957822799683\n",
      "  time_total_s: 14933.51099038124\n",
      "  timestamp: 1550808365\n",
      "  timesteps_since_restore: 6320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6320000\n",
      "  training_iteration: 632\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14933 s, 632 iter, 6320000 ts, 159 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-06-28\n",
      "  done: false\n",
      "  episode_len_mean: 90.89090909090909\n",
      "  episode_reward_max: 224.94082292401234\n",
      "  episode_reward_mean: 174.51501731040537\n",
      "  episode_reward_min: -127.59049014472606\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 66207\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3111.693\n",
      "    load_time_ms: 2.372\n",
      "    num_steps_sampled: 6330000\n",
      "    num_steps_trained: 6330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12016840279102325\n",
      "      kl: 0.03783035650849342\n",
      "      policy_loss: -8.378865459235385e-05\n",
      "      total_loss: 80.1293716430664\n",
      "      vf_explained_var: 0.884608805179596\n",
      "      vf_loss: 80.12945556640625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3029939830303192\n",
      "      kl: 0.027354363352060318\n",
      "      policy_loss: 0.002082789782434702\n",
      "      total_loss: 101.34503173828125\n",
      "      vf_explained_var: 0.8971673250198364\n",
      "      vf_loss: 101.34296417236328\n",
      "    sample_time_ms: 20031.681\n",
      "    update_time_ms: 6.91\n",
      "  iterations_since_restore: 633\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.35953498142817\n",
      "    rl_1: 101.15548232897719\n",
      "  time_since_restore: 14956.441053152084\n",
      "  time_this_iter_s: 22.930062770843506\n",
      "  time_total_s: 14956.441053152084\n",
      "  timestamp: 1550808388\n",
      "  timesteps_since_restore: 6330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6330000\n",
      "  training_iteration: 633\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14956 s, 633 iter, 6330000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-06-52\n",
      "  done: false\n",
      "  episode_len_mean: 91.14414414414415\n",
      "  episode_reward_max: 224.72850568562458\n",
      "  episode_reward_mean: 173.13651119019957\n",
      "  episode_reward_min: -153.31904583601568\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 66318\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.326\n",
      "    load_time_ms: 2.356\n",
      "    num_steps_sampled: 6340000\n",
      "    num_steps_trained: 6340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13984070718288422\n",
      "      kl: 0.02662825584411621\n",
      "      policy_loss: 0.0005237561417743564\n",
      "      total_loss: 78.25633239746094\n",
      "      vf_explained_var: 0.8857448101043701\n",
      "      vf_loss: 78.25582122802734\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.34955456852912903\n",
      "      kl: 0.017763987183570862\n",
      "      policy_loss: 0.006259419489651918\n",
      "      total_loss: 109.15283203125\n",
      "      vf_explained_var: 0.8866094946861267\n",
      "      vf_loss: 109.14656066894531\n",
      "    sample_time_ms: 20048.655\n",
      "    update_time_ms: 6.689\n",
      "  iterations_since_restore: 634\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.35739075961403\n",
      "    rl_1: 101.77912043058548\n",
      "  time_since_restore: 14980.151564598083\n",
      "  time_this_iter_s: 23.710511445999146\n",
      "  time_total_s: 14980.151564598083\n",
      "  timestamp: 1550808412\n",
      "  timesteps_since_restore: 6340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6340000\n",
      "  training_iteration: 634\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 14980 s, 634 iter, 6340000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-07-15\n",
      "  done: false\n",
      "  episode_len_mean: 89.41441441441441\n",
      "  episode_reward_max: 222.0891457754197\n",
      "  episode_reward_mean: 175.21989573255684\n",
      "  episode_reward_min: -165.84527118055257\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 66429\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.986\n",
      "    load_time_ms: 2.314\n",
      "    num_steps_sampled: 6350000\n",
      "    num_steps_trained: 6350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1701761931180954\n",
      "      kl: 0.014452364295721054\n",
      "      policy_loss: -0.00027444795705378056\n",
      "      total_loss: 81.84673309326172\n",
      "      vf_explained_var: 0.8864151239395142\n",
      "      vf_loss: 81.84700775146484\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.37529900670051575\n",
      "      kl: 0.018581572920084\n",
      "      policy_loss: 0.0016904359217733145\n",
      "      total_loss: 105.8408432006836\n",
      "      vf_explained_var: 0.8956404328346252\n",
      "      vf_loss: 105.83915710449219\n",
      "    sample_time_ms: 20082.483\n",
      "    update_time_ms: 6.984\n",
      "  iterations_since_restore: 635\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.25867153713827\n",
      "    rl_1: 101.96122419541858\n",
      "  time_since_restore: 15003.70534992218\n",
      "  time_this_iter_s: 23.55378532409668\n",
      "  time_total_s: 15003.70534992218\n",
      "  timestamp: 1550808435\n",
      "  timesteps_since_restore: 6350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6350000\n",
      "  training_iteration: 635\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15003 s, 635 iter, 6350000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-07-38\n",
      "  done: false\n",
      "  episode_len_mean: 91.54128440366972\n",
      "  episode_reward_max: 221.02292332286953\n",
      "  episode_reward_mean: 181.56671606582154\n",
      "  episode_reward_min: -152.72047699005373\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 66538\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.759\n",
      "    load_time_ms: 2.34\n",
      "    num_steps_sampled: 6360000\n",
      "    num_steps_trained: 6360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1335635483264923\n",
      "      kl: 0.0166262648999691\n",
      "      policy_loss: -0.0026104769203811884\n",
      "      total_loss: 27.688138961791992\n",
      "      vf_explained_var: 0.9587796330451965\n",
      "      vf_loss: 27.690746307373047\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3298426866531372\n",
      "      kl: 0.027591081336140633\n",
      "      policy_loss: 0.00601692171767354\n",
      "      total_loss: 34.26064682006836\n",
      "      vf_explained_var: 0.9655569791793823\n",
      "      vf_loss: 34.25463104248047\n",
      "    sample_time_ms: 20140.015\n",
      "    update_time_ms: 6.976\n",
      "  iterations_since_restore: 636\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.71270060403052\n",
      "    rl_1: 104.85401546179098\n",
      "  time_since_restore: 15026.754141330719\n",
      "  time_this_iter_s: 23.04879140853882\n",
      "  time_total_s: 15026.754141330719\n",
      "  timestamp: 1550808458\n",
      "  timesteps_since_restore: 6360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6360000\n",
      "  training_iteration: 636\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15026 s, 636 iter, 6360000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-08-01\n",
      "  done: false\n",
      "  episode_len_mean: 91.42727272727272\n",
      "  episode_reward_max: 219.53448648360845\n",
      "  episode_reward_mean: 177.8034026050417\n",
      "  episode_reward_min: 147.84617405226703\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 66648\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.741\n",
      "    load_time_ms: 2.217\n",
      "    num_steps_sampled: 6370000\n",
      "    num_steps_trained: 6370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09047834575176239\n",
      "      kl: 0.018069474026560783\n",
      "      policy_loss: 0.0014972856733947992\n",
      "      total_loss: 5.538733959197998\n",
      "      vf_explained_var: 0.9890342950820923\n",
      "      vf_loss: 5.537237167358398\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3446713089942932\n",
      "      kl: 0.013632673770189285\n",
      "      policy_loss: 0.0004783651093021035\n",
      "      total_loss: 3.700037717819214\n",
      "      vf_explained_var: 0.9957696795463562\n",
      "      vf_loss: 3.699558734893799\n",
      "    sample_time_ms: 20117.647\n",
      "    update_time_ms: 6.994\n",
      "  iterations_since_restore: 637\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.23620178028538\n",
      "    rl_1: 103.5672008247563\n",
      "  time_since_restore: 15049.783749103546\n",
      "  time_this_iter_s: 23.02960777282715\n",
      "  time_total_s: 15049.783749103546\n",
      "  timestamp: 1550808481\n",
      "  timesteps_since_restore: 6370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6370000\n",
      "  training_iteration: 637\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15049 s, 637 iter, 6370000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-08-24\n",
      "  done: false\n",
      "  episode_len_mean: 90.27272727272727\n",
      "  episode_reward_max: 225.15215801759786\n",
      "  episode_reward_mean: 172.7913820558694\n",
      "  episode_reward_min: -155.78647234654898\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 66758\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.098\n",
      "    load_time_ms: 2.197\n",
      "    num_steps_sampled: 6380000\n",
      "    num_steps_trained: 6380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12809279561042786\n",
      "      kl: 0.015912486240267754\n",
      "      policy_loss: -0.005460737738758326\n",
      "      total_loss: 42.21195602416992\n",
      "      vf_explained_var: 0.9407187104225159\n",
      "      vf_loss: 42.2174186706543\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3939807116985321\n",
      "      kl: 0.011632032692432404\n",
      "      policy_loss: 0.00047374487621709704\n",
      "      total_loss: 66.81324768066406\n",
      "      vf_explained_var: 0.9384499788284302\n",
      "      vf_loss: 66.8127670288086\n",
      "    sample_time_ms: 20060.182\n",
      "    update_time_ms: 6.989\n",
      "  iterations_since_restore: 638\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.82356203175941\n",
      "    rl_1: 100.96782002410994\n",
      "  time_since_restore: 15072.701608419418\n",
      "  time_this_iter_s: 22.917859315872192\n",
      "  time_total_s: 15072.701608419418\n",
      "  timestamp: 1550808504\n",
      "  timesteps_since_restore: 6380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6380000\n",
      "  training_iteration: 638\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15072 s, 638 iter, 6380000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-08-48\n",
      "  done: false\n",
      "  episode_len_mean: 91.27272727272727\n",
      "  episode_reward_max: 222.80904776652116\n",
      "  episode_reward_mean: 174.96872555420413\n",
      "  episode_reward_min: -167.4998827806497\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 66868\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.366\n",
      "    load_time_ms: 2.272\n",
      "    num_steps_sampled: 6390000\n",
      "    num_steps_trained: 6390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10611898452043533\n",
      "      kl: 0.018759647384285927\n",
      "      policy_loss: -0.0035504125989973545\n",
      "      total_loss: 39.49079513549805\n",
      "      vf_explained_var: 0.9442170858383179\n",
      "      vf_loss: 39.49434280395508\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3054634630680084\n",
      "      kl: 0.012244979850947857\n",
      "      policy_loss: -0.001945242052897811\n",
      "      total_loss: 50.255043029785156\n",
      "      vf_explained_var: 0.9564915299415588\n",
      "      vf_loss: 50.256988525390625\n",
      "    sample_time_ms: 20080.033\n",
      "    update_time_ms: 7.485\n",
      "  iterations_since_restore: 639\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.27434703597187\n",
      "    rl_1: 101.69437851823226\n",
      "  time_since_restore: 15096.121108293533\n",
      "  time_this_iter_s: 23.41949987411499\n",
      "  time_total_s: 15096.121108293533\n",
      "  timestamp: 1550808528\n",
      "  timesteps_since_restore: 6390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6390000\n",
      "  training_iteration: 639\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15096 s, 639 iter, 6390000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-09-11\n",
      "  done: false\n",
      "  episode_len_mean: 91.02727272727273\n",
      "  episode_reward_max: 222.1132908119571\n",
      "  episode_reward_mean: 172.58847687248516\n",
      "  episode_reward_min: -154.47142065991048\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 66978\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.854\n",
      "    load_time_ms: 2.372\n",
      "    num_steps_sampled: 6400000\n",
      "    num_steps_trained: 6400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08161045610904694\n",
      "      kl: 0.02304127626121044\n",
      "      policy_loss: -0.0007905973470769823\n",
      "      total_loss: 38.26313781738281\n",
      "      vf_explained_var: 0.9413055181503296\n",
      "      vf_loss: 38.26392364501953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2750631868839264\n",
      "      kl: 0.023807857185602188\n",
      "      policy_loss: -0.0028814382385462523\n",
      "      total_loss: 57.61187744140625\n",
      "      vf_explained_var: 0.9455450177192688\n",
      "      vf_loss: 57.61477279663086\n",
      "    sample_time_ms: 20165.194\n",
      "    update_time_ms: 7.28\n",
      "  iterations_since_restore: 640\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.06569267991158\n",
      "    rl_1: 101.5227841925736\n",
      "  time_since_restore: 15119.653094291687\n",
      "  time_this_iter_s: 23.531985998153687\n",
      "  time_total_s: 15119.653094291687\n",
      "  timestamp: 1550808551\n",
      "  timesteps_since_restore: 6400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6400000\n",
      "  training_iteration: 640\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15119 s, 640 iter, 6400000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-09-35\n",
      "  done: false\n",
      "  episode_len_mean: 90.45045045045045\n",
      "  episode_reward_max: 226.08006977699273\n",
      "  episode_reward_mean: 173.82184240640538\n",
      "  episode_reward_min: -160.67814570569791\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 67089\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.551\n",
      "    load_time_ms: 2.35\n",
      "    num_steps_sampled: 6410000\n",
      "    num_steps_trained: 6410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1328854262828827\n",
      "      kl: 0.017516035586595535\n",
      "      policy_loss: -0.000460676645161584\n",
      "      total_loss: 33.462432861328125\n",
      "      vf_explained_var: 0.950299859046936\n",
      "      vf_loss: 33.462890625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2567574083805084\n",
      "      kl: 0.0288617555052042\n",
      "      policy_loss: -0.0028590469155460596\n",
      "      total_loss: 49.430809020996094\n",
      "      vf_explained_var: 0.9522145390510559\n",
      "      vf_loss: 49.43366622924805\n",
      "    sample_time_ms: 20133.671\n",
      "    update_time_ms: 7.482\n",
      "  iterations_since_restore: 641\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.01799266230555\n",
      "    rl_1: 100.80384974409981\n",
      "  time_since_restore: 15142.858062028885\n",
      "  time_this_iter_s: 23.204967737197876\n",
      "  time_total_s: 15142.858062028885\n",
      "  timestamp: 1550808575\n",
      "  timesteps_since_restore: 6410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6410000\n",
      "  training_iteration: 641\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15142 s, 641 iter, 6410000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-09-58\n",
      "  done: false\n",
      "  episode_len_mean: 90.75454545454545\n",
      "  episode_reward_max: 217.50625166396017\n",
      "  episode_reward_mean: 178.87481618960945\n",
      "  episode_reward_min: -156.85476424939466\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 67199\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.667\n",
      "    load_time_ms: 2.388\n",
      "    num_steps_sampled: 6420000\n",
      "    num_steps_trained: 6420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.16935758292675018\n",
      "      kl: 0.02246507816016674\n",
      "      policy_loss: -0.000585551664698869\n",
      "      total_loss: 25.54501724243164\n",
      "      vf_explained_var: 0.9616421461105347\n",
      "      vf_loss: 25.54560089111328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2665185034275055\n",
      "      kl: 0.013191021047532558\n",
      "      policy_loss: -0.00432501221075654\n",
      "      total_loss: 39.17720413208008\n",
      "      vf_explained_var: 0.9638168215751648\n",
      "      vf_loss: 39.18153381347656\n",
      "    sample_time_ms: 20132.718\n",
      "    update_time_ms: 7.311\n",
      "  iterations_since_restore: 642\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.87445179860953\n",
      "    rl_1: 104.0003643909999\n",
      "  time_since_restore: 15166.352922439575\n",
      "  time_this_iter_s: 23.494860410690308\n",
      "  time_total_s: 15166.352922439575\n",
      "  timestamp: 1550808598\n",
      "  timesteps_since_restore: 6420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6420000\n",
      "  training_iteration: 642\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15166 s, 642 iter, 6420000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-10-22\n",
      "  done: false\n",
      "  episode_len_mean: 91.87155963302752\n",
      "  episode_reward_max: 224.03979258485063\n",
      "  episode_reward_mean: 177.12744016418117\n",
      "  episode_reward_min: -156.296805504462\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 67308\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.005\n",
      "    load_time_ms: 2.39\n",
      "    num_steps_sampled: 6430000\n",
      "    num_steps_trained: 6430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1345575451850891\n",
      "      kl: 0.018872572109103203\n",
      "      policy_loss: -0.005004947539418936\n",
      "      total_loss: 42.69377136230469\n",
      "      vf_explained_var: 0.9405378699302673\n",
      "      vf_loss: 42.69878387451172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3032688796520233\n",
      "      kl: 0.31960001587867737\n",
      "      policy_loss: 0.0008710293914191425\n",
      "      total_loss: 68.36929321289062\n",
      "      vf_explained_var: 0.9391418695449829\n",
      "      vf_loss: 68.3684310913086\n",
      "    sample_time_ms: 20151.864\n",
      "    update_time_ms: 7.248\n",
      "  iterations_since_restore: 643\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.39113312484972\n",
      "    rl_1: 104.73630703933144\n",
      "  time_since_restore: 15189.666335821152\n",
      "  time_this_iter_s: 23.313413381576538\n",
      "  time_total_s: 15189.666335821152\n",
      "  timestamp: 1550808622\n",
      "  timesteps_since_restore: 6430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6430000\n",
      "  training_iteration: 643\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15189 s, 643 iter, 6430000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-10-44\n",
      "  done: false\n",
      "  episode_len_mean: 91.78899082568807\n",
      "  episode_reward_max: 224.13325839670318\n",
      "  episode_reward_mean: 180.96719494243087\n",
      "  episode_reward_min: -158.71337565191052\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 67417\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.847\n",
      "    load_time_ms: 2.357\n",
      "    num_steps_sampled: 6440000\n",
      "    num_steps_trained: 6440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.14253732562065125\n",
      "      kl: 0.022717785090208054\n",
      "      policy_loss: -0.005738220177590847\n",
      "      total_loss: 25.76043701171875\n",
      "      vf_explained_var: 0.9593199491500854\n",
      "      vf_loss: 25.766178131103516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.298254132270813\n",
      "      kl: 0.011702112853527069\n",
      "      policy_loss: -0.0016530507709831\n",
      "      total_loss: 39.38221740722656\n",
      "      vf_explained_var: 0.963230550289154\n",
      "      vf_loss: 39.38386917114258\n",
      "    sample_time_ms: 20078.446\n",
      "    update_time_ms: 7.528\n",
      "  iterations_since_restore: 644\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.95273558979123\n",
      "    rl_1: 106.01445935263968\n",
      "  time_since_restore: 15212.50006866455\n",
      "  time_this_iter_s: 22.833732843399048\n",
      "  time_total_s: 15212.50006866455\n",
      "  timestamp: 1550808644\n",
      "  timesteps_since_restore: 6440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6440000\n",
      "  training_iteration: 644\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15212 s, 644 iter, 6440000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-11-08\n",
      "  done: false\n",
      "  episode_len_mean: 91.81481481481481\n",
      "  episode_reward_max: 224.30374629564204\n",
      "  episode_reward_mean: 180.28800021903626\n",
      "  episode_reward_min: 148.82605192478619\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 67525\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.485\n",
      "    load_time_ms: 2.413\n",
      "    num_steps_sampled: 6450000\n",
      "    num_steps_trained: 6450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06661023944616318\n",
      "      kl: 0.01831035129725933\n",
      "      policy_loss: 0.00015255837934091687\n",
      "      total_loss: 3.733290910720825\n",
      "      vf_explained_var: 0.9931367039680481\n",
      "      vf_loss: 3.733137845993042\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2632235884666443\n",
      "      kl: 0.01208886131644249\n",
      "      policy_loss: 0.0005432255566120148\n",
      "      total_loss: 3.0442569255828857\n",
      "      vf_explained_var: 0.9966804385185242\n",
      "      vf_loss: 3.0437140464782715\n",
      "    sample_time_ms: 20054.211\n",
      "    update_time_ms: 7.526\n",
      "  iterations_since_restore: 645\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.18892885144841\n",
      "    rl_1: 104.09907136758787\n",
      "  time_since_restore: 15235.83015537262\n",
      "  time_this_iter_s: 23.330086708068848\n",
      "  time_total_s: 15235.83015537262\n",
      "  timestamp: 1550808668\n",
      "  timesteps_since_restore: 6450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6450000\n",
      "  training_iteration: 645\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15235 s, 645 iter, 6450000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-11-31\n",
      "  done: false\n",
      "  episode_len_mean: 91.33636363636364\n",
      "  episode_reward_max: 224.0053533352508\n",
      "  episode_reward_mean: 180.6898050568985\n",
      "  episode_reward_min: 145.2401192377076\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 67635\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.872\n",
      "    load_time_ms: 2.374\n",
      "    num_steps_sampled: 6460000\n",
      "    num_steps_trained: 6460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08276143670082092\n",
      "      kl: 0.02033158205449581\n",
      "      policy_loss: 0.0032661419827491045\n",
      "      total_loss: 3.5745365619659424\n",
      "      vf_explained_var: 0.993520200252533\n",
      "      vf_loss: 3.571269989013672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3140031099319458\n",
      "      kl: 0.016113078221678734\n",
      "      policy_loss: -0.0009700875380076468\n",
      "      total_loss: 3.088552713394165\n",
      "      vf_explained_var: 0.9966740608215332\n",
      "      vf_loss: 3.089522361755371\n",
      "    sample_time_ms: 20085.269\n",
      "    update_time_ms: 7.685\n",
      "  iterations_since_restore: 646\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.83397541113483\n",
      "    rl_1: 104.85582964576365\n",
      "  time_since_restore: 15259.19179058075\n",
      "  time_this_iter_s: 23.361635208129883\n",
      "  time_total_s: 15259.19179058075\n",
      "  timestamp: 1550808691\n",
      "  timesteps_since_restore: 6460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6460000\n",
      "  training_iteration: 646\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15259 s, 646 iter, 6460000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-11-55\n",
      "  done: false\n",
      "  episode_len_mean: 90.75454545454545\n",
      "  episode_reward_max: 224.76114256271856\n",
      "  episode_reward_mean: 178.04529263181416\n",
      "  episode_reward_min: 148.83972869662765\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 67745\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.268\n",
      "    load_time_ms: 2.389\n",
      "    num_steps_sampled: 6470000\n",
      "    num_steps_trained: 6470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12775839865207672\n",
      "      kl: 0.02149694785475731\n",
      "      policy_loss: 0.004300933331251144\n",
      "      total_loss: 2.1494624614715576\n",
      "      vf_explained_var: 0.9955892562866211\n",
      "      vf_loss: 2.1451618671417236\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3068239390850067\n",
      "      kl: 0.01955566741526127\n",
      "      policy_loss: 0.005509805865585804\n",
      "      total_loss: 2.510295867919922\n",
      "      vf_explained_var: 0.9972607493400574\n",
      "      vf_loss: 2.504786252975464\n",
      "    sample_time_ms: 20121.584\n",
      "    update_time_ms: 7.667\n",
      "  iterations_since_restore: 647\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.35345998119824\n",
      "    rl_1: 103.6918326506159\n",
      "  time_since_restore: 15282.567164421082\n",
      "  time_this_iter_s: 23.37537384033203\n",
      "  time_total_s: 15282.567164421082\n",
      "  timestamp: 1550808715\n",
      "  timesteps_since_restore: 6470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6470000\n",
      "  training_iteration: 647\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15282 s, 647 iter, 6470000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-12-18\n",
      "  done: false\n",
      "  episode_len_mean: 91.70642201834862\n",
      "  episode_reward_max: 224.90107428593987\n",
      "  episode_reward_mean: 179.45390013343626\n",
      "  episode_reward_min: 144.2859326137631\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 67854\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.788\n",
      "    load_time_ms: 2.422\n",
      "    num_steps_sampled: 6480000\n",
      "    num_steps_trained: 6480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11099817603826523\n",
      "      kl: 0.0176699161529541\n",
      "      policy_loss: -0.0005090797203592956\n",
      "      total_loss: 2.2892167568206787\n",
      "      vf_explained_var: 0.9955140948295593\n",
      "      vf_loss: 2.2897260189056396\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.30190759897232056\n",
      "      kl: 0.01452108658850193\n",
      "      policy_loss: 0.0019703758880496025\n",
      "      total_loss: 2.7738940715789795\n",
      "      vf_explained_var: 0.9971030950546265\n",
      "      vf_loss: 2.7719240188598633\n",
      "    sample_time_ms: 20175.817\n",
      "    update_time_ms: 7.75\n",
      "  iterations_since_restore: 648\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.02359484489519\n",
      "    rl_1: 105.43030528854116\n",
      "  time_since_restore: 15306.044887065887\n",
      "  time_this_iter_s: 23.477722644805908\n",
      "  time_total_s: 15306.044887065887\n",
      "  timestamp: 1550808738\n",
      "  timesteps_since_restore: 6480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6480000\n",
      "  training_iteration: 648\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15306 s, 648 iter, 6480000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-12-42\n",
      "  done: false\n",
      "  episode_len_mean: 90.7927927927928\n",
      "  episode_reward_max: 225.7795941183314\n",
      "  episode_reward_mean: 177.63001853077688\n",
      "  episode_reward_min: -155.4670833150917\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 67965\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.096\n",
      "    load_time_ms: 2.412\n",
      "    num_steps_sampled: 6490000\n",
      "    num_steps_trained: 6490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11022930592298508\n",
      "      kl: 0.007730244193226099\n",
      "      policy_loss: -0.0018309748265892267\n",
      "      total_loss: 40.65203857421875\n",
      "      vf_explained_var: 0.9440237879753113\n",
      "      vf_loss: 40.65386962890625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2921612858772278\n",
      "      kl: 0.05105355754494667\n",
      "      policy_loss: 0.008553753606975079\n",
      "      total_loss: 71.23767852783203\n",
      "      vf_explained_var: 0.9358276724815369\n",
      "      vf_loss: 71.2291259765625\n",
      "    sample_time_ms: 20202.174\n",
      "    update_time_ms: 7.246\n",
      "  iterations_since_restore: 649\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.9980324900796\n",
      "    rl_1: 102.63198604069726\n",
      "  time_since_restore: 15329.715149402618\n",
      "  time_this_iter_s: 23.670262336730957\n",
      "  time_total_s: 15329.715149402618\n",
      "  timestamp: 1550808762\n",
      "  timesteps_since_restore: 6490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6490000\n",
      "  training_iteration: 649\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15329 s, 649 iter, 6490000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-13-05\n",
      "  done: false\n",
      "  episode_len_mean: 91.31192660550458\n",
      "  episode_reward_max: 223.78455717027902\n",
      "  episode_reward_mean: 177.27104169003456\n",
      "  episode_reward_min: 143.86107493206907\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 68074\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.757\n",
      "    load_time_ms: 2.416\n",
      "    num_steps_sampled: 6500000\n",
      "    num_steps_trained: 6500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11936338990926743\n",
      "      kl: 0.018680747598409653\n",
      "      policy_loss: 0.0006752089248038828\n",
      "      total_loss: 2.4256510734558105\n",
      "      vf_explained_var: 0.9951198101043701\n",
      "      vf_loss: 2.424975872039795\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3444265127182007\n",
      "      kl: 0.01201867125928402\n",
      "      policy_loss: 0.0009269790607504547\n",
      "      total_loss: 2.3459858894348145\n",
      "      vf_explained_var: 0.9973859786987305\n",
      "      vf_loss: 2.3450584411621094\n",
      "    sample_time_ms: 20174.92\n",
      "    update_time_ms: 7.292\n",
      "  iterations_since_restore: 650\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.07693660453613\n",
      "    rl_1: 103.19410508549846\n",
      "  time_since_restore: 15352.989519357681\n",
      "  time_this_iter_s: 23.274369955062866\n",
      "  time_total_s: 15352.989519357681\n",
      "  timestamp: 1550808785\n",
      "  timesteps_since_restore: 6500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6500000\n",
      "  training_iteration: 650\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15352 s, 650 iter, 6500000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-13-28\n",
      "  done: false\n",
      "  episode_len_mean: 91.41284403669725\n",
      "  episode_reward_max: 219.49714041216706\n",
      "  episode_reward_mean: 177.96313866510602\n",
      "  episode_reward_min: -157.75921412619817\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 68183\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.593\n",
      "    load_time_ms: 2.406\n",
      "    num_steps_sampled: 6510000\n",
      "    num_steps_trained: 6510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08468770235776901\n",
      "      kl: 0.013867609202861786\n",
      "      policy_loss: -0.0004712051304522902\n",
      "      total_loss: 23.516483306884766\n",
      "      vf_explained_var: 0.9634251594543457\n",
      "      vf_loss: 23.51695442199707\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2666392922401428\n",
      "      kl: 0.026008373126387596\n",
      "      policy_loss: -0.0011394524481147528\n",
      "      total_loss: 39.11929702758789\n",
      "      vf_explained_var: 0.9615986347198486\n",
      "      vf_loss: 39.12043380737305\n",
      "    sample_time_ms: 20137.091\n",
      "    update_time_ms: 7.155\n",
      "  iterations_since_restore: 651\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.01421505372932\n",
      "    rl_1: 102.94892361137669\n",
      "  time_since_restore: 15375.780972480774\n",
      "  time_this_iter_s: 22.79145312309265\n",
      "  time_total_s: 15375.780972480774\n",
      "  timestamp: 1550808808\n",
      "  timesteps_since_restore: 6510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6510000\n",
      "  training_iteration: 651\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15375 s, 651 iter, 6510000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-13-52\n",
      "  done: false\n",
      "  episode_len_mean: 92.22935779816514\n",
      "  episode_reward_max: 224.8161120476371\n",
      "  episode_reward_mean: 180.9572113557149\n",
      "  episode_reward_min: -75.11620334281216\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 68292\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.636\n",
      "    load_time_ms: 2.36\n",
      "    num_steps_sampled: 6520000\n",
      "    num_steps_trained: 6520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1302453875541687\n",
      "      kl: 0.02580898627638817\n",
      "      policy_loss: -0.0004927842528559268\n",
      "      total_loss: 11.04786491394043\n",
      "      vf_explained_var: 0.9841042757034302\n",
      "      vf_loss: 11.048357963562012\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.29998695850372314\n",
      "      kl: 0.00817115232348442\n",
      "      policy_loss: -0.0006776463706046343\n",
      "      total_loss: 11.707413673400879\n",
      "      vf_explained_var: 0.9895767569541931\n",
      "      vf_loss: 11.708091735839844\n",
      "    sample_time_ms: 20140.625\n",
      "    update_time_ms: 7.388\n",
      "  iterations_since_restore: 652\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.27349882009828\n",
      "    rl_1: 104.68371253561658\n",
      "  time_since_restore: 15399.52422785759\n",
      "  time_this_iter_s: 23.743255376815796\n",
      "  time_total_s: 15399.52422785759\n",
      "  timestamp: 1550808832\n",
      "  timesteps_since_restore: 6520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6520000\n",
      "  training_iteration: 652\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15399 s, 652 iter, 6520000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-14-15\n",
      "  done: false\n",
      "  episode_len_mean: 91.06363636363636\n",
      "  episode_reward_max: 222.97791867101267\n",
      "  episode_reward_mean: 180.5395993089568\n",
      "  episode_reward_min: 142.7672796859637\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 68402\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.721\n",
      "    load_time_ms: 2.383\n",
      "    num_steps_sampled: 6530000\n",
      "    num_steps_trained: 6530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10928506404161453\n",
      "      kl: 0.02560039423406124\n",
      "      policy_loss: -0.00023227185010910034\n",
      "      total_loss: 2.5660955905914307\n",
      "      vf_explained_var: 0.9949501156806946\n",
      "      vf_loss: 2.5663273334503174\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.27941346168518066\n",
      "      kl: 0.010274571366608143\n",
      "      policy_loss: 0.0013401779578998685\n",
      "      total_loss: 2.554534673690796\n",
      "      vf_explained_var: 0.9972572326660156\n",
      "      vf_loss: 2.553194522857666\n",
      "    sample_time_ms: 20156.114\n",
      "    update_time_ms: 7.534\n",
      "  iterations_since_restore: 653\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.96215910882859\n",
      "    rl_1: 104.57744020012818\n",
      "  time_since_restore: 15422.84316778183\n",
      "  time_this_iter_s: 23.318939924240112\n",
      "  time_total_s: 15422.84316778183\n",
      "  timestamp: 1550808855\n",
      "  timesteps_since_restore: 6530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6530000\n",
      "  training_iteration: 653\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15422 s, 653 iter, 6530000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-14-39\n",
      "  done: false\n",
      "  episode_len_mean: 91.22935779816514\n",
      "  episode_reward_max: 222.11389388131144\n",
      "  episode_reward_mean: 177.99281519979212\n",
      "  episode_reward_min: -151.34777462585168\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 68511\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.201\n",
      "    load_time_ms: 2.39\n",
      "    num_steps_sampled: 6540000\n",
      "    num_steps_trained: 6540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.15511468052864075\n",
      "      kl: 0.015935948118567467\n",
      "      policy_loss: 0.0007070161518640816\n",
      "      total_loss: 19.08758544921875\n",
      "      vf_explained_var: 0.9712605476379395\n",
      "      vf_loss: 19.086877822875977\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.307025283575058\n",
      "      kl: 0.021276503801345825\n",
      "      policy_loss: -0.006498577538877726\n",
      "      total_loss: 32.829776763916016\n",
      "      vf_explained_var: 0.96854567527771\n",
      "      vf_loss: 32.836280822753906\n",
      "    sample_time_ms: 20214.628\n",
      "    update_time_ms: 7.16\n",
      "  iterations_since_restore: 654\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.3140823994077\n",
      "    rl_1: 102.67873280038438\n",
      "  time_since_restore: 15446.355067253113\n",
      "  time_this_iter_s: 23.51189947128296\n",
      "  time_total_s: 15446.355067253113\n",
      "  timestamp: 1550808879\n",
      "  timesteps_since_restore: 6540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6540000\n",
      "  training_iteration: 654\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15446 s, 654 iter, 6540000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-15-03\n",
      "  done: false\n",
      "  episode_len_mean: 91.38181818181818\n",
      "  episode_reward_max: 215.98103785828707\n",
      "  episode_reward_mean: 174.99390516914127\n",
      "  episode_reward_min: -153.2166100682798\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 68621\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.449\n",
      "    load_time_ms: 2.32\n",
      "    num_steps_sampled: 6550000\n",
      "    num_steps_trained: 6550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13047093152999878\n",
      "      kl: 0.03480612859129906\n",
      "      policy_loss: -0.008637750521302223\n",
      "      total_loss: 56.16267395019531\n",
      "      vf_explained_var: 0.9193482995033264\n",
      "      vf_loss: 56.17129898071289\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.30232614278793335\n",
      "      kl: 0.012927941977977753\n",
      "      policy_loss: 7.307080522878096e-05\n",
      "      total_loss: 77.4613037109375\n",
      "      vf_explained_var: 0.9284875392913818\n",
      "      vf_loss: 77.46123504638672\n",
      "    sample_time_ms: 20265.254\n",
      "    update_time_ms: 7.476\n",
      "  iterations_since_restore: 655\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.41948641740089\n",
      "    rl_1: 102.57441875174037\n",
      "  time_since_restore: 15470.20417356491\n",
      "  time_this_iter_s: 23.849106311798096\n",
      "  time_total_s: 15470.20417356491\n",
      "  timestamp: 1550808903\n",
      "  timesteps_since_restore: 6550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6550000\n",
      "  training_iteration: 655\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15470 s, 655 iter, 6550000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-15-26\n",
      "  done: false\n",
      "  episode_len_mean: 91.03636363636363\n",
      "  episode_reward_max: 220.37081627079033\n",
      "  episode_reward_mean: 171.09686311066437\n",
      "  episode_reward_min: -155.95111977254925\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 68731\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.32\n",
      "    load_time_ms: 2.418\n",
      "    num_steps_sampled: 6560000\n",
      "    num_steps_trained: 6560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10371123999357224\n",
      "      kl: 0.01879926398396492\n",
      "      policy_loss: -0.0006310800672508776\n",
      "      total_loss: 37.99729919433594\n",
      "      vf_explained_var: 0.9423750042915344\n",
      "      vf_loss: 37.9979362487793\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23435071110725403\n",
      "      kl: 0.020588262006640434\n",
      "      policy_loss: -0.007657160516828299\n",
      "      total_loss: 65.28423309326172\n",
      "      vf_explained_var: 0.9379836916923523\n",
      "      vf_loss: 65.2918701171875\n",
      "    sample_time_ms: 20240.268\n",
      "    update_time_ms: 7.245\n",
      "  iterations_since_restore: 656\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.39793805375515\n",
      "    rl_1: 100.69892505690916\n",
      "  time_since_restore: 15493.325444936752\n",
      "  time_this_iter_s: 23.12127137184143\n",
      "  time_total_s: 15493.325444936752\n",
      "  timestamp: 1550808926\n",
      "  timesteps_since_restore: 6560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6560000\n",
      "  training_iteration: 656\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15493 s, 656 iter, 6560000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-15-50\n",
      "  done: false\n",
      "  episode_len_mean: 90.85321100917432\n",
      "  episode_reward_max: 224.56134934409206\n",
      "  episode_reward_mean: 174.55602425237643\n",
      "  episode_reward_min: -164.28858924279035\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 68840\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3166.477\n",
      "    load_time_ms: 2.412\n",
      "    num_steps_sampled: 6570000\n",
      "    num_steps_trained: 6570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11396801471710205\n",
      "      kl: 0.02417333982884884\n",
      "      policy_loss: -0.0024948162026703358\n",
      "      total_loss: 25.18672752380371\n",
      "      vf_explained_var: 0.9618055820465088\n",
      "      vf_loss: 25.189218521118164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22706256806850433\n",
      "      kl: 0.00857033021748066\n",
      "      policy_loss: -0.0013132758904248476\n",
      "      total_loss: 34.90386962890625\n",
      "      vf_explained_var: 0.96659916639328\n",
      "      vf_loss: 34.90517807006836\n",
      "    sample_time_ms: 20242.782\n",
      "    update_time_ms: 7.223\n",
      "  iterations_since_restore: 657\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.68626497902439\n",
      "    rl_1: 101.86975927335202\n",
      "  time_since_restore: 15516.93564081192\n",
      "  time_this_iter_s: 23.610195875167847\n",
      "  time_total_s: 15516.93564081192\n",
      "  timestamp: 1550808950\n",
      "  timesteps_since_restore: 6570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6570000\n",
      "  training_iteration: 657\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15516 s, 657 iter, 6570000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-16-13\n",
      "  done: false\n",
      "  episode_len_mean: 90.70270270270271\n",
      "  episode_reward_max: 225.2248721697533\n",
      "  episode_reward_mean: 173.2869424253469\n",
      "  episode_reward_min: -162.40657159997994\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 68951\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3166.457\n",
      "    load_time_ms: 2.418\n",
      "    num_steps_sampled: 6580000\n",
      "    num_steps_trained: 6580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09874250739812851\n",
      "      kl: 0.2607567012310028\n",
      "      policy_loss: 0.0024082253221422434\n",
      "      total_loss: 67.318115234375\n",
      "      vf_explained_var: 0.9031057953834534\n",
      "      vf_loss: 67.31570434570312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.20867440104484558\n",
      "      kl: 0.03013577125966549\n",
      "      policy_loss: -0.0025496084708720446\n",
      "      total_loss: 93.10318756103516\n",
      "      vf_explained_var: 0.9133718609809875\n",
      "      vf_loss: 93.10575103759766\n",
      "    sample_time_ms: 20211.897\n",
      "    update_time_ms: 7.13\n",
      "  iterations_since_restore: 658\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.99488751580341\n",
      "    rl_1: 101.29205490954345\n",
      "  time_since_restore: 15540.101895332336\n",
      "  time_this_iter_s: 23.16625452041626\n",
      "  time_total_s: 15540.101895332336\n",
      "  timestamp: 1550808973\n",
      "  timesteps_since_restore: 6580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6580000\n",
      "  training_iteration: 658\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15540 s, 658 iter, 6580000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 87.98245614035088\n",
      "  episode_reward_max: 220.52068356531743\n",
      "  episode_reward_mean: 152.4097702928476\n",
      "  episode_reward_min: -169.1706557055045\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 69065\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3166.336\n",
      "    load_time_ms: 2.475\n",
      "    num_steps_sampled: 6590000\n",
      "    num_steps_trained: 6590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11973758786916733\n",
      "      kl: 0.017128273844718933\n",
      "      policy_loss: -0.003320915624499321\n",
      "      total_loss: 260.4097900390625\n",
      "      vf_explained_var: 0.7060306072235107\n",
      "      vf_loss: 260.4130859375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.238718181848526\n",
      "      kl: 0.027248183265328407\n",
      "      policy_loss: -0.0004031755670439452\n",
      "      total_loss: 324.33056640625\n",
      "      vf_explained_var: 0.7453985214233398\n",
      "      vf_loss: 324.3309326171875\n",
      "    sample_time_ms: 20157.717\n",
      "    update_time_ms: 7.227\n",
      "  iterations_since_restore: 659\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.86141065298747\n",
      "    rl_1: 88.54835963986015\n",
      "  time_since_restore: 15563.229650974274\n",
      "  time_this_iter_s: 23.127755641937256\n",
      "  time_total_s: 15563.229650974274\n",
      "  timestamp: 1550808996\n",
      "  timesteps_since_restore: 6590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6590000\n",
      "  training_iteration: 659\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15563 s, 659 iter, 6590000 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-17-00\n",
      "  done: false\n",
      "  episode_len_mean: 90.05454545454545\n",
      "  episode_reward_max: 225.14177759715818\n",
      "  episode_reward_mean: 165.36094009796452\n",
      "  episode_reward_min: -158.78255422816954\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 69175\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3166.703\n",
      "    load_time_ms: 2.357\n",
      "    num_steps_sampled: 6600000\n",
      "    num_steps_trained: 6600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08921685069799423\n",
      "      kl: 0.02064974047243595\n",
      "      policy_loss: -0.0008990292553789914\n",
      "      total_loss: 107.06556701660156\n",
      "      vf_explained_var: 0.8399234414100647\n",
      "      vf_loss: 107.06646728515625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.27533408999443054\n",
      "      kl: 0.020673535764217377\n",
      "      policy_loss: -0.0009206506656482816\n",
      "      total_loss: 137.08010864257812\n",
      "      vf_explained_var: 0.8755740523338318\n",
      "      vf_loss: 137.08102416992188\n",
      "    sample_time_ms: 20186.599\n",
      "    update_time_ms: 7.608\n",
      "  iterations_since_restore: 660\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.0318752183801\n",
      "    rl_1: 97.32906487958438\n",
      "  time_since_restore: 15586.802812576294\n",
      "  time_this_iter_s: 23.573161602020264\n",
      "  time_total_s: 15586.802812576294\n",
      "  timestamp: 1550809020\n",
      "  timesteps_since_restore: 6600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6600000\n",
      "  training_iteration: 660\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15586 s, 660 iter, 6600000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-17-23\n",
      "  done: false\n",
      "  episode_len_mean: 87.8695652173913\n",
      "  episode_reward_max: 221.95464800554043\n",
      "  episode_reward_mean: 156.3375161687243\n",
      "  episode_reward_min: -160.62213926897527\n",
      "  episodes_this_iter: 115\n",
      "  episodes_total: 69290\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3171.26\n",
      "    load_time_ms: 2.391\n",
      "    num_steps_sampled: 6610000\n",
      "    num_steps_trained: 6610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13847015798091888\n",
      "      kl: 0.026229290291666985\n",
      "      policy_loss: -0.00270872307009995\n",
      "      total_loss: 204.15863037109375\n",
      "      vf_explained_var: 0.7668319344520569\n",
      "      vf_loss: 204.16134643554688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.296815425157547\n",
      "      kl: 0.016784433275461197\n",
      "      policy_loss: 0.0024828140158206224\n",
      "      total_loss: 277.22723388671875\n",
      "      vf_explained_var: 0.7772188186645508\n",
      "      vf_loss: 277.22479248046875\n",
      "    sample_time_ms: 20211.406\n",
      "    update_time_ms: 7.784\n",
      "  iterations_since_restore: 661\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.36389036698837\n",
      "    rl_1: 91.97362580173598\n",
      "  time_since_restore: 15609.891571044922\n",
      "  time_this_iter_s: 23.08875846862793\n",
      "  time_total_s: 15609.891571044922\n",
      "  timestamp: 1550809043\n",
      "  timesteps_since_restore: 6610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6610000\n",
      "  training_iteration: 661\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15609 s, 661 iter, 6610000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-17-46\n",
      "  done: false\n",
      "  episode_len_mean: 92.05607476635514\n",
      "  episode_reward_max: 226.5073240479727\n",
      "  episode_reward_mean: 170.65456677645543\n",
      "  episode_reward_min: -154.8005517550373\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 69397\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3153.68\n",
      "    load_time_ms: 2.401\n",
      "    num_steps_sampled: 6620000\n",
      "    num_steps_trained: 6620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.026793885976076126\n",
      "      kl: 0.027342664077878\n",
      "      policy_loss: -0.0009647649712860584\n",
      "      total_loss: 82.27345275878906\n",
      "      vf_explained_var: 0.8728327751159668\n",
      "      vf_loss: 82.27442932128906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22003205120563507\n",
      "      kl: 0.014791291207075119\n",
      "      policy_loss: -0.000558431725949049\n",
      "      total_loss: 98.73188781738281\n",
      "      vf_explained_var: 0.9042108058929443\n",
      "      vf_loss: 98.73245239257812\n",
      "    sample_time_ms: 20157.998\n",
      "    update_time_ms: 7.796\n",
      "  iterations_since_restore: 662\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.9285288279274\n",
      "    rl_1: 101.72603794852803\n",
      "  time_since_restore: 15632.925061702728\n",
      "  time_this_iter_s: 23.033490657806396\n",
      "  time_total_s: 15632.925061702728\n",
      "  timestamp: 1550809066\n",
      "  timesteps_since_restore: 6620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6620000\n",
      "  training_iteration: 662\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15632 s, 662 iter, 6620000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 89.72321428571429\n",
      "  episode_reward_max: 225.29314040223178\n",
      "  episode_reward_mean: 168.6133662028034\n",
      "  episode_reward_min: -166.25377444582298\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 69509\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3153.389\n",
      "    load_time_ms: 2.504\n",
      "    num_steps_sampled: 6630000\n",
      "    num_steps_trained: 6630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11922725290060043\n",
      "      kl: 0.023216968402266502\n",
      "      policy_loss: -0.0005137022235430777\n",
      "      total_loss: 94.77925872802734\n",
      "      vf_explained_var: 0.8688453435897827\n",
      "      vf_loss: 94.77977752685547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2893126904964447\n",
      "      kl: 0.012424733489751816\n",
      "      policy_loss: -0.002404572209343314\n",
      "      total_loss: 130.36349487304688\n",
      "      vf_explained_var: 0.8807501196861267\n",
      "      vf_loss: 130.3658905029297\n",
      "    sample_time_ms: 20213.043\n",
      "    update_time_ms: 7.924\n",
      "  iterations_since_restore: 663\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.49386627854759\n",
      "    rl_1: 99.1194999242558\n",
      "  time_since_restore: 15656.7936398983\n",
      "  time_this_iter_s: 23.8685781955719\n",
      "  time_total_s: 15656.7936398983\n",
      "  timestamp: 1550809090\n",
      "  timesteps_since_restore: 6630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6630000\n",
      "  training_iteration: 663\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15656 s, 663 iter, 6630000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-18-33\n",
      "  done: false\n",
      "  episode_len_mean: 89.53571428571429\n",
      "  episode_reward_max: 221.69125311598816\n",
      "  episode_reward_mean: 163.34548043135328\n",
      "  episode_reward_min: -151.69824684658929\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 69621\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.657\n",
      "    load_time_ms: 2.505\n",
      "    num_steps_sampled: 6640000\n",
      "    num_steps_trained: 6640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0881885290145874\n",
      "      kl: 0.021814819425344467\n",
      "      policy_loss: 0.0011232801480218768\n",
      "      total_loss: 141.26315307617188\n",
      "      vf_explained_var: 0.8180996179580688\n",
      "      vf_loss: 141.26202392578125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3029662072658539\n",
      "      kl: 0.034489963203668594\n",
      "      policy_loss: 0.007982095703482628\n",
      "      total_loss: 194.97705078125\n",
      "      vf_explained_var: 0.8281937837600708\n",
      "      vf_loss: 194.9690704345703\n",
      "    sample_time_ms: 20213.104\n",
      "    update_time_ms: 8.052\n",
      "  iterations_since_restore: 664\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.16078891491154\n",
      "    rl_1: 97.18469151644172\n",
      "  time_since_restore: 15680.23240852356\n",
      "  time_this_iter_s: 23.4387686252594\n",
      "  time_total_s: 15680.23240852356\n",
      "  timestamp: 1550809113\n",
      "  timesteps_since_restore: 6640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6640000\n",
      "  training_iteration: 664\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15680 s, 664 iter, 6640000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-18-56\n",
      "  done: false\n",
      "  episode_len_mean: 89.90990990990991\n",
      "  episode_reward_max: 217.7529109281921\n",
      "  episode_reward_mean: 164.74717937041774\n",
      "  episode_reward_min: -154.07153307877672\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 69732\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.491\n",
      "    load_time_ms: 2.561\n",
      "    num_steps_sampled: 6650000\n",
      "    num_steps_trained: 6650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.05971936136484146\n",
      "      kl: 0.023165855556726456\n",
      "      policy_loss: -0.000576937454752624\n",
      "      total_loss: 94.93838500976562\n",
      "      vf_explained_var: 0.8705177307128906\n",
      "      vf_loss: 94.93895721435547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.24126216769218445\n",
      "      kl: 0.02036539651453495\n",
      "      policy_loss: -0.0017410406144335866\n",
      "      total_loss: 139.06881713867188\n",
      "      vf_explained_var: 0.8639790415763855\n",
      "      vf_loss: 139.07057189941406\n",
      "    sample_time_ms: 20136.824\n",
      "    update_time_ms: 7.799\n",
      "  iterations_since_restore: 665\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.89256386622199\n",
      "    rl_1: 96.85461550419568\n",
      "  time_since_restore: 15703.305521249771\n",
      "  time_this_iter_s: 23.073112726211548\n",
      "  time_total_s: 15703.305521249771\n",
      "  timestamp: 1550809136\n",
      "  timesteps_since_restore: 6650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6650000\n",
      "  training_iteration: 665\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15703 s, 665 iter, 6650000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-19-20\n",
      "  done: false\n",
      "  episode_len_mean: 90.45945945945945\n",
      "  episode_reward_max: 217.8754712779146\n",
      "  episode_reward_mean: 170.7621514000513\n",
      "  episode_reward_min: -141.06205804365516\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 69843\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3159.574\n",
      "    load_time_ms: 2.554\n",
      "    num_steps_sampled: 6660000\n",
      "    num_steps_trained: 6660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.042960360646247864\n",
      "      kl: 0.024302955716848373\n",
      "      policy_loss: 0.002350323833525181\n",
      "      total_loss: 98.28956604003906\n",
      "      vf_explained_var: 0.8484625220298767\n",
      "      vf_loss: 98.2872085571289\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23812247812747955\n",
      "      kl: 0.006636721082031727\n",
      "      policy_loss: -0.001939099165610969\n",
      "      total_loss: 118.36995697021484\n",
      "      vf_explained_var: 0.879792332649231\n",
      "      vf_loss: 118.37191009521484\n",
      "    sample_time_ms: 20173.774\n",
      "    update_time_ms: 7.974\n",
      "  iterations_since_restore: 666\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.31808484143524\n",
      "    rl_1: 99.44406655861606\n",
      "  time_since_restore: 15726.950712680817\n",
      "  time_this_iter_s: 23.645191431045532\n",
      "  time_total_s: 15726.950712680817\n",
      "  timestamp: 1550809160\n",
      "  timesteps_since_restore: 6660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6660000\n",
      "  training_iteration: 666\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15726 s, 666 iter, 6660000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-19-43\n",
      "  done: false\n",
      "  episode_len_mean: 91.1\n",
      "  episode_reward_max: 220.8599484148012\n",
      "  episode_reward_mean: 175.3061958779076\n",
      "  episode_reward_min: -153.52270765480304\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 69953\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.893\n",
      "    load_time_ms: 2.575\n",
      "    num_steps_sampled: 6670000\n",
      "    num_steps_trained: 6670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08624421805143356\n",
      "      kl: 0.017173336818814278\n",
      "      policy_loss: -0.004365055821835995\n",
      "      total_loss: 42.5928955078125\n",
      "      vf_explained_var: 0.9385553002357483\n",
      "      vf_loss: 42.59726333618164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25765764713287354\n",
      "      kl: 0.02086767554283142\n",
      "      policy_loss: -0.001123555819503963\n",
      "      total_loss: 68.62860107421875\n",
      "      vf_explained_var: 0.9326621890068054\n",
      "      vf_loss: 68.62971496582031\n",
      "    sample_time_ms: 20156.147\n",
      "    update_time_ms: 8.276\n",
      "  iterations_since_restore: 667\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.01185486730807\n",
      "    rl_1: 103.2943410105996\n",
      "  time_since_restore: 15750.17041349411\n",
      "  time_this_iter_s: 23.219700813293457\n",
      "  time_total_s: 15750.17041349411\n",
      "  timestamp: 1550809183\n",
      "  timesteps_since_restore: 6670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6670000\n",
      "  training_iteration: 667\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15750 s, 667 iter, 6670000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-20-07\n",
      "  done: false\n",
      "  episode_len_mean: 91.11926605504587\n",
      "  episode_reward_max: 218.33188391790395\n",
      "  episode_reward_mean: 178.88630683586484\n",
      "  episode_reward_min: 147.56587336283687\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 70062\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.764\n",
      "    load_time_ms: 2.571\n",
      "    num_steps_sampled: 6680000\n",
      "    num_steps_trained: 6680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06914480030536652\n",
      "      kl: 0.01309926062822342\n",
      "      policy_loss: -0.0009906921768561006\n",
      "      total_loss: 6.478703498840332\n",
      "      vf_explained_var: 0.9887895584106445\n",
      "      vf_loss: 6.47969388961792\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2761130928993225\n",
      "      kl: 0.021245621144771576\n",
      "      policy_loss: 0.006207461468875408\n",
      "      total_loss: 6.048991680145264\n",
      "      vf_explained_var: 0.9939870834350586\n",
      "      vf_loss: 6.04278564453125\n",
      "    sample_time_ms: 20180.808\n",
      "    update_time_ms: 8.256\n",
      "  iterations_since_restore: 668\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.37025138876302\n",
      "    rl_1: 104.51605544710183\n",
      "  time_since_restore: 15773.601986408234\n",
      "  time_this_iter_s: 23.431572914123535\n",
      "  time_total_s: 15773.601986408234\n",
      "  timestamp: 1550809207\n",
      "  timesteps_since_restore: 6680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6680000\n",
      "  training_iteration: 668\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15773 s, 668 iter, 6680000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-20-30\n",
      "  done: false\n",
      "  episode_len_mean: 91.17272727272727\n",
      "  episode_reward_max: 219.45372365146855\n",
      "  episode_reward_mean: 181.39543331696746\n",
      "  episode_reward_min: 143.4127348925771\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 70172\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.07\n",
      "    load_time_ms: 2.474\n",
      "    num_steps_sampled: 6690000\n",
      "    num_steps_trained: 6690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08090086281299591\n",
      "      kl: 0.013678284361958504\n",
      "      policy_loss: -0.003881637006998062\n",
      "      total_loss: 3.8767285346984863\n",
      "      vf_explained_var: 0.9925059080123901\n",
      "      vf_loss: 3.880610942840576\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2779574394226074\n",
      "      kl: 0.009835726581513882\n",
      "      policy_loss: 0.0004623483691830188\n",
      "      total_loss: 2.9198460578918457\n",
      "      vf_explained_var: 0.9968765377998352\n",
      "      vf_loss: 2.919384002685547\n",
      "    sample_time_ms: 20170.23\n",
      "    update_time_ms: 8.273\n",
      "  iterations_since_restore: 669\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.13624475910736\n",
      "    rl_1: 105.25918855786011\n",
      "  time_since_restore: 15796.605030059814\n",
      "  time_this_iter_s: 23.00304365158081\n",
      "  time_total_s: 15796.605030059814\n",
      "  timestamp: 1550809230\n",
      "  timesteps_since_restore: 6690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6690000\n",
      "  training_iteration: 669\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15796 s, 669 iter, 6690000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-20-53\n",
      "  done: false\n",
      "  episode_len_mean: 91.68807339449542\n",
      "  episode_reward_max: 220.92207435873522\n",
      "  episode_reward_mean: 178.5587174706033\n",
      "  episode_reward_min: 142.15933751331974\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 70281\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.645\n",
      "    load_time_ms: 2.475\n",
      "    num_steps_sampled: 6700000\n",
      "    num_steps_trained: 6700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0831599086523056\n",
      "      kl: 0.01744709350168705\n",
      "      policy_loss: -0.0004627641465049237\n",
      "      total_loss: 3.279836654663086\n",
      "      vf_explained_var: 0.9937524795532227\n",
      "      vf_loss: 3.280299186706543\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28261783719062805\n",
      "      kl: 0.014336435124278069\n",
      "      policy_loss: 0.003085865406319499\n",
      "      total_loss: 2.9873783588409424\n",
      "      vf_explained_var: 0.9968405365943909\n",
      "      vf_loss: 2.9842922687530518\n",
      "    sample_time_ms: 20131.674\n",
      "    update_time_ms: 7.794\n",
      "  iterations_since_restore: 670\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.87480102442419\n",
      "    rl_1: 104.68391644617911\n",
      "  time_since_restore: 15819.757357597351\n",
      "  time_this_iter_s: 23.15232753753662\n",
      "  time_total_s: 15819.757357597351\n",
      "  timestamp: 1550809253\n",
      "  timesteps_since_restore: 6700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6700000\n",
      "  training_iteration: 670\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15819 s, 670 iter, 6700000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-21-16\n",
      "  done: false\n",
      "  episode_len_mean: 90.83636363636364\n",
      "  episode_reward_max: 218.92706490305724\n",
      "  episode_reward_mean: 179.88247918718412\n",
      "  episode_reward_min: -136.28169184997319\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 70391\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.559\n",
      "    load_time_ms: 2.431\n",
      "    num_steps_sampled: 6710000\n",
      "    num_steps_trained: 6710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.07250716537237167\n",
      "      kl: 0.019420884549617767\n",
      "      policy_loss: -0.0018035005778074265\n",
      "      total_loss: 36.584503173828125\n",
      "      vf_explained_var: 0.9431936740875244\n",
      "      vf_loss: 36.586299896240234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2693927586078644\n",
      "      kl: 0.02138408087193966\n",
      "      policy_loss: -0.0008244459750130773\n",
      "      total_loss: 46.09658432006836\n",
      "      vf_explained_var: 0.9562302827835083\n",
      "      vf_loss: 46.09740447998047\n",
      "    sample_time_ms: 20163.505\n",
      "    update_time_ms: 7.4\n",
      "  iterations_since_restore: 671\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.38998029971938\n",
      "    rl_1: 104.4924988874647\n",
      "  time_since_restore: 15843.117510080338\n",
      "  time_this_iter_s: 23.36015248298645\n",
      "  time_total_s: 15843.117510080338\n",
      "  timestamp: 1550809276\n",
      "  timesteps_since_restore: 6710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6710000\n",
      "  training_iteration: 671\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15843 s, 671 iter, 6710000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-21-40\n",
      "  done: false\n",
      "  episode_len_mean: 91.53211009174312\n",
      "  episode_reward_max: 218.3942382610229\n",
      "  episode_reward_mean: 177.9947404872793\n",
      "  episode_reward_min: 139.17509496174952\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 70500\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.45\n",
      "    load_time_ms: 2.459\n",
      "    num_steps_sampled: 6720000\n",
      "    num_steps_trained: 6720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.054737262427806854\n",
      "      kl: 0.01804722100496292\n",
      "      policy_loss: 0.00185812683776021\n",
      "      total_loss: 3.047668933868408\n",
      "      vf_explained_var: 0.9939332604408264\n",
      "      vf_loss: 3.0458109378814697\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35416921973228455\n",
      "      kl: 0.015117947943508625\n",
      "      policy_loss: -0.0019906950183212757\n",
      "      total_loss: 2.680298089981079\n",
      "      vf_explained_var: 0.997137188911438\n",
      "      vf_loss: 2.682288646697998\n",
      "    sample_time_ms: 20183.741\n",
      "    update_time_ms: 7.275\n",
      "  iterations_since_restore: 672\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.26911454728925\n",
      "    rl_1: 104.72562593999001\n",
      "  time_since_restore: 15866.353223085403\n",
      "  time_this_iter_s: 23.235713005065918\n",
      "  time_total_s: 15866.353223085403\n",
      "  timestamp: 1550809300\n",
      "  timesteps_since_restore: 6720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6720000\n",
      "  training_iteration: 672\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15866 s, 672 iter, 6720000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-22-02\n",
      "  done: false\n",
      "  episode_len_mean: 91.36363636363636\n",
      "  episode_reward_max: 219.93410100050392\n",
      "  episode_reward_mean: 172.20639897855276\n",
      "  episode_reward_min: -149.09658720180306\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 70610\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.278\n",
      "    load_time_ms: 2.343\n",
      "    num_steps_sampled: 6730000\n",
      "    num_steps_trained: 6730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.03694140911102295\n",
      "      kl: 0.007317102048546076\n",
      "      policy_loss: -0.0005089414189569652\n",
      "      total_loss: 49.316768646240234\n",
      "      vf_explained_var: 0.9253352284431458\n",
      "      vf_loss: 49.31727981567383\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.30807653069496155\n",
      "      kl: 0.011500443331897259\n",
      "      policy_loss: -0.0008193724206648767\n",
      "      total_loss: 69.13848114013672\n",
      "      vf_explained_var: 0.9360173344612122\n",
      "      vf_loss: 69.13931274414062\n",
      "    sample_time_ms: 20063.454\n",
      "    update_time_ms: 7.436\n",
      "  iterations_since_restore: 673\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.4272324180529\n",
      "    rl_1: 102.77916656049983\n",
      "  time_since_restore: 15889.00905919075\n",
      "  time_this_iter_s: 22.65583610534668\n",
      "  time_total_s: 15889.00905919075\n",
      "  timestamp: 1550809322\n",
      "  timesteps_since_restore: 6730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6730000\n",
      "  training_iteration: 673\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15889 s, 673 iter, 6730000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-22-26\n",
      "  done: false\n",
      "  episode_len_mean: 91.01818181818182\n",
      "  episode_reward_max: 219.29619138766958\n",
      "  episode_reward_mean: 177.1700312964771\n",
      "  episode_reward_min: -144.2985161342138\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 70720\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.732\n",
      "    load_time_ms: 2.519\n",
      "    num_steps_sampled: 6740000\n",
      "    num_steps_trained: 6740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09857518970966339\n",
      "      kl: 0.01968703791499138\n",
      "      policy_loss: 0.003037561196833849\n",
      "      total_loss: 49.02725601196289\n",
      "      vf_explained_var: 0.9245109558105469\n",
      "      vf_loss: 49.02422332763672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2474890798330307\n",
      "      kl: 0.05438126623630524\n",
      "      policy_loss: 0.0014592723455280066\n",
      "      total_loss: 61.33972930908203\n",
      "      vf_explained_var: 0.9444165229797363\n",
      "      vf_loss: 61.33826446533203\n",
      "    sample_time_ms: 20043.534\n",
      "    update_time_ms: 7.457\n",
      "  iterations_since_restore: 674\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.88831434042037\n",
      "    rl_1: 103.28171695605674\n",
      "  time_since_restore: 15912.213521957397\n",
      "  time_this_iter_s: 23.20446276664734\n",
      "  time_total_s: 15912.213521957397\n",
      "  timestamp: 1550809346\n",
      "  timesteps_since_restore: 6740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6740000\n",
      "  training_iteration: 674\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15912 s, 674 iter, 6740000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-22-49\n",
      "  done: false\n",
      "  episode_len_mean: 91.28440366972477\n",
      "  episode_reward_max: 221.67693049791126\n",
      "  episode_reward_mean: 180.2422602407744\n",
      "  episode_reward_min: 143.7158546881561\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 70829\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.271\n",
      "    load_time_ms: 2.468\n",
      "    num_steps_sampled: 6750000\n",
      "    num_steps_trained: 6750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06304167956113815\n",
      "      kl: 0.01722095161676407\n",
      "      policy_loss: 0.0007054475136101246\n",
      "      total_loss: 3.4612388610839844\n",
      "      vf_explained_var: 0.9931726455688477\n",
      "      vf_loss: 3.460533618927002\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26175880432128906\n",
      "      kl: 0.016173314303159714\n",
      "      policy_loss: 0.0027818616945296526\n",
      "      total_loss: 3.1715617179870605\n",
      "      vf_explained_var: 0.9965381026268005\n",
      "      vf_loss: 3.1687800884246826\n",
      "    sample_time_ms: 20039.772\n",
      "    update_time_ms: 7.398\n",
      "  iterations_since_restore: 675\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.32360189483006\n",
      "    rl_1: 104.91865834594432\n",
      "  time_since_restore: 15935.250817775726\n",
      "  time_this_iter_s: 23.037295818328857\n",
      "  time_total_s: 15935.250817775726\n",
      "  timestamp: 1550809369\n",
      "  timesteps_since_restore: 6750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6750000\n",
      "  training_iteration: 675\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15935 s, 675 iter, 6750000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-23-12\n",
      "  done: false\n",
      "  episode_len_mean: 91.04545454545455\n",
      "  episode_reward_max: 218.84247670765006\n",
      "  episode_reward_mean: 174.475011486649\n",
      "  episode_reward_min: -151.32773421157063\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 70939\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3110.809\n",
      "    load_time_ms: 2.373\n",
      "    num_steps_sampled: 6760000\n",
      "    num_steps_trained: 6760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06447163224220276\n",
      "      kl: 0.009672689251601696\n",
      "      policy_loss: -0.0026885811239480972\n",
      "      total_loss: 49.8955192565918\n",
      "      vf_explained_var: 0.9223442077636719\n",
      "      vf_loss: 49.8982048034668\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25571855902671814\n",
      "      kl: 0.012984214350581169\n",
      "      policy_loss: -0.002915972378104925\n",
      "      total_loss: 68.8284912109375\n",
      "      vf_explained_var: 0.9358107447624207\n",
      "      vf_loss: 68.8314208984375\n",
      "    sample_time_ms: 20064.317\n",
      "    update_time_ms: 7.343\n",
      "  iterations_since_restore: 676\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.40921170619235\n",
      "    rl_1: 102.06579978045662\n",
      "  time_since_restore: 15958.972987651825\n",
      "  time_this_iter_s: 23.722169876098633\n",
      "  time_total_s: 15958.972987651825\n",
      "  timestamp: 1550809392\n",
      "  timesteps_since_restore: 6760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6760000\n",
      "  training_iteration: 676\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15958 s, 676 iter, 6760000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-23-36\n",
      "  done: false\n",
      "  episode_len_mean: 90.50450450450451\n",
      "  episode_reward_max: 223.08279532655084\n",
      "  episode_reward_mean: 175.52415889028143\n",
      "  episode_reward_min: -153.9442046412441\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 71050\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3111.927\n",
      "    load_time_ms: 2.412\n",
      "    num_steps_sampled: 6770000\n",
      "    num_steps_trained: 6770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12876078486442566\n",
      "      kl: 0.02443493902683258\n",
      "      policy_loss: -0.0016881399787962437\n",
      "      total_loss: 50.432315826416016\n",
      "      vf_explained_var: 0.9219352602958679\n",
      "      vf_loss: 50.43400192260742\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.289672315120697\n",
      "      kl: 0.07132341712713242\n",
      "      policy_loss: 0.0008966997265815735\n",
      "      total_loss: 75.67213439941406\n",
      "      vf_explained_var: 0.9311103820800781\n",
      "      vf_loss: 75.67122650146484\n",
      "    sample_time_ms: 20090.655\n",
      "    update_time_ms: 7.054\n",
      "  iterations_since_restore: 677\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.62515614202647\n",
      "    rl_1: 102.89900274825496\n",
      "  time_since_restore: 15982.465613365173\n",
      "  time_this_iter_s: 23.49262571334839\n",
      "  time_total_s: 15982.465613365173\n",
      "  timestamp: 1550809416\n",
      "  timesteps_since_restore: 6770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6770000\n",
      "  training_iteration: 677\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 15982 s, 677 iter, 6770000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-23-59\n",
      "  done: false\n",
      "  episode_len_mean: 90.56363636363636\n",
      "  episode_reward_max: 221.7260580224722\n",
      "  episode_reward_mean: 166.01026256039623\n",
      "  episode_reward_min: -153.70013051614632\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 71160\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3111.422\n",
      "    load_time_ms: 2.402\n",
      "    num_steps_sampled: 6780000\n",
      "    num_steps_trained: 6780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06004283204674721\n",
      "      kl: 0.030735045671463013\n",
      "      policy_loss: 3.381226269993931e-05\n",
      "      total_loss: 99.53363800048828\n",
      "      vf_explained_var: 0.8495429754257202\n",
      "      vf_loss: 99.53360748291016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22367550432682037\n",
      "      kl: 0.016538603231310844\n",
      "      policy_loss: -0.004930482245981693\n",
      "      total_loss: 126.7430419921875\n",
      "      vf_explained_var: 0.8872132897377014\n",
      "      vf_loss: 126.74798583984375\n",
      "    sample_time_ms: 20078.416\n",
      "    update_time_ms: 6.954\n",
      "  iterations_since_restore: 678\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.57781239811322\n",
      "    rl_1: 98.43245016228293\n",
      "  time_since_restore: 16005.76959490776\n",
      "  time_this_iter_s: 23.30398154258728\n",
      "  time_total_s: 16005.76959490776\n",
      "  timestamp: 1550809439\n",
      "  timesteps_since_restore: 6780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6780000\n",
      "  training_iteration: 678\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16005 s, 678 iter, 6780000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-24-22\n",
      "  done: false\n",
      "  episode_len_mean: 89.35714285714286\n",
      "  episode_reward_max: 218.4290501509588\n",
      "  episode_reward_mean: 162.47624381190187\n",
      "  episode_reward_min: -168.44636936327072\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 71272\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3115.469\n",
      "    load_time_ms: 2.426\n",
      "    num_steps_sampled: 6790000\n",
      "    num_steps_trained: 6790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.020532606169581413\n",
      "      kl: 0.016586139798164368\n",
      "      policy_loss: -0.007185733411461115\n",
      "      total_loss: 121.07839965820312\n",
      "      vf_explained_var: 0.8467339873313904\n",
      "      vf_loss: 121.08556365966797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.16287565231323242\n",
      "      kl: 0.019736500456929207\n",
      "      policy_loss: -0.0017348488327115774\n",
      "      total_loss: 178.96737670898438\n",
      "      vf_explained_var: 0.8423503041267395\n",
      "      vf_loss: 178.96910095214844\n",
      "    sample_time_ms: 20076.572\n",
      "    update_time_ms: 7.034\n",
      "  iterations_since_restore: 679\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.00711012937512\n",
      "    rl_1: 95.46913368252673\n",
      "  time_since_restore: 16028.798219919205\n",
      "  time_this_iter_s: 23.028625011444092\n",
      "  time_total_s: 16028.798219919205\n",
      "  timestamp: 1550809462\n",
      "  timesteps_since_restore: 6790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6790000\n",
      "  training_iteration: 679\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16028 s, 679 iter, 6790000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-24-46\n",
      "  done: false\n",
      "  episode_len_mean: 90.23423423423424\n",
      "  episode_reward_max: 221.8132498134088\n",
      "  episode_reward_mean: 169.98824527450472\n",
      "  episode_reward_min: -137.35135543962954\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 71383\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3116.57\n",
      "    load_time_ms: 2.427\n",
      "    num_steps_sampled: 6800000\n",
      "    num_steps_trained: 6800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.05080167576670647\n",
      "      kl: 0.025006461888551712\n",
      "      policy_loss: -0.0012417647521942854\n",
      "      total_loss: 84.59830474853516\n",
      "      vf_explained_var: 0.862791121006012\n",
      "      vf_loss: 84.59955596923828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21681225299835205\n",
      "      kl: 0.020415320992469788\n",
      "      policy_loss: 0.0010647139279171824\n",
      "      total_loss: 106.30447387695312\n",
      "      vf_explained_var: 0.8959071040153503\n",
      "      vf_loss: 106.30339813232422\n",
      "    sample_time_ms: 20071.946\n",
      "    update_time_ms: 7.213\n",
      "  iterations_since_restore: 680\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.67992260798316\n",
      "    rl_1: 100.30832266652152\n",
      "  time_since_restore: 16051.916563510895\n",
      "  time_this_iter_s: 23.118343591690063\n",
      "  time_total_s: 16051.916563510895\n",
      "  timestamp: 1550809486\n",
      "  timesteps_since_restore: 6800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6800000\n",
      "  training_iteration: 680\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16051 s, 680 iter, 6800000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-25-09\n",
      "  done: false\n",
      "  episode_len_mean: 90.77272727272727\n",
      "  episode_reward_max: 221.81166792720032\n",
      "  episode_reward_mean: 177.90437604462903\n",
      "  episode_reward_min: -137.94510982536912\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 71493\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3117.928\n",
      "    load_time_ms: 2.591\n",
      "    num_steps_sampled: 6810000\n",
      "    num_steps_trained: 6810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.013983648270368576\n",
      "      kl: 0.014644309878349304\n",
      "      policy_loss: 0.0007478516199626029\n",
      "      total_loss: 27.61142921447754\n",
      "      vf_explained_var: 0.9539499282836914\n",
      "      vf_loss: 27.61067771911621\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.14879004657268524\n",
      "      kl: 0.009996422566473484\n",
      "      policy_loss: 0.0003088749072048813\n",
      "      total_loss: 32.662635803222656\n",
      "      vf_explained_var: 0.9670170545578003\n",
      "      vf_loss: 32.66232681274414\n",
      "    sample_time_ms: 20078.884\n",
      "    update_time_ms: 7.565\n",
      "  iterations_since_restore: 681\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.52987914160335\n",
      "    rl_1: 103.37449690302564\n",
      "  time_since_restore: 16075.36542391777\n",
      "  time_this_iter_s: 23.44886040687561\n",
      "  time_total_s: 16075.36542391777\n",
      "  timestamp: 1550809509\n",
      "  timesteps_since_restore: 6810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6810000\n",
      "  training_iteration: 681\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16075 s, 681 iter, 6810000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-25-32\n",
      "  done: false\n",
      "  episode_len_mean: 91.78703703703704\n",
      "  episode_reward_max: 221.8527406460435\n",
      "  episode_reward_mean: 168.31609167665266\n",
      "  episode_reward_min: -143.30928092267752\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 71601\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3117.031\n",
      "    load_time_ms: 2.586\n",
      "    num_steps_sampled: 6820000\n",
      "    num_steps_trained: 6820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.021014239639043808\n",
      "      kl: 0.020544182509183884\n",
      "      policy_loss: -0.0011635285336524248\n",
      "      total_loss: 117.91669464111328\n",
      "      vf_explained_var: 0.8242200016975403\n",
      "      vf_loss: 117.9178695678711\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0693221315741539\n",
      "      kl: 0.022794507443904877\n",
      "      policy_loss: -0.002061213366687298\n",
      "      total_loss: 145.8831787109375\n",
      "      vf_explained_var: 0.8608167767524719\n",
      "      vf_loss: 145.88525390625\n",
      "    sample_time_ms: 20077.0\n",
      "    update_time_ms: 7.511\n",
      "  iterations_since_restore: 682\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.23877179735307\n",
      "    rl_1: 99.07731987929957\n",
      "  time_since_restore: 16098.57035779953\n",
      "  time_this_iter_s: 23.204933881759644\n",
      "  time_total_s: 16098.57035779953\n",
      "  timestamp: 1550809532\n",
      "  timesteps_since_restore: 6820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6820000\n",
      "  training_iteration: 682\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16098 s, 682 iter, 6820000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-25-55\n",
      "  done: false\n",
      "  episode_len_mean: 90.53153153153153\n",
      "  episode_reward_max: 220.4713075472451\n",
      "  episode_reward_mean: 181.29196632228457\n",
      "  episode_reward_min: -137.26781897055747\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 71712\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3116.1\n",
      "    load_time_ms: 2.587\n",
      "    num_steps_sampled: 6830000\n",
      "    num_steps_trained: 6830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.03579980507493019\n",
      "      kl: 0.013020973652601242\n",
      "      policy_loss: -0.0014720067847520113\n",
      "      total_loss: 37.21907424926758\n",
      "      vf_explained_var: 0.939601480960846\n",
      "      vf_loss: 37.22054672241211\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.13219617307186127\n",
      "      kl: 0.012108822353184223\n",
      "      policy_loss: -0.0009598241886124015\n",
      "      total_loss: 45.59424591064453\n",
      "      vf_explained_var: 0.9525076746940613\n",
      "      vf_loss: 45.5952033996582\n",
      "    sample_time_ms: 20119.303\n",
      "    update_time_ms: 7.315\n",
      "  iterations_since_restore: 683\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.61844092568982\n",
      "    rl_1: 104.67352539659474\n",
      "  time_since_restore: 16121.637755155563\n",
      "  time_this_iter_s: 23.067397356033325\n",
      "  time_total_s: 16121.637755155563\n",
      "  timestamp: 1550809555\n",
      "  timesteps_since_restore: 6830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6830000\n",
      "  training_iteration: 683\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16121 s, 683 iter, 6830000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-26-19\n",
      "  done: false\n",
      "  episode_len_mean: 92.29357798165138\n",
      "  episode_reward_max: 220.9303278548644\n",
      "  episode_reward_mean: 180.55218850218134\n",
      "  episode_reward_min: 142.042285192339\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 71821\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3116.524\n",
      "    load_time_ms: 2.41\n",
      "    num_steps_sampled: 6840000\n",
      "    num_steps_trained: 6840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.012331731617450714\n",
      "      kl: 0.012763462029397488\n",
      "      policy_loss: -0.0004647265304811299\n",
      "      total_loss: 4.029332637786865\n",
      "      vf_explained_var: 0.9919281005859375\n",
      "      vf_loss: 4.029797554016113\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.17302939295768738\n",
      "      kl: 0.011072132736444473\n",
      "      policy_loss: -0.0005393122555688024\n",
      "      total_loss: 3.16526198387146\n",
      "      vf_explained_var: 0.9966591000556946\n",
      "      vf_loss: 3.1658008098602295\n",
      "    sample_time_ms: 20134.193\n",
      "    update_time_ms: 7.213\n",
      "  iterations_since_restore: 684\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.12760633647054\n",
      "    rl_1: 106.4245821657108\n",
      "  time_since_restore: 16144.995676755905\n",
      "  time_this_iter_s: 23.357921600341797\n",
      "  time_total_s: 16144.995676755905\n",
      "  timestamp: 1550809579\n",
      "  timesteps_since_restore: 6840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6840000\n",
      "  training_iteration: 684\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16144 s, 684 iter, 6840000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-26-42\n",
      "  done: false\n",
      "  episode_len_mean: 90.61818181818182\n",
      "  episode_reward_max: 222.66934995152656\n",
      "  episode_reward_mean: 178.39364468382925\n",
      "  episode_reward_min: 145.884800750026\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 71931\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3119.694\n",
      "    load_time_ms: 2.404\n",
      "    num_steps_sampled: 6850000\n",
      "    num_steps_trained: 6850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.017358461394906044\n",
      "      kl: 0.013726494275033474\n",
      "      policy_loss: -0.0022203971166163683\n",
      "      total_loss: 3.1192967891693115\n",
      "      vf_explained_var: 0.9937640428543091\n",
      "      vf_loss: 3.1215169429779053\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1449417769908905\n",
      "      kl: 0.012290015816688538\n",
      "      policy_loss: 0.0018771672621369362\n",
      "      total_loss: 2.8327434062957764\n",
      "      vf_explained_var: 0.9968302845954895\n",
      "      vf_loss: 2.8308660984039307\n",
      "    sample_time_ms: 20151.778\n",
      "    update_time_ms: 7.239\n",
      "  iterations_since_restore: 685\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.87566159481567\n",
      "    rl_1: 103.51798308901355\n",
      "  time_since_restore: 16168.242418050766\n",
      "  time_this_iter_s: 23.24674129486084\n",
      "  time_total_s: 16168.242418050766\n",
      "  timestamp: 1550809602\n",
      "  timesteps_since_restore: 6850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6850000\n",
      "  training_iteration: 685\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16168 s, 685 iter, 6850000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-27-06\n",
      "  done: false\n",
      "  episode_len_mean: 94.65094339622641\n",
      "  episode_reward_max: 218.38820678887126\n",
      "  episode_reward_mean: 175.96820248721812\n",
      "  episode_reward_min: -160.81035635504156\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 72037\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3120.522\n",
      "    load_time_ms: 2.415\n",
      "    num_steps_sampled: 6860000\n",
      "    num_steps_trained: 6860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.030692225322127342\n",
      "      kl: 0.009117797017097473\n",
      "      policy_loss: -0.005676132161170244\n",
      "      total_loss: 9.404884338378906\n",
      "      vf_explained_var: 0.9895147681236267\n",
      "      vf_loss: 9.410560607910156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10226539522409439\n",
      "      kl: 0.01721097342669964\n",
      "      policy_loss: -0.00014991589705459774\n",
      "      total_loss: 5.953982830047607\n",
      "      vf_explained_var: 0.9956173896789551\n",
      "      vf_loss: 5.954133033752441\n",
      "    sample_time_ms: 20159.938\n",
      "    update_time_ms: 7.152\n",
      "  iterations_since_restore: 686\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.64834788315373\n",
      "    rl_1: 102.31985460406439\n",
      "  time_since_restore: 16192.051331996918\n",
      "  time_this_iter_s: 23.808913946151733\n",
      "  time_total_s: 16192.051331996918\n",
      "  timestamp: 1550809626\n",
      "  timesteps_since_restore: 6860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6860000\n",
      "  training_iteration: 686\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16192 s, 686 iter, 6860000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-27-29\n",
      "  done: false\n",
      "  episode_len_mean: 91.70370370370371\n",
      "  episode_reward_max: 220.53543507286918\n",
      "  episode_reward_mean: 177.97895970486059\n",
      "  episode_reward_min: 143.16430788148276\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 72145\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3120.772\n",
      "    load_time_ms: 2.353\n",
      "    num_steps_sampled: 6870000\n",
      "    num_steps_trained: 6870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.026806214824318886\n",
      "      kl: 0.012321754358708858\n",
      "      policy_loss: -0.002168731763958931\n",
      "      total_loss: 2.790203332901001\n",
      "      vf_explained_var: 0.9941407442092896\n",
      "      vf_loss: 2.792372465133667\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.16124823689460754\n",
      "      kl: 0.007038196548819542\n",
      "      policy_loss: -0.0013275094097480178\n",
      "      total_loss: 2.504542589187622\n",
      "      vf_explained_var: 0.9972787499427795\n",
      "      vf_loss: 2.5058701038360596\n",
      "    sample_time_ms: 20141.055\n",
      "    update_time_ms: 7.192\n",
      "  iterations_since_restore: 687\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.27482944849311\n",
      "    rl_1: 104.70413025636746\n",
      "  time_since_restore: 16215.359177350998\n",
      "  time_this_iter_s: 23.3078453540802\n",
      "  time_total_s: 16215.359177350998\n",
      "  timestamp: 1550809649\n",
      "  timesteps_since_restore: 6870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6870000\n",
      "  training_iteration: 687\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16215 s, 687 iter, 6870000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-27-53\n",
      "  done: false\n",
      "  episode_len_mean: 91.91743119266054\n",
      "  episode_reward_max: 224.69242914091666\n",
      "  episode_reward_mean: 173.28598437353656\n",
      "  episode_reward_min: -147.45709091208613\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 72254\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3121.042\n",
      "    load_time_ms: 2.353\n",
      "    num_steps_sampled: 6880000\n",
      "    num_steps_trained: 6880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.024830179288983345\n",
      "      kl: 0.012269146740436554\n",
      "      policy_loss: -0.004649466834962368\n",
      "      total_loss: 17.819385528564453\n",
      "      vf_explained_var: 0.9743412733078003\n",
      "      vf_loss: 17.82403564453125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.14478540420532227\n",
      "      kl: 0.008929351344704628\n",
      "      policy_loss: -0.0033513056114315987\n",
      "      total_loss: 26.30374526977539\n",
      "      vf_explained_var: 0.9777542352676392\n",
      "      vf_loss: 26.307098388671875\n",
      "    sample_time_ms: 20141.44\n",
      "    update_time_ms: 7.276\n",
      "  iterations_since_restore: 688\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.68890092773258\n",
      "    rl_1: 102.597083445804\n",
      "  time_since_restore: 16238.67043685913\n",
      "  time_this_iter_s: 23.311259508132935\n",
      "  time_total_s: 16238.67043685913\n",
      "  timestamp: 1550809673\n",
      "  timesteps_since_restore: 6880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6880000\n",
      "  training_iteration: 688\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16238 s, 688 iter, 6880000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-28-16\n",
      "  done: false\n",
      "  episode_len_mean: 90.81981981981981\n",
      "  episode_reward_max: 219.15236825696897\n",
      "  episode_reward_mean: 175.64190078801747\n",
      "  episode_reward_min: -148.95272351964474\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 72365\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3118.268\n",
      "    load_time_ms: 2.377\n",
      "    num_steps_sampled: 6890000\n",
      "    num_steps_trained: 6890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.05552390217781067\n",
      "      kl: 0.012264606542885303\n",
      "      policy_loss: -0.003229902358725667\n",
      "      total_loss: 22.74497413635254\n",
      "      vf_explained_var: 0.9631137251853943\n",
      "      vf_loss: 22.74820327758789\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22883738577365875\n",
      "      kl: 0.029106076806783676\n",
      "      policy_loss: 0.0009526109788566828\n",
      "      total_loss: 38.74226379394531\n",
      "      vf_explained_var: 0.9638591408729553\n",
      "      vf_loss: 38.74130630493164\n",
      "    sample_time_ms: 20138.874\n",
      "    update_time_ms: 7.223\n",
      "  iterations_since_restore: 689\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.42298376605774\n",
      "    rl_1: 103.21891702195965\n",
      "  time_since_restore: 16261.643909215927\n",
      "  time_this_iter_s: 22.973472356796265\n",
      "  time_total_s: 16261.643909215927\n",
      "  timestamp: 1550809696\n",
      "  timesteps_since_restore: 6890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6890000\n",
      "  training_iteration: 689\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16261 s, 689 iter, 6890000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-28-39\n",
      "  done: false\n",
      "  episode_len_mean: 90.27927927927928\n",
      "  episode_reward_max: 226.22188268974006\n",
      "  episode_reward_mean: 170.56891026470413\n",
      "  episode_reward_min: -163.4780770622483\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 72476\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.571\n",
      "    load_time_ms: 2.441\n",
      "    num_steps_sampled: 6900000\n",
      "    num_steps_trained: 6900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06586579978466034\n",
      "      kl: 0.015218468382954597\n",
      "      policy_loss: -0.004484444856643677\n",
      "      total_loss: 55.443878173828125\n",
      "      vf_explained_var: 0.9255091547966003\n",
      "      vf_loss: 55.44837188720703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21056999266147614\n",
      "      kl: 0.019830545410513878\n",
      "      policy_loss: -0.0011605332838371396\n",
      "      total_loss: 86.18572998046875\n",
      "      vf_explained_var: 0.9233067035675049\n",
      "      vf_loss: 86.18688201904297\n",
      "    sample_time_ms: 20106.666\n",
      "    update_time_ms: 7.073\n",
      "  iterations_since_restore: 690\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.7459542799349\n",
      "    rl_1: 98.82295598476925\n",
      "  time_since_restore: 16284.524486541748\n",
      "  time_this_iter_s: 22.880577325820923\n",
      "  time_total_s: 16284.524486541748\n",
      "  timestamp: 1550809719\n",
      "  timesteps_since_restore: 6900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6900000\n",
      "  training_iteration: 690\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16284 s, 690 iter, 6900000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-29-01\n",
      "  done: false\n",
      "  episode_len_mean: 91.11926605504587\n",
      "  episode_reward_max: 221.43437686306396\n",
      "  episode_reward_mean: 182.2500850064034\n",
      "  episode_reward_min: 146.46261067420366\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 72585\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.227\n",
      "    load_time_ms: 2.27\n",
      "    num_steps_sampled: 6910000\n",
      "    num_steps_trained: 6910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08397489786148071\n",
      "      kl: 0.0166709553450346\n",
      "      policy_loss: -0.001542528159916401\n",
      "      total_loss: 2.679821252822876\n",
      "      vf_explained_var: 0.9950123429298401\n",
      "      vf_loss: 2.681363582611084\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2442377805709839\n",
      "      kl: 0.011315636336803436\n",
      "      policy_loss: 0.0025228913873434067\n",
      "      total_loss: 2.2202444076538086\n",
      "      vf_explained_var: 0.9975831508636475\n",
      "      vf_loss: 2.217721700668335\n",
      "    sample_time_ms: 20036.816\n",
      "    update_time_ms: 6.849\n",
      "  iterations_since_restore: 691\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 77.69871713092802\n",
      "    rl_1: 104.55136787547535\n",
      "  time_since_restore: 16307.268034934998\n",
      "  time_this_iter_s: 22.74354839324951\n",
      "  time_total_s: 16307.268034934998\n",
      "  timestamp: 1550809741\n",
      "  timesteps_since_restore: 6910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6910000\n",
      "  training_iteration: 691\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16307 s, 691 iter, 6910000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-29-25\n",
      "  done: false\n",
      "  episode_len_mean: 92.3425925925926\n",
      "  episode_reward_max: 219.4478681567287\n",
      "  episode_reward_mean: 178.34301957845545\n",
      "  episode_reward_min: 139.40604562069854\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 72693\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.282\n",
      "    load_time_ms: 2.228\n",
      "    num_steps_sampled: 6920000\n",
      "    num_steps_trained: 6920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.03634792938828468\n",
      "      kl: 0.020261840894818306\n",
      "      policy_loss: 0.0009731328464113176\n",
      "      total_loss: 2.7826006412506104\n",
      "      vf_explained_var: 0.9946383237838745\n",
      "      vf_loss: 2.781627655029297\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22316932678222656\n",
      "      kl: 0.01071628276258707\n",
      "      policy_loss: -0.003227180801331997\n",
      "      total_loss: 2.733672857284546\n",
      "      vf_explained_var: 0.9970118403434753\n",
      "      vf_loss: 2.7369003295898438\n",
      "    sample_time_ms: 20037.156\n",
      "    update_time_ms: 6.892\n",
      "  iterations_since_restore: 692\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.53036779876282\n",
      "    rl_1: 104.8126517796926\n",
      "  time_since_restore: 16330.46689081192\n",
      "  time_this_iter_s: 23.198855876922607\n",
      "  time_total_s: 16330.46689081192\n",
      "  timestamp: 1550809765\n",
      "  timesteps_since_restore: 6920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6920000\n",
      "  training_iteration: 692\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16330 s, 692 iter, 6920000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-29-48\n",
      "  done: false\n",
      "  episode_len_mean: 90.98198198198199\n",
      "  episode_reward_max: 220.19861186121975\n",
      "  episode_reward_mean: 175.8915415552106\n",
      "  episode_reward_min: -159.46824062794357\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 72804\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.095\n",
      "    load_time_ms: 2.304\n",
      "    num_steps_sampled: 6930000\n",
      "    num_steps_trained: 6930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.027443980798125267\n",
      "      kl: 0.014525153674185276\n",
      "      policy_loss: -0.008951242081820965\n",
      "      total_loss: 25.507078170776367\n",
      "      vf_explained_var: 0.9590049982070923\n",
      "      vf_loss: 25.516027450561523\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2685943841934204\n",
      "      kl: 0.01339894998818636\n",
      "      policy_loss: 0.00035406992537900805\n",
      "      total_loss: 39.55857467651367\n",
      "      vf_explained_var: 0.962600588798523\n",
      "      vf_loss: 39.558231353759766\n",
      "    sample_time_ms: 20096.482\n",
      "    update_time_ms: 7.353\n",
      "  iterations_since_restore: 693\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.9640918921822\n",
      "    rl_1: 102.9274496630284\n",
      "  time_since_restore: 16354.13161945343\n",
      "  time_this_iter_s: 23.66472864151001\n",
      "  time_total_s: 16354.13161945343\n",
      "  timestamp: 1550809788\n",
      "  timesteps_since_restore: 6930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6930000\n",
      "  training_iteration: 693\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16354 s, 693 iter, 6930000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-30-12\n",
      "  done: false\n",
      "  episode_len_mean: 91.36111111111111\n",
      "  episode_reward_max: 222.93991238520078\n",
      "  episode_reward_mean: 181.65038527517916\n",
      "  episode_reward_min: 139.6707646106214\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 72912\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.687\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 6940000\n",
      "    num_steps_trained: 6940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.06641066074371338\n",
      "      kl: 0.01762448064982891\n",
      "      policy_loss: 0.000873603334184736\n",
      "      total_loss: 2.4353957176208496\n",
      "      vf_explained_var: 0.9955485463142395\n",
      "      vf_loss: 2.4345219135284424\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.27471888065338135\n",
      "      kl: 0.025777386501431465\n",
      "      policy_loss: 0.004947086330503225\n",
      "      total_loss: 2.2417101860046387\n",
      "      vf_explained_var: 0.997662365436554\n",
      "      vf_loss: 2.2367630004882812\n",
      "    sample_time_ms: 20070.818\n",
      "    update_time_ms: 7.458\n",
      "  iterations_since_restore: 694\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.2482937209394\n",
      "    rl_1: 105.40209155423972\n",
      "  time_since_restore: 16377.247107744217\n",
      "  time_this_iter_s: 23.115488290786743\n",
      "  time_total_s: 16377.247107744217\n",
      "  timestamp: 1550809812\n",
      "  timesteps_since_restore: 6940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6940000\n",
      "  training_iteration: 694\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16377 s, 694 iter, 6940000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-30-35\n",
      "  done: false\n",
      "  episode_len_mean: 90.52678571428571\n",
      "  episode_reward_max: 223.45705366110593\n",
      "  episode_reward_mean: 182.1329359638472\n",
      "  episode_reward_min: 145.3819880127242\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 73024\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.779\n",
      "    load_time_ms: 2.414\n",
      "    num_steps_sampled: 6950000\n",
      "    num_steps_trained: 6950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0836658701300621\n",
      "      kl: 0.009956288151443005\n",
      "      policy_loss: -0.00026253730175085366\n",
      "      total_loss: 2.028019428253174\n",
      "      vf_explained_var: 0.9962798953056335\n",
      "      vf_loss: 2.0282819271087646\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2679353654384613\n",
      "      kl: 0.015180368907749653\n",
      "      policy_loss: 0.0007854117429815233\n",
      "      total_loss: 2.1002895832061768\n",
      "      vf_explained_var: 0.9978030323982239\n",
      "      vf_loss: 2.099503993988037\n",
      "    sample_time_ms: 20086.918\n",
      "    update_time_ms: 7.37\n",
      "  iterations_since_restore: 695\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.74376907710669\n",
      "    rl_1: 105.38916688674047\n",
      "  time_since_restore: 16400.615059375763\n",
      "  time_this_iter_s: 23.36795163154602\n",
      "  time_total_s: 16400.615059375763\n",
      "  timestamp: 1550809835\n",
      "  timesteps_since_restore: 6950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6950000\n",
      "  training_iteration: 695\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16400 s, 695 iter, 6950000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-30-59\n",
      "  done: false\n",
      "  episode_len_mean: 91.13761467889908\n",
      "  episode_reward_max: 216.2867614203318\n",
      "  episode_reward_mean: 179.5405413142406\n",
      "  episode_reward_min: -148.95740838158576\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 73133\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.387\n",
      "    load_time_ms: 2.447\n",
      "    num_steps_sampled: 6960000\n",
      "    num_steps_trained: 6960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.11728725582361221\n",
      "      kl: 0.010358049534261227\n",
      "      policy_loss: -0.0007395215216092765\n",
      "      total_loss: 18.874460220336914\n",
      "      vf_explained_var: 0.9725013971328735\n",
      "      vf_loss: 18.87519645690918\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26163458824157715\n",
      "      kl: 0.018059182912111282\n",
      "      policy_loss: -0.004279496613889933\n",
      "      total_loss: 35.81410598754883\n",
      "      vf_explained_var: 0.9675520062446594\n",
      "      vf_loss: 35.81838607788086\n",
      "    sample_time_ms: 20079.573\n",
      "    update_time_ms: 7.476\n",
      "  iterations_since_restore: 696\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.74219036609722\n",
      "    rl_1: 104.79835094814342\n",
      "  time_since_restore: 16424.35279083252\n",
      "  time_this_iter_s: 23.737731456756592\n",
      "  time_total_s: 16424.35279083252\n",
      "  timestamp: 1550809859\n",
      "  timesteps_since_restore: 6960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6960000\n",
      "  training_iteration: 696\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16424 s, 696 iter, 6960000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-31-22\n",
      "  done: false\n",
      "  episode_len_mean: 92.48148148148148\n",
      "  episode_reward_max: 221.2511493086168\n",
      "  episode_reward_mean: 178.42639495829164\n",
      "  episode_reward_min: 148.40246323335307\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 73241\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.577\n",
      "    load_time_ms: 2.516\n",
      "    num_steps_sampled: 6970000\n",
      "    num_steps_trained: 6970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09174900501966476\n",
      "      kl: 0.01784203015267849\n",
      "      policy_loss: -0.0017864039400592446\n",
      "      total_loss: 2.110039234161377\n",
      "      vf_explained_var: 0.9957399368286133\n",
      "      vf_loss: 2.111825942993164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23822762072086334\n",
      "      kl: 0.010542454198002815\n",
      "      policy_loss: 0.0020599907729774714\n",
      "      total_loss: 2.1832003593444824\n",
      "      vf_explained_var: 0.9976722598075867\n",
      "      vf_loss: 2.181140184402466\n",
      "    sample_time_ms: 20021.458\n",
      "    update_time_ms: 7.701\n",
      "  iterations_since_restore: 697\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.80343533779538\n",
      "    rl_1: 105.6229596204962\n",
      "  time_since_restore: 16447.2632791996\n",
      "  time_this_iter_s: 22.91048836708069\n",
      "  time_total_s: 16447.2632791996\n",
      "  timestamp: 1550809882\n",
      "  timesteps_since_restore: 6970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6970000\n",
      "  training_iteration: 697\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16447 s, 697 iter, 6970000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-31-45\n",
      "  done: false\n",
      "  episode_len_mean: 92.06422018348624\n",
      "  episode_reward_max: 221.20508013125684\n",
      "  episode_reward_mean: 175.7833207497664\n",
      "  episode_reward_min: 141.38820315897067\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 73350\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.617\n",
      "    load_time_ms: 2.508\n",
      "    num_steps_sampled: 6980000\n",
      "    num_steps_trained: 6980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08244813978672028\n",
      "      kl: 0.019033605232834816\n",
      "      policy_loss: 0.0008193339454010129\n",
      "      total_loss: 1.8622338771820068\n",
      "      vf_explained_var: 0.9960784912109375\n",
      "      vf_loss: 1.8614141941070557\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2477056086063385\n",
      "      kl: 0.013397030532360077\n",
      "      policy_loss: -0.0011199252912774682\n",
      "      total_loss: 2.3130245208740234\n",
      "      vf_explained_var: 0.9975521564483643\n",
      "      vf_loss: 2.3141443729400635\n",
      "    sample_time_ms: 19978.007\n",
      "    update_time_ms: 7.988\n",
      "  iterations_since_restore: 698\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.91937832869499\n",
      "    rl_1: 104.86394242107141\n",
      "  time_since_restore: 16470.14217567444\n",
      "  time_this_iter_s: 22.878896474838257\n",
      "  time_total_s: 16470.14217567444\n",
      "  timestamp: 1550809905\n",
      "  timesteps_since_restore: 6980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6980000\n",
      "  training_iteration: 698\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16470 s, 698 iter, 6980000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-32-08\n",
      "  done: false\n",
      "  episode_len_mean: 91.35779816513761\n",
      "  episode_reward_max: 220.84671988239623\n",
      "  episode_reward_mean: 182.09766467689718\n",
      "  episode_reward_min: 144.598378284926\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 73459\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.277\n",
      "    load_time_ms: 2.47\n",
      "    num_steps_sampled: 6990000\n",
      "    num_steps_trained: 6990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.08480683714151382\n",
      "      kl: 0.01385890506207943\n",
      "      policy_loss: 0.0003964986826758832\n",
      "      total_loss: 1.7785977125167847\n",
      "      vf_explained_var: 0.9965928196907043\n",
      "      vf_loss: 1.7782012224197388\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.24467132985591888\n",
      "      kl: 0.02421553246676922\n",
      "      policy_loss: 0.008049865253269672\n",
      "      total_loss: 2.1229960918426514\n",
      "      vf_explained_var: 0.9977578520774841\n",
      "      vf_loss: 2.1149466037750244\n",
      "    sample_time_ms: 19988.345\n",
      "    update_time_ms: 7.982\n",
      "  iterations_since_restore: 699\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.57394060461695\n",
      "    rl_1: 105.52372407228023\n",
      "  time_since_restore: 16493.214858293533\n",
      "  time_this_iter_s: 23.07268261909485\n",
      "  time_total_s: 16493.214858293533\n",
      "  timestamp: 1550809928\n",
      "  timesteps_since_restore: 6990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 6990000\n",
      "  training_iteration: 699\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16493 s, 699 iter, 6990000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-32-31\n",
      "  done: false\n",
      "  episode_len_mean: 92.21296296296296\n",
      "  episode_reward_max: 222.8401624720855\n",
      "  episode_reward_mean: 176.54996600911468\n",
      "  episode_reward_min: 144.21149133121483\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 73567\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.116\n",
      "    load_time_ms: 2.436\n",
      "    num_steps_sampled: 7000000\n",
      "    num_steps_trained: 7000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1038510873913765\n",
      "      kl: 0.016165856271982193\n",
      "      policy_loss: -0.002331838011741638\n",
      "      total_loss: 2.249987840652466\n",
      "      vf_explained_var: 0.9954359531402588\n",
      "      vf_loss: 2.2523193359375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26592695713043213\n",
      "      kl: 0.015048716217279434\n",
      "      policy_loss: -0.000591547810472548\n",
      "      total_loss: 2.0513246059417725\n",
      "      vf_explained_var: 0.9977565407752991\n",
      "      vf_loss: 2.0519161224365234\n",
      "    sample_time_ms: 20014.35\n",
      "    update_time_ms: 7.905\n",
      "  iterations_since_restore: 700\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.1701747658519\n",
      "    rl_1: 104.37979124326279\n",
      "  time_since_restore: 16516.292091846466\n",
      "  time_this_iter_s: 23.07723355293274\n",
      "  time_total_s: 16516.292091846466\n",
      "  timestamp: 1550809951\n",
      "  timesteps_since_restore: 7000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7000000\n",
      "  training_iteration: 700\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16516 s, 700 iter, 7000000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-32-54\n",
      "  done: false\n",
      "  episode_len_mean: 92.22018348623853\n",
      "  episode_reward_max: 217.72084630385453\n",
      "  episode_reward_mean: 183.56140686199396\n",
      "  episode_reward_min: 146.2473848043702\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 73676\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.695\n",
      "    load_time_ms: 2.442\n",
      "    num_steps_sampled: 7010000\n",
      "    num_steps_trained: 7010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12413281947374344\n",
      "      kl: 0.015431607142090797\n",
      "      policy_loss: 0.0007892222492955625\n",
      "      total_loss: 1.7686785459518433\n",
      "      vf_explained_var: 0.99673992395401\n",
      "      vf_loss: 1.767889380455017\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2719346582889557\n",
      "      kl: 0.010748502798378468\n",
      "      policy_loss: 0.0011210722150281072\n",
      "      total_loss: 1.7026441097259521\n",
      "      vf_explained_var: 0.9982221722602844\n",
      "      vf_loss: 1.7015231847763062\n",
      "    sample_time_ms: 20054.86\n",
      "    update_time_ms: 7.877\n",
      "  iterations_since_restore: 701\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.41891133339094\n",
      "    rl_1: 107.142495528603\n",
      "  time_since_restore: 16539.43542575836\n",
      "  time_this_iter_s: 23.143333911895752\n",
      "  time_total_s: 16539.43542575836\n",
      "  timestamp: 1550809974\n",
      "  timesteps_since_restore: 7010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7010000\n",
      "  training_iteration: 701\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16539 s, 701 iter, 7010000 ts, 184 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-33-17\n",
      "  done: false\n",
      "  episode_len_mean: 92.17592592592592\n",
      "  episode_reward_max: 216.41071161391392\n",
      "  episode_reward_mean: 180.28940993798804\n",
      "  episode_reward_min: 147.56795845640045\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 73784\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.781\n",
      "    load_time_ms: 2.497\n",
      "    num_steps_sampled: 7020000\n",
      "    num_steps_trained: 7020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12223947793245316\n",
      "      kl: 0.014687946066260338\n",
      "      policy_loss: -0.0014978910330682993\n",
      "      total_loss: 2.212911367416382\n",
      "      vf_explained_var: 0.9955986142158508\n",
      "      vf_loss: 2.214409351348877\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32010090351104736\n",
      "      kl: 0.014721479266881943\n",
      "      policy_loss: 0.0027015311643481255\n",
      "      total_loss: 2.209393262863159\n",
      "      vf_explained_var: 0.9977495074272156\n",
      "      vf_loss: 2.2066915035247803\n",
      "    sample_time_ms: 20038.393\n",
      "    update_time_ms: 7.756\n",
      "  iterations_since_restore: 702\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.83170663752234\n",
      "    rl_1: 107.45770330046574\n",
      "  time_since_restore: 16562.46033525467\n",
      "  time_this_iter_s: 23.024909496307373\n",
      "  time_total_s: 16562.46033525467\n",
      "  timestamp: 1550809997\n",
      "  timesteps_since_restore: 7020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7020000\n",
      "  training_iteration: 702\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16562 s, 702 iter, 7020000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-33-41\n",
      "  done: false\n",
      "  episode_len_mean: 92.73148148148148\n",
      "  episode_reward_max: 222.87943612311065\n",
      "  episode_reward_mean: 177.07711175258828\n",
      "  episode_reward_min: 144.22040449826838\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 73892\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.056\n",
      "    load_time_ms: 2.484\n",
      "    num_steps_sampled: 7030000\n",
      "    num_steps_trained: 7030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1475497931241989\n",
      "      kl: 0.016909649595618248\n",
      "      policy_loss: 0.005018901079893112\n",
      "      total_loss: 1.617579460144043\n",
      "      vf_explained_var: 0.9965745210647583\n",
      "      vf_loss: 1.612560749053955\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3100910782814026\n",
      "      kl: 0.012158122844994068\n",
      "      policy_loss: 0.0018109995871782303\n",
      "      total_loss: 1.7137895822525024\n",
      "      vf_explained_var: 0.998157799243927\n",
      "      vf_loss: 1.7119786739349365\n",
      "    sample_time_ms: 20006.456\n",
      "    update_time_ms: 7.199\n",
      "  iterations_since_restore: 703\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.2615842034406\n",
      "    rl_1: 104.81552754914767\n",
      "  time_since_restore: 16585.83226132393\n",
      "  time_this_iter_s: 23.371926069259644\n",
      "  time_total_s: 16585.83226132393\n",
      "  timestamp: 1550810021\n",
      "  timesteps_since_restore: 7030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7030000\n",
      "  training_iteration: 703\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16585 s, 703 iter, 7030000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-34-05\n",
      "  done: false\n",
      "  episode_len_mean: 90.8\n",
      "  episode_reward_max: 225.7105656711272\n",
      "  episode_reward_mean: 181.76563352844758\n",
      "  episode_reward_min: 146.80627173105995\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 74002\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.701\n",
      "    load_time_ms: 2.445\n",
      "    num_steps_sampled: 7040000\n",
      "    num_steps_trained: 7040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1694117933511734\n",
      "      kl: 0.018073484301567078\n",
      "      policy_loss: 0.001116744359023869\n",
      "      total_loss: 1.4184566736221313\n",
      "      vf_explained_var: 0.9973555207252502\n",
      "      vf_loss: 1.4173401594161987\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2938455045223236\n",
      "      kl: 0.015335734002292156\n",
      "      policy_loss: 0.005120968911796808\n",
      "      total_loss: 1.6424301862716675\n",
      "      vf_explained_var: 0.9982287883758545\n",
      "      vf_loss: 1.6373093128204346\n",
      "    sample_time_ms: 20083.897\n",
      "    update_time_ms: 7.351\n",
      "  iterations_since_restore: 704\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 77.49712865528973\n",
      "    rl_1: 104.26850487315784\n",
      "  time_since_restore: 16609.72899222374\n",
      "  time_this_iter_s: 23.89673089981079\n",
      "  time_total_s: 16609.72899222374\n",
      "  timestamp: 1550810045\n",
      "  timesteps_since_restore: 7040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7040000\n",
      "  training_iteration: 704\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16609 s, 704 iter, 7040000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-34-28\n",
      "  done: false\n",
      "  episode_len_mean: 92.35185185185185\n",
      "  episode_reward_max: 224.2069435773369\n",
      "  episode_reward_mean: 179.19188704456852\n",
      "  episode_reward_min: 142.99900566828313\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 74110\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.276\n",
      "    load_time_ms: 2.413\n",
      "    num_steps_sampled: 7050000\n",
      "    num_steps_trained: 7050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.16580216586589813\n",
      "      kl: 0.01754380576312542\n",
      "      policy_loss: 0.00014615636609960347\n",
      "      total_loss: 1.5124304294586182\n",
      "      vf_explained_var: 0.996985137462616\n",
      "      vf_loss: 1.5122841596603394\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3431326448917389\n",
      "      kl: 0.01476383674889803\n",
      "      policy_loss: 0.0015079658478498459\n",
      "      total_loss: 1.722470760345459\n",
      "      vf_explained_var: 0.9982125759124756\n",
      "      vf_loss: 1.7209628820419312\n",
      "    sample_time_ms: 20088.566\n",
      "    update_time_ms: 7.454\n",
      "  iterations_since_restore: 705\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.63043846759412\n",
      "    rl_1: 105.56144857697436\n",
      "  time_since_restore: 16633.14875817299\n",
      "  time_this_iter_s: 23.419765949249268\n",
      "  time_total_s: 16633.14875817299\n",
      "  timestamp: 1550810068\n",
      "  timesteps_since_restore: 7050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7050000\n",
      "  training_iteration: 705\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16633 s, 705 iter, 7050000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-34-52\n",
      "  done: false\n",
      "  episode_len_mean: 92.34862385321101\n",
      "  episode_reward_max: 221.34717071205216\n",
      "  episode_reward_mean: 178.62835179896786\n",
      "  episode_reward_min: 141.59013774031325\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 74219\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3155.059\n",
      "    load_time_ms: 2.403\n",
      "    num_steps_sampled: 7060000\n",
      "    num_steps_trained: 7060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.16489307582378387\n",
      "      kl: 0.017893653362989426\n",
      "      policy_loss: -0.0014458014629781246\n",
      "      total_loss: 1.5463382005691528\n",
      "      vf_explained_var: 0.9968142509460449\n",
      "      vf_loss: 1.5477838516235352\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.37874704599380493\n",
      "      kl: 0.01576804183423519\n",
      "      policy_loss: -0.0020494183991104364\n",
      "      total_loss: 1.7827504873275757\n",
      "      vf_explained_var: 0.9981539845466614\n",
      "      vf_loss: 1.784799575805664\n",
      "    sample_time_ms: 20066.832\n",
      "    update_time_ms: 7.518\n",
      "  iterations_since_restore: 706\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.81830874323914\n",
      "    rl_1: 106.81004305572864\n",
      "  time_since_restore: 16656.84535741806\n",
      "  time_this_iter_s: 23.69659924507141\n",
      "  time_total_s: 16656.84535741806\n",
      "  timestamp: 1550810092\n",
      "  timesteps_since_restore: 7060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7060000\n",
      "  training_iteration: 706\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16656 s, 706 iter, 7060000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-35-15\n",
      "  done: false\n",
      "  episode_len_mean: 91.36697247706422\n",
      "  episode_reward_max: 218.0142316277554\n",
      "  episode_reward_mean: 180.04511145939864\n",
      "  episode_reward_min: 139.90271814169685\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 74328\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.336\n",
      "    load_time_ms: 2.395\n",
      "    num_steps_sampled: 7070000\n",
      "    num_steps_trained: 7070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12604716420173645\n",
      "      kl: 0.016864975914359093\n",
      "      policy_loss: -0.0008011030731722713\n",
      "      total_loss: 1.7243603467941284\n",
      "      vf_explained_var: 0.9966112971305847\n",
      "      vf_loss: 1.7251616716384888\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3430866599082947\n",
      "      kl: 0.024909695610404015\n",
      "      policy_loss: 0.0049593946896493435\n",
      "      total_loss: 1.7331963777542114\n",
      "      vf_explained_var: 0.9981780648231506\n",
      "      vf_loss: 1.7282366752624512\n",
      "    sample_time_ms: 20141.625\n",
      "    update_time_ms: 7.814\n",
      "  iterations_since_restore: 707\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.94807078722569\n",
      "    rl_1: 105.09704067217295\n",
      "  time_since_restore: 16680.314741373062\n",
      "  time_this_iter_s: 23.46938395500183\n",
      "  time_total_s: 16680.314741373062\n",
      "  timestamp: 1550810115\n",
      "  timesteps_since_restore: 7070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7070000\n",
      "  training_iteration: 707\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16680 s, 707 iter, 7070000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-35-39\n",
      "  done: false\n",
      "  episode_len_mean: 92.08256880733946\n",
      "  episode_reward_max: 217.82440767417367\n",
      "  episode_reward_mean: 179.45577355122256\n",
      "  episode_reward_min: 144.9118046588753\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 74437\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.341\n",
      "    load_time_ms: 2.397\n",
      "    num_steps_sampled: 7080000\n",
      "    num_steps_trained: 7080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1356690526008606\n",
      "      kl: 0.012610390782356262\n",
      "      policy_loss: -0.0016561659285798669\n",
      "      total_loss: 1.5573698282241821\n",
      "      vf_explained_var: 0.9969164729118347\n",
      "      vf_loss: 1.5590258836746216\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3279457688331604\n",
      "      kl: 0.011790214106440544\n",
      "      policy_loss: -3.1659306841902435e-05\n",
      "      total_loss: 1.6414822340011597\n",
      "      vf_explained_var: 0.99825519323349\n",
      "      vf_loss: 1.641513705253601\n",
      "    sample_time_ms: 20187.941\n",
      "    update_time_ms: 7.404\n",
      "  iterations_since_restore: 708\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.86836007013926\n",
      "    rl_1: 104.58741348108332\n",
      "  time_since_restore: 16703.66432952881\n",
      "  time_this_iter_s: 23.34958815574646\n",
      "  time_total_s: 16703.66432952881\n",
      "  timestamp: 1550810139\n",
      "  timesteps_since_restore: 7080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7080000\n",
      "  training_iteration: 708\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16703 s, 708 iter, 7080000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-36-03\n",
      "  done: false\n",
      "  episode_len_mean: 91.89908256880734\n",
      "  episode_reward_max: 217.8085219560703\n",
      "  episode_reward_mean: 176.06239768341524\n",
      "  episode_reward_min: 147.12318131564956\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 74546\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.225\n",
      "    load_time_ms: 2.384\n",
      "    num_steps_sampled: 7090000\n",
      "    num_steps_trained: 7090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.0819433182477951\n",
      "      kl: 0.016526365652680397\n",
      "      policy_loss: 0.0015814730431884527\n",
      "      total_loss: 2.0174336433410645\n",
      "      vf_explained_var: 0.9957870244979858\n",
      "      vf_loss: 2.015852212905884\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.36419689655303955\n",
      "      kl: 0.012406598776578903\n",
      "      policy_loss: 2.7299332941765897e-05\n",
      "      total_loss: 2.2071235179901123\n",
      "      vf_explained_var: 0.9976575970649719\n",
      "      vf_loss: 2.2070960998535156\n",
      "    sample_time_ms: 20274.296\n",
      "    update_time_ms: 7.916\n",
      "  iterations_since_restore: 709\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.39528462535067\n",
      "    rl_1: 104.6671130580646\n",
      "  time_since_restore: 16727.632541418076\n",
      "  time_this_iter_s: 23.968211889266968\n",
      "  time_total_s: 16727.632541418076\n",
      "  timestamp: 1550810163\n",
      "  timesteps_since_restore: 7090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7090000\n",
      "  training_iteration: 709\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16727 s, 709 iter, 7090000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-36-26\n",
      "  done: false\n",
      "  episode_len_mean: 91.91743119266054\n",
      "  episode_reward_max: 223.5204336628174\n",
      "  episode_reward_mean: 179.10091943286957\n",
      "  episode_reward_min: 145.5228419220738\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 74655\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.776\n",
      "    load_time_ms: 2.39\n",
      "    num_steps_sampled: 7100000\n",
      "    num_steps_trained: 7100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.17525765299797058\n",
      "      kl: 0.016026634722948074\n",
      "      policy_loss: -0.0020791650749742985\n",
      "      total_loss: 1.5622011423110962\n",
      "      vf_explained_var: 0.9968801736831665\n",
      "      vf_loss: 1.5642805099487305\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.37984248995780945\n",
      "      kl: 0.015294704586267471\n",
      "      policy_loss: 4.4470030843513086e-05\n",
      "      total_loss: 1.8527334928512573\n",
      "      vf_explained_var: 0.9980436563491821\n",
      "      vf_loss: 1.8526891469955444\n",
      "    sample_time_ms: 20293.272\n",
      "    update_time_ms: 8.07\n",
      "  iterations_since_restore: 710\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.3108895898035\n",
      "    rl_1: 105.7900298430661\n",
      "  time_since_restore: 16750.906979322433\n",
      "  time_this_iter_s: 23.27443790435791\n",
      "  time_total_s: 16750.906979322433\n",
      "  timestamp: 1550810186\n",
      "  timesteps_since_restore: 7100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7100000\n",
      "  training_iteration: 710\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16750 s, 710 iter, 7100000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-36-49\n",
      "  done: false\n",
      "  episode_len_mean: 92.28440366972477\n",
      "  episode_reward_max: 219.24849223479148\n",
      "  episode_reward_mean: 181.4229585832127\n",
      "  episode_reward_min: 143.53560610856368\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 74764\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3164.156\n",
      "    load_time_ms: 2.395\n",
      "    num_steps_sampled: 7110000\n",
      "    num_steps_trained: 7110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.16192997992038727\n",
      "      kl: 0.012889327481389046\n",
      "      policy_loss: -0.0007546176784671843\n",
      "      total_loss: 2.7164204120635986\n",
      "      vf_explained_var: 0.9951881766319275\n",
      "      vf_loss: 2.717175245285034\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35056984424591064\n",
      "      kl: 0.008619437925517559\n",
      "      policy_loss: -0.00016423180932179093\n",
      "      total_loss: 2.841634750366211\n",
      "      vf_explained_var: 0.9972577095031738\n",
      "      vf_loss: 2.841798782348633\n",
      "    sample_time_ms: 20295.46\n",
      "    update_time_ms: 8.257\n",
      "  iterations_since_restore: 711\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.03772207761757\n",
      "    rl_1: 105.38523650559509\n",
      "  time_since_restore: 16774.319119930267\n",
      "  time_this_iter_s: 23.412140607833862\n",
      "  time_total_s: 16774.319119930267\n",
      "  timestamp: 1550810209\n",
      "  timesteps_since_restore: 7110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7110000\n",
      "  training_iteration: 711\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16774 s, 711 iter, 7110000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-37-13\n",
      "  done: false\n",
      "  episode_len_mean: 91.76851851851852\n",
      "  episode_reward_max: 218.31414627531655\n",
      "  episode_reward_mean: 179.72682800623522\n",
      "  episode_reward_min: 144.24160022633183\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 74872\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3164.835\n",
      "    load_time_ms: 2.342\n",
      "    num_steps_sampled: 7120000\n",
      "    num_steps_trained: 7120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22269324958324432\n",
      "      kl: 0.02832907810807228\n",
      "      policy_loss: 0.004593915306031704\n",
      "      total_loss: 1.434605360031128\n",
      "      vf_explained_var: 0.9971684217453003\n",
      "      vf_loss: 1.430011510848999\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.38090574741363525\n",
      "      kl: 0.02243354171514511\n",
      "      policy_loss: 0.0023231604136526585\n",
      "      total_loss: 1.4958261251449585\n",
      "      vf_explained_var: 0.9984136819839478\n",
      "      vf_loss: 1.493503212928772\n",
      "    sample_time_ms: 20315.82\n",
      "    update_time_ms: 8.859\n",
      "  iterations_since_restore: 712\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.80632843685001\n",
      "    rl_1: 104.92049956938524\n",
      "  time_since_restore: 16797.560955286026\n",
      "  time_this_iter_s: 23.241835355758667\n",
      "  time_total_s: 16797.560955286026\n",
      "  timestamp: 1550810233\n",
      "  timesteps_since_restore: 7120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7120000\n",
      "  training_iteration: 712\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16797 s, 712 iter, 7120000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-37-36\n",
      "  done: false\n",
      "  episode_len_mean: 91.38181818181818\n",
      "  episode_reward_max: 219.50356356555787\n",
      "  episode_reward_mean: 178.54824460834774\n",
      "  episode_reward_min: 143.7336425384284\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 74982\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3163.147\n",
      "    load_time_ms: 2.276\n",
      "    num_steps_sampled: 7130000\n",
      "    num_steps_trained: 7130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21649527549743652\n",
      "      kl: 0.018107984215021133\n",
      "      policy_loss: 0.000934117182623595\n",
      "      total_loss: 1.4743088483810425\n",
      "      vf_explained_var: 0.9971553087234497\n",
      "      vf_loss: 1.4733749628067017\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.37724751234054565\n",
      "      kl: 0.02674095705151558\n",
      "      policy_loss: 0.0048643373884260654\n",
      "      total_loss: 1.7725259065628052\n",
      "      vf_explained_var: 0.9980944395065308\n",
      "      vf_loss: 1.767661452293396\n",
      "    sample_time_ms: 20349.754\n",
      "    update_time_ms: 8.782\n",
      "  iterations_since_restore: 713\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.58997188216019\n",
      "    rl_1: 103.95827272618754\n",
      "  time_since_restore: 16821.254026174545\n",
      "  time_this_iter_s: 23.693070888519287\n",
      "  time_total_s: 16821.254026174545\n",
      "  timestamp: 1550810256\n",
      "  timesteps_since_restore: 7130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7130000\n",
      "  training_iteration: 713\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16821 s, 713 iter, 7130000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-38-00\n",
      "  done: false\n",
      "  episode_len_mean: 91.55963302752293\n",
      "  episode_reward_max: 218.31866624724702\n",
      "  episode_reward_mean: 178.23639997825933\n",
      "  episode_reward_min: 142.81337299233718\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 75091\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3162.182\n",
      "    load_time_ms: 2.267\n",
      "    num_steps_sampled: 7140000\n",
      "    num_steps_trained: 7140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.20844192802906036\n",
      "      kl: 0.0391261987388134\n",
      "      policy_loss: 0.007154292892664671\n",
      "      total_loss: 1.5142236948013306\n",
      "      vf_explained_var: 0.9969470500946045\n",
      "      vf_loss: 1.5070693492889404\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.370098739862442\n",
      "      kl: 0.013037145137786865\n",
      "      policy_loss: -9.693257743492723e-05\n",
      "      total_loss: 1.5998491048812866\n",
      "      vf_explained_var: 0.9982902407646179\n",
      "      vf_loss: 1.599946141242981\n",
      "    sample_time_ms: 20302.119\n",
      "    update_time_ms: 8.511\n",
      "  iterations_since_restore: 714\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.50044216808351\n",
      "    rl_1: 103.7359578101758\n",
      "  time_since_restore: 16844.666556835175\n",
      "  time_this_iter_s: 23.412530660629272\n",
      "  time_total_s: 16844.666556835175\n",
      "  timestamp: 1550810280\n",
      "  timesteps_since_restore: 7140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7140000\n",
      "  training_iteration: 714\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16844 s, 714 iter, 7140000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-38-23\n",
      "  done: false\n",
      "  episode_len_mean: 90.86363636363636\n",
      "  episode_reward_max: 223.77537813640677\n",
      "  episode_reward_mean: 182.57165161345458\n",
      "  episode_reward_min: 145.34089431151244\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 75201\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3160.922\n",
      "    load_time_ms: 2.337\n",
      "    num_steps_sampled: 7150000\n",
      "    num_steps_trained: 7150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2824693024158478\n",
      "      kl: 0.016784917563199997\n",
      "      policy_loss: -0.002655167831107974\n",
      "      total_loss: 1.2421571016311646\n",
      "      vf_explained_var: 0.9976475834846497\n",
      "      vf_loss: 1.244812250137329\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.41272979974746704\n",
      "      kl: 0.02570377290248871\n",
      "      policy_loss: 0.010402164421975613\n",
      "      total_loss: 1.6125731468200684\n",
      "      vf_explained_var: 0.9982993006706238\n",
      "      vf_loss: 1.6021708250045776\n",
      "    sample_time_ms: 20262.299\n",
      "    update_time_ms: 8.224\n",
      "  iterations_since_restore: 715\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.99345290839132\n",
      "    rl_1: 105.57819870506329\n",
      "  time_since_restore: 16867.676423072815\n",
      "  time_this_iter_s: 23.00986623764038\n",
      "  time_total_s: 16867.676423072815\n",
      "  timestamp: 1550810303\n",
      "  timesteps_since_restore: 7150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7150000\n",
      "  training_iteration: 715\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16867 s, 715 iter, 7150000 ts, 183 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-38-47\n",
      "  done: false\n",
      "  episode_len_mean: 91.55045871559633\n",
      "  episode_reward_max: 222.27563120038153\n",
      "  episode_reward_mean: 178.5464912220781\n",
      "  episode_reward_min: 145.75881947988697\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 75310\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.479\n",
      "    load_time_ms: 2.298\n",
      "    num_steps_sampled: 7160000\n",
      "    num_steps_trained: 7160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.18670624494552612\n",
      "      kl: 0.01909683085978031\n",
      "      policy_loss: 0.001890375278890133\n",
      "      total_loss: 1.5378080606460571\n",
      "      vf_explained_var: 0.9968497157096863\n",
      "      vf_loss: 1.53591787815094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35344198346138\n",
      "      kl: 0.029650097712874413\n",
      "      policy_loss: 0.012370782904326916\n",
      "      total_loss: 1.4859329462051392\n",
      "      vf_explained_var: 0.9984241127967834\n",
      "      vf_loss: 1.473562240600586\n",
      "    sample_time_ms: 20269.048\n",
      "    update_time_ms: 8.636\n",
      "  iterations_since_restore: 716\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.4414978696935\n",
      "    rl_1: 104.10499335238461\n",
      "  time_since_restore: 16891.27797627449\n",
      "  time_this_iter_s: 23.601553201675415\n",
      "  time_total_s: 16891.27797627449\n",
      "  timestamp: 1550810327\n",
      "  timesteps_since_restore: 7160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7160000\n",
      "  training_iteration: 716\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16891 s, 716 iter, 7160000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-39-10\n",
      "  done: false\n",
      "  episode_len_mean: 92.63888888888889\n",
      "  episode_reward_max: 222.21183489430499\n",
      "  episode_reward_mean: 179.64871806917344\n",
      "  episode_reward_min: 145.0368073634549\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 75418\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.843\n",
      "    load_time_ms: 2.29\n",
      "    num_steps_sampled: 7170000\n",
      "    num_steps_trained: 7170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2604341208934784\n",
      "      kl: 0.025643914937973022\n",
      "      policy_loss: 0.002206921111792326\n",
      "      total_loss: 1.5674779415130615\n",
      "      vf_explained_var: 0.9969056844711304\n",
      "      vf_loss: 1.565271019935608\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.453022837638855\n",
      "      kl: 0.019510187208652496\n",
      "      policy_loss: 0.0019351175287738442\n",
      "      total_loss: 1.5873043537139893\n",
      "      vf_explained_var: 0.9983827471733093\n",
      "      vf_loss: 1.5853691101074219\n",
      "    sample_time_ms: 20261.128\n",
      "    update_time_ms: 8.241\n",
      "  iterations_since_restore: 717\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.50578778158376\n",
      "    rl_1: 106.14293028758966\n",
      "  time_since_restore: 16914.675560235977\n",
      "  time_this_iter_s: 23.397583961486816\n",
      "  time_total_s: 16914.675560235977\n",
      "  timestamp: 1550810350\n",
      "  timesteps_since_restore: 7170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7170000\n",
      "  training_iteration: 717\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16914 s, 717 iter, 7170000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-39-33\n",
      "  done: false\n",
      "  episode_len_mean: 92.29629629629629\n",
      "  episode_reward_max: 223.49011026997107\n",
      "  episode_reward_mean: 177.67731429062098\n",
      "  episode_reward_min: 147.73088767862583\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 75526\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.361\n",
      "    load_time_ms: 2.319\n",
      "    num_steps_sampled: 7180000\n",
      "    num_steps_trained: 7180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25589650869369507\n",
      "      kl: 0.0241741631180048\n",
      "      policy_loss: 0.0036494762171059847\n",
      "      total_loss: 1.3702532052993774\n",
      "      vf_explained_var: 0.9972094893455505\n",
      "      vf_loss: 1.3666036128997803\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.43608397245407104\n",
      "      kl: 0.024263808503746986\n",
      "      policy_loss: 0.006727620493620634\n",
      "      total_loss: 1.5537621974945068\n",
      "      vf_explained_var: 0.9982982277870178\n",
      "      vf_loss: 1.547034502029419\n",
      "    sample_time_ms: 20222.726\n",
      "    update_time_ms: 8.347\n",
      "  iterations_since_restore: 718\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.96590043190284\n",
      "    rl_1: 104.71141385871815\n",
      "  time_since_restore: 16937.637557029724\n",
      "  time_this_iter_s: 22.96199679374695\n",
      "  time_total_s: 16937.637557029724\n",
      "  timestamp: 1550810373\n",
      "  timesteps_since_restore: 7180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7180000\n",
      "  training_iteration: 718\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16937 s, 718 iter, 7180000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-39-57\n",
      "  done: false\n",
      "  episode_len_mean: 92.95370370370371\n",
      "  episode_reward_max: 220.6129755556719\n",
      "  episode_reward_mean: 177.14641247501496\n",
      "  episode_reward_min: 143.37227518143692\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 75634\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.435\n",
      "    load_time_ms: 2.395\n",
      "    num_steps_sampled: 7190000\n",
      "    num_steps_trained: 7190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.20572811365127563\n",
      "      kl: 0.02152292989194393\n",
      "      policy_loss: 0.0009164768271148205\n",
      "      total_loss: 1.558721899986267\n",
      "      vf_explained_var: 0.9969273805618286\n",
      "      vf_loss: 1.5578054189682007\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.37862104177474976\n",
      "      kl: 0.01100209541618824\n",
      "      policy_loss: 0.003858659416437149\n",
      "      total_loss: 1.574861764907837\n",
      "      vf_explained_var: 0.9982936382293701\n",
      "      vf_loss: 1.5710031986236572\n",
      "    sample_time_ms: 20165.407\n",
      "    update_time_ms: 7.782\n",
      "  iterations_since_restore: 719\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.5384456143173\n",
      "    rl_1: 103.60796686069769\n",
      "  time_since_restore: 16961.02045059204\n",
      "  time_this_iter_s: 23.382893562316895\n",
      "  time_total_s: 16961.02045059204\n",
      "  timestamp: 1550810397\n",
      "  timesteps_since_restore: 7190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7190000\n",
      "  training_iteration: 719\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16961 s, 719 iter, 7190000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-40-20\n",
      "  done: false\n",
      "  episode_len_mean: 91.26605504587155\n",
      "  episode_reward_max: 220.21611666325532\n",
      "  episode_reward_mean: 179.31780523492338\n",
      "  episode_reward_min: 148.12464377826936\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 75743\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3156.747\n",
      "    load_time_ms: 2.426\n",
      "    num_steps_sampled: 7200000\n",
      "    num_steps_trained: 7200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25937938690185547\n",
      "      kl: 0.03582847863435745\n",
      "      policy_loss: 0.0031089261174201965\n",
      "      total_loss: 1.3023946285247803\n",
      "      vf_explained_var: 0.9974454045295715\n",
      "      vf_loss: 1.2992857694625854\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.39890044927597046\n",
      "      kl: 0.015250138938426971\n",
      "      policy_loss: 0.0010479451157152653\n",
      "      total_loss: 1.4075614213943481\n",
      "      vf_explained_var: 0.9984726309776306\n",
      "      vf_loss: 1.4065134525299072\n",
      "    sample_time_ms: 20135.159\n",
      "    update_time_ms: 7.907\n",
      "  iterations_since_restore: 720\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.09931944993521\n",
      "    rl_1: 104.2184857849882\n",
      "  time_since_restore: 16984.119243621826\n",
      "  time_this_iter_s: 23.098793029785156\n",
      "  time_total_s: 16984.119243621826\n",
      "  timestamp: 1550810420\n",
      "  timesteps_since_restore: 7200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7200000\n",
      "  training_iteration: 720\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 16984 s, 720 iter, 7200000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-40-43\n",
      "  done: false\n",
      "  episode_len_mean: 93.57943925233644\n",
      "  episode_reward_max: 224.12257042453547\n",
      "  episode_reward_mean: 178.39768091109391\n",
      "  episode_reward_min: 142.85405534052097\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 75850\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.177\n",
      "    load_time_ms: 2.434\n",
      "    num_steps_sampled: 7210000\n",
      "    num_steps_trained: 7210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.19122348725795746\n",
      "      kl: 0.01257716491818428\n",
      "      policy_loss: -0.0005612260429188609\n",
      "      total_loss: 1.5923374891281128\n",
      "      vf_explained_var: 0.9969071745872498\n",
      "      vf_loss: 1.5928986072540283\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.36284565925598145\n",
      "      kl: 0.0110542681068182\n",
      "      policy_loss: -0.0008809102000668645\n",
      "      total_loss: 1.6531763076782227\n",
      "      vf_explained_var: 0.9982433915138245\n",
      "      vf_loss: 1.6540573835372925\n",
      "    sample_time_ms: 20164.792\n",
      "    update_time_ms: 8.002\n",
      "  iterations_since_restore: 721\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.59792450528639\n",
      "    rl_1: 105.79975640580754\n",
      "  time_since_restore: 17007.58214044571\n",
      "  time_this_iter_s: 23.462896823883057\n",
      "  time_total_s: 17007.58214044571\n",
      "  timestamp: 1550810443\n",
      "  timesteps_since_restore: 7210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7210000\n",
      "  training_iteration: 721\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17007 s, 721 iter, 7210000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-41-07\n",
      "  done: false\n",
      "  episode_len_mean: 92.61111111111111\n",
      "  episode_reward_max: 220.53652819101185\n",
      "  episode_reward_mean: 180.2807745640421\n",
      "  episode_reward_min: 145.38895557083683\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 75958\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.6\n",
      "    load_time_ms: 2.528\n",
      "    num_steps_sampled: 7220000\n",
      "    num_steps_trained: 7220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23127678036689758\n",
      "      kl: 0.019146807491779327\n",
      "      policy_loss: 0.0008046355796977878\n",
      "      total_loss: 1.5633476972579956\n",
      "      vf_explained_var: 0.9969105124473572\n",
      "      vf_loss: 1.5625427961349487\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.39707130193710327\n",
      "      kl: 0.018394479528069496\n",
      "      policy_loss: 0.0026073213666677475\n",
      "      total_loss: 1.3433458805084229\n",
      "      vf_explained_var: 0.9985893368721008\n",
      "      vf_loss: 1.3407384157180786\n",
      "    sample_time_ms: 20178.538\n",
      "    update_time_ms: 7.541\n",
      "  iterations_since_restore: 722\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.36748030014003\n",
      "    rl_1: 105.91329426390205\n",
      "  time_since_restore: 17030.99415755272\n",
      "  time_this_iter_s: 23.412017107009888\n",
      "  time_total_s: 17030.99415755272\n",
      "  timestamp: 1550810467\n",
      "  timesteps_since_restore: 7220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7220000\n",
      "  training_iteration: 722\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17030 s, 722 iter, 7220000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-41-29\n",
      "  done: false\n",
      "  episode_len_mean: 92.3302752293578\n",
      "  episode_reward_max: 220.9533976648631\n",
      "  episode_reward_mean: 177.84780295458924\n",
      "  episode_reward_min: 140.81138058051857\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 76067\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.751\n",
      "    load_time_ms: 2.529\n",
      "    num_steps_sampled: 7230000\n",
      "    num_steps_trained: 7230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22441130876541138\n",
      "      kl: 0.01692212000489235\n",
      "      policy_loss: 0.002168468665331602\n",
      "      total_loss: 1.6460812091827393\n",
      "      vf_explained_var: 0.996736466884613\n",
      "      vf_loss: 1.643912434577942\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3963516056537628\n",
      "      kl: 0.01296873390674591\n",
      "      policy_loss: 0.001441080356016755\n",
      "      total_loss: 1.6310616731643677\n",
      "      vf_explained_var: 0.9982197284698486\n",
      "      vf_loss: 1.6296206712722778\n",
      "    sample_time_ms: 20094.229\n",
      "    update_time_ms: 7.731\n",
      "  iterations_since_restore: 723\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.14573750141071\n",
      "    rl_1: 103.7020654531785\n",
      "  time_since_restore: 17053.82561635971\n",
      "  time_this_iter_s: 22.831458806991577\n",
      "  time_total_s: 17053.82561635971\n",
      "  timestamp: 1550810489\n",
      "  timesteps_since_restore: 7230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7230000\n",
      "  training_iteration: 723\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17053 s, 723 iter, 7230000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-41-53\n",
      "  done: false\n",
      "  episode_len_mean: 92.58878504672897\n",
      "  episode_reward_max: 219.32925423375036\n",
      "  episode_reward_mean: 178.24529570371152\n",
      "  episode_reward_min: -156.36144788024643\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 76174\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.964\n",
      "    load_time_ms: 2.659\n",
      "    num_steps_sampled: 7240000\n",
      "    num_steps_trained: 7240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25928157567977905\n",
      "      kl: 0.009726264514029026\n",
      "      policy_loss: -0.0002957590331789106\n",
      "      total_loss: 25.159610748291016\n",
      "      vf_explained_var: 0.9615179896354675\n",
      "      vf_loss: 25.159902572631836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.40167030692100525\n",
      "      kl: 0.29072442650794983\n",
      "      policy_loss: -0.007412021514028311\n",
      "      total_loss: 42.2470588684082\n",
      "      vf_explained_var: 0.9614657163619995\n",
      "      vf_loss: 42.25446701049805\n",
      "    sample_time_ms: 20091.326\n",
      "    update_time_ms: 7.701\n",
      "  iterations_since_restore: 724\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.00024663254413\n",
      "    rl_1: 104.24504907116739\n",
      "  time_since_restore: 17077.20748615265\n",
      "  time_this_iter_s: 23.381869792938232\n",
      "  time_total_s: 17077.20748615265\n",
      "  timestamp: 1550810513\n",
      "  timesteps_since_restore: 7240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7240000\n",
      "  training_iteration: 724\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17077 s, 724 iter, 7240000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-42-16\n",
      "  done: false\n",
      "  episode_len_mean: 93.1574074074074\n",
      "  episode_reward_max: 220.41204191011795\n",
      "  episode_reward_mean: 173.554684517262\n",
      "  episode_reward_min: -152.35227974024602\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 76282\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.488\n",
      "    load_time_ms: 2.622\n",
      "    num_steps_sampled: 7250000\n",
      "    num_steps_trained: 7250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.19954290986061096\n",
      "      kl: 0.01485726423561573\n",
      "      policy_loss: -0.0006322638364508748\n",
      "      total_loss: 32.0499267578125\n",
      "      vf_explained_var: 0.9516229033470154\n",
      "      vf_loss: 32.05055618286133\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3459700047969818\n",
      "      kl: 0.5403577089309692\n",
      "      policy_loss: 0.005955422297120094\n",
      "      total_loss: 49.98979568481445\n",
      "      vf_explained_var: 0.9545559287071228\n",
      "      vf_loss: 49.98383712768555\n",
      "    sample_time_ms: 20118.206\n",
      "    update_time_ms: 7.879\n",
      "  iterations_since_restore: 725\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.09216454430232\n",
      "    rl_1: 101.46251997295968\n",
      "  time_since_restore: 17100.512760162354\n",
      "  time_this_iter_s: 23.30527400970459\n",
      "  time_total_s: 17100.512760162354\n",
      "  timestamp: 1550810536\n",
      "  timesteps_since_restore: 7250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7250000\n",
      "  training_iteration: 725\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17100 s, 725 iter, 7250000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-42-40\n",
      "  done: false\n",
      "  episode_len_mean: 92.44444444444444\n",
      "  episode_reward_max: 217.6595746781651\n",
      "  episode_reward_mean: 177.5051098418627\n",
      "  episode_reward_min: 143.8580065725557\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 76390\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.116\n",
      "    load_time_ms: 2.685\n",
      "    num_steps_sampled: 7260000\n",
      "    num_steps_trained: 7260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2379951775074005\n",
      "      kl: 0.029703469946980476\n",
      "      policy_loss: 0.0009595649316906929\n",
      "      total_loss: 1.9228533506393433\n",
      "      vf_explained_var: 0.9960513114929199\n",
      "      vf_loss: 1.92189359664917\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4341385066509247\n",
      "      kl: 0.031719256192445755\n",
      "      policy_loss: 0.010296500287950039\n",
      "      total_loss: 1.9926095008850098\n",
      "      vf_explained_var: 0.9979438781738281\n",
      "      vf_loss: 1.9823131561279297\n",
      "    sample_time_ms: 20083.611\n",
      "    update_time_ms: 7.562\n",
      "  iterations_since_restore: 726\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.01618294111584\n",
      "    rl_1: 106.48892690074685\n",
      "  time_since_restore: 17123.873997449875\n",
      "  time_this_iter_s: 23.361237287521362\n",
      "  time_total_s: 17123.873997449875\n",
      "  timestamp: 1550810560\n",
      "  timesteps_since_restore: 7260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7260000\n",
      "  training_iteration: 726\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17123 s, 726 iter, 7260000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-43-03\n",
      "  done: false\n",
      "  episode_len_mean: 91.89908256880734\n",
      "  episode_reward_max: 222.5360384624378\n",
      "  episode_reward_mean: 180.77017982501766\n",
      "  episode_reward_min: 146.77959528253768\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 76499\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.559\n",
      "    load_time_ms: 2.706\n",
      "    num_steps_sampled: 7270000\n",
      "    num_steps_trained: 7270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3034088611602783\n",
      "      kl: 0.025703653693199158\n",
      "      policy_loss: -0.0008655905839987099\n",
      "      total_loss: 1.743449330329895\n",
      "      vf_explained_var: 0.9966018795967102\n",
      "      vf_loss: 1.7443149089813232\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4863097369670868\n",
      "      kl: 0.02011980302631855\n",
      "      policy_loss: 0.004218084271997213\n",
      "      total_loss: 1.9077718257904053\n",
      "      vf_explained_var: 0.9980482459068298\n",
      "      vf_loss: 1.9035537242889404\n",
      "    sample_time_ms: 20068.911\n",
      "    update_time_ms: 7.49\n",
      "  iterations_since_restore: 727\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.89454017029945\n",
      "    rl_1: 106.8756396547182\n",
      "  time_since_restore: 17147.10564517975\n",
      "  time_this_iter_s: 23.231647729873657\n",
      "  time_total_s: 17147.10564517975\n",
      "  timestamp: 1550810583\n",
      "  timesteps_since_restore: 7270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7270000\n",
      "  training_iteration: 727\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17147 s, 727 iter, 7270000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-43-26\n",
      "  done: false\n",
      "  episode_len_mean: 93.25233644859813\n",
      "  episode_reward_max: 224.53116203448272\n",
      "  episode_reward_mean: 175.3934808391068\n",
      "  episode_reward_min: -156.50461126906035\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 76606\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.485\n",
      "    load_time_ms: 2.666\n",
      "    num_steps_sampled: 7280000\n",
      "    num_steps_trained: 7280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2483307272195816\n",
      "      kl: 0.024755865335464478\n",
      "      policy_loss: -0.00416349433362484\n",
      "      total_loss: 22.672548294067383\n",
      "      vf_explained_var: 0.9621174931526184\n",
      "      vf_loss: 22.676712036132812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.40875887870788574\n",
      "      kl: 0.015086240135133266\n",
      "      policy_loss: 0.0015061207814142108\n",
      "      total_loss: 39.03542709350586\n",
      "      vf_explained_var: 0.9646798372268677\n",
      "      vf_loss: 39.03392028808594\n",
      "    sample_time_ms: 20108.762\n",
      "    update_time_ms: 7.614\n",
      "  iterations_since_restore: 728\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.42968099172695\n",
      "    rl_1: 104.96379984737982\n",
      "  time_since_restore: 17170.454812526703\n",
      "  time_this_iter_s: 23.349167346954346\n",
      "  time_total_s: 17170.454812526703\n",
      "  timestamp: 1550810606\n",
      "  timesteps_since_restore: 7280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7280000\n",
      "  training_iteration: 728\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17170 s, 728 iter, 7280000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-43-50\n",
      "  done: false\n",
      "  episode_len_mean: 92.61467889908256\n",
      "  episode_reward_max: 224.33697782275092\n",
      "  episode_reward_mean: 180.173126287917\n",
      "  episode_reward_min: 144.04112642485882\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 76715\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3161.84\n",
      "    load_time_ms: 2.557\n",
      "    num_steps_sampled: 7290000\n",
      "    num_steps_trained: 7290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2437591552734375\n",
      "      kl: 0.04459846019744873\n",
      "      policy_loss: 0.00336441514082253\n",
      "      total_loss: 1.7977474927902222\n",
      "      vf_explained_var: 0.9965729117393494\n",
      "      vf_loss: 1.79438316822052\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.40168482065200806\n",
      "      kl: 0.02077396586537361\n",
      "      policy_loss: 0.007740769535303116\n",
      "      total_loss: 1.6476432085037231\n",
      "      vf_explained_var: 0.9982377290725708\n",
      "      vf_loss: 1.6399022340774536\n",
      "    sample_time_ms: 20116.194\n",
      "    update_time_ms: 7.665\n",
      "  iterations_since_restore: 729\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.02162206899546\n",
      "    rl_1: 105.15150421892154\n",
      "  time_since_restore: 17194.085035562515\n",
      "  time_this_iter_s: 23.630223035812378\n",
      "  time_total_s: 17194.085035562515\n",
      "  timestamp: 1550810630\n",
      "  timesteps_since_restore: 7290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7290000\n",
      "  training_iteration: 729\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17194 s, 729 iter, 7290000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 92.29629629629629\n",
      "  episode_reward_max: 220.85121838385732\n",
      "  episode_reward_mean: 176.97592564348167\n",
      "  episode_reward_min: 147.23320090491046\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 76823\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.309\n",
      "    load_time_ms: 2.49\n",
      "    num_steps_sampled: 7300000\n",
      "    num_steps_trained: 7300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.27340519428253174\n",
      "      kl: 0.023582259193062782\n",
      "      policy_loss: 0.002860462758690119\n",
      "      total_loss: 1.6872235536575317\n",
      "      vf_explained_var: 0.9964503049850464\n",
      "      vf_loss: 1.6843633651733398\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.42346495389938354\n",
      "      kl: 0.015438842587172985\n",
      "      policy_loss: 0.0015722739044576883\n",
      "      total_loss: 1.707952618598938\n",
      "      vf_explained_var: 0.9981590509414673\n",
      "      vf_loss: 1.706380009651184\n",
      "    sample_time_ms: 20096.305\n",
      "    update_time_ms: 7.996\n",
      "  iterations_since_restore: 730\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.45152126290749\n",
      "    rl_1: 104.52440438057418\n",
      "  time_since_restore: 17216.85201907158\n",
      "  time_this_iter_s: 22.76698350906372\n",
      "  time_total_s: 17216.85201907158\n",
      "  timestamp: 1550810653\n",
      "  timesteps_since_restore: 7300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7300000\n",
      "  training_iteration: 730\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17216 s, 730 iter, 7300000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-44-36\n",
      "  done: false\n",
      "  episode_len_mean: 92.16666666666667\n",
      "  episode_reward_max: 224.63630594931635\n",
      "  episode_reward_mean: 178.41623661686242\n",
      "  episode_reward_min: -153.03638222195585\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 76931\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.49\n",
      "    load_time_ms: 2.505\n",
      "    num_steps_sampled: 7310000\n",
      "    num_steps_trained: 7310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2636805772781372\n",
      "      kl: 0.016273008659482002\n",
      "      policy_loss: -0.006710362154990435\n",
      "      total_loss: 26.306312561035156\n",
      "      vf_explained_var: 0.9613823294639587\n",
      "      vf_loss: 26.31302261352539\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35290807485580444\n",
      "      kl: 0.02103295922279358\n",
      "      policy_loss: -0.0041997781954705715\n",
      "      total_loss: 43.14234161376953\n",
      "      vf_explained_var: 0.9603937864303589\n",
      "      vf_loss: 43.14654541015625\n",
      "    sample_time_ms: 20104.952\n",
      "    update_time_ms: 7.858\n",
      "  iterations_since_restore: 731\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.31368656064872\n",
      "    rl_1: 104.10255005621367\n",
      "  time_since_restore: 17240.38151526451\n",
      "  time_this_iter_s: 23.52949619293213\n",
      "  time_total_s: 17240.38151526451\n",
      "  timestamp: 1550810676\n",
      "  timesteps_since_restore: 7310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7310000\n",
      "  training_iteration: 731\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17240 s, 731 iter, 7310000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-45-00\n",
      "  done: false\n",
      "  episode_len_mean: 91.83333333333333\n",
      "  episode_reward_max: 219.11091596746434\n",
      "  episode_reward_mean: 176.21846651976907\n",
      "  episode_reward_min: -158.69704755150488\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 77039\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.716\n",
      "    load_time_ms: 2.416\n",
      "    num_steps_sampled: 7320000\n",
      "    num_steps_trained: 7320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2558428645133972\n",
      "      kl: 0.01437804289162159\n",
      "      policy_loss: -0.0027644771616905928\n",
      "      total_loss: 24.67574119567871\n",
      "      vf_explained_var: 0.960874617099762\n",
      "      vf_loss: 24.678510665893555\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31610700488090515\n",
      "      kl: 0.017583562061190605\n",
      "      policy_loss: 0.0009064467158168554\n",
      "      total_loss: 40.23318099975586\n",
      "      vf_explained_var: 0.9616061449050903\n",
      "      vf_loss: 40.23228073120117\n",
      "    sample_time_ms: 20104.198\n",
      "    update_time_ms: 7.989\n",
      "  iterations_since_restore: 732\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.85375054660777\n",
      "    rl_1: 103.36471597316128\n",
      "  time_since_restore: 17263.78500199318\n",
      "  time_this_iter_s: 23.403486728668213\n",
      "  time_total_s: 17263.78500199318\n",
      "  timestamp: 1550810700\n",
      "  timesteps_since_restore: 7320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7320000\n",
      "  training_iteration: 732\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17263 s, 732 iter, 7320000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-45-23\n",
      "  done: false\n",
      "  episode_len_mean: 91.5909090909091\n",
      "  episode_reward_max: 220.7854287543581\n",
      "  episode_reward_mean: 177.66637400026437\n",
      "  episode_reward_min: 145.01283452387736\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 77149\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.834\n",
      "    load_time_ms: 2.405\n",
      "    num_steps_sampled: 7330000\n",
      "    num_steps_trained: 7330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2415643334388733\n",
      "      kl: 0.07727416604757309\n",
      "      policy_loss: 0.007667608559131622\n",
      "      total_loss: 2.1227641105651855\n",
      "      vf_explained_var: 0.9959320425987244\n",
      "      vf_loss: 2.1150963306427\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.34838950634002686\n",
      "      kl: 0.013591784983873367\n",
      "      policy_loss: 0.0020561260171234608\n",
      "      total_loss: 1.714133620262146\n",
      "      vf_explained_var: 0.9981234669685364\n",
      "      vf_loss: 1.7120776176452637\n",
      "    sample_time_ms: 20129.359\n",
      "    update_time_ms: 7.84\n",
      "  iterations_since_restore: 733\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.38275532566207\n",
      "    rl_1: 103.28361867460232\n",
      "  time_since_restore: 17286.868594408035\n",
      "  time_this_iter_s: 23.083592414855957\n",
      "  time_total_s: 17286.868594408035\n",
      "  timestamp: 1550810723\n",
      "  timesteps_since_restore: 7330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7330000\n",
      "  training_iteration: 733\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17286 s, 733 iter, 7330000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 90.92727272727272\n",
      "  episode_reward_max: 225.28919535422534\n",
      "  episode_reward_mean: 174.20087110937993\n",
      "  episode_reward_min: -132.90461094522118\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 77259\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.113\n",
      "    load_time_ms: 2.305\n",
      "    num_steps_sampled: 7340000\n",
      "    num_steps_trained: 7340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21168789267539978\n",
      "      kl: 0.02202543430030346\n",
      "      policy_loss: -8.424266707152128e-05\n",
      "      total_loss: 68.26806640625\n",
      "      vf_explained_var: 0.8946605324745178\n",
      "      vf_loss: 68.26815032958984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31984567642211914\n",
      "      kl: 0.023172428831458092\n",
      "      policy_loss: -0.004489877261221409\n",
      "      total_loss: 83.16795349121094\n",
      "      vf_explained_var: 0.9246678352355957\n",
      "      vf_loss: 83.17244720458984\n",
      "    sample_time_ms: 20125.911\n",
      "    update_time_ms: 8.053\n",
      "  iterations_since_restore: 734\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.69785357876907\n",
      "    rl_1: 101.50301753061086\n",
      "  time_since_restore: 17310.192315340042\n",
      "  time_this_iter_s: 23.323720932006836\n",
      "  time_total_s: 17310.192315340042\n",
      "  timestamp: 1550810746\n",
      "  timesteps_since_restore: 7340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7340000\n",
      "  training_iteration: 734\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17310 s, 734 iter, 7340000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-46-09\n",
      "  done: false\n",
      "  episode_len_mean: 90.63636363636364\n",
      "  episode_reward_max: 221.74998957349842\n",
      "  episode_reward_mean: 176.21053479331917\n",
      "  episode_reward_min: -177.20617429547073\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 77369\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.267\n",
      "    load_time_ms: 2.309\n",
      "    num_steps_sampled: 7350000\n",
      "    num_steps_trained: 7350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.24847939610481262\n",
      "      kl: 0.027227338403463364\n",
      "      policy_loss: 0.0006476971902884543\n",
      "      total_loss: 55.74857711791992\n",
      "      vf_explained_var: 0.9175944328308105\n",
      "      vf_loss: 55.74793243408203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3007047474384308\n",
      "      kl: 0.05485227331519127\n",
      "      policy_loss: 0.0010642959969118237\n",
      "      total_loss: 79.78038787841797\n",
      "      vf_explained_var: 0.9281112551689148\n",
      "      vf_loss: 79.7793197631836\n",
      "    sample_time_ms: 20104.578\n",
      "    update_time_ms: 8.045\n",
      "  iterations_since_restore: 735\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.05676648222183\n",
      "    rl_1: 102.15376831109732\n",
      "  time_since_restore: 17333.263961315155\n",
      "  time_this_iter_s: 23.071645975112915\n",
      "  time_total_s: 17333.263961315155\n",
      "  timestamp: 1550810769\n",
      "  timesteps_since_restore: 7350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7350000\n",
      "  training_iteration: 735\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17333 s, 735 iter, 7350000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-46-33\n",
      "  done: false\n",
      "  episode_len_mean: 91.00909090909092\n",
      "  episode_reward_max: 225.8265806558379\n",
      "  episode_reward_mean: 172.26365275220547\n",
      "  episode_reward_min: -153.21797128157783\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 77479\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.932\n",
      "    load_time_ms: 2.311\n",
      "    num_steps_sampled: 7360000\n",
      "    num_steps_trained: 7360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23637302219867706\n",
      "      kl: 0.016164660453796387\n",
      "      policy_loss: -0.0017464455449953675\n",
      "      total_loss: 48.95684814453125\n",
      "      vf_explained_var: 0.9240884184837341\n",
      "      vf_loss: 48.95859146118164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2840885519981384\n",
      "      kl: 0.050079356878995895\n",
      "      policy_loss: 0.005619429051876068\n",
      "      total_loss: 70.76888275146484\n",
      "      vf_explained_var: 0.9330754280090332\n",
      "      vf_loss: 70.76325988769531\n",
      "    sample_time_ms: 20099.984\n",
      "    update_time_ms: 7.794\n",
      "  iterations_since_restore: 736\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.98895951646794\n",
      "    rl_1: 101.27469323573752\n",
      "  time_since_restore: 17356.512845754623\n",
      "  time_this_iter_s: 23.248884439468384\n",
      "  time_total_s: 17356.512845754623\n",
      "  timestamp: 1550810793\n",
      "  timesteps_since_restore: 7360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7360000\n",
      "  training_iteration: 736\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17356 s, 736 iter, 7360000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-46-57\n",
      "  done: false\n",
      "  episode_len_mean: 91.90825688073394\n",
      "  episode_reward_max: 221.93375192226776\n",
      "  episode_reward_mean: 175.44419218706756\n",
      "  episode_reward_min: -38.99544665296173\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 77588\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.129\n",
      "    load_time_ms: 2.273\n",
      "    num_steps_sampled: 7370000\n",
      "    num_steps_trained: 7370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21702712774276733\n",
      "      kl: 0.012834694236516953\n",
      "      policy_loss: -0.001534361857920885\n",
      "      total_loss: 13.846352577209473\n",
      "      vf_explained_var: 0.9784382581710815\n",
      "      vf_loss: 13.847887992858887\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25549739599227905\n",
      "      kl: 0.008523784577846527\n",
      "      policy_loss: -0.0030973535031080246\n",
      "      total_loss: 12.61126708984375\n",
      "      vf_explained_var: 0.9885501265525818\n",
      "      vf_loss: 12.614364624023438\n",
      "    sample_time_ms: 20145.747\n",
      "    update_time_ms: 8.02\n",
      "  iterations_since_restore: 737\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.41592806785573\n",
      "    rl_1: 103.02826411921185\n",
      "  time_since_restore: 17380.215418815613\n",
      "  time_this_iter_s: 23.70257306098938\n",
      "  time_total_s: 17380.215418815613\n",
      "  timestamp: 1550810817\n",
      "  timesteps_since_restore: 7370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7370000\n",
      "  training_iteration: 737\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17380 s, 737 iter, 7370000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-47-19\n",
      "  done: false\n",
      "  episode_len_mean: 91.79816513761467\n",
      "  episode_reward_max: 225.07944086478204\n",
      "  episode_reward_mean: 176.551648701993\n",
      "  episode_reward_min: -129.22119009258984\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 77697\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3149.664\n",
      "    load_time_ms: 2.313\n",
      "    num_steps_sampled: 7380000\n",
      "    num_steps_trained: 7380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.24321673810482025\n",
      "      kl: 0.02203289233148098\n",
      "      policy_loss: -0.0016578513896092772\n",
      "      total_loss: 34.786231994628906\n",
      "      vf_explained_var: 0.9405003786087036\n",
      "      vf_loss: 34.787899017333984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.317306250333786\n",
      "      kl: 0.045624978840351105\n",
      "      policy_loss: 0.006008770316839218\n",
      "      total_loss: 42.915870666503906\n",
      "      vf_explained_var: 0.9575836062431335\n",
      "      vf_loss: 42.90986633300781\n",
      "    sample_time_ms: 20088.519\n",
      "    update_time_ms: 7.926\n",
      "  iterations_since_restore: 738\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.82756158280466\n",
      "    rl_1: 103.72408711918838\n",
      "  time_since_restore: 17403.11765885353\n",
      "  time_this_iter_s: 22.90224003791809\n",
      "  time_total_s: 17403.11765885353\n",
      "  timestamp: 1550810839\n",
      "  timesteps_since_restore: 7380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7380000\n",
      "  training_iteration: 738\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17403 s, 738 iter, 7380000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-47-43\n",
      "  done: false\n",
      "  episode_len_mean: 90.15315315315316\n",
      "  episode_reward_max: 225.84077935573507\n",
      "  episode_reward_mean: 171.58747586009636\n",
      "  episode_reward_min: -143.55936706497909\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 77808\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.901\n",
      "    load_time_ms: 2.324\n",
      "    num_steps_sampled: 7390000\n",
      "    num_steps_trained: 7390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28228628635406494\n",
      "      kl: 0.019177213311195374\n",
      "      policy_loss: 0.002908002119511366\n",
      "      total_loss: 87.37554931640625\n",
      "      vf_explained_var: 0.8686541318893433\n",
      "      vf_loss: 87.37264251708984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3195842206478119\n",
      "      kl: 0.03826652094721794\n",
      "      policy_loss: 0.00018207452376373112\n",
      "      total_loss: 107.96880340576172\n",
      "      vf_explained_var: 0.8994418382644653\n",
      "      vf_loss: 107.9686279296875\n",
      "    sample_time_ms: 20060.522\n",
      "    update_time_ms: 7.828\n",
      "  iterations_since_restore: 739\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.859604834092\n",
      "    rl_1: 99.72787102600435\n",
      "  time_since_restore: 17426.245894670486\n",
      "  time_this_iter_s: 23.128235816955566\n",
      "  time_total_s: 17426.245894670486\n",
      "  timestamp: 1550810863\n",
      "  timesteps_since_restore: 7390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7390000\n",
      "  training_iteration: 739\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17426 s, 739 iter, 7390000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-48-06\n",
      "  done: false\n",
      "  episode_len_mean: 91.55963302752293\n",
      "  episode_reward_max: 219.41344687741292\n",
      "  episode_reward_mean: 179.94739325634586\n",
      "  episode_reward_min: 147.98414333365693\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 77917\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.984\n",
      "    load_time_ms: 2.371\n",
      "    num_steps_sampled: 7400000\n",
      "    num_steps_trained: 7400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23687365651130676\n",
      "      kl: 0.01511272881180048\n",
      "      policy_loss: -0.0010953368619084358\n",
      "      total_loss: 2.725785493850708\n",
      "      vf_explained_var: 0.9946942329406738\n",
      "      vf_loss: 2.7268807888031006\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26255983114242554\n",
      "      kl: 0.01661756820976734\n",
      "      policy_loss: 0.0014647624921053648\n",
      "      total_loss: 2.0876100063323975\n",
      "      vf_explained_var: 0.9976199269294739\n",
      "      vf_loss: 2.0861456394195557\n",
      "    sample_time_ms: 20117.23\n",
      "    update_time_ms: 7.62\n",
      "  iterations_since_restore: 740\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.49747676513847\n",
      "    rl_1: 103.44991649120736\n",
      "  time_since_restore: 17449.588520288467\n",
      "  time_this_iter_s: 23.342625617980957\n",
      "  time_total_s: 17449.588520288467\n",
      "  timestamp: 1550810886\n",
      "  timesteps_since_restore: 7400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7400000\n",
      "  training_iteration: 740\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17449 s, 740 iter, 7400000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-48-29\n",
      "  done: false\n",
      "  episode_len_mean: 90.9\n",
      "  episode_reward_max: 222.0490992278727\n",
      "  episode_reward_mean: 165.37849777392162\n",
      "  episode_reward_min: -152.93460987553456\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 78027\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.935\n",
      "    load_time_ms: 2.345\n",
      "    num_steps_sampled: 7410000\n",
      "    num_steps_trained: 7410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22557274997234344\n",
      "      kl: 0.018065311014652252\n",
      "      policy_loss: -0.01032952032983303\n",
      "      total_loss: 113.74085998535156\n",
      "      vf_explained_var: 0.8434162735939026\n",
      "      vf_loss: 113.75117492675781\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28666412830352783\n",
      "      kl: 0.020276615396142006\n",
      "      policy_loss: 0.00025335795362479985\n",
      "      total_loss: 157.88137817382812\n",
      "      vf_explained_var: 0.8637570142745972\n",
      "      vf_loss: 157.8811492919922\n",
      "    sample_time_ms: 20098.751\n",
      "    update_time_ms: 7.987\n",
      "  iterations_since_restore: 741\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.37074643745554\n",
      "    rl_1: 98.00775133646606\n",
      "  time_since_restore: 17472.955805063248\n",
      "  time_this_iter_s: 23.367284774780273\n",
      "  time_total_s: 17472.955805063248\n",
      "  timestamp: 1550810909\n",
      "  timesteps_since_restore: 7410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7410000\n",
      "  training_iteration: 741\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17472 s, 741 iter, 7410000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-48-53\n",
      "  done: false\n",
      "  episode_len_mean: 90.91818181818182\n",
      "  episode_reward_max: 220.29273435076135\n",
      "  episode_reward_mean: 170.33626720879568\n",
      "  episode_reward_min: -140.48918574607012\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 78137\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.196\n",
      "    load_time_ms: 2.441\n",
      "    num_steps_sampled: 7420000\n",
      "    num_steps_trained: 7420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23255516588687897\n",
      "      kl: 0.021565888077020645\n",
      "      policy_loss: -0.0011177277192473412\n",
      "      total_loss: 100.4661636352539\n",
      "      vf_explained_var: 0.8537176251411438\n",
      "      vf_loss: 100.46729278564453\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.27085086703300476\n",
      "      kl: 0.024467987939715385\n",
      "      policy_loss: -0.004316624719649553\n",
      "      total_loss: 129.07867431640625\n",
      "      vf_explained_var: 0.8824345469474792\n",
      "      vf_loss: 129.08297729492188\n",
      "    sample_time_ms: 20086.786\n",
      "    update_time_ms: 7.808\n",
      "  iterations_since_restore: 742\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.91541564775297\n",
      "    rl_1: 100.42085156104277\n",
      "  time_since_restore: 17496.21170067787\n",
      "  time_this_iter_s: 23.255895614624023\n",
      "  time_total_s: 17496.21170067787\n",
      "  timestamp: 1550810933\n",
      "  timesteps_since_restore: 7420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7420000\n",
      "  training_iteration: 742\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17496 s, 742 iter, 7420000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-49-16\n",
      "  done: false\n",
      "  episode_len_mean: 90.64864864864865\n",
      "  episode_reward_max: 223.92911346580192\n",
      "  episode_reward_mean: 171.9874585764745\n",
      "  episode_reward_min: -149.6983441528967\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 78248\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.662\n",
      "    load_time_ms: 2.455\n",
      "    num_steps_sampled: 7430000\n",
      "    num_steps_trained: 7430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22027668356895447\n",
      "      kl: 0.02422843873500824\n",
      "      policy_loss: -0.0015219193883240223\n",
      "      total_loss: 84.05591583251953\n",
      "      vf_explained_var: 0.8710812330245972\n",
      "      vf_loss: 84.05744171142578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25784966349601746\n",
      "      kl: 0.012097497470676899\n",
      "      policy_loss: -0.0021857931278645992\n",
      "      total_loss: 109.1107177734375\n",
      "      vf_explained_var: 0.8932426571846008\n",
      "      vf_loss: 109.11287689208984\n",
      "    sample_time_ms: 20120.876\n",
      "    update_time_ms: 7.902\n",
      "  iterations_since_restore: 743\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.70259592840597\n",
      "    rl_1: 100.28486264806854\n",
      "  time_since_restore: 17519.651340007782\n",
      "  time_this_iter_s: 23.43963932991028\n",
      "  time_total_s: 17519.651340007782\n",
      "  timestamp: 1550810956\n",
      "  timesteps_since_restore: 7430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7430000\n",
      "  training_iteration: 743\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17519 s, 743 iter, 7430000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-49-40\n",
      "  done: false\n",
      "  episode_len_mean: 90.8256880733945\n",
      "  episode_reward_max: 220.80131562939894\n",
      "  episode_reward_mean: 169.4246834617924\n",
      "  episode_reward_min: -161.85162102103953\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 78357\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.507\n",
      "    load_time_ms: 2.385\n",
      "    num_steps_sampled: 7440000\n",
      "    num_steps_trained: 7440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23198069632053375\n",
      "      kl: 0.01885882392525673\n",
      "      policy_loss: 0.0022063504438847303\n",
      "      total_loss: 109.72021484375\n",
      "      vf_explained_var: 0.8351913690567017\n",
      "      vf_loss: 109.71800994873047\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25781556963920593\n",
      "      kl: 0.024721277877688408\n",
      "      policy_loss: -0.0019387641223147511\n",
      "      total_loss: 142.347900390625\n",
      "      vf_explained_var: 0.8700238466262817\n",
      "      vf_loss: 142.349853515625\n",
      "    sample_time_ms: 20150.688\n",
      "    update_time_ms: 7.816\n",
      "  iterations_since_restore: 744\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.00190988966533\n",
      "    rl_1: 100.42277357212706\n",
      "  time_since_restore: 17543.257177114487\n",
      "  time_this_iter_s: 23.605837106704712\n",
      "  time_total_s: 17543.257177114487\n",
      "  timestamp: 1550810980\n",
      "  timesteps_since_restore: 7440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7440000\n",
      "  training_iteration: 744\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17543 s, 744 iter, 7440000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-50-03\n",
      "  done: false\n",
      "  episode_len_mean: 91.60550458715596\n",
      "  episode_reward_max: 220.097755721328\n",
      "  episode_reward_mean: 171.81893100817217\n",
      "  episode_reward_min: -175.84557633796123\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 78466\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.275\n",
      "    load_time_ms: 2.453\n",
      "    num_steps_sampled: 7450000\n",
      "    num_steps_trained: 7450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.20764392614364624\n",
      "      kl: 0.016968922689557076\n",
      "      policy_loss: 0.0014369008131325245\n",
      "      total_loss: 99.4048080444336\n",
      "      vf_explained_var: 0.8563997149467468\n",
      "      vf_loss: 99.40337371826172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22697590291500092\n",
      "      kl: 0.020091775804758072\n",
      "      policy_loss: 0.0013616651995107532\n",
      "      total_loss: 126.89287567138672\n",
      "      vf_explained_var: 0.8827505111694336\n",
      "      vf_loss: 126.89152526855469\n",
      "    sample_time_ms: 20168.515\n",
      "    update_time_ms: 7.762\n",
      "  iterations_since_restore: 745\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.63570633378492\n",
      "    rl_1: 100.18322467438722\n",
      "  time_since_restore: 17566.51874113083\n",
      "  time_this_iter_s: 23.261564016342163\n",
      "  time_total_s: 17566.51874113083\n",
      "  timestamp: 1550811003\n",
      "  timesteps_since_restore: 7450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7450000\n",
      "  training_iteration: 745\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17566 s, 745 iter, 7450000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-50-26\n",
      "  done: false\n",
      "  episode_len_mean: 90.85585585585585\n",
      "  episode_reward_max: 220.80220455927898\n",
      "  episode_reward_mean: 174.28410286419603\n",
      "  episode_reward_min: -153.19330415708973\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 78577\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.208\n",
      "    load_time_ms: 2.425\n",
      "    num_steps_sampled: 7460000\n",
      "    num_steps_trained: 7460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.20865313708782196\n",
      "      kl: 0.016909465193748474\n",
      "      policy_loss: -0.0024558992590755224\n",
      "      total_loss: 52.76670837402344\n",
      "      vf_explained_var: 0.9216099381446838\n",
      "      vf_loss: 52.769168853759766\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.20315444469451904\n",
      "      kl: 0.019020799547433853\n",
      "      policy_loss: 0.001979729626327753\n",
      "      total_loss: 71.93502807617188\n",
      "      vf_explained_var: 0.9279853701591492\n",
      "      vf_loss: 71.93305206298828\n",
      "    sample_time_ms: 20156.496\n",
      "    update_time_ms: 7.853\n",
      "  iterations_since_restore: 746\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.7734138122778\n",
      "    rl_1: 100.51068905191826\n",
      "  time_since_restore: 17589.598279237747\n",
      "  time_this_iter_s: 23.079538106918335\n",
      "  time_total_s: 17589.598279237747\n",
      "  timestamp: 1550811026\n",
      "  timesteps_since_restore: 7460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7460000\n",
      "  training_iteration: 746\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17589 s, 746 iter, 7460000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-50-50\n",
      "  done: false\n",
      "  episode_len_mean: 92.53703703703704\n",
      "  episode_reward_max: 223.48561805221803\n",
      "  episode_reward_mean: 179.24494029130477\n",
      "  episode_reward_min: -47.743520691259846\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 78685\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.552\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 7470000\n",
      "    num_steps_trained: 7470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.20579005777835846\n",
      "      kl: 0.012888803146779537\n",
      "      policy_loss: 0.00016685125592630357\n",
      "      total_loss: 17.704378128051758\n",
      "      vf_explained_var: 0.9704070687294006\n",
      "      vf_loss: 17.70421028137207\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.17999567091464996\n",
      "      kl: 0.005633553024381399\n",
      "      policy_loss: -0.0017095559742301702\n",
      "      total_loss: 16.592803955078125\n",
      "      vf_explained_var: 0.9848337173461914\n",
      "      vf_loss: 16.594511032104492\n",
      "    sample_time_ms: 20099.38\n",
      "    update_time_ms: 7.554\n",
      "  iterations_since_restore: 747\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.62090622176815\n",
      "    rl_1: 105.62403406953662\n",
      "  time_since_restore: 17612.929957151413\n",
      "  time_this_iter_s: 23.33167791366577\n",
      "  time_total_s: 17612.929957151413\n",
      "  timestamp: 1550811050\n",
      "  timesteps_since_restore: 7470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7470000\n",
      "  training_iteration: 747\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17612 s, 747 iter, 7470000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-51-13\n",
      "  done: false\n",
      "  episode_len_mean: 91.78899082568807\n",
      "  episode_reward_max: 223.97270504721948\n",
      "  episode_reward_mean: 173.2090263656097\n",
      "  episode_reward_min: -137.18395606130002\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 78794\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.706\n",
      "    load_time_ms: 2.377\n",
      "    num_steps_sampled: 7480000\n",
      "    num_steps_trained: 7480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2427666187286377\n",
      "      kl: 0.030050406232476234\n",
      "      policy_loss: 0.00039981582085601985\n",
      "      total_loss: 57.850486755371094\n",
      "      vf_explained_var: 0.909602701663971\n",
      "      vf_loss: 57.85009002685547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.16560418903827667\n",
      "      kl: 0.020797645673155785\n",
      "      policy_loss: -0.0019705493468791246\n",
      "      total_loss: 73.46624755859375\n",
      "      vf_explained_var: 0.9308021664619446\n",
      "      vf_loss: 73.46820831298828\n",
      "    sample_time_ms: 20117.133\n",
      "    update_time_ms: 7.43\n",
      "  iterations_since_restore: 748\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.8236964482943\n",
      "    rl_1: 102.3853299173154\n",
      "  time_since_restore: 17635.900690555573\n",
      "  time_this_iter_s: 22.970733404159546\n",
      "  time_total_s: 17635.900690555573\n",
      "  timestamp: 1550811073\n",
      "  timesteps_since_restore: 7480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7480000\n",
      "  training_iteration: 748\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17635 s, 748 iter, 7480000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-51-36\n",
      "  done: false\n",
      "  episode_len_mean: 92.25925925925925\n",
      "  episode_reward_max: 221.41802370384113\n",
      "  episode_reward_mean: 179.8506855957364\n",
      "  episode_reward_min: -150.85093077015898\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 78902\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.08\n",
      "    load_time_ms: 2.408\n",
      "    num_steps_sampled: 7490000\n",
      "    num_steps_trained: 7490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25918012857437134\n",
      "      kl: 0.024066690355539322\n",
      "      policy_loss: -0.003098056884482503\n",
      "      total_loss: 22.196834564208984\n",
      "      vf_explained_var: 0.9646907448768616\n",
      "      vf_loss: 22.19993019104004\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.14482374489307404\n",
      "      kl: 0.02258577011525631\n",
      "      policy_loss: -0.0023511990439146757\n",
      "      total_loss: 36.55009078979492\n",
      "      vf_explained_var: 0.9641978144645691\n",
      "      vf_loss: 36.55244064331055\n",
      "    sample_time_ms: 20120.431\n",
      "    update_time_ms: 7.418\n",
      "  iterations_since_restore: 749\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.13411707784772\n",
      "    rl_1: 105.71656851788859\n",
      "  time_since_restore: 17659.117888450623\n",
      "  time_this_iter_s: 23.21719789505005\n",
      "  time_total_s: 17659.117888450623\n",
      "  timestamp: 1550811096\n",
      "  timesteps_since_restore: 7490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7490000\n",
      "  training_iteration: 749\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17659 s, 749 iter, 7490000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-51-59\n",
      "  done: false\n",
      "  episode_len_mean: 91.45871559633028\n",
      "  episode_reward_max: 223.33234598105352\n",
      "  episode_reward_mean: 181.85175332948674\n",
      "  episode_reward_min: 144.09937476151802\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 79011\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.951\n",
      "    load_time_ms: 2.389\n",
      "    num_steps_sampled: 7500000\n",
      "    num_steps_trained: 7500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2768994867801666\n",
      "      kl: 0.01603863574564457\n",
      "      policy_loss: -0.0017581498250365257\n",
      "      total_loss: 3.201545238494873\n",
      "      vf_explained_var: 0.994027853012085\n",
      "      vf_loss: 3.203303098678589\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12866216897964478\n",
      "      kl: 0.013567402958869934\n",
      "      policy_loss: 0.0018237268086522818\n",
      "      total_loss: 2.859729290008545\n",
      "      vf_explained_var: 0.9969249367713928\n",
      "      vf_loss: 2.8579061031341553\n",
      "    sample_time_ms: 20092.256\n",
      "    update_time_ms: 7.615\n",
      "  iterations_since_restore: 750\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.61171947707548\n",
      "    rl_1: 105.2400338524113\n",
      "  time_since_restore: 17682.16638326645\n",
      "  time_this_iter_s: 23.048494815826416\n",
      "  time_total_s: 17682.16638326645\n",
      "  timestamp: 1550811119\n",
      "  timesteps_since_restore: 7500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7500000\n",
      "  training_iteration: 750\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17682 s, 750 iter, 7500000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-52-22\n",
      "  done: false\n",
      "  episode_len_mean: 92.63888888888889\n",
      "  episode_reward_max: 218.75012816748372\n",
      "  episode_reward_mean: 173.1326261011513\n",
      "  episode_reward_min: -148.948945583174\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 79119\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3157.43\n",
      "    load_time_ms: 2.418\n",
      "    num_steps_sampled: 7510000\n",
      "    num_steps_trained: 7510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.24757754802703857\n",
      "      kl: 0.02032998763024807\n",
      "      policy_loss: -0.0006449436768889427\n",
      "      total_loss: 53.05684280395508\n",
      "      vf_explained_var: 0.9212900996208191\n",
      "      vf_loss: 53.05747985839844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.12185037136077881\n",
      "      kl: 0.01410622801631689\n",
      "      policy_loss: -0.0007675134693272412\n",
      "      total_loss: 73.10247039794922\n",
      "      vf_explained_var: 0.930180013179779\n",
      "      vf_loss: 73.10324096679688\n",
      "    sample_time_ms: 20008.613\n",
      "    update_time_ms: 7.164\n",
      "  iterations_since_restore: 751\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.61214565792893\n",
      "    rl_1: 101.52048044322234\n",
      "  time_since_restore: 17704.89067697525\n",
      "  time_this_iter_s: 22.72429370880127\n",
      "  time_total_s: 17704.89067697525\n",
      "  timestamp: 1550811142\n",
      "  timesteps_since_restore: 7510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7510000\n",
      "  training_iteration: 751\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17704 s, 751 iter, 7510000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-52-45\n",
      "  done: false\n",
      "  episode_len_mean: 92.21100917431193\n",
      "  episode_reward_max: 224.91728963018133\n",
      "  episode_reward_mean: 171.55360074286858\n",
      "  episode_reward_min: -178.03363053315093\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 79228\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3158.272\n",
      "    load_time_ms: 2.344\n",
      "    num_steps_sampled: 7520000\n",
      "    num_steps_trained: 7520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23149177432060242\n",
      "      kl: 0.028392208740115166\n",
      "      policy_loss: 0.0015323612606152892\n",
      "      total_loss: 54.05056381225586\n",
      "      vf_explained_var: 0.9208633303642273\n",
      "      vf_loss: 54.04902267456055\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.09969598799943924\n",
      "      kl: 0.010255292057991028\n",
      "      policy_loss: -0.000967464002314955\n",
      "      total_loss: 75.55353546142578\n",
      "      vf_explained_var: 0.9288528561592102\n",
      "      vf_loss: 75.55449676513672\n",
      "    sample_time_ms: 19989.861\n",
      "    update_time_ms: 7.488\n",
      "  iterations_since_restore: 752\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.03274949038509\n",
      "    rl_1: 100.52085125248347\n",
      "  time_since_restore: 17727.970074892044\n",
      "  time_this_iter_s: 23.079397916793823\n",
      "  time_total_s: 17727.970074892044\n",
      "  timestamp: 1550811165\n",
      "  timesteps_since_restore: 7520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7520000\n",
      "  training_iteration: 752\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17727 s, 752 iter, 7520000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-53-08\n",
      "  done: false\n",
      "  episode_len_mean: 92.08256880733946\n",
      "  episode_reward_max: 225.22789059258255\n",
      "  episode_reward_mean: 178.55558894270482\n",
      "  episode_reward_min: -101.42011446563814\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 79337\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3159.285\n",
      "    load_time_ms: 2.339\n",
      "    num_steps_sampled: 7530000\n",
      "    num_steps_trained: 7530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.29049134254455566\n",
      "      kl: 0.017689453437924385\n",
      "      policy_loss: -0.003554157679900527\n",
      "      total_loss: 11.724596977233887\n",
      "      vf_explained_var: 0.982146143913269\n",
      "      vf_loss: 11.728150367736816\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1388387382030487\n",
      "      kl: 0.013020419515669346\n",
      "      policy_loss: -0.0004923716187477112\n",
      "      total_loss: 12.718329429626465\n",
      "      vf_explained_var: 0.9881893992424011\n",
      "      vf_loss: 12.71882152557373\n",
      "    sample_time_ms: 19974.489\n",
      "    update_time_ms: 7.324\n",
      "  iterations_since_restore: 753\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.55224170276213\n",
      "    rl_1: 104.00334723994266\n",
      "  time_since_restore: 17751.26634335518\n",
      "  time_this_iter_s: 23.296268463134766\n",
      "  time_total_s: 17751.26634335518\n",
      "  timestamp: 1550811188\n",
      "  timesteps_since_restore: 7530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7530000\n",
      "  training_iteration: 753\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17751 s, 753 iter, 7530000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-53-32\n",
      "  done: false\n",
      "  episode_len_mean: 91.52777777777777\n",
      "  episode_reward_max: 217.80275921691788\n",
      "  episode_reward_mean: 182.15554396389365\n",
      "  episode_reward_min: 143.2301223890471\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 79445\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3166.501\n",
      "    load_time_ms: 2.342\n",
      "    num_steps_sampled: 7540000\n",
      "    num_steps_trained: 7540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3057044744491577\n",
      "      kl: 0.025333015248179436\n",
      "      policy_loss: -0.000815867620985955\n",
      "      total_loss: 2.78368878364563\n",
      "      vf_explained_var: 0.9947786331176758\n",
      "      vf_loss: 2.7845051288604736\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1579122692346573\n",
      "      kl: 0.012134341523051262\n",
      "      policy_loss: 0.003458781400695443\n",
      "      total_loss: 2.2419791221618652\n",
      "      vf_explained_var: 0.997614860534668\n",
      "      vf_loss: 2.2385196685791016\n",
      "    sample_time_ms: 20007.431\n",
      "    update_time_ms: 7.334\n",
      "  iterations_since_restore: 754\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.4223264218901\n",
      "    rl_1: 105.73321754200352\n",
      "  time_since_restore: 17775.28222799301\n",
      "  time_this_iter_s: 24.01588463783264\n",
      "  time_total_s: 17775.28222799301\n",
      "  timestamp: 1550811212\n",
      "  timesteps_since_restore: 7540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7540000\n",
      "  training_iteration: 754\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17775 s, 754 iter, 7540000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-53-55\n",
      "  done: false\n",
      "  episode_len_mean: 92.10909090909091\n",
      "  episode_reward_max: 220.16528832197167\n",
      "  episode_reward_mean: 171.35727061070799\n",
      "  episode_reward_min: -163.50787164730275\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 79555\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3168.054\n",
      "    load_time_ms: 2.259\n",
      "    num_steps_sampled: 7550000\n",
      "    num_steps_trained: 7550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2269664704799652\n",
      "      kl: 0.03184931352734566\n",
      "      policy_loss: -0.0033201612532138824\n",
      "      total_loss: 68.8515396118164\n",
      "      vf_explained_var: 0.8863381743431091\n",
      "      vf_loss: 68.85486602783203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.139850452542305\n",
      "      kl: 0.026141587644815445\n",
      "      policy_loss: 0.0003166233072988689\n",
      "      total_loss: 82.76194763183594\n",
      "      vf_explained_var: 0.9232220649719238\n",
      "      vf_loss: 82.76162719726562\n",
      "    sample_time_ms: 19981.156\n",
      "    update_time_ms: 7.618\n",
      "  iterations_since_restore: 755\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.01804966283548\n",
      "    rl_1: 101.33922094787252\n",
      "  time_since_restore: 17798.296270608902\n",
      "  time_this_iter_s: 23.014042615890503\n",
      "  time_total_s: 17798.296270608902\n",
      "  timestamp: 1550811235\n",
      "  timesteps_since_restore: 7550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7550000\n",
      "  training_iteration: 755\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17798 s, 755 iter, 7550000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-54-18\n",
      "  done: false\n",
      "  episode_len_mean: 91.64220183486239\n",
      "  episode_reward_max: 213.4188340683348\n",
      "  episode_reward_mean: 176.00734175825838\n",
      "  episode_reward_min: 144.28913123648852\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 79664\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3168.59\n",
      "    load_time_ms: 2.233\n",
      "    num_steps_sampled: 7560000\n",
      "    num_steps_trained: 7560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.23342110216617584\n",
      "      kl: 0.041234973818063736\n",
      "      policy_loss: 0.0011838390491902828\n",
      "      total_loss: 2.4660093784332275\n",
      "      vf_explained_var: 0.994645357131958\n",
      "      vf_loss: 2.4648256301879883\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.10369564592838287\n",
      "      kl: 0.009682394564151764\n",
      "      policy_loss: 0.0015248944982886314\n",
      "      total_loss: 2.184178113937378\n",
      "      vf_explained_var: 0.9974583387374878\n",
      "      vf_loss: 2.1826536655426025\n",
      "    sample_time_ms: 19942.314\n",
      "    update_time_ms: 7.651\n",
      "  iterations_since_restore: 756\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.10671588485613\n",
      "    rl_1: 101.9006258734022\n",
      "  time_since_restore: 17820.99012875557\n",
      "  time_this_iter_s: 22.69385814666748\n",
      "  time_total_s: 17820.99012875557\n",
      "  timestamp: 1550811258\n",
      "  timesteps_since_restore: 7560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7560000\n",
      "  training_iteration: 756\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17820 s, 756 iter, 7560000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-54-42\n",
      "  done: false\n",
      "  episode_len_mean: 92.10185185185185\n",
      "  episode_reward_max: 219.00193796641202\n",
      "  episode_reward_mean: 179.79977034390194\n",
      "  episode_reward_min: 144.65053338032604\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 79772\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3153.479\n",
      "    load_time_ms: 2.274\n",
      "    num_steps_sampled: 7570000\n",
      "    num_steps_trained: 7570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.29502037167549133\n",
      "      kl: 0.029798761010169983\n",
      "      policy_loss: 0.0018728259019553661\n",
      "      total_loss: 2.3494813442230225\n",
      "      vf_explained_var: 0.9954521059989929\n",
      "      vf_loss: 2.3476085662841797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.18532347679138184\n",
      "      kl: 0.0083167077973485\n",
      "      policy_loss: 0.0005799788050353527\n",
      "      total_loss: 2.106924295425415\n",
      "      vf_explained_var: 0.9977312088012695\n",
      "      vf_loss: 2.106343984603882\n",
      "    sample_time_ms: 19989.61\n",
      "    update_time_ms: 7.73\n",
      "  iterations_since_restore: 757\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.83349622461186\n",
      "    rl_1: 104.9662741192901\n",
      "  time_since_restore: 17844.64548945427\n",
      "  time_this_iter_s: 23.65536069869995\n",
      "  time_total_s: 17844.64548945427\n",
      "  timestamp: 1550811282\n",
      "  timesteps_since_restore: 7570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7570000\n",
      "  training_iteration: 757\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17844 s, 757 iter, 7570000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-55-06\n",
      "  done: false\n",
      "  episode_len_mean: 91.71559633027523\n",
      "  episode_reward_max: 217.75223720133783\n",
      "  episode_reward_mean: 177.7700195997361\n",
      "  episode_reward_min: 142.13359660659012\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 79881\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3152.353\n",
      "    load_time_ms: 2.35\n",
      "    num_steps_sampled: 7580000\n",
      "    num_steps_trained: 7580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2831580936908722\n",
      "      kl: 0.0166088305413723\n",
      "      policy_loss: -0.0007169589516706765\n",
      "      total_loss: 1.749829649925232\n",
      "      vf_explained_var: 0.9965519905090332\n",
      "      vf_loss: 1.7505463361740112\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.17420697212219238\n",
      "      kl: 0.014058617874979973\n",
      "      policy_loss: -0.0005022797267884016\n",
      "      total_loss: 1.9022296667099\n",
      "      vf_explained_var: 0.9979556202888489\n",
      "      vf_loss: 1.902732253074646\n",
      "    sample_time_ms: 20066.518\n",
      "    update_time_ms: 7.862\n",
      "  iterations_since_restore: 758\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.28682910246181\n",
      "    rl_1: 103.48319049727424\n",
      "  time_since_restore: 17868.377557992935\n",
      "  time_this_iter_s: 23.73206853866577\n",
      "  time_total_s: 17868.377557992935\n",
      "  timestamp: 1550811306\n",
      "  timesteps_since_restore: 7580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7580000\n",
      "  training_iteration: 758\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17868 s, 758 iter, 7580000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-55-29\n",
      "  done: false\n",
      "  episode_len_mean: 91.53211009174312\n",
      "  episode_reward_max: 224.41476988346045\n",
      "  episode_reward_mean: 184.05483884796655\n",
      "  episode_reward_min: 140.80303437525873\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 79990\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.545\n",
      "    load_time_ms: 2.338\n",
      "    num_steps_sampled: 7590000\n",
      "    num_steps_trained: 7590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3171951472759247\n",
      "      kl: 0.01799287274479866\n",
      "      policy_loss: -0.00013120968651492149\n",
      "      total_loss: 1.6341567039489746\n",
      "      vf_explained_var: 0.9970505833625793\n",
      "      vf_loss: 1.63428795337677\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2066870480775833\n",
      "      kl: 0.010130045004189014\n",
      "      policy_loss: 0.0006435581599362195\n",
      "      total_loss: 1.726130723953247\n",
      "      vf_explained_var: 0.9981971383094788\n",
      "      vf_loss: 1.7254871129989624\n",
      "    sample_time_ms: 20100.968\n",
      "    update_time_ms: 7.993\n",
      "  iterations_since_restore: 759\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 77.75974619147786\n",
      "    rl_1: 106.29509265648869\n",
      "  time_since_restore: 17891.92163825035\n",
      "  time_this_iter_s: 23.54408025741577\n",
      "  time_total_s: 17891.92163825035\n",
      "  timestamp: 1550811329\n",
      "  timesteps_since_restore: 7590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7590000\n",
      "  training_iteration: 759\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17891 s, 759 iter, 7590000 ts, 184 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-55-52\n",
      "  done: false\n",
      "  episode_len_mean: 91.42201834862385\n",
      "  episode_reward_max: 218.34576177361183\n",
      "  episode_reward_mean: 181.34799578229428\n",
      "  episode_reward_min: 143.5919719849726\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 80099\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3154.895\n",
      "    load_time_ms: 2.374\n",
      "    num_steps_sampled: 7600000\n",
      "    num_steps_trained: 7600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28729048371315\n",
      "      kl: 0.021372202783823013\n",
      "      policy_loss: 0.001637088949792087\n",
      "      total_loss: 1.7155873775482178\n",
      "      vf_explained_var: 0.9968758821487427\n",
      "      vf_loss: 1.7139501571655273\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.1860448718070984\n",
      "      kl: 0.011403484269976616\n",
      "      policy_loss: -0.00425300095230341\n",
      "      total_loss: 1.8079617023468018\n",
      "      vf_explained_var: 0.998114287853241\n",
      "      vf_loss: 1.812214732170105\n",
      "    sample_time_ms: 20094.078\n",
      "    update_time_ms: 7.662\n",
      "  iterations_since_restore: 760\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 76.1887730515363\n",
      "    rl_1: 105.159222730758\n",
      "  time_since_restore: 17914.944722414017\n",
      "  time_this_iter_s: 23.02308416366577\n",
      "  time_total_s: 17914.944722414017\n",
      "  timestamp: 1550811352\n",
      "  timesteps_since_restore: 7600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7600000\n",
      "  training_iteration: 760\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17914 s, 760 iter, 7600000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-56-16\n",
      "  done: false\n",
      "  episode_len_mean: 92.8425925925926\n",
      "  episode_reward_max: 219.31488501135354\n",
      "  episode_reward_mean: 175.69962418368\n",
      "  episode_reward_min: -150.98724112564963\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 80207\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.487\n",
      "    load_time_ms: 2.328\n",
      "    num_steps_sampled: 7610000\n",
      "    num_steps_trained: 7610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25262901186943054\n",
      "      kl: 0.011025514453649521\n",
      "      policy_loss: -0.003438725369051099\n",
      "      total_loss: 20.78603744506836\n",
      "      vf_explained_var: 0.9655929803848267\n",
      "      vf_loss: 20.789474487304688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2254905253648758\n",
      "      kl: 30.07457160949707\n",
      "      policy_loss: 0.04319421574473381\n",
      "      total_loss: 38.24248504638672\n",
      "      vf_explained_var: 0.96353679895401\n",
      "      vf_loss: 38.19928741455078\n",
      "    sample_time_ms: 20161.868\n",
      "    update_time_ms: 7.736\n",
      "  iterations_since_restore: 761\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.11695644838969\n",
      "    rl_1: 104.58266773529031\n",
      "  time_since_restore: 17938.14042377472\n",
      "  time_this_iter_s: 23.195701360702515\n",
      "  time_total_s: 17938.14042377472\n",
      "  timestamp: 1550811376\n",
      "  timesteps_since_restore: 7610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7610000\n",
      "  training_iteration: 761\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17938 s, 761 iter, 7610000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-56-39\n",
      "  done: false\n",
      "  episode_len_mean: 89.990990990991\n",
      "  episode_reward_max: 218.58542394464777\n",
      "  episode_reward_mean: 179.5374558096992\n",
      "  episode_reward_min: 146.65205853936382\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 80318\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.811\n",
      "    load_time_ms: 2.318\n",
      "    num_steps_sampled: 7620000\n",
      "    num_steps_trained: 7620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3100515604019165\n",
      "      kl: 0.023762604221701622\n",
      "      policy_loss: 0.0025130289141088724\n",
      "      total_loss: 1.5892918109893799\n",
      "      vf_explained_var: 0.9969750642776489\n",
      "      vf_loss: 1.5867786407470703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26322251558303833\n",
      "      kl: 0.005324373487383127\n",
      "      policy_loss: -0.0016508534317836165\n",
      "      total_loss: 1.5595874786376953\n",
      "      vf_explained_var: 0.9983609318733215\n",
      "      vf_loss: 1.561238169670105\n",
      "    sample_time_ms: 20214.827\n",
      "    update_time_ms: 7.589\n",
      "  iterations_since_restore: 762\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.93565727129597\n",
      "    rl_1: 105.60179853840323\n",
      "  time_since_restore: 17961.84149980545\n",
      "  time_this_iter_s: 23.7010760307312\n",
      "  time_total_s: 17961.84149980545\n",
      "  timestamp: 1550811399\n",
      "  timesteps_since_restore: 7620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7620000\n",
      "  training_iteration: 762\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17961 s, 762 iter, 7620000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-57-03\n",
      "  done: false\n",
      "  episode_len_mean: 90.10810810810811\n",
      "  episode_reward_max: 223.79006552139927\n",
      "  episode_reward_mean: 176.9725223179718\n",
      "  episode_reward_min: 141.66702358659836\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 80429\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.047\n",
      "    load_time_ms: 2.353\n",
      "    num_steps_sampled: 7630000\n",
      "    num_steps_trained: 7630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28336161375045776\n",
      "      kl: 0.021257221698760986\n",
      "      policy_loss: -0.0011389456922188401\n",
      "      total_loss: 1.501591444015503\n",
      "      vf_explained_var: 0.9969357252120972\n",
      "      vf_loss: 1.5027302503585815\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22998760640621185\n",
      "      kl: 0.00901662278920412\n",
      "      policy_loss: 0.0005271545378491282\n",
      "      total_loss: 1.6664447784423828\n",
      "      vf_explained_var: 0.9982539415359497\n",
      "      vf_loss: 1.6659176349639893\n",
      "    sample_time_ms: 20242.876\n",
      "    update_time_ms: 7.759\n",
      "  iterations_since_restore: 763\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.88664404836031\n",
      "    rl_1: 105.08587826961154\n",
      "  time_since_restore: 17985.36235666275\n",
      "  time_this_iter_s: 23.520856857299805\n",
      "  time_total_s: 17985.36235666275\n",
      "  timestamp: 1550811423\n",
      "  timesteps_since_restore: 7630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7630000\n",
      "  training_iteration: 763\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 17985 s, 763 iter, 7630000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 89.80357142857143\n",
      "  episode_reward_max: 222.31338305744723\n",
      "  episode_reward_mean: 180.58421613942133\n",
      "  episode_reward_min: 142.05044430542156\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 80541\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.959\n",
      "    load_time_ms: 2.355\n",
      "    num_steps_sampled: 7640000\n",
      "    num_steps_trained: 7640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3025803565979004\n",
      "      kl: 0.015321685932576656\n",
      "      policy_loss: 0.0006709635490551591\n",
      "      total_loss: 1.3586821556091309\n",
      "      vf_explained_var: 0.9973552227020264\n",
      "      vf_loss: 1.3580113649368286\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.24712669849395752\n",
      "      kl: 0.005470415111631155\n",
      "      policy_loss: -0.00022468190582003444\n",
      "      total_loss: 1.5285892486572266\n",
      "      vf_explained_var: 0.9983989000320435\n",
      "      vf_loss: 1.5288138389587402\n",
      "    sample_time_ms: 20146.043\n",
      "    update_time_ms: 7.66\n",
      "  iterations_since_restore: 764\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.18407558279486\n",
      "    rl_1: 105.40014055662644\n",
      "  time_since_restore: 18008.351313591003\n",
      "  time_this_iter_s: 22.988956928253174\n",
      "  time_total_s: 18008.351313591003\n",
      "  timestamp: 1550811446\n",
      "  timesteps_since_restore: 7640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7640000\n",
      "  training_iteration: 764\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18008 s, 764 iter, 7640000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-57-50\n",
      "  done: false\n",
      "  episode_len_mean: 91.1\n",
      "  episode_reward_max: 221.01739479499224\n",
      "  episode_reward_mean: 179.325691576646\n",
      "  episode_reward_min: 145.61857210534714\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 80651\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.29\n",
      "    load_time_ms: 2.41\n",
      "    num_steps_sampled: 7650000\n",
      "    num_steps_trained: 7650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2563319504261017\n",
      "      kl: 0.023108532652258873\n",
      "      policy_loss: 0.0016250688349828124\n",
      "      total_loss: 1.588989496231079\n",
      "      vf_explained_var: 0.9968273639678955\n",
      "      vf_loss: 1.5873644351959229\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2508450150489807\n",
      "      kl: 0.010874892584979534\n",
      "      policy_loss: 0.002222648588940501\n",
      "      total_loss: 1.5110161304473877\n",
      "      vf_explained_var: 0.9984831213951111\n",
      "      vf_loss: 1.5087934732437134\n",
      "    sample_time_ms: 20193.348\n",
      "    update_time_ms: 7.27\n",
      "  iterations_since_restore: 765\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.6431179926126\n",
      "    rl_1: 106.68257358403345\n",
      "  time_since_restore: 18031.97185564041\n",
      "  time_this_iter_s: 23.62054204940796\n",
      "  time_total_s: 18031.97185564041\n",
      "  timestamp: 1550811470\n",
      "  timesteps_since_restore: 7650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7650000\n",
      "  training_iteration: 765\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18031 s, 765 iter, 7650000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-58-12\n",
      "  done: false\n",
      "  episode_len_mean: 90.06363636363636\n",
      "  episode_reward_max: 220.1906128385576\n",
      "  episode_reward_mean: 178.1779867931322\n",
      "  episode_reward_min: 141.95561713818367\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 80761\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.653\n",
      "    load_time_ms: 2.422\n",
      "    num_steps_sampled: 7660000\n",
      "    num_steps_trained: 7660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25785356760025024\n",
      "      kl: 0.012049823068082333\n",
      "      policy_loss: -0.0016172730829566717\n",
      "      total_loss: 1.4295613765716553\n",
      "      vf_explained_var: 0.9971499443054199\n",
      "      vf_loss: 1.4311785697937012\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.22794175148010254\n",
      "      kl: 0.01789999194443226\n",
      "      policy_loss: 0.002641866682097316\n",
      "      total_loss: 1.3940976858139038\n",
      "      vf_explained_var: 0.9984887838363647\n",
      "      vf_loss: 1.3914560079574585\n",
      "    sample_time_ms: 20196.842\n",
      "    update_time_ms: 7.102\n",
      "  iterations_since_restore: 766\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.4447354587176\n",
      "    rl_1: 103.73325133441456\n",
      "  time_since_restore: 18054.686453580856\n",
      "  time_this_iter_s: 22.714597940444946\n",
      "  time_total_s: 18054.686453580856\n",
      "  timestamp: 1550811492\n",
      "  timesteps_since_restore: 7660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7660000\n",
      "  training_iteration: 766\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18054 s, 766 iter, 7660000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-58-36\n",
      "  done: false\n",
      "  episode_len_mean: 90.05357142857143\n",
      "  episode_reward_max: 215.9747319693194\n",
      "  episode_reward_mean: 179.85172738031932\n",
      "  episode_reward_min: 142.03029315234997\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 80873\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.447\n",
      "    load_time_ms: 2.388\n",
      "    num_steps_sampled: 7670000\n",
      "    num_steps_trained: 7670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2966651916503906\n",
      "      kl: 0.02408744767308235\n",
      "      policy_loss: 0.00014079858374316245\n",
      "      total_loss: 1.1595498323440552\n",
      "      vf_explained_var: 0.9977459907531738\n",
      "      vf_loss: 1.1594088077545166\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.24589504301548004\n",
      "      kl: 0.006087689660489559\n",
      "      policy_loss: -0.0004888485418632627\n",
      "      total_loss: 1.4763742685317993\n",
      "      vf_explained_var: 0.9984921216964722\n",
      "      vf_loss: 1.476863145828247\n",
      "    sample_time_ms: 20182.265\n",
      "    update_time_ms: 7.078\n",
      "  iterations_since_restore: 767\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.52721769083792\n",
      "    rl_1: 106.3245096894814\n",
      "  time_since_restore: 18078.142245054245\n",
      "  time_this_iter_s: 23.455791473388672\n",
      "  time_total_s: 18078.142245054245\n",
      "  timestamp: 1550811516\n",
      "  timesteps_since_restore: 7670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7670000\n",
      "  training_iteration: 767\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18078 s, 767 iter, 7670000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-58-59\n",
      "  done: false\n",
      "  episode_len_mean: 90.35454545454546\n",
      "  episode_reward_max: 218.11434019293486\n",
      "  episode_reward_mean: 178.09247094093334\n",
      "  episode_reward_min: 142.89577070316076\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 80983\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.388\n",
      "    load_time_ms: 2.351\n",
      "    num_steps_sampled: 7680000\n",
      "    num_steps_trained: 7680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26320576667785645\n",
      "      kl: 0.017260955646634102\n",
      "      policy_loss: -0.0005953765357844532\n",
      "      total_loss: 1.321972131729126\n",
      "      vf_explained_var: 0.9972430467605591\n",
      "      vf_loss: 1.3225674629211426\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.219075009226799\n",
      "      kl: 0.013186665251851082\n",
      "      policy_loss: 0.001670593279413879\n",
      "      total_loss: 1.3523322343826294\n",
      "      vf_explained_var: 0.998558521270752\n",
      "      vf_loss: 1.3506617546081543\n",
      "    sample_time_ms: 20155.615\n",
      "    update_time_ms: 6.978\n",
      "  iterations_since_restore: 768\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.44813941698298\n",
      "    rl_1: 104.64433152395033\n",
      "  time_since_restore: 18101.624384641647\n",
      "  time_this_iter_s: 23.482139587402344\n",
      "  time_total_s: 18101.624384641647\n",
      "  timestamp: 1550811539\n",
      "  timesteps_since_restore: 7680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7680000\n",
      "  training_iteration: 768\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18101 s, 768 iter, 7680000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-59-23\n",
      "  done: false\n",
      "  episode_len_mean: 90.01801801801801\n",
      "  episode_reward_max: 222.50782725712983\n",
      "  episode_reward_mean: 177.58561966114272\n",
      "  episode_reward_min: 140.51832231911456\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 81094\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.59\n",
      "    load_time_ms: 2.328\n",
      "    num_steps_sampled: 7690000\n",
      "    num_steps_trained: 7690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3443978428840637\n",
      "      kl: 0.022947469726204872\n",
      "      policy_loss: 0.0023300577886402607\n",
      "      total_loss: 1.1697081327438354\n",
      "      vf_explained_var: 0.9975720643997192\n",
      "      vf_loss: 1.1673781871795654\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2980821430683136\n",
      "      kl: 0.011420329101383686\n",
      "      policy_loss: 0.0018719694344326854\n",
      "      total_loss: 1.3256293535232544\n",
      "      vf_explained_var: 0.9985908269882202\n",
      "      vf_loss: 1.3237570524215698\n",
      "    sample_time_ms: 20120.889\n",
      "    update_time_ms: 7.048\n",
      "  iterations_since_restore: 769\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.97864683870844\n",
      "    rl_1: 104.6069728224342\n",
      "  time_since_restore: 18124.824734210968\n",
      "  time_this_iter_s: 23.20034956932068\n",
      "  time_total_s: 18124.824734210968\n",
      "  timestamp: 1550811563\n",
      "  timesteps_since_restore: 7690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7690000\n",
      "  training_iteration: 769\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18124 s, 769 iter, 7690000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_05-59-46\n",
      "  done: false\n",
      "  episode_len_mean: 89.82142857142857\n",
      "  episode_reward_max: 220.30834021900097\n",
      "  episode_reward_mean: 176.65597660186975\n",
      "  episode_reward_min: -149.27418602375698\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 81206\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.55\n",
      "    load_time_ms: 2.301\n",
      "    num_steps_sampled: 7700000\n",
      "    num_steps_trained: 7700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.29543638229370117\n",
      "      kl: 0.01766241155564785\n",
      "      policy_loss: -0.003496799385175109\n",
      "      total_loss: 21.829174041748047\n",
      "      vf_explained_var: 0.9665766954421997\n",
      "      vf_loss: 21.832672119140625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2553861439228058\n",
      "      kl: 0.03595875948667526\n",
      "      policy_loss: 0.00638963095843792\n",
      "      total_loss: 37.66886901855469\n",
      "      vf_explained_var: 0.9646787643432617\n",
      "      vf_loss: 37.662479400634766\n",
      "    sample_time_ms: 20141.667\n",
      "    update_time_ms: 7.036\n",
      "  iterations_since_restore: 770\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.80524095067084\n",
      "    rl_1: 103.8507356511989\n",
      "  time_since_restore: 18147.993982553482\n",
      "  time_this_iter_s: 23.169248342514038\n",
      "  time_total_s: 18147.993982553482\n",
      "  timestamp: 1550811586\n",
      "  timesteps_since_restore: 7700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7700000\n",
      "  training_iteration: 770\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18147 s, 770 iter, 7700000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-00-09\n",
      "  done: false\n",
      "  episode_len_mean: 89.55357142857143\n",
      "  episode_reward_max: 219.11915765987175\n",
      "  episode_reward_mean: 176.14391694462802\n",
      "  episode_reward_min: -149.36519529893522\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 81318\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.78\n",
      "    load_time_ms: 2.317\n",
      "    num_steps_sampled: 7710000\n",
      "    num_steps_trained: 7710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32188522815704346\n",
      "      kl: 0.01159936748445034\n",
      "      policy_loss: -0.001896460889838636\n",
      "      total_loss: 17.589496612548828\n",
      "      vf_explained_var: 0.9738685488700867\n",
      "      vf_loss: 17.59139060974121\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2762797176837921\n",
      "      kl: 0.694503128528595\n",
      "      policy_loss: -0.004251144360750914\n",
      "      total_loss: 32.57773208618164\n",
      "      vf_explained_var: 0.9700742959976196\n",
      "      vf_loss: 32.581974029541016\n",
      "    sample_time_ms: 20118.921\n",
      "    update_time_ms: 7.341\n",
      "  iterations_since_restore: 771\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.63358116639688\n",
      "    rl_1: 102.51033577823114\n",
      "  time_since_restore: 18170.98696923256\n",
      "  time_this_iter_s: 22.99298667907715\n",
      "  time_total_s: 18170.98696923256\n",
      "  timestamp: 1550811609\n",
      "  timesteps_since_restore: 7710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7710000\n",
      "  training_iteration: 771\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18170 s, 771 iter, 7710000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-00-32\n",
      "  done: false\n",
      "  episode_len_mean: 90.0909090909091\n",
      "  episode_reward_max: 221.40275302780375\n",
      "  episode_reward_mean: 178.89811041481033\n",
      "  episode_reward_min: 138.79772496577846\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 81428\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.891\n",
      "    load_time_ms: 2.294\n",
      "    num_steps_sampled: 7720000\n",
      "    num_steps_trained: 7720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28403714299201965\n",
      "      kl: 0.013110657222568989\n",
      "      policy_loss: 5.012597830500454e-05\n",
      "      total_loss: 1.5332365036010742\n",
      "      vf_explained_var: 0.996916651725769\n",
      "      vf_loss: 1.5331861972808838\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25318819284439087\n",
      "      kl: 0.011172487400472164\n",
      "      policy_loss: 0.0005758996703661978\n",
      "      total_loss: 1.353997826576233\n",
      "      vf_explained_var: 0.9985837936401367\n",
      "      vf_loss: 1.353421926498413\n",
      "    sample_time_ms: 20087.455\n",
      "    update_time_ms: 7.23\n",
      "  iterations_since_restore: 772\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.41493528571996\n",
      "    rl_1: 105.48317512909037\n",
      "  time_since_restore: 18194.283623456955\n",
      "  time_this_iter_s: 23.296654224395752\n",
      "  time_total_s: 18194.283623456955\n",
      "  timestamp: 1550811632\n",
      "  timesteps_since_restore: 7720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7720000\n",
      "  training_iteration: 772\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18194 s, 772 iter, 7720000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-00-56\n",
      "  done: false\n",
      "  episode_len_mean: 89.00884955752213\n",
      "  episode_reward_max: 222.7798303055614\n",
      "  episode_reward_mean: 180.2516188619129\n",
      "  episode_reward_min: 143.1415850969239\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 81541\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.912\n",
      "    load_time_ms: 2.333\n",
      "    num_steps_sampled: 7730000\n",
      "    num_steps_trained: 7730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.345405638217926\n",
      "      kl: 0.022889427840709686\n",
      "      policy_loss: -0.0019709260668605566\n",
      "      total_loss: 1.3248080015182495\n",
      "      vf_explained_var: 0.9974865913391113\n",
      "      vf_loss: 1.326778769493103\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31282371282577515\n",
      "      kl: 0.011700480245053768\n",
      "      policy_loss: 0.002844520378857851\n",
      "      total_loss: 1.234628438949585\n",
      "      vf_explained_var: 0.9986847043037415\n",
      "      vf_loss: 1.2317837476730347\n",
      "    sample_time_ms: 20054.046\n",
      "    update_time_ms: 7.466\n",
      "  iterations_since_restore: 773\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.78387523673796\n",
      "    rl_1: 104.46774362517495\n",
      "  time_since_restore: 18217.52410197258\n",
      "  time_this_iter_s: 23.240478515625\n",
      "  time_total_s: 18217.52410197258\n",
      "  timestamp: 1550811656\n",
      "  timesteps_since_restore: 7730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7730000\n",
      "  training_iteration: 773\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18217 s, 773 iter, 7730000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-01-19\n",
      "  done: false\n",
      "  episode_len_mean: 89.08928571428571\n",
      "  episode_reward_max: 218.3225457256064\n",
      "  episode_reward_mean: 176.52960449442966\n",
      "  episode_reward_min: 137.38739153368206\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 81653\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.737\n",
      "    load_time_ms: 2.367\n",
      "    num_steps_sampled: 7740000\n",
      "    num_steps_trained: 7740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31032174825668335\n",
      "      kl: 0.01672523282468319\n",
      "      policy_loss: 0.0007958064670674503\n",
      "      total_loss: 1.427042007446289\n",
      "      vf_explained_var: 0.9971545338630676\n",
      "      vf_loss: 1.426246166229248\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28734076023101807\n",
      "      kl: 0.008111773990094662\n",
      "      policy_loss: 0.0006585795199498534\n",
      "      total_loss: 1.2564629316329956\n",
      "      vf_explained_var: 0.9986184239387512\n",
      "      vf_loss: 1.2558043003082275\n",
      "    sample_time_ms: 20063.771\n",
      "    update_time_ms: 7.612\n",
      "  iterations_since_restore: 774\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.72187730147354\n",
      "    rl_1: 102.80772719295611\n",
      "  time_since_restore: 18240.600166082382\n",
      "  time_this_iter_s: 23.076064109802246\n",
      "  time_total_s: 18240.600166082382\n",
      "  timestamp: 1550811679\n",
      "  timesteps_since_restore: 7740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7740000\n",
      "  training_iteration: 774\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18240 s, 774 iter, 7740000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-01-42\n",
      "  done: false\n",
      "  episode_len_mean: 89.20535714285714\n",
      "  episode_reward_max: 218.9829261032319\n",
      "  episode_reward_mean: 172.5370099606956\n",
      "  episode_reward_min: -175.7670076258406\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 81765\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3117.25\n",
      "    load_time_ms: 2.3\n",
      "    num_steps_sampled: 7750000\n",
      "    num_steps_trained: 7750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3373015522956848\n",
      "      kl: 0.011108504608273506\n",
      "      policy_loss: -0.0016420785104855895\n",
      "      total_loss: 43.44187927246094\n",
      "      vf_explained_var: 0.9394416809082031\n",
      "      vf_loss: 43.44351577758789\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2727995216846466\n",
      "      kl: 0.022360503673553467\n",
      "      policy_loss: -0.003907100297510624\n",
      "      total_loss: 72.02513122558594\n",
      "      vf_explained_var: 0.936834990978241\n",
      "      vf_loss: 72.02903747558594\n",
      "    sample_time_ms: 20007.574\n",
      "    update_time_ms: 7.597\n",
      "  iterations_since_restore: 775\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.00834596934706\n",
      "    rl_1: 101.5286639913485\n",
      "  time_since_restore: 18263.49679517746\n",
      "  time_this_iter_s: 22.896629095077515\n",
      "  time_total_s: 18263.49679517746\n",
      "  timestamp: 1550811702\n",
      "  timesteps_since_restore: 7750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7750000\n",
      "  training_iteration: 775\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18263 s, 775 iter, 7750000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-02-05\n",
      "  done: false\n",
      "  episode_len_mean: 90.07207207207207\n",
      "  episode_reward_max: 218.30371132980355\n",
      "  episode_reward_mean: 174.0523718322263\n",
      "  episode_reward_min: -158.83855964684207\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 81876\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3118.694\n",
      "    load_time_ms: 2.296\n",
      "    num_steps_sampled: 7760000\n",
      "    num_steps_trained: 7760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.343006432056427\n",
      "      kl: 0.012882750481367111\n",
      "      policy_loss: -0.006119679193943739\n",
      "      total_loss: 27.191402435302734\n",
      "      vf_explained_var: 0.9584292769432068\n",
      "      vf_loss: 27.19752311706543\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31195658445358276\n",
      "      kl: 0.04409245029091835\n",
      "      policy_loss: 0.011915591545403004\n",
      "      total_loss: 48.72718048095703\n",
      "      vf_explained_var: 0.9583587646484375\n",
      "      vf_loss: 48.715267181396484\n",
      "    sample_time_ms: 20071.932\n",
      "    update_time_ms: 7.778\n",
      "  iterations_since_restore: 776\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.90036022638425\n",
      "    rl_1: 104.15201160584203\n",
      "  time_since_restore: 18286.871549606323\n",
      "  time_this_iter_s: 23.374754428863525\n",
      "  time_total_s: 18286.871549606323\n",
      "  timestamp: 1550811725\n",
      "  timesteps_since_restore: 7760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7760000\n",
      "  training_iteration: 776\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18286 s, 776 iter, 7760000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-02-29\n",
      "  done: false\n",
      "  episode_len_mean: 88.95535714285714\n",
      "  episode_reward_max: 218.62973181083404\n",
      "  episode_reward_mean: 172.66227891623166\n",
      "  episode_reward_min: -163.1836034029621\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 81988\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3117.123\n",
      "    load_time_ms: 2.312\n",
      "    num_steps_sampled: 7770000\n",
      "    num_steps_trained: 7770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3171570599079132\n",
      "      kl: 0.016486283391714096\n",
      "      policy_loss: -0.0022747009061276913\n",
      "      total_loss: 17.836841583251953\n",
      "      vf_explained_var: 0.9706162810325623\n",
      "      vf_loss: 17.839115142822266\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25105521082878113\n",
      "      kl: 0.016336672008037567\n",
      "      policy_loss: -0.0006514228880405426\n",
      "      total_loss: 31.00954246520996\n",
      "      vf_explained_var: 0.9696431159973145\n",
      "      vf_loss: 31.010190963745117\n",
      "    sample_time_ms: 20072.974\n",
      "    update_time_ms: 7.797\n",
      "  iterations_since_restore: 777\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.28068536888246\n",
      "    rl_1: 101.38159354734918\n",
      "  time_since_restore: 18310.32388162613\n",
      "  time_this_iter_s: 23.452332019805908\n",
      "  time_total_s: 18310.32388162613\n",
      "  timestamp: 1550811749\n",
      "  timesteps_since_restore: 7770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7770000\n",
      "  training_iteration: 777\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18310 s, 777 iter, 7770000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-02-52\n",
      "  done: false\n",
      "  episode_len_mean: 89.60714285714286\n",
      "  episode_reward_max: 215.49097763723606\n",
      "  episode_reward_mean: 176.92195063111063\n",
      "  episode_reward_min: -154.9636591737048\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 82100\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.531\n",
      "    load_time_ms: 2.383\n",
      "    num_steps_sampled: 7780000\n",
      "    num_steps_trained: 7780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3485962450504303\n",
      "      kl: 0.01222298014909029\n",
      "      policy_loss: -0.0004258966655470431\n",
      "      total_loss: 18.69103240966797\n",
      "      vf_explained_var: 0.9708500504493713\n",
      "      vf_loss: 18.691457748413086\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26079005002975464\n",
      "      kl: 0.010204690508544445\n",
      "      policy_loss: 0.0013324408791959286\n",
      "      total_loss: 30.179658889770508\n",
      "      vf_explained_var: 0.9721242189407349\n",
      "      vf_loss: 30.178325653076172\n",
      "    sample_time_ms: 20085.726\n",
      "    update_time_ms: 7.966\n",
      "  iterations_since_restore: 778\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.53918678683851\n",
      "    rl_1: 104.3827638442721\n",
      "  time_since_restore: 18334.0604968071\n",
      "  time_this_iter_s: 23.73661518096924\n",
      "  time_total_s: 18334.0604968071\n",
      "  timestamp: 1550811772\n",
      "  timesteps_since_restore: 7780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7780000\n",
      "  training_iteration: 778\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18334 s, 778 iter, 7780000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-03-15\n",
      "  done: false\n",
      "  episode_len_mean: 89.65765765765765\n",
      "  episode_reward_max: 215.45063370507563\n",
      "  episode_reward_mean: 177.13219035747701\n",
      "  episode_reward_min: -158.63324236729943\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 82211\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.133\n",
      "    load_time_ms: 2.484\n",
      "    num_steps_sampled: 7790000\n",
      "    num_steps_trained: 7790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35510504245758057\n",
      "      kl: 0.013694501481950283\n",
      "      policy_loss: -0.0006545293726958334\n",
      "      total_loss: 16.541950225830078\n",
      "      vf_explained_var: 0.9742956757545471\n",
      "      vf_loss: 16.542606353759766\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26233628392219543\n",
      "      kl: 0.026860836893320084\n",
      "      policy_loss: 0.0011276688892394304\n",
      "      total_loss: 26.46970558166504\n",
      "      vf_explained_var: 0.9746035933494568\n",
      "      vf_loss: 26.468578338623047\n",
      "    sample_time_ms: 20033.886\n",
      "    update_time_ms: 7.807\n",
      "  iterations_since_restore: 779\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.74632576009422\n",
      "    rl_1: 104.38586459738275\n",
      "  time_since_restore: 18356.72781062126\n",
      "  time_this_iter_s: 22.667313814163208\n",
      "  time_total_s: 18356.72781062126\n",
      "  timestamp: 1550811795\n",
      "  timesteps_since_restore: 7790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7790000\n",
      "  training_iteration: 779\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18356 s, 779 iter, 7790000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-03-38\n",
      "  done: false\n",
      "  episode_len_mean: 89.15044247787611\n",
      "  episode_reward_max: 222.90081408891552\n",
      "  episode_reward_mean: 164.66877451544372\n",
      "  episode_reward_min: -157.59686354271844\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 82324\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.743\n",
      "    load_time_ms: 2.472\n",
      "    num_steps_sampled: 7800000\n",
      "    num_steps_trained: 7800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28313204646110535\n",
      "      kl: 0.012840967625379562\n",
      "      policy_loss: -0.00503659900277853\n",
      "      total_loss: 48.04631042480469\n",
      "      vf_explained_var: 0.9420625567436218\n",
      "      vf_loss: 48.05133819580078\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.24679061770439148\n",
      "      kl: 0.051991675049066544\n",
      "      policy_loss: 0.00878171343356371\n",
      "      total_loss: 84.60515594482422\n",
      "      vf_explained_var: 0.9269708395004272\n",
      "      vf_loss: 84.59636688232422\n",
      "    sample_time_ms: 20031.337\n",
      "    update_time_ms: 8.044\n",
      "  iterations_since_restore: 780\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.08557241532455\n",
      "    rl_1: 98.58320210011915\n",
      "  time_since_restore: 18379.90046262741\n",
      "  time_this_iter_s: 23.172652006149292\n",
      "  time_total_s: 18379.90046262741\n",
      "  timestamp: 1550811818\n",
      "  timesteps_since_restore: 7800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7800000\n",
      "  training_iteration: 780\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18379 s, 780 iter, 7800000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-04-01\n",
      "  done: false\n",
      "  episode_len_mean: 89.6036036036036\n",
      "  episode_reward_max: 220.6946467404602\n",
      "  episode_reward_mean: 181.19877468086682\n",
      "  episode_reward_min: 144.07133399447463\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 82435\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.763\n",
      "    load_time_ms: 2.479\n",
      "    num_steps_sampled: 7810000\n",
      "    num_steps_trained: 7810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3578960597515106\n",
      "      kl: 0.028744684532284737\n",
      "      policy_loss: -2.7900658096768893e-05\n",
      "      total_loss: 2.6674907207489014\n",
      "      vf_explained_var: 0.9949920773506165\n",
      "      vf_loss: 2.6675186157226562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2811775803565979\n",
      "      kl: 0.013886699452996254\n",
      "      policy_loss: 0.0021342432592064142\n",
      "      total_loss: 2.560126304626465\n",
      "      vf_explained_var: 0.9974859356880188\n",
      "      vf_loss: 2.5579915046691895\n",
      "    sample_time_ms: 20012.672\n",
      "    update_time_ms: 7.585\n",
      "  iterations_since_restore: 781\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.95387492272337\n",
      "    rl_1: 106.24489975814348\n",
      "  time_since_restore: 18402.694713830948\n",
      "  time_this_iter_s: 22.794251203536987\n",
      "  time_total_s: 18402.694713830948\n",
      "  timestamp: 1550811841\n",
      "  timesteps_since_restore: 7810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7810000\n",
      "  training_iteration: 781\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18402 s, 781 iter, 7810000 ts, 181 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-04-24\n",
      "  done: false\n",
      "  episode_len_mean: 89.46017699115045\n",
      "  episode_reward_max: 219.20156935775168\n",
      "  episode_reward_mean: 179.98989977121522\n",
      "  episode_reward_min: 140.40426842634258\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 82548\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.55\n",
      "    load_time_ms: 2.485\n",
      "    num_steps_sampled: 7820000\n",
      "    num_steps_trained: 7820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32948732376098633\n",
      "      kl: 0.022125188261270523\n",
      "      policy_loss: -0.002623470965772867\n",
      "      total_loss: 2.9434142112731934\n",
      "      vf_explained_var: 0.9944832921028137\n",
      "      vf_loss: 2.946037530899048\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26429271697998047\n",
      "      kl: 0.017259864136576653\n",
      "      policy_loss: -0.002899936866015196\n",
      "      total_loss: 2.3211424350738525\n",
      "      vf_explained_var: 0.9975622892379761\n",
      "      vf_loss: 2.324042797088623\n",
      "    sample_time_ms: 20005.436\n",
      "    update_time_ms: 7.521\n",
      "  iterations_since_restore: 782\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.58339803635701\n",
      "    rl_1: 105.4065017348582\n",
      "  time_since_restore: 18425.907658100128\n",
      "  time_this_iter_s: 23.212944269180298\n",
      "  time_total_s: 18425.907658100128\n",
      "  timestamp: 1550811864\n",
      "  timesteps_since_restore: 7820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7820000\n",
      "  training_iteration: 782\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18425 s, 782 iter, 7820000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-04-48\n",
      "  done: false\n",
      "  episode_len_mean: 89.33333333333333\n",
      "  episode_reward_max: 215.43141428618978\n",
      "  episode_reward_mean: 171.20886431908568\n",
      "  episode_reward_min: -159.2836350747459\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 82659\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.517\n",
      "    load_time_ms: 2.476\n",
      "    num_steps_sampled: 7830000\n",
      "    num_steps_trained: 7830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3109266459941864\n",
      "      kl: 0.018706221133470535\n",
      "      policy_loss: 0.00019601073290687054\n",
      "      total_loss: 29.53227996826172\n",
      "      vf_explained_var: 0.9556655883789062\n",
      "      vf_loss: 29.532079696655273\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2322133183479309\n",
      "      kl: 0.027445439249277115\n",
      "      policy_loss: 0.0018145990325137973\n",
      "      total_loss: 50.55571746826172\n",
      "      vf_explained_var: 0.9516100883483887\n",
      "      vf_loss: 50.55390930175781\n",
      "    sample_time_ms: 20031.474\n",
      "    update_time_ms: 7.331\n",
      "  iterations_since_restore: 783\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.87523674160128\n",
      "    rl_1: 101.33362757748431\n",
      "  time_since_restore: 18449.395765304565\n",
      "  time_this_iter_s: 23.488107204437256\n",
      "  time_total_s: 18449.395765304565\n",
      "  timestamp: 1550811888\n",
      "  timesteps_since_restore: 7830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7830000\n",
      "  training_iteration: 783\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18449 s, 783 iter, 7830000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-05-11\n",
      "  done: false\n",
      "  episode_len_mean: 90.03571428571429\n",
      "  episode_reward_max: 221.020253195965\n",
      "  episode_reward_mean: 180.43517936633128\n",
      "  episode_reward_min: 145.83353038773066\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 82771\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.362\n",
      "    load_time_ms: 2.439\n",
      "    num_steps_sampled: 7840000\n",
      "    num_steps_trained: 7840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3595886528491974\n",
      "      kl: 0.024240825325250626\n",
      "      policy_loss: -0.0015245294198393822\n",
      "      total_loss: 2.548896312713623\n",
      "      vf_explained_var: 0.9954002499580383\n",
      "      vf_loss: 2.5504202842712402\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2895919680595398\n",
      "      kl: 0.018468201160430908\n",
      "      policy_loss: -0.0005298007163219154\n",
      "      total_loss: 2.2939651012420654\n",
      "      vf_explained_var: 0.9977870583534241\n",
      "      vf_loss: 2.294495105743408\n",
      "    sample_time_ms: 20030.723\n",
      "    update_time_ms: 7.134\n",
      "  iterations_since_restore: 784\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.18604063556803\n",
      "    rl_1: 106.24913873076324\n",
      "  time_since_restore: 18472.471690416336\n",
      "  time_this_iter_s: 23.07592511177063\n",
      "  time_total_s: 18472.471690416336\n",
      "  timestamp: 1550811911\n",
      "  timesteps_since_restore: 7840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7840000\n",
      "  training_iteration: 784\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18472 s, 784 iter, 7840000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-05-34\n",
      "  done: false\n",
      "  episode_len_mean: 88.29203539823008\n",
      "  episode_reward_max: 211.20047747092337\n",
      "  episode_reward_mean: 172.87004784698615\n",
      "  episode_reward_min: -155.2585250382166\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 82884\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.374\n",
      "    load_time_ms: 2.439\n",
      "    num_steps_sampled: 7850000\n",
      "    num_steps_trained: 7850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31146329641342163\n",
      "      kl: 0.016456443816423416\n",
      "      policy_loss: -0.003766549052670598\n",
      "      total_loss: 21.56549835205078\n",
      "      vf_explained_var: 0.9695574641227722\n",
      "      vf_loss: 21.569265365600586\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2620790898799896\n",
      "      kl: 0.035828378051519394\n",
      "      policy_loss: 0.004601325839757919\n",
      "      total_loss: 43.29220199584961\n",
      "      vf_explained_var: 0.9639166593551636\n",
      "      vf_loss: 43.28760528564453\n",
      "    sample_time_ms: 20041.828\n",
      "    update_time_ms: 7.449\n",
      "  iterations_since_restore: 785\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.47752065801286\n",
      "    rl_1: 101.39252718897328\n",
      "  time_since_restore: 18495.483883857727\n",
      "  time_this_iter_s: 23.01219344139099\n",
      "  time_total_s: 18495.483883857727\n",
      "  timestamp: 1550811934\n",
      "  timesteps_since_restore: 7850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7850000\n",
      "  training_iteration: 785\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18495 s, 785 iter, 7850000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-05-57\n",
      "  done: false\n",
      "  episode_len_mean: 88.9375\n",
      "  episode_reward_max: 219.58396095198674\n",
      "  episode_reward_mean: 176.50610287967842\n",
      "  episode_reward_min: -165.65803118313153\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 82996\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.908\n",
      "    load_time_ms: 2.439\n",
      "    num_steps_sampled: 7860000\n",
      "    num_steps_trained: 7860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3339219391345978\n",
      "      kl: 0.01809251867234707\n",
      "      policy_loss: 9.289345325669274e-05\n",
      "      total_loss: 12.88543701171875\n",
      "      vf_explained_var: 0.9786645770072937\n",
      "      vf_loss: 12.885343551635742\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32322317361831665\n",
      "      kl: 0.05942496284842491\n",
      "      policy_loss: 0.014596447348594666\n",
      "      total_loss: 19.06733512878418\n",
      "      vf_explained_var: 0.9828162789344788\n",
      "      vf_loss: 19.052738189697266\n",
      "    sample_time_ms: 20004.104\n",
      "    update_time_ms: 7.345\n",
      "  iterations_since_restore: 786\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.16365540452418\n",
      "    rl_1: 103.34244747515426\n",
      "  time_since_restore: 18518.525020599365\n",
      "  time_this_iter_s: 23.041136741638184\n",
      "  time_total_s: 18518.525020599365\n",
      "  timestamp: 1550811957\n",
      "  timesteps_since_restore: 7860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7860000\n",
      "  training_iteration: 786\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18518 s, 786 iter, 7860000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-06-20\n",
      "  done: false\n",
      "  episode_len_mean: 88.87610619469027\n",
      "  episode_reward_max: 216.9015487576392\n",
      "  episode_reward_mean: 176.08831155288783\n",
      "  episode_reward_min: -149.60138488154058\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 83109\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3152.864\n",
      "    load_time_ms: 2.49\n",
      "    num_steps_sampled: 7870000\n",
      "    num_steps_trained: 7870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35631486773490906\n",
      "      kl: 0.012664830312132835\n",
      "      policy_loss: -0.001126931863836944\n",
      "      total_loss: 10.93329906463623\n",
      "      vf_explained_var: 0.9826269745826721\n",
      "      vf_loss: 10.93442440032959\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2556160092353821\n",
      "      kl: 0.02115308865904808\n",
      "      policy_loss: -0.0009512476972304285\n",
      "      total_loss: 20.12456703186035\n",
      "      vf_explained_var: 0.9811884760856628\n",
      "      vf_loss: 20.125518798828125\n",
      "    sample_time_ms: 19967.141\n",
      "    update_time_ms: 7.247\n",
      "  iterations_since_restore: 787\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.19536462862416\n",
      "    rl_1: 102.89294692426365\n",
      "  time_since_restore: 18541.80793952942\n",
      "  time_this_iter_s: 23.28291893005371\n",
      "  time_total_s: 18541.80793952942\n",
      "  timestamp: 1550811980\n",
      "  timesteps_since_restore: 7870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7870000\n",
      "  training_iteration: 787\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18541 s, 787 iter, 7870000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-06-44\n",
      "  done: false\n",
      "  episode_len_mean: 89.72072072072072\n",
      "  episode_reward_max: 222.16527117422658\n",
      "  episode_reward_mean: 177.23453301762754\n",
      "  episode_reward_min: 140.12771394082\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 83220\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.515\n",
      "    load_time_ms: 2.352\n",
      "    num_steps_sampled: 7880000\n",
      "    num_steps_trained: 7880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2930825352668762\n",
      "      kl: 0.023662444204092026\n",
      "      policy_loss: -0.0027255271561443806\n",
      "      total_loss: 4.406993865966797\n",
      "      vf_explained_var: 0.9926800727844238\n",
      "      vf_loss: 4.409718990325928\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.21366862952709198\n",
      "      kl: 0.012226522900164127\n",
      "      policy_loss: -0.0004471267166081816\n",
      "      total_loss: 3.6195855140686035\n",
      "      vf_explained_var: 0.9964011907577515\n",
      "      vf_loss: 3.62003231048584\n",
      "    sample_time_ms: 19937.437\n",
      "    update_time_ms: 7.54\n",
      "  iterations_since_restore: 788\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.81120524673936\n",
      "    rl_1: 103.42332777088816\n",
      "  time_since_restore: 18565.094982624054\n",
      "  time_this_iter_s: 23.28704309463501\n",
      "  time_total_s: 18565.094982624054\n",
      "  timestamp: 1550812004\n",
      "  timesteps_since_restore: 7880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7880000\n",
      "  training_iteration: 788\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18565 s, 788 iter, 7880000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-07-07\n",
      "  done: false\n",
      "  episode_len_mean: 89.74107142857143\n",
      "  episode_reward_max: 222.3566084431631\n",
      "  episode_reward_mean: 170.5311595158871\n",
      "  episode_reward_min: -152.23566275032138\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 83332\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.99\n",
      "    load_time_ms: 2.251\n",
      "    num_steps_sampled: 7890000\n",
      "    num_steps_trained: 7890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.278737336397171\n",
      "      kl: 0.014539087191224098\n",
      "      policy_loss: 0.0002684266655705869\n",
      "      total_loss: 22.617582321166992\n",
      "      vf_explained_var: 0.9668249487876892\n",
      "      vf_loss: 22.617311477661133\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2353663146495819\n",
      "      kl: 0.02281232178211212\n",
      "      policy_loss: -0.001956929685547948\n",
      "      total_loss: 40.32645034790039\n",
      "      vf_explained_var: 0.9638819694519043\n",
      "      vf_loss: 40.32840347290039\n",
      "    sample_time_ms: 19997.349\n",
      "    update_time_ms: 7.646\n",
      "  iterations_since_restore: 789\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.56725601009921\n",
      "    rl_1: 101.96390350578788\n",
      "  time_since_restore: 18588.378125429153\n",
      "  time_this_iter_s: 23.283142805099487\n",
      "  time_total_s: 18588.378125429153\n",
      "  timestamp: 1550812027\n",
      "  timesteps_since_restore: 7890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7890000\n",
      "  training_iteration: 789\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18588 s, 789 iter, 7890000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-07-30\n",
      "  done: false\n",
      "  episode_len_mean: 89.97272727272727\n",
      "  episode_reward_max: 224.0485990968463\n",
      "  episode_reward_mean: 181.7052581382619\n",
      "  episode_reward_min: 144.9719493518001\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 83442\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.519\n",
      "    load_time_ms: 2.287\n",
      "    num_steps_sampled: 7900000\n",
      "    num_steps_trained: 7900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.36864423751831055\n",
      "      kl: 0.020896902307868004\n",
      "      policy_loss: -0.0020364795345813036\n",
      "      total_loss: 1.677895188331604\n",
      "      vf_explained_var: 0.9967607259750366\n",
      "      vf_loss: 1.6799312829971313\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25791704654693604\n",
      "      kl: 0.014700078405439854\n",
      "      policy_loss: -0.00044038594933226705\n",
      "      total_loss: 1.6499090194702148\n",
      "      vf_explained_var: 0.9982933402061462\n",
      "      vf_loss: 1.6503493785858154\n",
      "    sample_time_ms: 19961.249\n",
      "    update_time_ms: 7.152\n",
      "  iterations_since_restore: 790\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.94105912207345\n",
      "    rl_1: 106.76419901618846\n",
      "  time_since_restore: 18611.18094778061\n",
      "  time_this_iter_s: 22.80282235145569\n",
      "  time_total_s: 18611.18094778061\n",
      "  timestamp: 1550812050\n",
      "  timesteps_since_restore: 7900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7900000\n",
      "  training_iteration: 790\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18611 s, 790 iter, 7900000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-07-53\n",
      "  done: false\n",
      "  episode_len_mean: 89.625\n",
      "  episode_reward_max: 216.7427927343851\n",
      "  episode_reward_mean: 175.05412036304838\n",
      "  episode_reward_min: -152.4556055226659\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 83554\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.467\n",
      "    load_time_ms: 2.339\n",
      "    num_steps_sampled: 7910000\n",
      "    num_steps_trained: 7910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3272401690483093\n",
      "      kl: 0.017808346077799797\n",
      "      policy_loss: -0.0033244986552745104\n",
      "      total_loss: 14.031286239624023\n",
      "      vf_explained_var: 0.9772500395774841\n",
      "      vf_loss: 14.034613609313965\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.24861027300357819\n",
      "      kl: 0.015131991356611252\n",
      "      policy_loss: 0.0032575030345469713\n",
      "      total_loss: 23.1612491607666\n",
      "      vf_explained_var: 0.9784982204437256\n",
      "      vf_loss: 23.157989501953125\n",
      "    sample_time_ms: 20022.732\n",
      "    update_time_ms: 7.161\n",
      "  iterations_since_restore: 791\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.92557304727309\n",
      "    rl_1: 105.12854731577525\n",
      "  time_since_restore: 18634.632244110107\n",
      "  time_this_iter_s: 23.45129632949829\n",
      "  time_total_s: 18634.632244110107\n",
      "  timestamp: 1550812073\n",
      "  timesteps_since_restore: 7910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7910000\n",
      "  training_iteration: 791\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18634 s, 791 iter, 7910000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-08-17\n",
      "  done: false\n",
      "  episode_len_mean: 89.94594594594595\n",
      "  episode_reward_max: 215.18474768041972\n",
      "  episode_reward_mean: 173.18524744506516\n",
      "  episode_reward_min: -157.89246020929173\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 83665\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.747\n",
      "    load_time_ms: 2.338\n",
      "    num_steps_sampled: 7920000\n",
      "    num_steps_trained: 7920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28739726543426514\n",
      "      kl: 0.014500469900667667\n",
      "      policy_loss: -0.0060353525914251804\n",
      "      total_loss: 14.680830001831055\n",
      "      vf_explained_var: 0.9744370579719543\n",
      "      vf_loss: 14.686867713928223\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.19918674230575562\n",
      "      kl: 0.01276667881757021\n",
      "      policy_loss: 0.004870460368692875\n",
      "      total_loss: 23.978954315185547\n",
      "      vf_explained_var: 0.9776261448860168\n",
      "      vf_loss: 23.974084854125977\n",
      "    sample_time_ms: 20024.831\n",
      "    update_time_ms: 7.456\n",
      "  iterations_since_restore: 792\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.62345905762427\n",
      "    rl_1: 103.5617883874409\n",
      "  time_since_restore: 18657.860205173492\n",
      "  time_this_iter_s: 23.22796106338501\n",
      "  time_total_s: 18657.860205173492\n",
      "  timestamp: 1550812097\n",
      "  timesteps_since_restore: 7920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7920000\n",
      "  training_iteration: 792\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18657 s, 792 iter, 7920000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-08-40\n",
      "  done: false\n",
      "  episode_len_mean: 89.74107142857143\n",
      "  episode_reward_max: 218.19842531932105\n",
      "  episode_reward_mean: 177.55880075997737\n",
      "  episode_reward_min: 140.94769686776493\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 83777\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.448\n",
      "    load_time_ms: 2.298\n",
      "    num_steps_sampled: 7930000\n",
      "    num_steps_trained: 7930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3054536283016205\n",
      "      kl: 0.020210111513733864\n",
      "      policy_loss: -0.0018841020064428449\n",
      "      total_loss: 9.111034393310547\n",
      "      vf_explained_var: 0.9846817255020142\n",
      "      vf_loss: 9.11291790008545\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.256974995136261\n",
      "      kl: 0.015703367069363594\n",
      "      policy_loss: 0.0028273293282836676\n",
      "      total_loss: 10.426960945129395\n",
      "      vf_explained_var: 0.9906428456306458\n",
      "      vf_loss: 10.424135208129883\n",
      "    sample_time_ms: 19963.358\n",
      "    update_time_ms: 7.285\n",
      "  iterations_since_restore: 793\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.36375183086236\n",
      "    rl_1: 106.19504892911503\n",
      "  time_since_restore: 18680.818964481354\n",
      "  time_this_iter_s: 22.958759307861328\n",
      "  time_total_s: 18680.818964481354\n",
      "  timestamp: 1550812120\n",
      "  timesteps_since_restore: 7930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7930000\n",
      "  training_iteration: 793\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18680 s, 793 iter, 7930000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-09-02\n",
      "  done: false\n",
      "  episode_len_mean: 89.77477477477477\n",
      "  episode_reward_max: 221.12123875122163\n",
      "  episode_reward_mean: 182.13782451543872\n",
      "  episode_reward_min: 137.97495924083913\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 83888\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3152.549\n",
      "    load_time_ms: 2.375\n",
      "    num_steps_sampled: 7940000\n",
      "    num_steps_trained: 7940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3904012441635132\n",
      "      kl: 0.027473226189613342\n",
      "      policy_loss: 0.000583803397603333\n",
      "      total_loss: 2.253570795059204\n",
      "      vf_explained_var: 0.9959602355957031\n",
      "      vf_loss: 2.2529871463775635\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2697238028049469\n",
      "      kl: 0.010390980169177055\n",
      "      policy_loss: -0.001617305213585496\n",
      "      total_loss: 2.273009777069092\n",
      "      vf_explained_var: 0.9979232549667358\n",
      "      vf_loss: 2.274627208709717\n",
      "    sample_time_ms: 19907.19\n",
      "    update_time_ms: 7.645\n",
      "  iterations_since_restore: 794\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.53981729977448\n",
      "    rl_1: 106.59800721566428\n",
      "  time_since_restore: 18703.35916876793\n",
      "  time_this_iter_s: 22.540204286575317\n",
      "  time_total_s: 18703.35916876793\n",
      "  timestamp: 1550812142\n",
      "  timesteps_since_restore: 7940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7940000\n",
      "  training_iteration: 794\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18703 s, 794 iter, 7940000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-09-26\n",
      "  done: false\n",
      "  episode_len_mean: 89.95495495495496\n",
      "  episode_reward_max: 220.625371259626\n",
      "  episode_reward_mean: 179.90494478388442\n",
      "  episode_reward_min: 147.4972671443526\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 83999\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3155.006\n",
      "    load_time_ms: 2.361\n",
      "    num_steps_sampled: 7950000\n",
      "    num_steps_trained: 7950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.36393457651138306\n",
      "      kl: 0.03423184156417847\n",
      "      policy_loss: -6.918948201928288e-05\n",
      "      total_loss: 2.0898170471191406\n",
      "      vf_explained_var: 0.9960331320762634\n",
      "      vf_loss: 2.08988618850708\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2842966616153717\n",
      "      kl: 0.016534488648176193\n",
      "      policy_loss: 0.00012342221452854574\n",
      "      total_loss: 2.205564022064209\n",
      "      vf_explained_var: 0.9978601336479187\n",
      "      vf_loss: 2.2054405212402344\n",
      "    sample_time_ms: 19929.871\n",
      "    update_time_ms: 7.467\n",
      "  iterations_since_restore: 795\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.52418338285196\n",
      "    rl_1: 106.38076140103244\n",
      "  time_since_restore: 18726.62028479576\n",
      "  time_this_iter_s: 23.26111602783203\n",
      "  time_total_s: 18726.62028479576\n",
      "  timestamp: 1550812166\n",
      "  timesteps_since_restore: 7950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7950000\n",
      "  training_iteration: 795\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18726 s, 795 iter, 7950000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-09-49\n",
      "  done: false\n",
      "  episode_len_mean: 89.26548672566372\n",
      "  episode_reward_max: 219.81897408843335\n",
      "  episode_reward_mean: 179.00919600302512\n",
      "  episode_reward_min: 142.05299539509247\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 84112\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3154.302\n",
      "    load_time_ms: 2.422\n",
      "    num_steps_sampled: 7960000\n",
      "    num_steps_trained: 7960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3593803346157074\n",
      "      kl: 0.6195900440216064\n",
      "      policy_loss: 0.04252060130238533\n",
      "      total_loss: 1.9813534021377563\n",
      "      vf_explained_var: 0.9962201118469238\n",
      "      vf_loss: 1.938832402229309\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32807084918022156\n",
      "      kl: 0.010711411945521832\n",
      "      policy_loss: 0.0028494305443018675\n",
      "      total_loss: 1.888706088066101\n",
      "      vf_explained_var: 0.9980587363243103\n",
      "      vf_loss: 1.8858566284179688\n",
      "    sample_time_ms: 19953.374\n",
      "    update_time_ms: 7.59\n",
      "  iterations_since_restore: 796\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.55112376602948\n",
      "    rl_1: 105.45807223699559\n",
      "  time_since_restore: 18749.896838903427\n",
      "  time_this_iter_s: 23.276554107666016\n",
      "  time_total_s: 18749.896838903427\n",
      "  timestamp: 1550812189\n",
      "  timesteps_since_restore: 7960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7960000\n",
      "  training_iteration: 796\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18749 s, 796 iter, 7960000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-10-12\n",
      "  done: false\n",
      "  episode_len_mean: 89.65765765765765\n",
      "  episode_reward_max: 219.20623292033\n",
      "  episode_reward_mean: 176.76883637805466\n",
      "  episode_reward_min: 144.0315160908154\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 84223\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.754\n",
      "    load_time_ms: 2.372\n",
      "    num_steps_sampled: 7970000\n",
      "    num_steps_trained: 7970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3457344174385071\n",
      "      kl: 0.017180591821670532\n",
      "      policy_loss: -0.0032676339615136385\n",
      "      total_loss: 1.6470822095870972\n",
      "      vf_explained_var: 0.9966235160827637\n",
      "      vf_loss: 1.650349736213684\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3096965551376343\n",
      "      kl: 0.017192592844367027\n",
      "      policy_loss: 0.0016394754638895392\n",
      "      total_loss: 1.737642765045166\n",
      "      vf_explained_var: 0.9981202483177185\n",
      "      vf_loss: 1.7360033988952637\n",
      "    sample_time_ms: 19977.914\n",
      "    update_time_ms: 8.171\n",
      "  iterations_since_restore: 797\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.95121745660626\n",
      "    rl_1: 103.81761892144841\n",
      "  time_since_restore: 18773.26358103752\n",
      "  time_this_iter_s: 23.36674213409424\n",
      "  time_total_s: 18773.26358103752\n",
      "  timestamp: 1550812212\n",
      "  timesteps_since_restore: 7970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7970000\n",
      "  training_iteration: 797\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18773 s, 797 iter, 7970000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-10-36\n",
      "  done: false\n",
      "  episode_len_mean: 89.51351351351352\n",
      "  episode_reward_max: 219.71127431475404\n",
      "  episode_reward_mean: 177.60095270498488\n",
      "  episode_reward_min: 145.56186785303348\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 84334\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.383\n",
      "    load_time_ms: 2.372\n",
      "    num_steps_sampled: 7980000\n",
      "    num_steps_trained: 7980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32000496983528137\n",
      "      kl: 0.01559542864561081\n",
      "      policy_loss: 2.0243418475729413e-05\n",
      "      total_loss: 1.9184093475341797\n",
      "      vf_explained_var: 0.996186375617981\n",
      "      vf_loss: 1.9183892011642456\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3469293713569641\n",
      "      kl: 0.010280461050570011\n",
      "      policy_loss: -0.0005966615863144398\n",
      "      total_loss: 2.131153106689453\n",
      "      vf_explained_var: 0.9977921843528748\n",
      "      vf_loss: 2.1317498683929443\n",
      "    sample_time_ms: 19980.841\n",
      "    update_time_ms: 7.684\n",
      "  iterations_since_restore: 798\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.97990690562251\n",
      "    rl_1: 105.62104579936232\n",
      "  time_since_restore: 18796.589789628983\n",
      "  time_this_iter_s: 23.32620859146118\n",
      "  time_total_s: 18796.589789628983\n",
      "  timestamp: 1550812236\n",
      "  timesteps_since_restore: 7980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7980000\n",
      "  training_iteration: 798\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18796 s, 798 iter, 7980000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-10-59\n",
      "  done: false\n",
      "  episode_len_mean: 89.16071428571429\n",
      "  episode_reward_max: 222.84138198788878\n",
      "  episode_reward_mean: 179.26934762570912\n",
      "  episode_reward_min: -154.2365675618906\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 84446\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.251\n",
      "    load_time_ms: 2.376\n",
      "    num_steps_sampled: 7990000\n",
      "    num_steps_trained: 7990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3607652485370636\n",
      "      kl: 0.01126466691493988\n",
      "      policy_loss: -0.003191988915205002\n",
      "      total_loss: 21.2303524017334\n",
      "      vf_explained_var: 0.9705176949501038\n",
      "      vf_loss: 21.23354148864746\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28094181418418884\n",
      "      kl: 0.02105480059981346\n",
      "      policy_loss: -0.003876088187098503\n",
      "      total_loss: 38.18138885498047\n",
      "      vf_explained_var: 0.9694852828979492\n",
      "      vf_loss: 38.18526077270508\n",
      "    sample_time_ms: 20007.988\n",
      "    update_time_ms: 7.909\n",
      "  iterations_since_restore: 799\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.80010634256361\n",
      "    rl_1: 105.4692412831455\n",
      "  time_since_restore: 18820.123207092285\n",
      "  time_this_iter_s: 23.533417463302612\n",
      "  time_total_s: 18820.123207092285\n",
      "  timestamp: 1550812259\n",
      "  timesteps_since_restore: 7990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 7990000\n",
      "  training_iteration: 799\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18820 s, 799 iter, 7990000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-11-23\n",
      "  done: false\n",
      "  episode_len_mean: 90.54464285714286\n",
      "  episode_reward_max: 220.72215398982866\n",
      "  episode_reward_mean: 176.9643686137028\n",
      "  episode_reward_min: 140.28921785377318\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 84558\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.561\n",
      "    load_time_ms: 2.311\n",
      "    num_steps_sampled: 8000000\n",
      "    num_steps_trained: 8000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31011763215065\n",
      "      kl: 0.019441528245806694\n",
      "      policy_loss: -0.0007729888893663883\n",
      "      total_loss: 1.8367098569869995\n",
      "      vf_explained_var: 0.9962888956069946\n",
      "      vf_loss: 1.8374825716018677\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.27331727743148804\n",
      "      kl: 0.01635773852467537\n",
      "      policy_loss: 0.0032971722539514303\n",
      "      total_loss: 1.6391710042953491\n",
      "      vf_explained_var: 0.9982658624649048\n",
      "      vf_loss: 1.6358739137649536\n",
      "    sample_time_ms: 20093.441\n",
      "    update_time_ms: 7.928\n",
      "  iterations_since_restore: 800\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.48558353159558\n",
      "    rl_1: 104.47878508210722\n",
      "  time_since_restore: 18843.772488594055\n",
      "  time_this_iter_s: 23.64928150177002\n",
      "  time_total_s: 18843.772488594055\n",
      "  timestamp: 1550812283\n",
      "  timesteps_since_restore: 8000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8000000\n",
      "  training_iteration: 800\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18843 s, 800 iter, 8000000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-11-47\n",
      "  done: false\n",
      "  episode_len_mean: 89.12612612612612\n",
      "  episode_reward_max: 220.0022066577547\n",
      "  episode_reward_mean: 176.3310830607376\n",
      "  episode_reward_min: 136.09084344891355\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 84669\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.555\n",
      "    load_time_ms: 2.326\n",
      "    num_steps_sampled: 8010000\n",
      "    num_steps_trained: 8010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3375432789325714\n",
      "      kl: 0.019028428941965103\n",
      "      policy_loss: 0.0009623178630135953\n",
      "      total_loss: 1.6167383193969727\n",
      "      vf_explained_var: 0.9966936111450195\n",
      "      vf_loss: 1.6157763004302979\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28777727484703064\n",
      "      kl: 0.016816336661577225\n",
      "      policy_loss: 0.00260002538561821\n",
      "      total_loss: 1.789377212524414\n",
      "      vf_explained_var: 0.9980887174606323\n",
      "      vf_loss: 1.7867770195007324\n",
      "    sample_time_ms: 20117.668\n",
      "    update_time_ms: 8.132\n",
      "  iterations_since_restore: 801\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.63899630503278\n",
      "    rl_1: 103.6920867557048\n",
      "  time_since_restore: 18867.41502046585\n",
      "  time_this_iter_s: 23.642531871795654\n",
      "  time_total_s: 18867.41502046585\n",
      "  timestamp: 1550812307\n",
      "  timesteps_since_restore: 8010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8010000\n",
      "  training_iteration: 801\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18867 s, 801 iter, 8010000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-12-10\n",
      "  done: false\n",
      "  episode_len_mean: 89.8018018018018\n",
      "  episode_reward_max: 220.6195337381827\n",
      "  episode_reward_mean: 177.0630903684617\n",
      "  episode_reward_min: 140.94986577164937\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 84780\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.57\n",
      "    load_time_ms: 2.318\n",
      "    num_steps_sampled: 8020000\n",
      "    num_steps_trained: 8020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3421974182128906\n",
      "      kl: 0.020083457231521606\n",
      "      policy_loss: 0.001670515164732933\n",
      "      total_loss: 1.5489990711212158\n",
      "      vf_explained_var: 0.9967581629753113\n",
      "      vf_loss: 1.5473287105560303\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3145538568496704\n",
      "      kl: 0.008973702788352966\n",
      "      policy_loss: -0.0006129059474915266\n",
      "      total_loss: 1.5719400644302368\n",
      "      vf_explained_var: 0.998358428478241\n",
      "      vf_loss: 1.572553038597107\n",
      "    sample_time_ms: 20123.823\n",
      "    update_time_ms: 7.795\n",
      "  iterations_since_restore: 802\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.01709371416209\n",
      "    rl_1: 105.04599665429961\n",
      "  time_since_restore: 18890.70058965683\n",
      "  time_this_iter_s: 23.285569190979004\n",
      "  time_total_s: 18890.70058965683\n",
      "  timestamp: 1550812330\n",
      "  timesteps_since_restore: 8020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8020000\n",
      "  training_iteration: 802\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18890 s, 802 iter, 8020000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-12-33\n",
      "  done: false\n",
      "  episode_len_mean: 89.83928571428571\n",
      "  episode_reward_max: 218.176899087463\n",
      "  episode_reward_mean: 176.91784102159053\n",
      "  episode_reward_min: 139.04503057758004\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 84892\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3121.216\n",
      "    load_time_ms: 2.286\n",
      "    num_steps_sampled: 8030000\n",
      "    num_steps_trained: 8030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3557937741279602\n",
      "      kl: 0.015357790514826775\n",
      "      policy_loss: -0.00017121777636930346\n",
      "      total_loss: 1.5145304203033447\n",
      "      vf_explained_var: 0.9970067143440247\n",
      "      vf_loss: 1.5147016048431396\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31033021211624146\n",
      "      kl: 0.012845484539866447\n",
      "      policy_loss: 0.004516600631177425\n",
      "      total_loss: 1.6162196397781372\n",
      "      vf_explained_var: 0.9982612729072571\n",
      "      vf_loss: 1.6117030382156372\n",
      "    sample_time_ms: 20144.395\n",
      "    update_time_ms: 7.747\n",
      "  iterations_since_restore: 803\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.45463821938785\n",
      "    rl_1: 104.46320280220267\n",
      "  time_since_restore: 18913.76304101944\n",
      "  time_this_iter_s: 23.062451362609863\n",
      "  time_total_s: 18913.76304101944\n",
      "  timestamp: 1550812353\n",
      "  timesteps_since_restore: 8030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8030000\n",
      "  training_iteration: 803\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18913 s, 803 iter, 8030000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-12-57\n",
      "  done: false\n",
      "  episode_len_mean: 89.44642857142857\n",
      "  episode_reward_max: 218.9348397827705\n",
      "  episode_reward_mean: 180.2173192155576\n",
      "  episode_reward_min: 142.29867794889543\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 85004\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.082\n",
      "    load_time_ms: 2.282\n",
      "    num_steps_sampled: 8040000\n",
      "    num_steps_trained: 8040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3746216595172882\n",
      "      kl: 0.03448501601815224\n",
      "      policy_loss: 0.007011357229202986\n",
      "      total_loss: 1.4802500009536743\n",
      "      vf_explained_var: 0.9972070455551147\n",
      "      vf_loss: 1.4732385873794556\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32949328422546387\n",
      "      kl: 0.02065727487206459\n",
      "      policy_loss: -0.0007868999382480979\n",
      "      total_loss: 1.7163687944412231\n",
      "      vf_explained_var: 0.9982457160949707\n",
      "      vf_loss: 1.7171556949615479\n",
      "    sample_time_ms: 20247.924\n",
      "    update_time_ms: 7.614\n",
      "  iterations_since_restore: 804\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.07971769794199\n",
      "    rl_1: 106.1376015176156\n",
      "  time_since_restore: 18937.34681725502\n",
      "  time_this_iter_s: 23.583776235580444\n",
      "  time_total_s: 18937.34681725502\n",
      "  timestamp: 1550812377\n",
      "  timesteps_since_restore: 8040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8040000\n",
      "  training_iteration: 804\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18937 s, 804 iter, 8040000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-13-20\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_reward_max: 218.80439379120457\n",
      "  episode_reward_mean: 173.60114722675493\n",
      "  episode_reward_min: -150.05600910190594\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 85115\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.97\n",
      "    load_time_ms: 2.288\n",
      "    num_steps_sampled: 8050000\n",
      "    num_steps_trained: 8050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.37014517188072205\n",
      "      kl: 0.014954652637243271\n",
      "      policy_loss: -0.0035031007137149572\n",
      "      total_loss: 20.48056411743164\n",
      "      vf_explained_var: 0.9671716094017029\n",
      "      vf_loss: 20.484066009521484\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3658905625343323\n",
      "      kl: 0.022101256996393204\n",
      "      policy_loss: 0.0018580517498776317\n",
      "      total_loss: 38.64344024658203\n",
      "      vf_explained_var: 0.9665355086326599\n",
      "      vf_loss: 38.64158630371094\n",
      "    sample_time_ms: 20260.088\n",
      "    update_time_ms: 7.981\n",
      "  iterations_since_restore: 805\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.96299819656164\n",
      "    rl_1: 105.63814903019333\n",
      "  time_since_restore: 18960.913514614105\n",
      "  time_this_iter_s: 23.566697359085083\n",
      "  time_total_s: 18960.913514614105\n",
      "  timestamp: 1550812400\n",
      "  timesteps_since_restore: 8050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8050000\n",
      "  training_iteration: 805\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18960 s, 805 iter, 8050000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-13-43\n",
      "  done: false\n",
      "  episode_len_mean: 89.61607142857143\n",
      "  episode_reward_max: 214.69999606150705\n",
      "  episode_reward_mean: 178.45374384618637\n",
      "  episode_reward_min: 143.94095212732782\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 85227\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.531\n",
      "    load_time_ms: 2.252\n",
      "    num_steps_sampled: 8060000\n",
      "    num_steps_trained: 8060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3774193227291107\n",
      "      kl: 0.03218195214867592\n",
      "      policy_loss: 0.009716426022350788\n",
      "      total_loss: 1.4802180528640747\n",
      "      vf_explained_var: 0.9970217943191528\n",
      "      vf_loss: 1.4705015420913696\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.25630322098731995\n",
      "      kl: 0.011759653687477112\n",
      "      policy_loss: 0.001003345474600792\n",
      "      total_loss: 1.3270660638809204\n",
      "      vf_explained_var: 0.9985936284065247\n",
      "      vf_loss: 1.3260629177093506\n",
      "    sample_time_ms: 20220.015\n",
      "    update_time_ms: 7.885\n",
      "  iterations_since_restore: 806\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.78906678003185\n",
      "    rl_1: 104.6646770661545\n",
      "  time_since_restore: 18983.73620581627\n",
      "  time_this_iter_s: 22.822691202163696\n",
      "  time_total_s: 18983.73620581627\n",
      "  timestamp: 1550812423\n",
      "  timesteps_since_restore: 8060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8060000\n",
      "  training_iteration: 806\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 18983 s, 806 iter, 8060000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-14-07\n",
      "  done: false\n",
      "  episode_len_mean: 88.86607142857143\n",
      "  episode_reward_max: 223.5859871497661\n",
      "  episode_reward_mean: 178.44863018888398\n",
      "  episode_reward_min: 144.65998428893485\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 85339\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.025\n",
      "    load_time_ms: 2.243\n",
      "    num_steps_sampled: 8070000\n",
      "    num_steps_trained: 8070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3952120840549469\n",
      "      kl: 0.01574018970131874\n",
      "      policy_loss: 2.0211418814142235e-05\n",
      "      total_loss: 1.3514372110366821\n",
      "      vf_explained_var: 0.9972700476646423\n",
      "      vf_loss: 1.3514169454574585\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2929539978504181\n",
      "      kl: 0.012787293642759323\n",
      "      policy_loss: 0.003927611280232668\n",
      "      total_loss: 1.4943288564682007\n",
      "      vf_explained_var: 0.9984237551689148\n",
      "      vf_loss: 1.4904013872146606\n",
      "    sample_time_ms: 20209.562\n",
      "    update_time_ms: 7.453\n",
      "  iterations_since_restore: 807\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.78061039566015\n",
      "    rl_1: 104.66801979322383\n",
      "  time_since_restore: 19006.97646665573\n",
      "  time_this_iter_s: 23.24026083946228\n",
      "  time_total_s: 19006.97646665573\n",
      "  timestamp: 1550812447\n",
      "  timesteps_since_restore: 8070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8070000\n",
      "  training_iteration: 807\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19006 s, 807 iter, 8070000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-14-30\n",
      "  done: false\n",
      "  episode_len_mean: 89.66071428571429\n",
      "  episode_reward_max: 219.41835698738194\n",
      "  episode_reward_mean: 176.15151609108125\n",
      "  episode_reward_min: 140.3746762177708\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 85451\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.145\n",
      "    load_time_ms: 2.243\n",
      "    num_steps_sampled: 8080000\n",
      "    num_steps_trained: 8080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3681510090827942\n",
      "      kl: 0.02222178317606449\n",
      "      policy_loss: 0.0014143189182505012\n",
      "      total_loss: 1.27177894115448\n",
      "      vf_explained_var: 0.997407078742981\n",
      "      vf_loss: 1.270364761352539\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.240083247423172\n",
      "      kl: 0.008778293617069721\n",
      "      policy_loss: -0.003001550445333123\n",
      "      total_loss: 1.3269908428192139\n",
      "      vf_explained_var: 0.9985772967338562\n",
      "      vf_loss: 1.329992413520813\n",
      "    sample_time_ms: 20239.498\n",
      "    update_time_ms: 7.571\n",
      "  iterations_since_restore: 808\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.45542501578723\n",
      "    rl_1: 103.69609107529402\n",
      "  time_since_restore: 19030.615946292877\n",
      "  time_this_iter_s: 23.639479637145996\n",
      "  time_total_s: 19030.615946292877\n",
      "  timestamp: 1550812470\n",
      "  timesteps_since_restore: 8080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8080000\n",
      "  training_iteration: 808\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19030 s, 808 iter, 8080000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-14-54\n",
      "  done: false\n",
      "  episode_len_mean: 90.04504504504504\n",
      "  episode_reward_max: 221.92827326556022\n",
      "  episode_reward_mean: 174.64397283947937\n",
      "  episode_reward_min: 142.1075028427723\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 85562\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.126\n",
      "    load_time_ms: 2.239\n",
      "    num_steps_sampled: 8090000\n",
      "    num_steps_trained: 8090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.36764705181121826\n",
      "      kl: 0.01652504876255989\n",
      "      policy_loss: -0.0017213347600772977\n",
      "      total_loss: 1.816784143447876\n",
      "      vf_explained_var: 0.9962336421012878\n",
      "      vf_loss: 1.8185056447982788\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.30523186922073364\n",
      "      kl: 0.017796438187360764\n",
      "      policy_loss: 0.003977459389716387\n",
      "      total_loss: 1.631618618965149\n",
      "      vf_explained_var: 0.9982622861862183\n",
      "      vf_loss: 1.6276410818099976\n",
      "    sample_time_ms: 20256.619\n",
      "    update_time_ms: 7.496\n",
      "  iterations_since_restore: 809\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.25268173447076\n",
      "    rl_1: 104.3912911050086\n",
      "  time_since_restore: 19054.310079097748\n",
      "  time_this_iter_s: 23.694132804870605\n",
      "  time_total_s: 19054.310079097748\n",
      "  timestamp: 1550812494\n",
      "  timesteps_since_restore: 8090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8090000\n",
      "  training_iteration: 809\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19054 s, 809 iter, 8090000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-15-17\n",
      "  done: false\n",
      "  episode_len_mean: 89.22522522522523\n",
      "  episode_reward_max: 215.24795441548704\n",
      "  episode_reward_mean: 173.14853096047557\n",
      "  episode_reward_min: -154.75489245589927\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 85673\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.451\n",
      "    load_time_ms: 2.245\n",
      "    num_steps_sampled: 8100000\n",
      "    num_steps_trained: 8100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3862001597881317\n",
      "      kl: 0.013036680407822132\n",
      "      policy_loss: -0.0009314786293543875\n",
      "      total_loss: 22.59684944152832\n",
      "      vf_explained_var: 0.9626415967941284\n",
      "      vf_loss: 22.597780227661133\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.27049753069877625\n",
      "      kl: 0.014825950376689434\n",
      "      policy_loss: -0.0009945064084604383\n",
      "      total_loss: 39.919857025146484\n",
      "      vf_explained_var: 0.9621753096580505\n",
      "      vf_loss: 39.92084884643555\n",
      "    sample_time_ms: 20181.567\n",
      "    update_time_ms: 7.779\n",
      "  iterations_since_restore: 810\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.23633773650221\n",
      "    rl_1: 102.91219322397336\n",
      "  time_since_restore: 19077.20437335968\n",
      "  time_this_iter_s: 22.894294261932373\n",
      "  time_total_s: 19077.20437335968\n",
      "  timestamp: 1550812517\n",
      "  timesteps_since_restore: 8100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8100000\n",
      "  training_iteration: 810\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19077 s, 810 iter, 8100000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-15-41\n",
      "  done: false\n",
      "  episode_len_mean: 88.82456140350877\n",
      "  episode_reward_max: 218.22960589452094\n",
      "  episode_reward_mean: 176.09755345396013\n",
      "  episode_reward_min: 140.55613564332936\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 85787\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.471\n",
      "    load_time_ms: 2.296\n",
      "    num_steps_sampled: 8110000\n",
      "    num_steps_trained: 8110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.47893911600112915\n",
      "      kl: 0.01886541210114956\n",
      "      policy_loss: 0.0005213368567638099\n",
      "      total_loss: 1.1935055255889893\n",
      "      vf_explained_var: 0.9974725246429443\n",
      "      vf_loss: 1.1929842233657837\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3161472976207733\n",
      "      kl: 0.017172889783978462\n",
      "      policy_loss: 0.0022377001587301493\n",
      "      total_loss: 1.2179338932037354\n",
      "      vf_explained_var: 0.9986968636512756\n",
      "      vf_loss: 1.215696096420288\n",
      "    sample_time_ms: 20167.953\n",
      "    update_time_ms: 7.709\n",
      "  iterations_since_restore: 811\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.15085395739399\n",
      "    rl_1: 103.94669949656615\n",
      "  time_since_restore: 19100.73118877411\n",
      "  time_this_iter_s: 23.52681541442871\n",
      "  time_total_s: 19100.73118877411\n",
      "  timestamp: 1550812541\n",
      "  timesteps_since_restore: 8110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8110000\n",
      "  training_iteration: 811\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19100 s, 811 iter, 8110000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-16-04\n",
      "  done: false\n",
      "  episode_len_mean: 89.6036036036036\n",
      "  episode_reward_max: 222.4437429094467\n",
      "  episode_reward_mean: 177.0785186510186\n",
      "  episode_reward_min: 143.6469208372504\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 85898\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.767\n",
      "    load_time_ms: 2.352\n",
      "    num_steps_sampled: 8120000\n",
      "    num_steps_trained: 8120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4094932973384857\n",
      "      kl: 0.01690930686891079\n",
      "      policy_loss: 0.0016591919120401144\n",
      "      total_loss: 1.5763683319091797\n",
      "      vf_explained_var: 0.9967671036720276\n",
      "      vf_loss: 1.5747092962265015\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28206053376197815\n",
      "      kl: 0.012202163226902485\n",
      "      policy_loss: 0.0005557342083193362\n",
      "      total_loss: 1.3765474557876587\n",
      "      vf_explained_var: 0.9985418915748596\n",
      "      vf_loss: 1.375991702079773\n",
      "    sample_time_ms: 20154.719\n",
      "    update_time_ms: 7.687\n",
      "  iterations_since_restore: 812\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.30957158087134\n",
      "    rl_1: 104.7689470701473\n",
      "  time_since_restore: 19123.880904197693\n",
      "  time_this_iter_s: 23.149715423583984\n",
      "  time_total_s: 19123.880904197693\n",
      "  timestamp: 1550812564\n",
      "  timesteps_since_restore: 8120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8120000\n",
      "  training_iteration: 812\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19123 s, 812 iter, 8120000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-16-27\n",
      "  done: false\n",
      "  episode_len_mean: 89.44642857142857\n",
      "  episode_reward_max: 217.82458742469564\n",
      "  episode_reward_mean: 179.63614281482657\n",
      "  episode_reward_min: 143.34982576911705\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 86010\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.961\n",
      "    load_time_ms: 2.378\n",
      "    num_steps_sampled: 8130000\n",
      "    num_steps_trained: 8130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.45844101905822754\n",
      "      kl: 0.027969984337687492\n",
      "      policy_loss: 0.0005577467381954193\n",
      "      total_loss: 1.2449467182159424\n",
      "      vf_explained_var: 0.9974775910377502\n",
      "      vf_loss: 1.2443888187408447\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3151768445968628\n",
      "      kl: 0.012057880870997906\n",
      "      policy_loss: 0.002680053934454918\n",
      "      total_loss: 1.299146294593811\n",
      "      vf_explained_var: 0.9986488223075867\n",
      "      vf_loss: 1.2964664697647095\n",
      "    sample_time_ms: 20158.26\n",
      "    update_time_ms: 7.737\n",
      "  iterations_since_restore: 813\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.92618575153868\n",
      "    rl_1: 105.70995706328787\n",
      "  time_since_restore: 19146.99900650978\n",
      "  time_this_iter_s: 23.118102312088013\n",
      "  time_total_s: 19146.99900650978\n",
      "  timestamp: 1550812587\n",
      "  timesteps_since_restore: 8130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8130000\n",
      "  training_iteration: 813\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19146 s, 813 iter, 8130000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-16-51\n",
      "  done: false\n",
      "  episode_len_mean: 89.82727272727273\n",
      "  episode_reward_max: 219.3870731716723\n",
      "  episode_reward_mean: 177.4611045548621\n",
      "  episode_reward_min: 139.6150905988054\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 86120\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.731\n",
      "    load_time_ms: 2.33\n",
      "    num_steps_sampled: 8140000\n",
      "    num_steps_trained: 8140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.44001108407974243\n",
      "      kl: 0.02602621167898178\n",
      "      policy_loss: 0.004397264681756496\n",
      "      total_loss: 1.4137040376663208\n",
      "      vf_explained_var: 0.9971233010292053\n",
      "      vf_loss: 1.4093068838119507\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31586897373199463\n",
      "      kl: 0.015169177204370499\n",
      "      policy_loss: 0.0003145094960927963\n",
      "      total_loss: 1.2451750040054321\n",
      "      vf_explained_var: 0.9987014532089233\n",
      "      vf_loss: 1.2448606491088867\n",
      "    sample_time_ms: 20158.486\n",
      "    update_time_ms: 7.77\n",
      "  iterations_since_restore: 814\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.29926419044102\n",
      "    rl_1: 105.16184036442108\n",
      "  time_since_restore: 19170.687458753586\n",
      "  time_this_iter_s: 23.68845224380493\n",
      "  time_total_s: 19170.687458753586\n",
      "  timestamp: 1550812611\n",
      "  timesteps_since_restore: 8140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8140000\n",
      "  training_iteration: 814\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19170 s, 814 iter, 8140000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-17-14\n",
      "  done: false\n",
      "  episode_len_mean: 89.76785714285714\n",
      "  episode_reward_max: 219.22667693625732\n",
      "  episode_reward_mean: 176.12513185508462\n",
      "  episode_reward_min: 144.33094384446758\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 86232\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.874\n",
      "    load_time_ms: 2.337\n",
      "    num_steps_sampled: 8150000\n",
      "    num_steps_trained: 8150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.42589691281318665\n",
      "      kl: 0.02063382975757122\n",
      "      policy_loss: 0.00017419907089788467\n",
      "      total_loss: 1.4498319625854492\n",
      "      vf_explained_var: 0.9969172477722168\n",
      "      vf_loss: 1.449657917022705\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.33218473196029663\n",
      "      kl: 0.014625275507569313\n",
      "      policy_loss: -0.0021853952202945948\n",
      "      total_loss: 1.4016454219818115\n",
      "      vf_explained_var: 0.9985159635543823\n",
      "      vf_loss: 1.403830885887146\n",
      "    sample_time_ms: 20158.549\n",
      "    update_time_ms: 7.368\n",
      "  iterations_since_restore: 815\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.24374175968431\n",
      "    rl_1: 104.88139009540032\n",
      "  time_since_restore: 19194.052758455276\n",
      "  time_this_iter_s: 23.365299701690674\n",
      "  time_total_s: 19194.052758455276\n",
      "  timestamp: 1550812634\n",
      "  timesteps_since_restore: 8150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8150000\n",
      "  training_iteration: 815\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19194 s, 815 iter, 8150000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-17-37\n",
      "  done: false\n",
      "  episode_len_mean: 89.75\n",
      "  episode_reward_max: 220.31426938904784\n",
      "  episode_reward_mean: 172.49874993848152\n",
      "  episode_reward_min: 138.7070215950769\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 86344\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.42\n",
      "    load_time_ms: 2.399\n",
      "    num_steps_sampled: 8160000\n",
      "    num_steps_trained: 8160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.39400801062583923\n",
      "      kl: 0.025622831657528877\n",
      "      policy_loss: 0.0009275751071982086\n",
      "      total_loss: 1.8080565929412842\n",
      "      vf_explained_var: 0.9960910081863403\n",
      "      vf_loss: 1.80712890625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3467816710472107\n",
      "      kl: 0.013843534514307976\n",
      "      policy_loss: -0.0004314289253670722\n",
      "      total_loss: 1.5154985189437866\n",
      "      vf_explained_var: 0.9983534216880798\n",
      "      vf_loss: 1.515929937362671\n",
      "    sample_time_ms: 20155.524\n",
      "    update_time_ms: 7.583\n",
      "  iterations_since_restore: 816\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.52989872204624\n",
      "    rl_1: 102.9688512164353\n",
      "  time_since_restore: 19216.84750366211\n",
      "  time_this_iter_s: 22.794745206832886\n",
      "  time_total_s: 19216.84750366211\n",
      "  timestamp: 1550812657\n",
      "  timesteps_since_restore: 8160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8160000\n",
      "  training_iteration: 816\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19216 s, 816 iter, 8160000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-18-00\n",
      "  done: false\n",
      "  episode_len_mean: 89.95495495495496\n",
      "  episode_reward_max: 222.39164243778868\n",
      "  episode_reward_mean: 177.94454910197584\n",
      "  episode_reward_min: 135.99730499311343\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 86455\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.394\n",
      "    load_time_ms: 2.392\n",
      "    num_steps_sampled: 8170000\n",
      "    num_steps_trained: 8170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4628708064556122\n",
      "      kl: 0.017689138650894165\n",
      "      policy_loss: -0.0016106126131489873\n",
      "      total_loss: 1.4202500581741333\n",
      "      vf_explained_var: 0.9972657561302185\n",
      "      vf_loss: 1.4218605756759644\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3745737671852112\n",
      "      kl: 0.01583150029182434\n",
      "      policy_loss: 0.0006198586779646575\n",
      "      total_loss: 1.2471970319747925\n",
      "      vf_explained_var: 0.9986621141433716\n",
      "      vf_loss: 1.2465773820877075\n",
      "    sample_time_ms: 20138.898\n",
      "    update_time_ms: 7.58\n",
      "  iterations_since_restore: 817\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.85261399853091\n",
      "    rl_1: 105.09193510344501\n",
      "  time_since_restore: 19239.932735204697\n",
      "  time_this_iter_s: 23.08523154258728\n",
      "  time_total_s: 19239.932735204697\n",
      "  timestamp: 1550812680\n",
      "  timesteps_since_restore: 8170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8170000\n",
      "  training_iteration: 817\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19239 s, 817 iter, 8170000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-18-23\n",
      "  done: false\n",
      "  episode_len_mean: 88.70796460176992\n",
      "  episode_reward_max: 218.45024390336786\n",
      "  episode_reward_mean: 174.9004844507548\n",
      "  episode_reward_min: -154.06324292250633\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 86568\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.207\n",
      "    load_time_ms: 2.544\n",
      "    num_steps_sampled: 8180000\n",
      "    num_steps_trained: 8180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.46935945749282837\n",
      "      kl: 0.01283940114080906\n",
      "      policy_loss: -0.004125895444303751\n",
      "      total_loss: 27.721893310546875\n",
      "      vf_explained_var: 0.9534600377082825\n",
      "      vf_loss: 27.726016998291016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28940194845199585\n",
      "      kl: 0.019738441333174706\n",
      "      policy_loss: -0.006370107177644968\n",
      "      total_loss: 43.53473663330078\n",
      "      vf_explained_var: 0.958277702331543\n",
      "      vf_loss: 43.54110336303711\n",
      "    sample_time_ms: 20118.312\n",
      "    update_time_ms: 7.662\n",
      "  iterations_since_restore: 818\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.24023878860994\n",
      "    rl_1: 102.66024566214486\n",
      "  time_since_restore: 19263.348212242126\n",
      "  time_this_iter_s: 23.41547703742981\n",
      "  time_total_s: 19263.348212242126\n",
      "  timestamp: 1550812703\n",
      "  timesteps_since_restore: 8180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8180000\n",
      "  training_iteration: 818\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19263 s, 818 iter, 8180000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-18-47\n",
      "  done: false\n",
      "  episode_len_mean: 89.75\n",
      "  episode_reward_max: 221.305007165611\n",
      "  episode_reward_mean: 179.98434214039662\n",
      "  episode_reward_min: 144.18337137592917\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 86680\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.433\n",
      "    load_time_ms: 2.578\n",
      "    num_steps_sampled: 8190000\n",
      "    num_steps_trained: 8190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.46049773693084717\n",
      "      kl: 0.21745970845222473\n",
      "      policy_loss: 0.0064347065053880215\n",
      "      total_loss: 1.5291111469268799\n",
      "      vf_explained_var: 0.9971064925193787\n",
      "      vf_loss: 1.5226764678955078\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.30033624172210693\n",
      "      kl: 0.013447584584355354\n",
      "      policy_loss: -0.0022347751073539257\n",
      "      total_loss: 1.4712169170379639\n",
      "      vf_explained_var: 0.9984750747680664\n",
      "      vf_loss: 1.4734516143798828\n",
      "    sample_time_ms: 20086.856\n",
      "    update_time_ms: 7.656\n",
      "  iterations_since_restore: 819\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.55488125795692\n",
      "    rl_1: 106.4294608824397\n",
      "  time_since_restore: 19286.77827525139\n",
      "  time_this_iter_s: 23.430063009262085\n",
      "  time_total_s: 19286.77827525139\n",
      "  timestamp: 1550812727\n",
      "  timesteps_since_restore: 8190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8190000\n",
      "  training_iteration: 819\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19286 s, 819 iter, 8190000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-19-11\n",
      "  done: false\n",
      "  episode_len_mean: 89.32432432432432\n",
      "  episode_reward_max: 217.93389498347392\n",
      "  episode_reward_mean: 174.335092734104\n",
      "  episode_reward_min: 145.49283166329582\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 86791\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.647\n",
      "    load_time_ms: 2.653\n",
      "    num_steps_sampled: 8200000\n",
      "    num_steps_trained: 8200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4245564043521881\n",
      "      kl: 0.025637630373239517\n",
      "      policy_loss: -0.0030846791341900826\n",
      "      total_loss: 1.6968038082122803\n",
      "      vf_explained_var: 0.9965104460716248\n",
      "      vf_loss: 1.6998882293701172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3069339394569397\n",
      "      kl: 0.010122904554009438\n",
      "      policy_loss: 0.0011817928170785308\n",
      "      total_loss: 1.7314239740371704\n",
      "      vf_explained_var: 0.9981138706207275\n",
      "      vf_loss: 1.7302417755126953\n",
      "    sample_time_ms: 20157.891\n",
      "    update_time_ms: 7.59\n",
      "  iterations_since_restore: 820\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.50345556375323\n",
      "    rl_1: 102.83163717035075\n",
      "  time_since_restore: 19310.417978286743\n",
      "  time_this_iter_s: 23.639703035354614\n",
      "  time_total_s: 19310.417978286743\n",
      "  timestamp: 1550812751\n",
      "  timesteps_since_restore: 8200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8200000\n",
      "  training_iteration: 820\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19310 s, 820 iter, 8200000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-19-34\n",
      "  done: false\n",
      "  episode_len_mean: 89.90090090090091\n",
      "  episode_reward_max: 215.63782713809394\n",
      "  episode_reward_mean: 177.59329590642682\n",
      "  episode_reward_min: 144.14893904344325\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 86902\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.552\n",
      "    load_time_ms: 2.536\n",
      "    num_steps_sampled: 8210000\n",
      "    num_steps_trained: 8210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4721134901046753\n",
      "      kl: 0.018738124519586563\n",
      "      policy_loss: -0.0011389779392629862\n",
      "      total_loss: 1.4751547574996948\n",
      "      vf_explained_var: 0.9970299005508423\n",
      "      vf_loss: 1.4762935638427734\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.33916690945625305\n",
      "      kl: 0.01702851429581642\n",
      "      policy_loss: 0.003912210930138826\n",
      "      total_loss: 1.3137362003326416\n",
      "      vf_explained_var: 0.9986482262611389\n",
      "      vf_loss: 1.3098241090774536\n",
      "    sample_time_ms: 20154.805\n",
      "    update_time_ms: 7.517\n",
      "  iterations_since_restore: 821\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.0084424820815\n",
      "    rl_1: 104.58485342434535\n",
      "  time_since_restore: 19333.899465322495\n",
      "  time_this_iter_s: 23.481487035751343\n",
      "  time_total_s: 19333.899465322495\n",
      "  timestamp: 1550812774\n",
      "  timesteps_since_restore: 8210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8210000\n",
      "  training_iteration: 821\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19333 s, 821 iter, 8210000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-19-58\n",
      "  done: false\n",
      "  episode_len_mean: 88.1842105263158\n",
      "  episode_reward_max: 224.35607992323753\n",
      "  episode_reward_mean: 172.69428018204016\n",
      "  episode_reward_min: -151.7206534368238\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 87016\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3136.108\n",
      "    load_time_ms: 2.537\n",
      "    num_steps_sampled: 8220000\n",
      "    num_steps_trained: 8220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.46986013650894165\n",
      "      kl: 0.010656772181391716\n",
      "      policy_loss: -0.002068233909085393\n",
      "      total_loss: 23.74875259399414\n",
      "      vf_explained_var: 0.9614178538322449\n",
      "      vf_loss: 23.75082015991211\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.29656925797462463\n",
      "      kl: 0.011456744745373726\n",
      "      policy_loss: -0.0047965338453650475\n",
      "      total_loss: 39.42165756225586\n",
      "      vf_explained_var: 0.9608832001686096\n",
      "      vf_loss: 39.42646026611328\n",
      "    sample_time_ms: 20192.192\n",
      "    update_time_ms: 7.533\n",
      "  iterations_since_restore: 822\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.83101729446054\n",
      "    rl_1: 100.86326288757955\n",
      "  time_since_restore: 19357.447857618332\n",
      "  time_this_iter_s: 23.548392295837402\n",
      "  time_total_s: 19357.447857618332\n",
      "  timestamp: 1550812798\n",
      "  timesteps_since_restore: 8220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8220000\n",
      "  training_iteration: 822\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19357 s, 822 iter, 8220000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-20-22\n",
      "  done: false\n",
      "  episode_len_mean: 90.2909090909091\n",
      "  episode_reward_max: 218.8287402897143\n",
      "  episode_reward_mean: 177.80751499565991\n",
      "  episode_reward_min: 142.59068671319704\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 87126\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3155.694\n",
      "    load_time_ms: 2.52\n",
      "    num_steps_sampled: 8230000\n",
      "    num_steps_trained: 8230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4670538306236267\n",
      "      kl: 0.019889535382390022\n",
      "      policy_loss: -0.0017941778060048819\n",
      "      total_loss: 1.4993692636489868\n",
      "      vf_explained_var: 0.9969643950462341\n",
      "      vf_loss: 1.5011636018753052\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.34690356254577637\n",
      "      kl: 0.015673870220780373\n",
      "      policy_loss: 0.0012206294341012836\n",
      "      total_loss: 1.4186124801635742\n",
      "      vf_explained_var: 0.9985531568527222\n",
      "      vf_loss: 1.4173920154571533\n",
      "    sample_time_ms: 20247.653\n",
      "    update_time_ms: 7.546\n",
      "  iterations_since_restore: 823\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.11225458901575\n",
      "    rl_1: 106.69526040664422\n",
      "  time_since_restore: 19381.31577897072\n",
      "  time_this_iter_s: 23.867921352386475\n",
      "  time_total_s: 19381.31577897072\n",
      "  timestamp: 1550812822\n",
      "  timesteps_since_restore: 8230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8230000\n",
      "  training_iteration: 823\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19381 s, 823 iter, 8230000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-20-45\n",
      "  done: false\n",
      "  episode_len_mean: 90.02678571428571\n",
      "  episode_reward_max: 222.2418288020493\n",
      "  episode_reward_mean: 180.31698980690126\n",
      "  episode_reward_min: 139.43850780581818\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 87238\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.653\n",
      "    load_time_ms: 2.611\n",
      "    num_steps_sampled: 8240000\n",
      "    num_steps_trained: 8240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5269933342933655\n",
      "      kl: 0.028669528663158417\n",
      "      policy_loss: 0.000374104012735188\n",
      "      total_loss: 1.1777937412261963\n",
      "      vf_explained_var: 0.997735857963562\n",
      "      vf_loss: 1.177419662475586\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3336126208305359\n",
      "      kl: 0.01765744760632515\n",
      "      policy_loss: 0.003724278649315238\n",
      "      total_loss: 1.051820993423462\n",
      "      vf_explained_var: 0.9989186525344849\n",
      "      vf_loss: 1.0480965375900269\n",
      "    sample_time_ms: 20207.281\n",
      "    update_time_ms: 7.854\n",
      "  iterations_since_restore: 824\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 74.60514374315811\n",
      "    rl_1: 105.7118460637431\n",
      "  time_since_restore: 19404.488998174667\n",
      "  time_this_iter_s: 23.173219203948975\n",
      "  time_total_s: 19404.488998174667\n",
      "  timestamp: 1550812845\n",
      "  timesteps_since_restore: 8240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8240000\n",
      "  training_iteration: 824\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19404 s, 824 iter, 8240000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-21-08\n",
      "  done: false\n",
      "  episode_len_mean: 90.46846846846847\n",
      "  episode_reward_max: 219.00934513464762\n",
      "  episode_reward_mean: 179.2243628386112\n",
      "  episode_reward_min: 142.07473303573872\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 87349\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.511\n",
      "    load_time_ms: 2.615\n",
      "    num_steps_sampled: 8250000\n",
      "    num_steps_trained: 8250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4696820080280304\n",
      "      kl: 0.021395424380898476\n",
      "      policy_loss: 0.0009912388632073998\n",
      "      total_loss: 1.5099833011627197\n",
      "      vf_explained_var: 0.9968988299369812\n",
      "      vf_loss: 1.5089921951293945\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.34962573647499084\n",
      "      kl: 0.017098598182201385\n",
      "      policy_loss: 0.0004541740345302969\n",
      "      total_loss: 1.3143881559371948\n",
      "      vf_explained_var: 0.998656153678894\n",
      "      vf_loss: 1.313934087753296\n",
      "    sample_time_ms: 20179.531\n",
      "    update_time_ms: 7.896\n",
      "  iterations_since_restore: 825\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.27800473164181\n",
      "    rl_1: 106.9463581069694\n",
      "  time_since_restore: 19427.58480167389\n",
      "  time_this_iter_s: 23.0958034992218\n",
      "  time_total_s: 19427.58480167389\n",
      "  timestamp: 1550812868\n",
      "  timesteps_since_restore: 8250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8250000\n",
      "  training_iteration: 825\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19427 s, 825 iter, 8250000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-21-31\n",
      "  done: false\n",
      "  episode_len_mean: 88.76785714285714\n",
      "  episode_reward_max: 223.276822965157\n",
      "  episode_reward_mean: 179.81069209745428\n",
      "  episode_reward_min: 139.2203595257943\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 87461\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3156.233\n",
      "    load_time_ms: 2.601\n",
      "    num_steps_sampled: 8260000\n",
      "    num_steps_trained: 8260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5300477743148804\n",
      "      kl: 0.029669499024748802\n",
      "      policy_loss: 0.00315936584956944\n",
      "      total_loss: 1.31302011013031\n",
      "      vf_explained_var: 0.9975293278694153\n",
      "      vf_loss: 1.3098605871200562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3893442153930664\n",
      "      kl: 0.008564517833292484\n",
      "      policy_loss: 0.0003495818527881056\n",
      "      total_loss: 1.2821413278579712\n",
      "      vf_explained_var: 0.9986311793327332\n",
      "      vf_loss: 1.2817916870117188\n",
      "    sample_time_ms: 20232.419\n",
      "    update_time_ms: 8.017\n",
      "  iterations_since_restore: 826\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.33368165123166\n",
      "    rl_1: 104.47701044622262\n",
      "  time_since_restore: 19451.02554154396\n",
      "  time_this_iter_s: 23.44073987007141\n",
      "  time_total_s: 19451.02554154396\n",
      "  timestamp: 1550812891\n",
      "  timesteps_since_restore: 8260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8260000\n",
      "  training_iteration: 826\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19451 s, 826 iter, 8260000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-21-55\n",
      "  done: false\n",
      "  episode_len_mean: 89.12612612612612\n",
      "  episode_reward_max: 223.80606544566095\n",
      "  episode_reward_mean: 180.29559811312225\n",
      "  episode_reward_min: 143.70844485402628\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 87572\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3153.128\n",
      "    load_time_ms: 2.587\n",
      "    num_steps_sampled: 8270000\n",
      "    num_steps_trained: 8270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5212500691413879\n",
      "      kl: 0.018144305795431137\n",
      "      policy_loss: -0.0018340670503675938\n",
      "      total_loss: 1.2128807306289673\n",
      "      vf_explained_var: 0.9976688027381897\n",
      "      vf_loss: 1.2147148847579956\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3657156229019165\n",
      "      kl: 0.018226610496640205\n",
      "      policy_loss: 0.003804562147706747\n",
      "      total_loss: 1.163234829902649\n",
      "      vf_explained_var: 0.9987834095954895\n",
      "      vf_loss: 1.1594301462173462\n",
      "    sample_time_ms: 20239.244\n",
      "    update_time_ms: 8.083\n",
      "  iterations_since_restore: 827\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 75.22586093349791\n",
      "    rl_1: 105.06973717962434\n",
      "  time_since_restore: 19474.14715218544\n",
      "  time_this_iter_s: 23.121610641479492\n",
      "  time_total_s: 19474.14715218544\n",
      "  timestamp: 1550812915\n",
      "  timesteps_since_restore: 8270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8270000\n",
      "  training_iteration: 827\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19474 s, 827 iter, 8270000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-22-18\n",
      "  done: false\n",
      "  episode_len_mean: 90.20535714285714\n",
      "  episode_reward_max: 222.4464748288579\n",
      "  episode_reward_mean: 175.2821412484038\n",
      "  episode_reward_min: 142.07909126622764\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 87684\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.11\n",
      "    load_time_ms: 2.424\n",
      "    num_steps_sampled: 8280000\n",
      "    num_steps_trained: 8280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4698549509048462\n",
      "      kl: 0.02729887142777443\n",
      "      policy_loss: 0.005173773504793644\n",
      "      total_loss: 1.4956891536712646\n",
      "      vf_explained_var: 0.9969026446342468\n",
      "      vf_loss: 1.490515112876892\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.32851821184158325\n",
      "      kl: 0.012773772701621056\n",
      "      policy_loss: 0.0013932455331087112\n",
      "      total_loss: 1.362610936164856\n",
      "      vf_explained_var: 0.9985437393188477\n",
      "      vf_loss: 1.3612176179885864\n",
      "    sample_time_ms: 20194.99\n",
      "    update_time_ms: 8.113\n",
      "  iterations_since_restore: 828\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.98559989738739\n",
      "    rl_1: 104.29654135101639\n",
      "  time_since_restore: 19497.084602594376\n",
      "  time_this_iter_s: 22.937450408935547\n",
      "  time_total_s: 19497.084602594376\n",
      "  timestamp: 1550812938\n",
      "  timesteps_since_restore: 8280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8280000\n",
      "  training_iteration: 828\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19497 s, 828 iter, 8280000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-22-41\n",
      "  done: false\n",
      "  episode_len_mean: 89.95495495495496\n",
      "  episode_reward_max: 217.8090238650866\n",
      "  episode_reward_mean: 177.7309928814374\n",
      "  episode_reward_min: 142.4996864072968\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 87795\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3149.199\n",
      "    load_time_ms: 2.429\n",
      "    num_steps_sampled: 8290000\n",
      "    num_steps_trained: 8290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.47918519377708435\n",
      "      kl: 0.019220998510718346\n",
      "      policy_loss: -0.0007789851515553892\n",
      "      total_loss: 1.4529523849487305\n",
      "      vf_explained_var: 0.9971235394477844\n",
      "      vf_loss: 1.4537314176559448\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3174302577972412\n",
      "      kl: 0.014457633718848228\n",
      "      policy_loss: 0.0012783887796103954\n",
      "      total_loss: 1.3476669788360596\n",
      "      vf_explained_var: 0.998586893081665\n",
      "      vf_loss: 1.3463886976242065\n",
      "    sample_time_ms: 20204.923\n",
      "    update_time_ms: 7.893\n",
      "  iterations_since_restore: 829\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.08345107486487\n",
      "    rl_1: 104.64754180657256\n",
      "  time_since_restore: 19520.605020046234\n",
      "  time_this_iter_s: 23.52041745185852\n",
      "  time_total_s: 19520.605020046234\n",
      "  timestamp: 1550812961\n",
      "  timesteps_since_restore: 8290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8290000\n",
      "  training_iteration: 829\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19520 s, 829 iter, 8290000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-23-05\n",
      "  done: false\n",
      "  episode_len_mean: 89.29464285714286\n",
      "  episode_reward_max: 219.44186599859162\n",
      "  episode_reward_mean: 175.22676694475928\n",
      "  episode_reward_min: -149.71409281880068\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 87907\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.611\n",
      "    load_time_ms: 2.417\n",
      "    num_steps_sampled: 8300000\n",
      "    num_steps_trained: 8300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5268866419792175\n",
      "      kl: 0.0140357855707407\n",
      "      policy_loss: -0.006036193110048771\n",
      "      total_loss: 20.51220703125\n",
      "      vf_explained_var: 0.9690998792648315\n",
      "      vf_loss: 20.51824188232422\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3270900547504425\n",
      "      kl: 0.03287225961685181\n",
      "      policy_loss: -0.0024355368223041296\n",
      "      total_loss: 37.954559326171875\n",
      "      vf_explained_var: 0.9654133319854736\n",
      "      vf_loss: 37.95698928833008\n",
      "    sample_time_ms: 20171.641\n",
      "    update_time_ms: 7.967\n",
      "  iterations_since_restore: 830\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.97412904693763\n",
      "    rl_1: 103.25263789782164\n",
      "  time_since_restore: 19543.874593257904\n",
      "  time_this_iter_s: 23.269573211669922\n",
      "  time_total_s: 19543.874593257904\n",
      "  timestamp: 1550812985\n",
      "  timesteps_since_restore: 8300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8300000\n",
      "  training_iteration: 830\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19543 s, 830 iter, 8300000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-23-29\n",
      "  done: false\n",
      "  episode_len_mean: 89.34821428571429\n",
      "  episode_reward_max: 224.10680787064783\n",
      "  episode_reward_mean: 178.5404205229632\n",
      "  episode_reward_min: 144.0531153490753\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 88019\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.124\n",
      "    load_time_ms: 2.424\n",
      "    num_steps_sampled: 8310000\n",
      "    num_steps_trained: 8310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.534593403339386\n",
      "      kl: 0.02574453316628933\n",
      "      policy_loss: 0.0017210779478773475\n",
      "      total_loss: 1.3628144264221191\n",
      "      vf_explained_var: 0.9973788857460022\n",
      "      vf_loss: 1.361093521118164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.34536972641944885\n",
      "      kl: 0.01217953022569418\n",
      "      policy_loss: 0.0025964160449802876\n",
      "      total_loss: 1.384582281112671\n",
      "      vf_explained_var: 0.9985564947128296\n",
      "      vf_loss: 1.3819855451583862\n",
      "    sample_time_ms: 20234.936\n",
      "    update_time_ms: 7.926\n",
      "  iterations_since_restore: 831\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.06429915083253\n",
      "    rl_1: 105.47612137213068\n",
      "  time_since_restore: 19567.986201286316\n",
      "  time_this_iter_s: 24.111608028411865\n",
      "  time_total_s: 19567.986201286316\n",
      "  timestamp: 1550813009\n",
      "  timesteps_since_restore: 8310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8310000\n",
      "  training_iteration: 831\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19567 s, 831 iter, 8310000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-23-52\n",
      "  done: false\n",
      "  episode_len_mean: 89.36607142857143\n",
      "  episode_reward_max: 223.78012971472438\n",
      "  episode_reward_mean: 175.8772912064719\n",
      "  episode_reward_min: -151.14228497515901\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 88131\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3160.109\n",
      "    load_time_ms: 2.404\n",
      "    num_steps_sampled: 8320000\n",
      "    num_steps_trained: 8320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.49655434489250183\n",
      "      kl: 0.009578018449246883\n",
      "      policy_loss: 0.0004250252968631685\n",
      "      total_loss: 24.33562660217285\n",
      "      vf_explained_var: 0.9630991816520691\n",
      "      vf_loss: 24.335203170776367\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2860678434371948\n",
      "      kl: 0.01626433990895748\n",
      "      policy_loss: -0.0015115817077457905\n",
      "      total_loss: 40.84665298461914\n",
      "      vf_explained_var: 0.962885856628418\n",
      "      vf_loss: 40.84817123413086\n",
      "    sample_time_ms: 20189.456\n",
      "    update_time_ms: 8.256\n",
      "  iterations_since_restore: 832\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.84362937636588\n",
      "    rl_1: 104.03366183010601\n",
      "  time_since_restore: 19591.23188495636\n",
      "  time_this_iter_s: 23.245683670043945\n",
      "  time_total_s: 19591.23188495636\n",
      "  timestamp: 1550813032\n",
      "  timesteps_since_restore: 8320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8320000\n",
      "  training_iteration: 832\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19591 s, 832 iter, 8320000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-24-16\n",
      "  done: false\n",
      "  episode_len_mean: 89.15178571428571\n",
      "  episode_reward_max: 223.8078165555257\n",
      "  episode_reward_mean: 178.6585565971896\n",
      "  episode_reward_min: 141.94268623057854\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 88243\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.657\n",
      "    load_time_ms: 2.412\n",
      "    num_steps_sampled: 8330000\n",
      "    num_steps_trained: 8330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5116623640060425\n",
      "      kl: 30.298681259155273\n",
      "      policy_loss: 0.10769237577915192\n",
      "      total_loss: 1.499359130859375\n",
      "      vf_explained_var: 0.9972604513168335\n",
      "      vf_loss: 1.3916666507720947\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3280426859855652\n",
      "      kl: 0.011743533425033092\n",
      "      policy_loss: 0.0022968715056777\n",
      "      total_loss: 1.2136715650558472\n",
      "      vf_explained_var: 0.998692512512207\n",
      "      vf_loss: 1.2113745212554932\n",
      "    sample_time_ms: 20181.788\n",
      "    update_time_ms: 8.234\n",
      "  iterations_since_restore: 833\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 73.55671752668682\n",
      "    rl_1: 105.10183907050279\n",
      "  time_since_restore: 19614.817718982697\n",
      "  time_this_iter_s: 23.58583402633667\n",
      "  time_total_s: 19614.817718982697\n",
      "  timestamp: 1550813056\n",
      "  timesteps_since_restore: 8330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8330000\n",
      "  training_iteration: 833\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19614 s, 833 iter, 8330000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-24-39\n",
      "  done: false\n",
      "  episode_len_mean: 89.63963963963964\n",
      "  episode_reward_max: 220.51607536387243\n",
      "  episode_reward_mean: 173.0242477688859\n",
      "  episode_reward_min: 141.98469720580687\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 88354\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3149.216\n",
      "    load_time_ms: 2.303\n",
      "    num_steps_sampled: 8340000\n",
      "    num_steps_trained: 8340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.46765920519828796\n",
      "      kl: 0.01769147254526615\n",
      "      policy_loss: -0.001757694175466895\n",
      "      total_loss: 1.606484293937683\n",
      "      vf_explained_var: 0.99672931432724\n",
      "      vf_loss: 1.6082417964935303\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.29090267419815063\n",
      "      kl: 0.015306896530091763\n",
      "      policy_loss: 0.0013190805912017822\n",
      "      total_loss: 1.5108494758605957\n",
      "      vf_explained_var: 0.9983832240104675\n",
      "      vf_loss: 1.5095304250717163\n",
      "    sample_time_ms: 20167.206\n",
      "    update_time_ms: 7.738\n",
      "  iterations_since_restore: 834\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.7412336499693\n",
      "    rl_1: 104.28301411891663\n",
      "  time_since_restore: 19637.930396556854\n",
      "  time_this_iter_s: 23.112677574157715\n",
      "  time_total_s: 19637.930396556854\n",
      "  timestamp: 1550813079\n",
      "  timesteps_since_restore: 8340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8340000\n",
      "  training_iteration: 834\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19637 s, 834 iter, 8340000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-25-02\n",
      "  done: false\n",
      "  episode_len_mean: 89.59821428571429\n",
      "  episode_reward_max: 216.2271439971493\n",
      "  episode_reward_mean: 174.14678116597133\n",
      "  episode_reward_min: 136.96212395954032\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 88466\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3151.033\n",
      "    load_time_ms: 2.295\n",
      "    num_steps_sampled: 8350000\n",
      "    num_steps_trained: 8350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.48428794741630554\n",
      "      kl: 0.021191412582993507\n",
      "      policy_loss: -0.002487204037606716\n",
      "      total_loss: 1.598827600479126\n",
      "      vf_explained_var: 0.996574878692627\n",
      "      vf_loss: 1.601314663887024\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3670421540737152\n",
      "      kl: 0.015518813394010067\n",
      "      policy_loss: 0.001188049092888832\n",
      "      total_loss: 1.5054149627685547\n",
      "      vf_explained_var: 0.9984309673309326\n",
      "      vf_loss: 1.5042269229888916\n",
      "    sample_time_ms: 20192.631\n",
      "    update_time_ms: 7.672\n",
      "  iterations_since_restore: 835\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.45244271046124\n",
      "    rl_1: 105.69433845551009\n",
      "  time_since_restore: 19661.297676563263\n",
      "  time_this_iter_s: 23.36728000640869\n",
      "  time_total_s: 19661.297676563263\n",
      "  timestamp: 1550813102\n",
      "  timesteps_since_restore: 8350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8350000\n",
      "  training_iteration: 835\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19661 s, 835 iter, 8350000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-25-26\n",
      "  done: false\n",
      "  episode_len_mean: 89.81981981981981\n",
      "  episode_reward_max: 213.38433524492743\n",
      "  episode_reward_mean: 173.64220966610944\n",
      "  episode_reward_min: 135.8518626557246\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 88577\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.091\n",
      "    load_time_ms: 2.243\n",
      "    num_steps_sampled: 8360000\n",
      "    num_steps_trained: 8360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4824591875076294\n",
      "      kl: 0.04786904901266098\n",
      "      policy_loss: 0.001650679623708129\n",
      "      total_loss: 1.435711145401001\n",
      "      vf_explained_var: 0.9968685507774353\n",
      "      vf_loss: 1.4340605735778809\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3365859389305115\n",
      "      kl: 0.014541358686983585\n",
      "      policy_loss: 0.0011648866347968578\n",
      "      total_loss: 1.459956407546997\n",
      "      vf_explained_var: 0.9984925389289856\n",
      "      vf_loss: 1.458791732788086\n",
      "    sample_time_ms: 20205.617\n",
      "    update_time_ms: 7.239\n",
      "  iterations_since_restore: 836\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.00130624801857\n",
      "    rl_1: 105.64090341809093\n",
      "  time_since_restore: 19684.741049528122\n",
      "  time_this_iter_s: 23.44337296485901\n",
      "  time_total_s: 19684.741049528122\n",
      "  timestamp: 1550813126\n",
      "  timesteps_since_restore: 8360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8360000\n",
      "  training_iteration: 836\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19684 s, 836 iter, 8360000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-25-49\n",
      "  done: false\n",
      "  episode_len_mean: 89.97297297297297\n",
      "  episode_reward_max: 216.8788387550425\n",
      "  episode_reward_mean: 173.65373283576398\n",
      "  episode_reward_min: 140.243116271194\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 88688\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.538\n",
      "    load_time_ms: 2.353\n",
      "    num_steps_sampled: 8370000\n",
      "    num_steps_trained: 8370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4740579128265381\n",
      "      kl: 0.015120943076908588\n",
      "      policy_loss: -0.0020119037944823503\n",
      "      total_loss: 1.4791183471679688\n",
      "      vf_explained_var: 0.9966608881950378\n",
      "      vf_loss: 1.4811303615570068\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.329602986574173\n",
      "      kl: 0.012617907486855984\n",
      "      policy_loss: 0.0017131927888840437\n",
      "      total_loss: 1.379143238067627\n",
      "      vf_explained_var: 0.9985532760620117\n",
      "      vf_loss: 1.3774302005767822\n",
      "    sample_time_ms: 20177.425\n",
      "    update_time_ms: 7.074\n",
      "  iterations_since_restore: 837\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.24386273484211\n",
      "    rl_1: 106.40987010092188\n",
      "  time_since_restore: 19707.60608792305\n",
      "  time_this_iter_s: 22.86503839492798\n",
      "  time_total_s: 19707.60608792305\n",
      "  timestamp: 1550813149\n",
      "  timesteps_since_restore: 8370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8370000\n",
      "  training_iteration: 837\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19707 s, 837 iter, 8370000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-26-12\n",
      "  done: false\n",
      "  episode_len_mean: 89.28318584070796\n",
      "  episode_reward_max: 216.9215420657126\n",
      "  episode_reward_mean: 172.99537683690212\n",
      "  episode_reward_min: 135.26256651325954\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 88801\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.638\n",
      "    load_time_ms: 2.383\n",
      "    num_steps_sampled: 8380000\n",
      "    num_steps_trained: 8380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.48404213786125183\n",
      "      kl: 0.017613109201192856\n",
      "      policy_loss: -0.0004240893467795104\n",
      "      total_loss: 1.4268499612808228\n",
      "      vf_explained_var: 0.9969322085380554\n",
      "      vf_loss: 1.4272741079330444\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.28924140334129333\n",
      "      kl: 0.01056886836886406\n",
      "      policy_loss: -0.00154741236474365\n",
      "      total_loss: 1.2425220012664795\n",
      "      vf_explained_var: 0.9986631870269775\n",
      "      vf_loss: 1.2440693378448486\n",
      "    sample_time_ms: 20170.855\n",
      "    update_time_ms: 6.999\n",
      "  iterations_since_restore: 838\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.77877636434509\n",
      "    rl_1: 103.21660047255699\n",
      "  time_since_restore: 19730.53190612793\n",
      "  time_this_iter_s: 22.92581820487976\n",
      "  time_total_s: 19730.53190612793\n",
      "  timestamp: 1550813172\n",
      "  timesteps_since_restore: 8380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8380000\n",
      "  training_iteration: 838\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19730 s, 838 iter, 8380000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-26-35\n",
      "  done: false\n",
      "  episode_len_mean: 90.09909909909909\n",
      "  episode_reward_max: 217.14736300740384\n",
      "  episode_reward_mean: 175.45624617396496\n",
      "  episode_reward_min: 130.78903579096817\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 88912\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.929\n",
      "    load_time_ms: 2.352\n",
      "    num_steps_sampled: 8390000\n",
      "    num_steps_trained: 8390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5337855815887451\n",
      "      kl: 0.026516037061810493\n",
      "      policy_loss: 0.0032381589990109205\n",
      "      total_loss: 1.2430419921875\n",
      "      vf_explained_var: 0.9973731637001038\n",
      "      vf_loss: 1.2398040294647217\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3171400725841522\n",
      "      kl: 0.015224458649754524\n",
      "      policy_loss: -0.0007264238665811718\n",
      "      total_loss: 1.1425472497940063\n",
      "      vf_explained_var: 0.9988058805465698\n",
      "      vf_loss: 1.1432734727859497\n",
      "    sample_time_ms: 20167.445\n",
      "    update_time_ms: 6.997\n",
      "  iterations_since_restore: 839\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.42801021126898\n",
      "    rl_1: 106.02823596269596\n",
      "  time_since_restore: 19753.98949456215\n",
      "  time_this_iter_s: 23.45758843421936\n",
      "  time_total_s: 19753.98949456215\n",
      "  timestamp: 1550813195\n",
      "  timesteps_since_restore: 8390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8390000\n",
      "  training_iteration: 839\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19753 s, 839 iter, 8390000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-26-58\n",
      "  done: false\n",
      "  episode_len_mean: 89.73873873873873\n",
      "  episode_reward_max: 221.75416158124574\n",
      "  episode_reward_mean: 174.06698722586864\n",
      "  episode_reward_min: 139.81846908207592\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 89023\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.446\n",
      "    load_time_ms: 2.285\n",
      "    num_steps_sampled: 8400000\n",
      "    num_steps_trained: 8400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.48794862627983093\n",
      "      kl: 0.016479989513754845\n",
      "      policy_loss: 0.0022083139047026634\n",
      "      total_loss: 1.2697595357894897\n",
      "      vf_explained_var: 0.9972804188728333\n",
      "      vf_loss: 1.2675511837005615\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.26109084486961365\n",
      "      kl: 0.013317828066647053\n",
      "      policy_loss: 0.0002454510540701449\n",
      "      total_loss: 1.199682593345642\n",
      "      vf_explained_var: 0.9987397789955139\n",
      "      vf_loss: 1.1994373798370361\n",
      "    sample_time_ms: 20121.508\n",
      "    update_time_ms: 7.262\n",
      "  iterations_since_restore: 840\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.35582959556118\n",
      "    rl_1: 104.71115763030745\n",
      "  time_since_restore: 19776.775085687637\n",
      "  time_this_iter_s: 22.78559112548828\n",
      "  time_total_s: 19776.775085687637\n",
      "  timestamp: 1550813218\n",
      "  timesteps_since_restore: 8400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8400000\n",
      "  training_iteration: 840\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19776 s, 840 iter, 8400000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-27-21\n",
      "  done: false\n",
      "  episode_len_mean: 89.6036036036036\n",
      "  episode_reward_max: 213.81074274273624\n",
      "  episode_reward_mean: 172.2834903567079\n",
      "  episode_reward_min: 142.4690618283814\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 89134\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3165.906\n",
      "    load_time_ms: 2.269\n",
      "    num_steps_sampled: 8410000\n",
      "    num_steps_trained: 8410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5055573582649231\n",
      "      kl: 0.023098651319742203\n",
      "      policy_loss: 0.003713436657562852\n",
      "      total_loss: 1.1547490358352661\n",
      "      vf_explained_var: 0.9972985982894897\n",
      "      vf_loss: 1.1510355472564697\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3220803737640381\n",
      "      kl: 0.009115449152886868\n",
      "      policy_loss: 0.0025613149628043175\n",
      "      total_loss: 1.3018687963485718\n",
      "      vf_explained_var: 0.99863600730896\n",
      "      vf_loss: 1.2993075847625732\n",
      "    sample_time_ms: 20036.267\n",
      "    update_time_ms: 7.49\n",
      "  iterations_since_restore: 841\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.23859549352477\n",
      "    rl_1: 105.04489486318313\n",
      "  time_since_restore: 19800.279096126556\n",
      "  time_this_iter_s: 23.504010438919067\n",
      "  time_total_s: 19800.279096126556\n",
      "  timestamp: 1550813241\n",
      "  timesteps_since_restore: 8410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8410000\n",
      "  training_iteration: 841\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19800 s, 841 iter, 8410000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-27-45\n",
      "  done: false\n",
      "  episode_len_mean: 90.07142857142857\n",
      "  episode_reward_max: 217.66942278574777\n",
      "  episode_reward_mean: 173.41585347341393\n",
      "  episode_reward_min: 135.55921351004227\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 89246\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.176\n",
      "    load_time_ms: 2.301\n",
      "    num_steps_sampled: 8420000\n",
      "    num_steps_trained: 8420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5172669291496277\n",
      "      kl: 0.02287233993411064\n",
      "      policy_loss: 0.0018333945190533996\n",
      "      total_loss: 1.169948697090149\n",
      "      vf_explained_var: 0.9974096417427063\n",
      "      vf_loss: 1.1681153774261475\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.34544888138771057\n",
      "      kl: 0.016605079174041748\n",
      "      policy_loss: 0.001321389339864254\n",
      "      total_loss: 1.3212898969650269\n",
      "      vf_explained_var: 0.9986523389816284\n",
      "      vf_loss: 1.319968581199646\n",
      "    sample_time_ms: 20046.388\n",
      "    update_time_ms: 7.286\n",
      "  iterations_since_restore: 842\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.58848858448042\n",
      "    rl_1: 105.82736488893347\n",
      "  time_since_restore: 19823.46706700325\n",
      "  time_this_iter_s: 23.187970876693726\n",
      "  time_total_s: 19823.46706700325\n",
      "  timestamp: 1550813265\n",
      "  timesteps_since_restore: 8420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8420000\n",
      "  training_iteration: 842\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19823 s, 842 iter, 8420000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-28-08\n",
      "  done: false\n",
      "  episode_len_mean: 89.46846846846847\n",
      "  episode_reward_max: 215.36857125927978\n",
      "  episode_reward_mean: 175.5743228611713\n",
      "  episode_reward_min: 137.05025277073915\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 89357\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.991\n",
      "    load_time_ms: 2.379\n",
      "    num_steps_sampled: 8430000\n",
      "    num_steps_trained: 8430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5692671537399292\n",
      "      kl: 0.02872924879193306\n",
      "      policy_loss: 0.0024634229484945536\n",
      "      total_loss: 1.2306628227233887\n",
      "      vf_explained_var: 0.9973276853561401\n",
      "      vf_loss: 1.2281994819641113\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31080809235572815\n",
      "      kl: 0.015674127265810966\n",
      "      policy_loss: 0.0015253990422934294\n",
      "      total_loss: 1.0616075992584229\n",
      "      vf_explained_var: 0.9988542199134827\n",
      "      vf_loss: 1.0600823163986206\n",
      "    sample_time_ms: 20023.737\n",
      "    update_time_ms: 7.337\n",
      "  iterations_since_restore: 843\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.20940510070736\n",
      "    rl_1: 105.36491776046385\n",
      "  time_since_restore: 19846.83987236023\n",
      "  time_this_iter_s: 23.37280535697937\n",
      "  time_total_s: 19846.83987236023\n",
      "  timestamp: 1550813288\n",
      "  timesteps_since_restore: 8430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8430000\n",
      "  training_iteration: 843\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19846 s, 843 iter, 8430000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-28-32\n",
      "  done: false\n",
      "  episode_len_mean: 90.5945945945946\n",
      "  episode_reward_max: 221.67123107937792\n",
      "  episode_reward_mean: 171.3800670188782\n",
      "  episode_reward_min: 138.85974425029627\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 89468\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.224\n",
      "    load_time_ms: 2.469\n",
      "    num_steps_sampled: 8440000\n",
      "    num_steps_trained: 8440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4793994724750519\n",
      "      kl: 0.026164637878537178\n",
      "      policy_loss: 0.0015633591683581471\n",
      "      total_loss: 1.4478826522827148\n",
      "      vf_explained_var: 0.9965317845344543\n",
      "      vf_loss: 1.446319341659546\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.33258116245269775\n",
      "      kl: 0.009232548996806145\n",
      "      policy_loss: -0.0005864827544428408\n",
      "      total_loss: 1.2100228071212769\n",
      "      vf_explained_var: 0.9987660050392151\n",
      "      vf_loss: 1.2106093168258667\n",
      "    sample_time_ms: 20064.591\n",
      "    update_time_ms: 7.253\n",
      "  iterations_since_restore: 844\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.68690617152502\n",
      "    rl_1: 105.69316084735311\n",
      "  time_since_restore: 19870.23723578453\n",
      "  time_this_iter_s: 23.397363424301147\n",
      "  time_total_s: 19870.23723578453\n",
      "  timestamp: 1550813312\n",
      "  timesteps_since_restore: 8440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8440000\n",
      "  training_iteration: 844\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19870 s, 844 iter, 8440000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-28-54\n",
      "  done: false\n",
      "  episode_len_mean: 89.66666666666667\n",
      "  episode_reward_max: 217.31310784135525\n",
      "  episode_reward_mean: 177.7458119615533\n",
      "  episode_reward_min: 145.68370889792763\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 89579\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.138\n",
      "    load_time_ms: 2.454\n",
      "    num_steps_sampled: 8450000\n",
      "    num_steps_trained: 8450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.578363835811615\n",
      "      kl: 0.019382594153285027\n",
      "      policy_loss: 0.00046377486432902515\n",
      "      total_loss: 1.214735507965088\n",
      "      vf_explained_var: 0.9974514842033386\n",
      "      vf_loss: 1.2142717838287354\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3564750552177429\n",
      "      kl: 0.012669267132878304\n",
      "      policy_loss: 0.0022387984208762646\n",
      "      total_loss: 1.0609979629516602\n",
      "      vf_explained_var: 0.9989104866981506\n",
      "      vf_loss: 1.0587592124938965\n",
      "    sample_time_ms: 20023.074\n",
      "    update_time_ms: 7.286\n",
      "  iterations_since_restore: 845\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.48135497760875\n",
      "    rl_1: 107.26445698394451\n",
      "  time_since_restore: 19893.15716600418\n",
      "  time_this_iter_s: 22.91993021965027\n",
      "  time_total_s: 19893.15716600418\n",
      "  timestamp: 1550813334\n",
      "  timesteps_since_restore: 8450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8450000\n",
      "  training_iteration: 845\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19893 s, 845 iter, 8450000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-29-17\n",
      "  done: false\n",
      "  episode_len_mean: 89.92857142857143\n",
      "  episode_reward_max: 215.3918233423365\n",
      "  episode_reward_mean: 174.13947488873762\n",
      "  episode_reward_min: 134.7476884175769\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 89691\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.312\n",
      "    load_time_ms: 2.424\n",
      "    num_steps_sampled: 8460000\n",
      "    num_steps_trained: 8460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.537898063659668\n",
      "      kl: 0.01896253228187561\n",
      "      policy_loss: -8.164475002558902e-05\n",
      "      total_loss: 1.1680161952972412\n",
      "      vf_explained_var: 0.9974085688591003\n",
      "      vf_loss: 1.168097734451294\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2887939214706421\n",
      "      kl: 0.009943614713847637\n",
      "      policy_loss: -0.00018371708574704826\n",
      "      total_loss: 1.1991088390350342\n",
      "      vf_explained_var: 0.9987552165985107\n",
      "      vf_loss: 1.199292540550232\n",
      "    sample_time_ms: 19971.723\n",
      "    update_time_ms: 7.327\n",
      "  iterations_since_restore: 846\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.27412197486964\n",
      "    rl_1: 104.865352913868\n",
      "  time_since_restore: 19916.107789754868\n",
      "  time_this_iter_s: 22.950623750686646\n",
      "  time_total_s: 19916.107789754868\n",
      "  timestamp: 1550813357\n",
      "  timesteps_since_restore: 8460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8460000\n",
      "  training_iteration: 846\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19916 s, 846 iter, 8460000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-29-40\n",
      "  done: false\n",
      "  episode_len_mean: 90.03603603603604\n",
      "  episode_reward_max: 214.06174352163728\n",
      "  episode_reward_mean: 171.29562446130743\n",
      "  episode_reward_min: 138.40722234747346\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 89802\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.979\n",
      "    load_time_ms: 2.358\n",
      "    num_steps_sampled: 8470000\n",
      "    num_steps_trained: 8470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5175988674163818\n",
      "      kl: 0.02321026660501957\n",
      "      policy_loss: 0.00012712209718301892\n",
      "      total_loss: 1.2857799530029297\n",
      "      vf_explained_var: 0.9968848824501038\n",
      "      vf_loss: 1.2856528759002686\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31380289793014526\n",
      "      kl: 0.01236997451633215\n",
      "      policy_loss: -0.0013437699526548386\n",
      "      total_loss: 1.3544800281524658\n",
      "      vf_explained_var: 0.998571515083313\n",
      "      vf_loss: 1.3558237552642822\n",
      "    sample_time_ms: 19956.975\n",
      "    update_time_ms: 7.367\n",
      "  iterations_since_restore: 847\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.66051116841906\n",
      "    rl_1: 104.63511329288836\n",
      "  time_since_restore: 19938.8109562397\n",
      "  time_this_iter_s: 22.703166484832764\n",
      "  time_total_s: 19938.8109562397\n",
      "  timestamp: 1550813380\n",
      "  timesteps_since_restore: 8470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8470000\n",
      "  training_iteration: 847\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19938 s, 847 iter, 8470000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-30-03\n",
      "  done: false\n",
      "  episode_len_mean: 90.0909090909091\n",
      "  episode_reward_max: 217.8618049767301\n",
      "  episode_reward_mean: 173.35009806242564\n",
      "  episode_reward_min: 135.75414181990257\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 89912\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.852\n",
      "    load_time_ms: 2.33\n",
      "    num_steps_sampled: 8480000\n",
      "    num_steps_trained: 8480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5353484749794006\n",
      "      kl: 0.031459223479032516\n",
      "      policy_loss: 0.0036774694453924894\n",
      "      total_loss: 1.3372924327850342\n",
      "      vf_explained_var: 0.9971407651901245\n",
      "      vf_loss: 1.3336148262023926\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.33076363801956177\n",
      "      kl: 0.014417493715882301\n",
      "      policy_loss: 0.0021869391202926636\n",
      "      total_loss: 1.3246369361877441\n",
      "      vf_explained_var: 0.9986184239387512\n",
      "      vf_loss: 1.322449803352356\n",
      "    sample_time_ms: 19953.545\n",
      "    update_time_ms: 7.373\n",
      "  iterations_since_restore: 848\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.93330391350415\n",
      "    rl_1: 105.41679414892151\n",
      "  time_since_restore: 19961.658992528915\n",
      "  time_this_iter_s: 22.848036289215088\n",
      "  time_total_s: 19961.658992528915\n",
      "  timestamp: 1550813403\n",
      "  timesteps_since_restore: 8480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8480000\n",
      "  training_iteration: 848\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19961 s, 848 iter, 8480000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-30-27\n",
      "  done: false\n",
      "  episode_len_mean: 89.76785714285714\n",
      "  episode_reward_max: 216.59224027859977\n",
      "  episode_reward_mean: 171.348674067363\n",
      "  episode_reward_min: 137.2893108549631\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 90024\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.616\n",
      "    load_time_ms: 2.352\n",
      "    num_steps_sampled: 8490000\n",
      "    num_steps_trained: 8490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5189767479896545\n",
      "      kl: 0.022808879613876343\n",
      "      policy_loss: 0.0021876711398363113\n",
      "      total_loss: 1.3671425580978394\n",
      "      vf_explained_var: 0.9966992139816284\n",
      "      vf_loss: 1.364954948425293\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35174810886383057\n",
      "      kl: 0.020407089963555336\n",
      "      policy_loss: 0.004623484332114458\n",
      "      total_loss: 1.4276014566421509\n",
      "      vf_explained_var: 0.9984819293022156\n",
      "      vf_loss: 1.4229780435562134\n",
      "    sample_time_ms: 19973.49\n",
      "    update_time_ms: 7.429\n",
      "  iterations_since_restore: 849\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.56428141851299\n",
      "    rl_1: 104.78439264884999\n",
      "  time_since_restore: 19985.355135679245\n",
      "  time_this_iter_s: 23.69614315032959\n",
      "  time_total_s: 19985.355135679245\n",
      "  timestamp: 1550813427\n",
      "  timesteps_since_restore: 8490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8490000\n",
      "  training_iteration: 849\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 19985 s, 849 iter, 8490000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-30-51\n",
      "  done: false\n",
      "  episode_len_mean: 89.19642857142857\n",
      "  episode_reward_max: 219.8790740052961\n",
      "  episode_reward_mean: 175.45813956405354\n",
      "  episode_reward_min: 138.61901526813526\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 90136\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3156.125\n",
      "    load_time_ms: 2.445\n",
      "    num_steps_sampled: 8500000\n",
      "    num_steps_trained: 8500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5733966827392578\n",
      "      kl: 1.08303964138031\n",
      "      policy_loss: 0.010883764363825321\n",
      "      total_loss: 1.1716030836105347\n",
      "      vf_explained_var: 0.9975143671035767\n",
      "      vf_loss: 1.1607191562652588\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3441465198993683\n",
      "      kl: 0.0129038505256176\n",
      "      policy_loss: -0.0009023858583532274\n",
      "      total_loss: 1.1944010257720947\n",
      "      vf_explained_var: 0.9987585544586182\n",
      "      vf_loss: 1.1953034400939941\n",
      "    sample_time_ms: 20071.376\n",
      "    update_time_ms: 7.069\n",
      "  iterations_since_restore: 850\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.61396759110805\n",
      "    rl_1: 104.84417197294546\n",
      "  time_since_restore: 20009.32445192337\n",
      "  time_this_iter_s: 23.969316244125366\n",
      "  time_total_s: 20009.32445192337\n",
      "  timestamp: 1550813451\n",
      "  timesteps_since_restore: 8500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8500000\n",
      "  training_iteration: 850\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20009 s, 850 iter, 8500000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-31-15\n",
      "  done: false\n",
      "  episode_len_mean: 89.875\n",
      "  episode_reward_max: 219.0866820501295\n",
      "  episode_reward_mean: 173.63307237618866\n",
      "  episode_reward_min: 141.92162275555046\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 90248\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.861\n",
      "    load_time_ms: 2.452\n",
      "    num_steps_sampled: 8510000\n",
      "    num_steps_trained: 8510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5592742562294006\n",
      "      kl: 0.018441390246152878\n",
      "      policy_loss: 0.0007145958952605724\n",
      "      total_loss: 1.228713035583496\n",
      "      vf_explained_var: 0.9972197413444519\n",
      "      vf_loss: 1.2279986143112183\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.37597891688346863\n",
      "      kl: 0.020149286836385727\n",
      "      policy_loss: 0.004590647295117378\n",
      "      total_loss: 1.1189194917678833\n",
      "      vf_explained_var: 0.9988456964492798\n",
      "      vf_loss: 1.1143287420272827\n",
      "    sample_time_ms: 20101.698\n",
      "    update_time_ms: 7.063\n",
      "  iterations_since_restore: 851\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.32459358682492\n",
      "    rl_1: 106.30847878936378\n",
      "  time_since_restore: 20032.89069032669\n",
      "  time_this_iter_s: 23.566238403320312\n",
      "  time_total_s: 20032.89069032669\n",
      "  timestamp: 1550813475\n",
      "  timesteps_since_restore: 8510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8510000\n",
      "  training_iteration: 851\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20032 s, 851 iter, 8510000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 89.68468468468468\n",
      "  episode_reward_max: 220.30472226057606\n",
      "  episode_reward_mean: 174.783043509244\n",
      "  episode_reward_min: 136.07500193531627\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 90359\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.85\n",
      "    load_time_ms: 2.406\n",
      "    num_steps_sampled: 8520000\n",
      "    num_steps_trained: 8520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5854913592338562\n",
      "      kl: 0.026838360354304314\n",
      "      policy_loss: 0.00010907665273407474\n",
      "      total_loss: 0.957091212272644\n",
      "      vf_explained_var: 0.9979366660118103\n",
      "      vf_loss: 0.9569821357727051\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.33179977536201477\n",
      "      kl: 0.01638483628630638\n",
      "      policy_loss: 0.007894997484982014\n",
      "      total_loss: 1.0271471738815308\n",
      "      vf_explained_var: 0.9989416599273682\n",
      "      vf_loss: 1.0192519426345825\n",
      "    sample_time_ms: 20112.959\n",
      "    update_time_ms: 6.991\n",
      "  iterations_since_restore: 852\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.80359559778083\n",
      "    rl_1: 104.97944791146315\n",
      "  time_since_restore: 20056.189351797104\n",
      "  time_this_iter_s: 23.298661470413208\n",
      "  time_total_s: 20056.189351797104\n",
      "  timestamp: 1550813498\n",
      "  timesteps_since_restore: 8520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8520000\n",
      "  training_iteration: 852\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20056 s, 852 iter, 8520000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-32-01\n",
      "  done: false\n",
      "  episode_len_mean: 89.94594594594595\n",
      "  episode_reward_max: 217.13843080559656\n",
      "  episode_reward_mean: 170.24162135891544\n",
      "  episode_reward_min: 135.44826622700506\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 90470\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.314\n",
      "    load_time_ms: 2.332\n",
      "    num_steps_sampled: 8530000\n",
      "    num_steps_trained: 8530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5236713886260986\n",
      "      kl: 0.03020597994327545\n",
      "      policy_loss: -0.00017542506975587457\n",
      "      total_loss: 1.2343339920043945\n",
      "      vf_explained_var: 0.9971243143081665\n",
      "      vf_loss: 1.2345095872879028\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3338804841041565\n",
      "      kl: 0.021069249138236046\n",
      "      policy_loss: 0.005652338266372681\n",
      "      total_loss: 1.114547610282898\n",
      "      vf_explained_var: 0.9988064765930176\n",
      "      vf_loss: 1.108895182609558\n",
      "    sample_time_ms: 20072.454\n",
      "    update_time_ms: 7.025\n",
      "  iterations_since_restore: 853\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.76754456914314\n",
      "    rl_1: 103.47407678977228\n",
      "  time_since_restore: 20079.148113965988\n",
      "  time_this_iter_s: 22.958762168884277\n",
      "  time_total_s: 20079.148113965988\n",
      "  timestamp: 1550813521\n",
      "  timesteps_since_restore: 8530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8530000\n",
      "  training_iteration: 853\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20079 s, 853 iter, 8530000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-32-24\n",
      "  done: false\n",
      "  episode_len_mean: 89.92792792792793\n",
      "  episode_reward_max: 215.28955615411112\n",
      "  episode_reward_mean: 174.16352860488445\n",
      "  episode_reward_min: 136.6641407347826\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 90581\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.979\n",
      "    load_time_ms: 2.252\n",
      "    num_steps_sampled: 8540000\n",
      "    num_steps_trained: 8540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.549345076084137\n",
      "      kl: 0.029122475534677505\n",
      "      policy_loss: -0.0005944606964476407\n",
      "      total_loss: 1.1756670475006104\n",
      "      vf_explained_var: 0.9973829984664917\n",
      "      vf_loss: 1.1762616634368896\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35591089725494385\n",
      "      kl: 0.026241358369588852\n",
      "      policy_loss: 0.00440731318667531\n",
      "      total_loss: 1.2078148126602173\n",
      "      vf_explained_var: 0.9987481236457825\n",
      "      vf_loss: 1.2034075260162354\n",
      "    sample_time_ms: 20084.901\n",
      "    update_time_ms: 7.072\n",
      "  iterations_since_restore: 854\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.36612441097796\n",
      "    rl_1: 105.79740419390647\n",
      "  time_since_restore: 20102.69332051277\n",
      "  time_this_iter_s: 23.545206546783447\n",
      "  time_total_s: 20102.69332051277\n",
      "  timestamp: 1550813544\n",
      "  timesteps_since_restore: 8540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8540000\n",
      "  training_iteration: 854\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20102 s, 854 iter, 8540000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-32-48\n",
      "  done: false\n",
      "  episode_len_mean: 89.10619469026548\n",
      "  episode_reward_max: 221.10072754301285\n",
      "  episode_reward_mean: 175.28932025877802\n",
      "  episode_reward_min: 134.19090013502026\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 90694\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.91\n",
      "    load_time_ms: 2.257\n",
      "    num_steps_sampled: 8550000\n",
      "    num_steps_trained: 8550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6177864670753479\n",
      "      kl: 0.029921675100922585\n",
      "      policy_loss: 0.0022981560323387384\n",
      "      total_loss: 1.1149132251739502\n",
      "      vf_explained_var: 0.997701108455658\n",
      "      vf_loss: 1.1126149892807007\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.40513333678245544\n",
      "      kl: 0.026892805472016335\n",
      "      policy_loss: 0.0019067847169935703\n",
      "      total_loss: 1.0878465175628662\n",
      "      vf_explained_var: 0.9988313317298889\n",
      "      vf_loss: 1.0859397649765015\n",
      "    sample_time_ms: 20125.657\n",
      "    update_time_ms: 6.995\n",
      "  iterations_since_restore: 855\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.97599309897403\n",
      "    rl_1: 104.31332715980399\n",
      "  time_since_restore: 20126.001823425293\n",
      "  time_this_iter_s: 23.308502912521362\n",
      "  time_total_s: 20126.001823425293\n",
      "  timestamp: 1550813568\n",
      "  timesteps_since_restore: 8550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8550000\n",
      "  training_iteration: 855\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20126 s, 855 iter, 8550000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-33-11\n",
      "  done: false\n",
      "  episode_len_mean: 90.5909090909091\n",
      "  episode_reward_max: 218.4305280854406\n",
      "  episode_reward_mean: 171.91017252500708\n",
      "  episode_reward_min: 136.68429409337358\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 90804\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.066\n",
      "    load_time_ms: 2.253\n",
      "    num_steps_sampled: 8560000\n",
      "    num_steps_trained: 8560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5701631307601929\n",
      "      kl: 0.034364230930805206\n",
      "      policy_loss: 0.00393022270873189\n",
      "      total_loss: 1.2280069589614868\n",
      "      vf_explained_var: 0.997154712677002\n",
      "      vf_loss: 1.2240766286849976\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4613485634326935\n",
      "      kl: 0.014575712382793427\n",
      "      policy_loss: 0.0010521941585466266\n",
      "      total_loss: 1.0759536027908325\n",
      "      vf_explained_var: 0.9988737106323242\n",
      "      vf_loss: 1.0749013423919678\n",
      "    sample_time_ms: 20155.568\n",
      "    update_time_ms: 7.151\n",
      "  iterations_since_restore: 856\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.10459724825189\n",
      "    rl_1: 105.8055752767552\n",
      "  time_since_restore: 20149.257509946823\n",
      "  time_this_iter_s: 23.25568652153015\n",
      "  time_total_s: 20149.257509946823\n",
      "  timestamp: 1550813591\n",
      "  timesteps_since_restore: 8560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8560000\n",
      "  training_iteration: 856\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20149 s, 856 iter, 8560000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-33-35\n",
      "  done: false\n",
      "  episode_len_mean: 89.78378378378379\n",
      "  episode_reward_max: 220.180449149527\n",
      "  episode_reward_mean: 177.25066850037723\n",
      "  episode_reward_min: 140.81307943767106\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 90915\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.228\n",
      "    load_time_ms: 2.219\n",
      "    num_steps_sampled: 8570000\n",
      "    num_steps_trained: 8570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5991055369377136\n",
      "      kl: 0.017797525972127914\n",
      "      policy_loss: 0.0008168993517756462\n",
      "      total_loss: 1.2954795360565186\n",
      "      vf_explained_var: 0.9971699118614197\n",
      "      vf_loss: 1.2946627140045166\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4554736912250519\n",
      "      kl: 0.015852710232138634\n",
      "      policy_loss: 0.00335541064850986\n",
      "      total_loss: 1.3172746896743774\n",
      "      vf_explained_var: 0.9986886382102966\n",
      "      vf_loss: 1.3139194250106812\n",
      "    sample_time_ms: 20236.599\n",
      "    update_time_ms: 7.153\n",
      "  iterations_since_restore: 857\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.83988829460513\n",
      "    rl_1: 107.41078020577213\n",
      "  time_since_restore: 20172.831900835037\n",
      "  time_this_iter_s: 23.57439088821411\n",
      "  time_total_s: 20172.831900835037\n",
      "  timestamp: 1550813615\n",
      "  timesteps_since_restore: 8570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8570000\n",
      "  training_iteration: 857\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20172 s, 857 iter, 8570000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-33-58\n",
      "  done: false\n",
      "  episode_len_mean: 89.60714285714286\n",
      "  episode_reward_max: 217.36894529624436\n",
      "  episode_reward_mean: 173.5576365893746\n",
      "  episode_reward_min: 138.53090569147452\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 91027\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.409\n",
      "    load_time_ms: 2.22\n",
      "    num_steps_sampled: 8580000\n",
      "    num_steps_trained: 8580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.627182126045227\n",
      "      kl: 0.024872969835996628\n",
      "      policy_loss: -0.003872048342600465\n",
      "      total_loss: 1.1428065299987793\n",
      "      vf_explained_var: 0.9974859952926636\n",
      "      vf_loss: 1.1466788053512573\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.44530194997787476\n",
      "      kl: 0.904617190361023\n",
      "      policy_loss: 0.05089053139090538\n",
      "      total_loss: 1.2645636796951294\n",
      "      vf_explained_var: 0.9987449645996094\n",
      "      vf_loss: 1.2136731147766113\n",
      "    sample_time_ms: 20285.218\n",
      "    update_time_ms: 7.072\n",
      "  iterations_since_restore: 858\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.97475399601292\n",
      "    rl_1: 105.58288259336167\n",
      "  time_since_restore: 20196.219099521637\n",
      "  time_this_iter_s: 23.38719868659973\n",
      "  time_total_s: 20196.219099521637\n",
      "  timestamp: 1550813638\n",
      "  timesteps_since_restore: 8580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8580000\n",
      "  training_iteration: 858\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20196 s, 858 iter, 8580000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-34-21\n",
      "  done: false\n",
      "  episode_len_mean: 90.66363636363636\n",
      "  episode_reward_max: 218.53691928676128\n",
      "  episode_reward_mean: 175.38788158008376\n",
      "  episode_reward_min: 134.42353261577986\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 91137\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3155.518\n",
      "    load_time_ms: 2.211\n",
      "    num_steps_sampled: 8590000\n",
      "    num_steps_trained: 8590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.602744460105896\n",
      "      kl: 0.032877545803785324\n",
      "      policy_loss: 0.0020594620145857334\n",
      "      total_loss: 1.204521656036377\n",
      "      vf_explained_var: 0.9973862767219543\n",
      "      vf_loss: 1.2024621963500977\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3998677730560303\n",
      "      kl: 0.022033585235476494\n",
      "      policy_loss: 0.0057794032618403435\n",
      "      total_loss: 1.1515594720840454\n",
      "      vf_explained_var: 0.9988221526145935\n",
      "      vf_loss: 1.1457799673080444\n",
      "    sample_time_ms: 20193.798\n",
      "    update_time_ms: 6.934\n",
      "  iterations_since_restore: 859\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.60652476744067\n",
      "    rl_1: 106.78135681264304\n",
      "  time_since_restore: 20219.11975336075\n",
      "  time_this_iter_s: 22.900653839111328\n",
      "  time_total_s: 20219.11975336075\n",
      "  timestamp: 1550813661\n",
      "  timesteps_since_restore: 8590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8590000\n",
      "  training_iteration: 859\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20219 s, 859 iter, 8590000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-34-45\n",
      "  done: false\n",
      "  episode_len_mean: 89.25892857142857\n",
      "  episode_reward_max: 218.29739043352012\n",
      "  episode_reward_mean: 168.76083537237324\n",
      "  episode_reward_min: -158.41400734981852\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 91249\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.746\n",
      "    load_time_ms: 2.137\n",
      "    num_steps_sampled: 8600000\n",
      "    num_steps_trained: 8600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6006514430046082\n",
      "      kl: 0.0235245693475008\n",
      "      policy_loss: -0.008886785246431828\n",
      "      total_loss: 49.39741134643555\n",
      "      vf_explained_var: 0.9215718507766724\n",
      "      vf_loss: 49.40629577636719\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3876318335533142\n",
      "      kl: 0.009945042431354523\n",
      "      policy_loss: -0.0010066404938697815\n",
      "      total_loss: 82.49788665771484\n",
      "      vf_explained_var: 0.9280072450637817\n",
      "      vf_loss: 82.4989013671875\n",
      "    sample_time_ms: 20206.27\n",
      "    update_time_ms: 6.715\n",
      "  iterations_since_restore: 860\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.35104686406143\n",
      "    rl_1: 102.40978850831179\n",
      "  time_since_restore: 20243.05193567276\n",
      "  time_this_iter_s: 23.93218231201172\n",
      "  time_total_s: 20243.05193567276\n",
      "  timestamp: 1550813685\n",
      "  timesteps_since_restore: 8600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8600000\n",
      "  training_iteration: 860\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20243 s, 860 iter, 8600000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-35-09\n",
      "  done: false\n",
      "  episode_len_mean: 90.35135135135135\n",
      "  episode_reward_max: 218.80661300449006\n",
      "  episode_reward_mean: 174.96015437194652\n",
      "  episode_reward_min: 135.38356048409938\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 91360\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.318\n",
      "    load_time_ms: 2.167\n",
      "    num_steps_sampled: 8610000\n",
      "    num_steps_trained: 8610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5994157791137695\n",
      "      kl: 0.03334793075919151\n",
      "      policy_loss: 0.0019373622490093112\n",
      "      total_loss: 1.3678967952728271\n",
      "      vf_explained_var: 0.9970629811286926\n",
      "      vf_loss: 1.3659594058990479\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4211016297340393\n",
      "      kl: 0.01062736101448536\n",
      "      policy_loss: 0.0008878092048689723\n",
      "      total_loss: 1.2148185968399048\n",
      "      vf_explained_var: 0.9987495541572571\n",
      "      vf_loss: 1.2139308452606201\n",
      "    sample_time_ms: 20209.365\n",
      "    update_time_ms: 6.817\n",
      "  iterations_since_restore: 861\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.45614455975901\n",
      "    rl_1: 106.50400981218756\n",
      "  time_since_restore: 20266.66491651535\n",
      "  time_this_iter_s: 23.612980842590332\n",
      "  time_total_s: 20266.66491651535\n",
      "  timestamp: 1550813709\n",
      "  timesteps_since_restore: 8610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8610000\n",
      "  training_iteration: 861\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20266 s, 861 iter, 8610000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-35-32\n",
      "  done: false\n",
      "  episode_len_mean: 90.01801801801801\n",
      "  episode_reward_max: 220.0037458017541\n",
      "  episode_reward_mean: 170.70086020355686\n",
      "  episode_reward_min: 137.3197478973683\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 91471\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.222\n",
      "    load_time_ms: 2.183\n",
      "    num_steps_sampled: 8620000\n",
      "    num_steps_trained: 8620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5446970462799072\n",
      "      kl: 0.03189374879002571\n",
      "      policy_loss: 0.005242271348834038\n",
      "      total_loss: 1.4881051778793335\n",
      "      vf_explained_var: 0.996525228023529\n",
      "      vf_loss: 1.4828630685806274\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.42132315039634705\n",
      "      kl: 0.01800944283604622\n",
      "      policy_loss: 0.0038533632177859545\n",
      "      total_loss: 1.4208471775054932\n",
      "      vf_explained_var: 0.9984930753707886\n",
      "      vf_loss: 1.4169938564300537\n",
      "    sample_time_ms: 20217.222\n",
      "    update_time_ms: 7.101\n",
      "  iterations_since_restore: 862\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.4684622015491\n",
      "    rl_1: 104.23239800200776\n",
      "  time_since_restore: 20290.05334377289\n",
      "  time_this_iter_s: 23.388427257537842\n",
      "  time_total_s: 20290.05334377289\n",
      "  timestamp: 1550813732\n",
      "  timesteps_since_restore: 8620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8620000\n",
      "  training_iteration: 862\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20290 s, 862 iter, 8620000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-35-56\n",
      "  done: false\n",
      "  episode_len_mean: 90.13513513513513\n",
      "  episode_reward_max: 217.07791472312272\n",
      "  episode_reward_mean: 172.09057424170612\n",
      "  episode_reward_min: 134.44036658060094\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 91582\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.604\n",
      "    load_time_ms: 2.212\n",
      "    num_steps_sampled: 8630000\n",
      "    num_steps_trained: 8630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6040940880775452\n",
      "      kl: 0.019479909911751747\n",
      "      policy_loss: 0.0007484522648155689\n",
      "      total_loss: 0.9935373067855835\n",
      "      vf_explained_var: 0.997776210308075\n",
      "      vf_loss: 0.9927891492843628\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.38013315200805664\n",
      "      kl: 0.02181011624634266\n",
      "      policy_loss: 0.003642757423222065\n",
      "      total_loss: 1.0301647186279297\n",
      "      vf_explained_var: 0.9988824725151062\n",
      "      vf_loss: 1.026521921157837\n",
      "    sample_time_ms: 20275.735\n",
      "    update_time_ms: 7.306\n",
      "  iterations_since_restore: 863\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.31130191420858\n",
      "    rl_1: 103.77927232749755\n",
      "  time_since_restore: 20313.603811264038\n",
      "  time_this_iter_s: 23.550467491149902\n",
      "  time_total_s: 20313.603811264038\n",
      "  timestamp: 1550813756\n",
      "  timesteps_since_restore: 8630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8630000\n",
      "  training_iteration: 863\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20313 s, 863 iter, 8630000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-36-19\n",
      "  done: false\n",
      "  episode_len_mean: 89.90090090090091\n",
      "  episode_reward_max: 215.26281382347867\n",
      "  episode_reward_mean: 169.724957731186\n",
      "  episode_reward_min: 138.28750845090963\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 91693\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.893\n",
      "    load_time_ms: 2.194\n",
      "    num_steps_sampled: 8640000\n",
      "    num_steps_trained: 8640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5593602061271667\n",
      "      kl: 0.44847291707992554\n",
      "      policy_loss: 0.0073813083581626415\n",
      "      total_loss: 1.3233578205108643\n",
      "      vf_explained_var: 0.9970144629478455\n",
      "      vf_loss: 1.3159763813018799\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.37531712651252747\n",
      "      kl: 0.022780677303671837\n",
      "      policy_loss: 0.004998301155865192\n",
      "      total_loss: 1.261451244354248\n",
      "      vf_explained_var: 0.998581051826477\n",
      "      vf_loss: 1.256453037261963\n",
      "    sample_time_ms: 20205.241\n",
      "    update_time_ms: 7.426\n",
      "  iterations_since_restore: 864\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.74650401399253\n",
      "    rl_1: 102.97845371719346\n",
      "  time_since_restore: 20336.48761987686\n",
      "  time_this_iter_s: 22.883808612823486\n",
      "  time_total_s: 20336.48761987686\n",
      "  timestamp: 1550813779\n",
      "  timesteps_since_restore: 8640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8640000\n",
      "  training_iteration: 864\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20336 s, 864 iter, 8640000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-36-42\n",
      "  done: false\n",
      "  episode_len_mean: 88.929203539823\n",
      "  episode_reward_max: 220.5033557409695\n",
      "  episode_reward_mean: 175.9342591681071\n",
      "  episode_reward_min: 143.63713674378394\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 91806\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3152.454\n",
      "    load_time_ms: 2.21\n",
      "    num_steps_sampled: 8650000\n",
      "    num_steps_trained: 8650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6630882620811462\n",
      "      kl: 0.03247763589024544\n",
      "      policy_loss: 0.004951892886310816\n",
      "      total_loss: 1.066086769104004\n",
      "      vf_explained_var: 0.9976599216461182\n",
      "      vf_loss: 1.0611348152160645\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4409097135066986\n",
      "      kl: 0.021313678473234177\n",
      "      policy_loss: 0.003235533135011792\n",
      "      total_loss: 1.0736435651779175\n",
      "      vf_explained_var: 0.9988831877708435\n",
      "      vf_loss: 1.0704081058502197\n",
      "    sample_time_ms: 20232.049\n",
      "    update_time_ms: 8.23\n",
      "  iterations_since_restore: 865\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.63213170851513\n",
      "    rl_1: 105.30212745959194\n",
      "  time_since_restore: 20360.126685857773\n",
      "  time_this_iter_s: 23.639065980911255\n",
      "  time_total_s: 20360.126685857773\n",
      "  timestamp: 1550813802\n",
      "  timesteps_since_restore: 8650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8650000\n",
      "  training_iteration: 865\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20360 s, 865 iter, 8650000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-37-06\n",
      "  done: false\n",
      "  episode_len_mean: 89.98181818181818\n",
      "  episode_reward_max: 215.59608383466258\n",
      "  episode_reward_mean: 169.23247468443313\n",
      "  episode_reward_min: 131.9383818238228\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 91916\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.932\n",
      "    load_time_ms: 2.353\n",
      "    num_steps_sampled: 8660000\n",
      "    num_steps_trained: 8660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6011514663696289\n",
      "      kl: 0.020510008558630943\n",
      "      policy_loss: 0.0021006944589316845\n",
      "      total_loss: 1.2815639972686768\n",
      "      vf_explained_var: 0.9970098733901978\n",
      "      vf_loss: 1.279463291168213\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.49016281962394714\n",
      "      kl: 0.027279887348413467\n",
      "      policy_loss: 0.01295471005141735\n",
      "      total_loss: 1.2540323734283447\n",
      "      vf_explained_var: 0.9986501932144165\n",
      "      vf_loss: 1.2410776615142822\n",
      "    sample_time_ms: 20280.692\n",
      "    update_time_ms: 8.529\n",
      "  iterations_since_restore: 866\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.71383015677524\n",
      "    rl_1: 103.51864452765784\n",
      "  time_since_restore: 20383.85760998726\n",
      "  time_this_iter_s: 23.730924129486084\n",
      "  time_total_s: 20383.85760998726\n",
      "  timestamp: 1550813826\n",
      "  timesteps_since_restore: 8660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8660000\n",
      "  training_iteration: 866\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20383 s, 866 iter, 8660000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-37-30\n",
      "  done: false\n",
      "  episode_len_mean: 89.01769911504425\n",
      "  episode_reward_max: 216.89388051373862\n",
      "  episode_reward_mean: 174.81820593538436\n",
      "  episode_reward_min: 138.31696922667015\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 92029\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.669\n",
      "    load_time_ms: 2.352\n",
      "    num_steps_sampled: 8670000\n",
      "    num_steps_trained: 8670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6347174644470215\n",
      "      kl: 0.02892649546265602\n",
      "      policy_loss: 0.000279690109891817\n",
      "      total_loss: 1.1496431827545166\n",
      "      vf_explained_var: 0.9975081086158752\n",
      "      vf_loss: 1.1493637561798096\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4281066060066223\n",
      "      kl: 0.013391649350523949\n",
      "      policy_loss: 0.00334058771841228\n",
      "      total_loss: 1.3046282529830933\n",
      "      vf_explained_var: 0.9986124038696289\n",
      "      vf_loss: 1.3012876510620117\n",
      "    sample_time_ms: 20274.871\n",
      "    update_time_ms: 8.845\n",
      "  iterations_since_restore: 867\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.40966284900378\n",
      "    rl_1: 104.40854308638059\n",
      "  time_since_restore: 20407.32647752762\n",
      "  time_this_iter_s: 23.468867540359497\n",
      "  time_total_s: 20407.32647752762\n",
      "  timestamp: 1550813850\n",
      "  timesteps_since_restore: 8670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8670000\n",
      "  training_iteration: 867\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20407 s, 867 iter, 8670000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-37-53\n",
      "  done: false\n",
      "  episode_len_mean: 89.10714285714286\n",
      "  episode_reward_max: 220.47050415350697\n",
      "  episode_reward_mean: 169.40048387867722\n",
      "  episode_reward_min: -149.94282432021484\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 92141\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.701\n",
      "    load_time_ms: 2.437\n",
      "    num_steps_sampled: 8680000\n",
      "    num_steps_trained: 8680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6537236571311951\n",
      "      kl: 0.017036238685250282\n",
      "      policy_loss: -0.002534930594265461\n",
      "      total_loss: 22.098594665527344\n",
      "      vf_explained_var: 0.961925208568573\n",
      "      vf_loss: 22.10112953186035\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4266030192375183\n",
      "      kl: 0.010004366748034954\n",
      "      policy_loss: -0.0008326521492563188\n",
      "      total_loss: 38.028053283691406\n",
      "      vf_explained_var: 0.9644651412963867\n",
      "      vf_loss: 38.02888107299805\n",
      "    sample_time_ms: 20289.004\n",
      "    update_time_ms: 8.874\n",
      "  iterations_since_restore: 868\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.74338457192975\n",
      "    rl_1: 102.65709930674748\n",
      "  time_since_restore: 20430.845833539963\n",
      "  time_this_iter_s: 23.51935601234436\n",
      "  time_total_s: 20430.845833539963\n",
      "  timestamp: 1550813873\n",
      "  timesteps_since_restore: 8680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8680000\n",
      "  training_iteration: 868\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20430 s, 868 iter, 8680000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-38-17\n",
      "  done: false\n",
      "  episode_len_mean: 90.29729729729729\n",
      "  episode_reward_max: 218.62238517772076\n",
      "  episode_reward_mean: 172.30280024794146\n",
      "  episode_reward_min: 138.1128569629145\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 92252\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.902\n",
      "    load_time_ms: 2.478\n",
      "    num_steps_sampled: 8690000\n",
      "    num_steps_trained: 8690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6257821917533875\n",
      "      kl: 0.10040868073701859\n",
      "      policy_loss: 0.00991551112383604\n",
      "      total_loss: 1.298251986503601\n",
      "      vf_explained_var: 0.9970157146453857\n",
      "      vf_loss: 1.2883366346359253\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.44409239292144775\n",
      "      kl: 0.023941993713378906\n",
      "      policy_loss: 0.007549161557108164\n",
      "      total_loss: 1.2232544422149658\n",
      "      vf_explained_var: 0.9987174868583679\n",
      "      vf_loss: 1.215705394744873\n",
      "    sample_time_ms: 20343.09\n",
      "    update_time_ms: 9.213\n",
      "  iterations_since_restore: 869\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.0849025539069\n",
      "    rl_1: 105.21789769403453\n",
      "  time_since_restore: 20454.135068178177\n",
      "  time_this_iter_s: 23.28923463821411\n",
      "  time_total_s: 20454.135068178177\n",
      "  timestamp: 1550813897\n",
      "  timesteps_since_restore: 8690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8690000\n",
      "  training_iteration: 869\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20454 s, 869 iter, 8690000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-38-40\n",
      "  done: false\n",
      "  episode_len_mean: 89.59821428571429\n",
      "  episode_reward_max: 215.6376407020544\n",
      "  episode_reward_mean: 170.52061478535794\n",
      "  episode_reward_min: -150.10780932826833\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 92364\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.187\n",
      "    load_time_ms: 2.46\n",
      "    num_steps_sampled: 8700000\n",
      "    num_steps_trained: 8700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6696062088012695\n",
      "      kl: 0.03446514904499054\n",
      "      policy_loss: 0.0012294587213546038\n",
      "      total_loss: 19.40547752380371\n",
      "      vf_explained_var: 0.9674104452133179\n",
      "      vf_loss: 19.404247283935547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3486102521419525\n",
      "      kl: 0.0191491786390543\n",
      "      policy_loss: -0.010497922077775002\n",
      "      total_loss: 35.032142639160156\n",
      "      vf_explained_var: 0.9675999879837036\n",
      "      vf_loss: 35.042633056640625\n",
      "    sample_time_ms: 20287.873\n",
      "    update_time_ms: 9.241\n",
      "  iterations_since_restore: 870\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.17404834831186\n",
      "    rl_1: 102.34656643704605\n",
      "  time_since_restore: 20477.60667824745\n",
      "  time_this_iter_s: 23.471610069274902\n",
      "  time_total_s: 20477.60667824745\n",
      "  timestamp: 1550813920\n",
      "  timesteps_since_restore: 8700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8700000\n",
      "  training_iteration: 870\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20477 s, 870 iter, 8700000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-39-04\n",
      "  done: false\n",
      "  episode_len_mean: 90.07207207207207\n",
      "  episode_reward_max: 214.77540850891435\n",
      "  episode_reward_mean: 173.59745635471353\n",
      "  episode_reward_min: 139.32987231355406\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 92475\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.782\n",
      "    load_time_ms: 2.471\n",
      "    num_steps_sampled: 8710000\n",
      "    num_steps_trained: 8710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6286512613296509\n",
      "      kl: 0.18159395456314087\n",
      "      policy_loss: 0.013201878406107426\n",
      "      total_loss: 1.422088384628296\n",
      "      vf_explained_var: 0.9968186616897583\n",
      "      vf_loss: 1.4088865518569946\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35758793354034424\n",
      "      kl: 0.020481983199715614\n",
      "      policy_loss: 0.0031296652741730213\n",
      "      total_loss: 1.2093284130096436\n",
      "      vf_explained_var: 0.9987255930900574\n",
      "      vf_loss: 1.206198811531067\n",
      "    sample_time_ms: 20300.964\n",
      "    update_time_ms: 8.937\n",
      "  iterations_since_restore: 871\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.2205870281269\n",
      "    rl_1: 105.3768693265866\n",
      "  time_since_restore: 20501.354558706284\n",
      "  time_this_iter_s: 23.747880458831787\n",
      "  time_total_s: 20501.354558706284\n",
      "  timestamp: 1550813944\n",
      "  timesteps_since_restore: 8710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8710000\n",
      "  training_iteration: 871\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20501 s, 871 iter, 8710000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-39-27\n",
      "  done: false\n",
      "  episode_len_mean: 90.2\n",
      "  episode_reward_max: 220.0939603905886\n",
      "  episode_reward_mean: 175.02641986903893\n",
      "  episode_reward_min: 139.4077166164518\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 92585\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.572\n",
      "    load_time_ms: 2.461\n",
      "    num_steps_sampled: 8720000\n",
      "    num_steps_trained: 8720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6787906289100647\n",
      "      kl: 0.047243859618902206\n",
      "      policy_loss: 0.0023074126802384853\n",
      "      total_loss: 1.0894886255264282\n",
      "      vf_explained_var: 0.9975382685661316\n",
      "      vf_loss: 1.0871810913085938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3843868374824524\n",
      "      kl: 0.017267726361751556\n",
      "      policy_loss: 0.006973929703235626\n",
      "      total_loss: 1.0794590711593628\n",
      "      vf_explained_var: 0.9988846182823181\n",
      "      vf_loss: 1.072485089302063\n",
      "    sample_time_ms: 20263.608\n",
      "    update_time_ms: 8.646\n",
      "  iterations_since_restore: 872\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.83595826644827\n",
      "    rl_1: 106.19046160259069\n",
      "  time_since_restore: 20524.36683344841\n",
      "  time_this_iter_s: 23.012274742126465\n",
      "  time_total_s: 20524.36683344841\n",
      "  timestamp: 1550813967\n",
      "  timesteps_since_restore: 8720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8720000\n",
      "  training_iteration: 872\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20524 s, 872 iter, 8720000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-39-50\n",
      "  done: false\n",
      "  episode_len_mean: 89.9375\n",
      "  episode_reward_max: 221.40216208502906\n",
      "  episode_reward_mean: 172.01415369365927\n",
      "  episode_reward_min: 138.4180444079839\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 92697\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3157.679\n",
      "    load_time_ms: 2.447\n",
      "    num_steps_sampled: 8730000\n",
      "    num_steps_trained: 8730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6035716533660889\n",
      "      kl: 0.04380054771900177\n",
      "      policy_loss: 0.002514821942895651\n",
      "      total_loss: 1.5044699907302856\n",
      "      vf_explained_var: 0.9966931343078613\n",
      "      vf_loss: 1.5019550323486328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.39724254608154297\n",
      "      kl: 0.011007537133991718\n",
      "      policy_loss: 0.0011740676127374172\n",
      "      total_loss: 1.334774136543274\n",
      "      vf_explained_var: 0.9985791444778442\n",
      "      vf_loss: 1.3336000442504883\n",
      "    sample_time_ms: 20187.609\n",
      "    update_time_ms: 8.547\n",
      "  iterations_since_restore: 873\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.49468803211165\n",
      "    rl_1: 104.51946566154763\n",
      "  time_since_restore: 20547.348081111908\n",
      "  time_this_iter_s: 22.981247663497925\n",
      "  time_total_s: 20547.348081111908\n",
      "  timestamp: 1550813990\n",
      "  timesteps_since_restore: 8730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8730000\n",
      "  training_iteration: 873\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20547 s, 873 iter, 8730000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-40-13\n",
      "  done: false\n",
      "  episode_len_mean: 89.43243243243244\n",
      "  episode_reward_max: 212.95814756736274\n",
      "  episode_reward_mean: 167.37293722320328\n",
      "  episode_reward_min: -150.0920120785935\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 92808\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3152.034\n",
      "    load_time_ms: 2.441\n",
      "    num_steps_sampled: 8740000\n",
      "    num_steps_trained: 8740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6245802640914917\n",
      "      kl: 0.024274157360196114\n",
      "      policy_loss: -0.002698973286896944\n",
      "      total_loss: 22.24824333190918\n",
      "      vf_explained_var: 0.9579175114631653\n",
      "      vf_loss: 22.250938415527344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.37179329991340637\n",
      "      kl: 0.011809978634119034\n",
      "      policy_loss: -0.0016989046707749367\n",
      "      total_loss: 39.23667907714844\n",
      "      vf_explained_var: 0.9625482559204102\n",
      "      vf_loss: 39.23838424682617\n",
      "    sample_time_ms: 20223.62\n",
      "    update_time_ms: 8.421\n",
      "  iterations_since_restore: 874\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.60327665414097\n",
      "    rl_1: 102.76966056906231\n",
      "  time_since_restore: 20570.534445762634\n",
      "  time_this_iter_s: 23.18636465072632\n",
      "  time_total_s: 20570.534445762634\n",
      "  timestamp: 1550814013\n",
      "  timesteps_since_restore: 8740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8740000\n",
      "  training_iteration: 874\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20570 s, 874 iter, 8740000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-40-37\n",
      "  done: false\n",
      "  episode_len_mean: 90.25\n",
      "  episode_reward_max: 216.42500008271145\n",
      "  episode_reward_mean: 174.75130072303935\n",
      "  episode_reward_min: 138.15459344217592\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 92920\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.587\n",
      "    load_time_ms: 2.434\n",
      "    num_steps_sampled: 8750000\n",
      "    num_steps_trained: 8750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6426583528518677\n",
      "      kl: 0.09418211877346039\n",
      "      policy_loss: 0.013170046731829643\n",
      "      total_loss: 1.3562729358673096\n",
      "      vf_explained_var: 0.9971230626106262\n",
      "      vf_loss: 1.3431028127670288\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3955581784248352\n",
      "      kl: 0.01552601344883442\n",
      "      policy_loss: 0.0038827131502330303\n",
      "      total_loss: 1.304861307144165\n",
      "      vf_explained_var: 0.9986855983734131\n",
      "      vf_loss: 1.3009787797927856\n",
      "    sample_time_ms: 20195.679\n",
      "    update_time_ms: 7.571\n",
      "  iterations_since_restore: 875\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.63882814492901\n",
      "    rl_1: 107.11247257811036\n",
      "  time_since_restore: 20593.83116030693\n",
      "  time_this_iter_s: 23.296714544296265\n",
      "  time_total_s: 20593.83116030693\n",
      "  timestamp: 1550814037\n",
      "  timesteps_since_restore: 8750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8750000\n",
      "  training_iteration: 875\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20593 s, 875 iter, 8750000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-41-00\n",
      "  done: false\n",
      "  episode_len_mean: 89.53153153153153\n",
      "  episode_reward_max: 216.18723176532546\n",
      "  episode_reward_mean: 170.0933441287828\n",
      "  episode_reward_min: 140.2208441994152\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 93031\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3149.304\n",
      "    load_time_ms: 2.292\n",
      "    num_steps_sampled: 8760000\n",
      "    num_steps_trained: 8760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6164337396621704\n",
      "      kl: 0.025054283440113068\n",
      "      policy_loss: -0.001502498285844922\n",
      "      total_loss: 1.3133845329284668\n",
      "      vf_explained_var: 0.9969707131385803\n",
      "      vf_loss: 1.3148870468139648\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.38243669271469116\n",
      "      kl: 0.012042716145515442\n",
      "      policy_loss: 0.0009917328134179115\n",
      "      total_loss: 1.3309354782104492\n",
      "      vf_explained_var: 0.9985551238059998\n",
      "      vf_loss: 1.3299437761306763\n",
      "    sample_time_ms: 20169.411\n",
      "    update_time_ms: 7.301\n",
      "  iterations_since_restore: 876\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.75315110167222\n",
      "    rl_1: 103.34019302711056\n",
      "  time_since_restore: 20617.319857120514\n",
      "  time_this_iter_s: 23.488696813583374\n",
      "  time_total_s: 20617.319857120514\n",
      "  timestamp: 1550814060\n",
      "  timesteps_since_restore: 8760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8760000\n",
      "  training_iteration: 876\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20617 s, 876 iter, 8760000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-41-23\n",
      "  done: false\n",
      "  episode_len_mean: 89.49107142857143\n",
      "  episode_reward_max: 212.20496309698663\n",
      "  episode_reward_mean: 171.34630031097336\n",
      "  episode_reward_min: 140.9164153850164\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 93143\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.397\n",
      "    load_time_ms: 2.291\n",
      "    num_steps_sampled: 8770000\n",
      "    num_steps_trained: 8770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6617470383644104\n",
      "      kl: 0.03620472550392151\n",
      "      policy_loss: 0.0032521975226700306\n",
      "      total_loss: 1.058739185333252\n",
      "      vf_explained_var: 0.9974766373634338\n",
      "      vf_loss: 1.055487036705017\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.406205952167511\n",
      "      kl: 0.01613646373152733\n",
      "      policy_loss: 0.003179375547915697\n",
      "      total_loss: 1.124160885810852\n",
      "      vf_explained_var: 0.9988017678260803\n",
      "      vf_loss: 1.1209814548492432\n",
      "    sample_time_ms: 20146.161\n",
      "    update_time_ms: 6.933\n",
      "  iterations_since_restore: 877\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.10518821970905\n",
      "    rl_1: 104.24111209126431\n",
      "  time_since_restore: 20640.563190698624\n",
      "  time_this_iter_s: 23.24333357810974\n",
      "  time_total_s: 20640.563190698624\n",
      "  timestamp: 1550814083\n",
      "  timesteps_since_restore: 8770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8770000\n",
      "  training_iteration: 877\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20640 s, 877 iter, 8770000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-41-46\n",
      "  done: false\n",
      "  episode_len_mean: 89.94594594594595\n",
      "  episode_reward_max: 220.03498114982924\n",
      "  episode_reward_mean: 172.52495002291113\n",
      "  episode_reward_min: 142.33952502064122\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 93254\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.034\n",
      "    load_time_ms: 2.233\n",
      "    num_steps_sampled: 8780000\n",
      "    num_steps_trained: 8780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6288426518440247\n",
      "      kl: 0.023763740435242653\n",
      "      policy_loss: 0.002196292160078883\n",
      "      total_loss: 1.1566352844238281\n",
      "      vf_explained_var: 0.9973981976509094\n",
      "      vf_loss: 1.1544389724731445\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3310476839542389\n",
      "      kl: 0.025504864752292633\n",
      "      policy_loss: 0.00544554041698575\n",
      "      total_loss: 1.1116853952407837\n",
      "      vf_explained_var: 0.9988343715667725\n",
      "      vf_loss: 1.106239676475525\n",
      "    sample_time_ms: 20091.31\n",
      "    update_time_ms: 7.176\n",
      "  iterations_since_restore: 878\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.09056291748159\n",
      "    rl_1: 104.43438710542956\n",
      "  time_since_restore: 20663.51095533371\n",
      "  time_this_iter_s: 22.94776463508606\n",
      "  time_total_s: 20663.51095533371\n",
      "  timestamp: 1550814106\n",
      "  timesteps_since_restore: 8780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8780000\n",
      "  training_iteration: 878\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20663 s, 878 iter, 8780000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-42-10\n",
      "  done: false\n",
      "  episode_len_mean: 89.91891891891892\n",
      "  episode_reward_max: 220.57536504995795\n",
      "  episode_reward_mean: 174.8433282617905\n",
      "  episode_reward_min: 133.1276740642798\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 93365\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.819\n",
      "    load_time_ms: 2.185\n",
      "    num_steps_sampled: 8790000\n",
      "    num_steps_trained: 8790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6683624982833862\n",
      "      kl: 0.036788616329431534\n",
      "      policy_loss: 0.0001283343881368637\n",
      "      total_loss: 1.1473925113677979\n",
      "      vf_explained_var: 0.9975225925445557\n",
      "      vf_loss: 1.147264003753662\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.36371779441833496\n",
      "      kl: 0.017087159678339958\n",
      "      policy_loss: -5.2277857321314514e-05\n",
      "      total_loss: 1.0645580291748047\n",
      "      vf_explained_var: 0.998856782913208\n",
      "      vf_loss: 1.0646103620529175\n",
      "    sample_time_ms: 20094.215\n",
      "    update_time_ms: 6.874\n",
      "  iterations_since_restore: 879\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.71780975148413\n",
      "    rl_1: 105.12551851030632\n",
      "  time_since_restore: 20686.82416653633\n",
      "  time_this_iter_s: 23.31321120262146\n",
      "  time_total_s: 20686.82416653633\n",
      "  timestamp: 1550814130\n",
      "  timesteps_since_restore: 8790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8790000\n",
      "  training_iteration: 879\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20686 s, 879 iter, 8790000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-42-34\n",
      "  done: false\n",
      "  episode_len_mean: 90.7909090909091\n",
      "  episode_reward_max: 218.29542089417043\n",
      "  episode_reward_mean: 170.64670615596833\n",
      "  episode_reward_min: 137.78611371729275\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 93475\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.153\n",
      "    load_time_ms: 2.18\n",
      "    num_steps_sampled: 8800000\n",
      "    num_steps_trained: 8800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6193140745162964\n",
      "      kl: 0.03452860563993454\n",
      "      policy_loss: 0.000130237007397227\n",
      "      total_loss: 1.1343647241592407\n",
      "      vf_explained_var: 0.9972547888755798\n",
      "      vf_loss: 1.1342345476150513\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3889297544956207\n",
      "      kl: 0.02252262644469738\n",
      "      policy_loss: 0.0006082924082875252\n",
      "      total_loss: 1.349955439567566\n",
      "      vf_explained_var: 0.998615562915802\n",
      "      vf_loss: 1.3493471145629883\n",
      "    sample_time_ms: 20133.456\n",
      "    update_time_ms: 7.009\n",
      "  iterations_since_restore: 880\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.5351619316762\n",
      "    rl_1: 106.11154422429213\n",
      "  time_since_restore: 20710.5644865036\n",
      "  time_this_iter_s: 23.740319967269897\n",
      "  time_total_s: 20710.5644865036\n",
      "  timestamp: 1550814154\n",
      "  timesteps_since_restore: 8800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8800000\n",
      "  training_iteration: 880\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20710 s, 880 iter, 8800000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-42-57\n",
      "  done: false\n",
      "  episode_len_mean: 89.625\n",
      "  episode_reward_max: 217.08572713062983\n",
      "  episode_reward_mean: 175.70401298135798\n",
      "  episode_reward_min: 140.5886127191713\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 93587\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.739\n",
      "    load_time_ms: 2.131\n",
      "    num_steps_sampled: 8810000\n",
      "    num_steps_trained: 8810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7421932816505432\n",
      "      kl: 0.0408610962331295\n",
      "      policy_loss: 0.005419390741735697\n",
      "      total_loss: 0.9356878399848938\n",
      "      vf_explained_var: 0.9979881048202515\n",
      "      vf_loss: 0.9302682876586914\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4199305772781372\n",
      "      kl: 0.012166700325906277\n",
      "      policy_loss: 0.0019538914784789085\n",
      "      total_loss: 0.9386987090110779\n",
      "      vf_explained_var: 0.9990063905715942\n",
      "      vf_loss: 0.9367449879646301\n",
      "    sample_time_ms: 20108.242\n",
      "    update_time_ms: 7.237\n",
      "  iterations_since_restore: 881\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.00556206030618\n",
      "    rl_1: 105.69845092105182\n",
      "  time_since_restore: 20734.035952091217\n",
      "  time_this_iter_s: 23.471465587615967\n",
      "  time_total_s: 20734.035952091217\n",
      "  timestamp: 1550814177\n",
      "  timesteps_since_restore: 8810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8810000\n",
      "  training_iteration: 881\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20734 s, 881 iter, 8810000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-43-21\n",
      "  done: false\n",
      "  episode_len_mean: 89.32142857142857\n",
      "  episode_reward_max: 215.97649739601835\n",
      "  episode_reward_mean: 175.02074611274102\n",
      "  episode_reward_min: 137.18233413923681\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 93699\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.449\n",
      "    load_time_ms: 2.189\n",
      "    num_steps_sampled: 8820000\n",
      "    num_steps_trained: 8820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7032332420349121\n",
      "      kl: 0.02903495356440544\n",
      "      policy_loss: 0.005713951773941517\n",
      "      total_loss: 1.0372374057769775\n",
      "      vf_explained_var: 0.9977847337722778\n",
      "      vf_loss: 1.031523585319519\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.43038880825042725\n",
      "      kl: 0.025697903707623482\n",
      "      policy_loss: 0.008694577030837536\n",
      "      total_loss: 1.1623643636703491\n",
      "      vf_explained_var: 0.9987906217575073\n",
      "      vf_loss: 1.1536699533462524\n",
      "    sample_time_ms: 20166.36\n",
      "    update_time_ms: 7.557\n",
      "  iterations_since_restore: 882\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.9451767745352\n",
      "    rl_1: 105.07556933820581\n",
      "  time_since_restore: 20757.80941271782\n",
      "  time_this_iter_s: 23.773460626602173\n",
      "  time_total_s: 20757.80941271782\n",
      "  timestamp: 1550814201\n",
      "  timesteps_since_restore: 8820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8820000\n",
      "  training_iteration: 882\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20757 s, 882 iter, 8820000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-43-44\n",
      "  done: false\n",
      "  episode_len_mean: 89.34821428571429\n",
      "  episode_reward_max: 217.33547561618485\n",
      "  episode_reward_mean: 172.16396274985286\n",
      "  episode_reward_min: 137.60855578974952\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 93811\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.516\n",
      "    load_time_ms: 2.214\n",
      "    num_steps_sampled: 8830000\n",
      "    num_steps_trained: 8830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6912590861320496\n",
      "      kl: 0.042871419340372086\n",
      "      policy_loss: 0.004844441544264555\n",
      "      total_loss: 1.0590474605560303\n",
      "      vf_explained_var: 0.9976621866226196\n",
      "      vf_loss: 1.054202914237976\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3955577313899994\n",
      "      kl: 0.011420873925089836\n",
      "      policy_loss: 0.0009247733396477997\n",
      "      total_loss: 1.0924413204193115\n",
      "      vf_explained_var: 0.9988018870353699\n",
      "      vf_loss: 1.0915164947509766\n",
      "    sample_time_ms: 20183.091\n",
      "    update_time_ms: 7.416\n",
      "  iterations_since_restore: 883\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.62259958483901\n",
      "    rl_1: 103.54136316501383\n",
      "  time_since_restore: 20780.72798013687\n",
      "  time_this_iter_s: 22.918567419052124\n",
      "  time_total_s: 20780.72798013687\n",
      "  timestamp: 1550814224\n",
      "  timesteps_since_restore: 8830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8830000\n",
      "  training_iteration: 883\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20780 s, 883 iter, 8830000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-44-07\n",
      "  done: false\n",
      "  episode_len_mean: 89.90990990990991\n",
      "  episode_reward_max: 215.13261374428672\n",
      "  episode_reward_mean: 174.30891891594857\n",
      "  episode_reward_min: 140.5193772889586\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 93922\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.761\n",
      "    load_time_ms: 2.307\n",
      "    num_steps_sampled: 8840000\n",
      "    num_steps_trained: 8840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6743737459182739\n",
      "      kl: 0.035911910235881805\n",
      "      policy_loss: 0.004530043341219425\n",
      "      total_loss: 1.155395746231079\n",
      "      vf_explained_var: 0.9974374771118164\n",
      "      vf_loss: 1.1508657932281494\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4460262954235077\n",
      "      kl: 0.014597933739423752\n",
      "      policy_loss: 0.003959472291171551\n",
      "      total_loss: 1.3366007804870605\n",
      "      vf_explained_var: 0.998630940914154\n",
      "      vf_loss: 1.332641363143921\n",
      "    sample_time_ms: 20194.789\n",
      "    update_time_ms: 7.806\n",
      "  iterations_since_restore: 884\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.07717582827831\n",
      "    rl_1: 106.23174308767028\n",
      "  time_since_restore: 20804.05124115944\n",
      "  time_this_iter_s: 23.32326102256775\n",
      "  time_total_s: 20804.05124115944\n",
      "  timestamp: 1550814247\n",
      "  timesteps_since_restore: 8840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8840000\n",
      "  training_iteration: 884\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20804 s, 884 iter, 8840000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-44-31\n",
      "  done: false\n",
      "  episode_len_mean: 89.52678571428571\n",
      "  episode_reward_max: 212.15651203957123\n",
      "  episode_reward_mean: 172.09708676963288\n",
      "  episode_reward_min: 137.7787668064128\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 94034\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.656\n",
      "    load_time_ms: 2.377\n",
      "    num_steps_sampled: 8850000\n",
      "    num_steps_trained: 8850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7113729119300842\n",
      "      kl: 0.026757655665278435\n",
      "      policy_loss: 0.005819134879857302\n",
      "      total_loss: 1.0801491737365723\n",
      "      vf_explained_var: 0.9975260496139526\n",
      "      vf_loss: 1.07433021068573\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.41015127301216125\n",
      "      kl: 0.018545759841799736\n",
      "      policy_loss: 0.00473242811858654\n",
      "      total_loss: 0.9213035702705383\n",
      "      vf_explained_var: 0.9990075826644897\n",
      "      vf_loss: 0.9165710806846619\n",
      "    sample_time_ms: 20223.597\n",
      "    update_time_ms: 8.062\n",
      "  iterations_since_restore: 885\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.74354242936919\n",
      "    rl_1: 103.35354434026372\n",
      "  time_since_restore: 20827.650294303894\n",
      "  time_this_iter_s: 23.599053144454956\n",
      "  time_total_s: 20827.650294303894\n",
      "  timestamp: 1550814271\n",
      "  timesteps_since_restore: 8850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8850000\n",
      "  training_iteration: 885\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20827 s, 885 iter, 8850000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-44-54\n",
      "  done: false\n",
      "  episode_len_mean: 90.25454545454545\n",
      "  episode_reward_max: 216.24678264363513\n",
      "  episode_reward_mean: 172.61539058290558\n",
      "  episode_reward_min: 134.1354367368186\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 94144\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.015\n",
      "    load_time_ms: 2.504\n",
      "    num_steps_sampled: 8860000\n",
      "    num_steps_trained: 8860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6687721014022827\n",
      "      kl: 0.04391788691282272\n",
      "      policy_loss: 0.003750402480363846\n",
      "      total_loss: 1.0874216556549072\n",
      "      vf_explained_var: 0.9975317716598511\n",
      "      vf_loss: 1.0836713314056396\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.45516878366470337\n",
      "      kl: 0.03616880625486374\n",
      "      policy_loss: 0.011138751171529293\n",
      "      total_loss: 1.2931848764419556\n",
      "      vf_explained_var: 0.9986982941627502\n",
      "      vf_loss: 1.2820461988449097\n",
      "    sample_time_ms: 20207.236\n",
      "    update_time_ms: 7.98\n",
      "  iterations_since_restore: 886\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.70905514266549\n",
      "    rl_1: 105.90633544024006\n",
      "  time_since_restore: 20850.964683055878\n",
      "  time_this_iter_s: 23.314388751983643\n",
      "  time_total_s: 20850.964683055878\n",
      "  timestamp: 1550814294\n",
      "  timesteps_since_restore: 8860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8860000\n",
      "  training_iteration: 886\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20850 s, 886 iter, 8860000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-45-17\n",
      "  done: false\n",
      "  episode_len_mean: 89.72321428571429\n",
      "  episode_reward_max: 214.9534261807248\n",
      "  episode_reward_mean: 173.30297293653985\n",
      "  episode_reward_min: 138.86370209209952\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 94256\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3125.315\n",
      "    load_time_ms: 2.623\n",
      "    num_steps_sampled: 8870000\n",
      "    num_steps_trained: 8870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7021505832672119\n",
      "      kl: 0.02704317681491375\n",
      "      policy_loss: 0.0006114855641499162\n",
      "      total_loss: 1.0626482963562012\n",
      "      vf_explained_var: 0.9976322650909424\n",
      "      vf_loss: 1.0620367527008057\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4082886278629303\n",
      "      kl: 0.02038474567234516\n",
      "      policy_loss: 0.0031126635149121284\n",
      "      total_loss: 1.0206102132797241\n",
      "      vf_explained_var: 0.9989222884178162\n",
      "      vf_loss: 1.0174975395202637\n",
      "    sample_time_ms: 20162.585\n",
      "    update_time_ms: 8.298\n",
      "  iterations_since_restore: 887\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.8546743892309\n",
      "    rl_1: 104.44829854730894\n",
      "  time_since_restore: 20873.739703178406\n",
      "  time_this_iter_s: 22.775020122528076\n",
      "  time_total_s: 20873.739703178406\n",
      "  timestamp: 1550814317\n",
      "  timesteps_since_restore: 8870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8870000\n",
      "  training_iteration: 887\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20873 s, 887 iter, 8870000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-45-40\n",
      "  done: false\n",
      "  episode_len_mean: 90.04545454545455\n",
      "  episode_reward_max: 216.91819860620558\n",
      "  episode_reward_mean: 174.96022427928514\n",
      "  episode_reward_min: 138.2642230261187\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 94366\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3123.989\n",
      "    load_time_ms: 2.614\n",
      "    num_steps_sampled: 8880000\n",
      "    num_steps_trained: 8880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6719504594802856\n",
      "      kl: 0.02456916682422161\n",
      "      policy_loss: 0.00023666273045819253\n",
      "      total_loss: 1.0859274864196777\n",
      "      vf_explained_var: 0.9975118637084961\n",
      "      vf_loss: 1.0856907367706299\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4220963418483734\n",
      "      kl: 0.02264571003615856\n",
      "      policy_loss: 0.006251015234738588\n",
      "      total_loss: 1.1961299180984497\n",
      "      vf_explained_var: 0.9987824559211731\n",
      "      vf_loss: 1.1898789405822754\n",
      "    sample_time_ms: 20168.056\n",
      "    update_time_ms: 8.103\n",
      "  iterations_since_restore: 888\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.59077065119054\n",
      "    rl_1: 106.3694536280946\n",
      "  time_since_restore: 20896.728063106537\n",
      "  time_this_iter_s: 22.988359928131104\n",
      "  time_total_s: 20896.728063106537\n",
      "  timestamp: 1550814340\n",
      "  timesteps_since_restore: 8880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8880000\n",
      "  training_iteration: 888\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20896 s, 888 iter, 8880000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 90.0\n",
      "  episode_reward_max: 220.033536842019\n",
      "  episode_reward_mean: 171.09287310938356\n",
      "  episode_reward_min: 135.76475561658415\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 94478\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.783\n",
      "    load_time_ms: 2.601\n",
      "    num_steps_sampled: 8890000\n",
      "    num_steps_trained: 8890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6549288630485535\n",
      "      kl: 0.028381124138832092\n",
      "      policy_loss: 0.0014849387807771564\n",
      "      total_loss: 1.0972633361816406\n",
      "      vf_explained_var: 0.9974867105484009\n",
      "      vf_loss: 1.0957785844802856\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.40277817845344543\n",
      "      kl: 0.03385976329445839\n",
      "      policy_loss: 0.016390062868595123\n",
      "      total_loss: 1.0752308368682861\n",
      "      vf_explained_var: 0.9988754391670227\n",
      "      vf_loss: 1.0588408708572388\n",
      "    sample_time_ms: 20189.885\n",
      "    update_time_ms: 8.092\n",
      "  iterations_since_restore: 889\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.1357111239548\n",
      "    rl_1: 103.95716198542871\n",
      "  time_since_restore: 20920.286627054214\n",
      "  time_this_iter_s: 23.558563947677612\n",
      "  time_total_s: 20920.286627054214\n",
      "  timestamp: 1550814364\n",
      "  timesteps_since_restore: 8890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8890000\n",
      "  training_iteration: 889\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20920 s, 889 iter, 8890000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-46-27\n",
      "  done: false\n",
      "  episode_len_mean: 89.21428571428571\n",
      "  episode_reward_max: 219.3332217406815\n",
      "  episode_reward_mean: 172.8101576755608\n",
      "  episode_reward_min: 141.93093341047447\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 94590\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.502\n",
      "    load_time_ms: 2.671\n",
      "    num_steps_sampled: 8900000\n",
      "    num_steps_trained: 8900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6562435030937195\n",
      "      kl: 0.0276467427611351\n",
      "      policy_loss: 0.00044390998664312065\n",
      "      total_loss: 0.9597673416137695\n",
      "      vf_explained_var: 0.9978926181793213\n",
      "      vf_loss: 0.9593234658241272\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3775908052921295\n",
      "      kl: 0.022476186975836754\n",
      "      policy_loss: 0.0011643044417724013\n",
      "      total_loss: 1.2295572757720947\n",
      "      vf_explained_var: 0.9986649751663208\n",
      "      vf_loss: 1.2283931970596313\n",
      "    sample_time_ms: 20093.787\n",
      "    update_time_ms: 8.377\n",
      "  iterations_since_restore: 890\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.02357220825627\n",
      "    rl_1: 103.78658546730449\n",
      "  time_since_restore: 20943.096467494965\n",
      "  time_this_iter_s: 22.809840440750122\n",
      "  time_total_s: 20943.096467494965\n",
      "  timestamp: 1550814387\n",
      "  timesteps_since_restore: 8900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8900000\n",
      "  training_iteration: 890\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20943 s, 890 iter, 8900000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-46-50\n",
      "  done: false\n",
      "  episode_len_mean: 89.75675675675676\n",
      "  episode_reward_max: 216.68577845816267\n",
      "  episode_reward_mean: 174.88005122707827\n",
      "  episode_reward_min: 139.70202595186544\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 94701\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.348\n",
      "    load_time_ms: 2.678\n",
      "    num_steps_sampled: 8910000\n",
      "    num_steps_trained: 8910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6754377484321594\n",
      "      kl: 0.021640555933117867\n",
      "      policy_loss: 0.0022362046875059605\n",
      "      total_loss: 1.0222796201705933\n",
      "      vf_explained_var: 0.9976927042007446\n",
      "      vf_loss: 1.0200433731079102\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.423469215631485\n",
      "      kl: 0.022606583312153816\n",
      "      policy_loss: 0.0033928423654288054\n",
      "      total_loss: 1.2654632329940796\n",
      "      vf_explained_var: 0.9987196326255798\n",
      "      vf_loss: 1.2620704174041748\n",
      "    sample_time_ms: 20079.288\n",
      "    update_time_ms: 8.364\n",
      "  iterations_since_restore: 891\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.09276755657567\n",
      "    rl_1: 106.78728367050263\n",
      "  time_since_restore: 20966.61486840248\n",
      "  time_this_iter_s: 23.51840090751648\n",
      "  time_total_s: 20966.61486840248\n",
      "  timestamp: 1550814410\n",
      "  timesteps_since_restore: 8910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8910000\n",
      "  training_iteration: 891\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20966 s, 891 iter, 8910000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-47-13\n",
      "  done: false\n",
      "  episode_len_mean: 89.89285714285714\n",
      "  episode_reward_max: 219.02545343384878\n",
      "  episode_reward_mean: 173.765165371647\n",
      "  episode_reward_min: 139.13840982182484\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 94813\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.209\n",
      "    load_time_ms: 2.65\n",
      "    num_steps_sampled: 8920000\n",
      "    num_steps_trained: 8920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6825482249259949\n",
      "      kl: 0.024997670203447342\n",
      "      policy_loss: 0.002035425743088126\n",
      "      total_loss: 1.1966755390167236\n",
      "      vf_explained_var: 0.9975578188896179\n",
      "      vf_loss: 1.1946401596069336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4715081751346588\n",
      "      kl: 0.017722396180033684\n",
      "      policy_loss: 0.005013121757656336\n",
      "      total_loss: 1.1251511573791504\n",
      "      vf_explained_var: 0.9988124370574951\n",
      "      vf_loss: 1.1201380491256714\n",
      "    sample_time_ms: 20009.743\n",
      "    update_time_ms: 8.035\n",
      "  iterations_since_restore: 892\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.60641088811998\n",
      "    rl_1: 105.15875448352699\n",
      "  time_since_restore: 20989.47983455658\n",
      "  time_this_iter_s: 22.86496615409851\n",
      "  time_total_s: 20989.47983455658\n",
      "  timestamp: 1550814433\n",
      "  timesteps_since_restore: 8920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8920000\n",
      "  training_iteration: 892\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 20989 s, 892 iter, 8920000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-47-36\n",
      "  done: false\n",
      "  episode_len_mean: 90.009009009009\n",
      "  episode_reward_max: 216.1041163113884\n",
      "  episode_reward_mean: 172.09640223136054\n",
      "  episode_reward_min: 140.09067910657808\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 94924\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.106\n",
      "    load_time_ms: 2.619\n",
      "    num_steps_sampled: 8930000\n",
      "    num_steps_trained: 8930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6581603288650513\n",
      "      kl: 0.03684549033641815\n",
      "      policy_loss: 0.005552280228585005\n",
      "      total_loss: 1.066555142402649\n",
      "      vf_explained_var: 0.9975698590278625\n",
      "      vf_loss: 1.0610027313232422\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4231310486793518\n",
      "      kl: 0.03266594558954239\n",
      "      policy_loss: 0.009090450592339039\n",
      "      total_loss: 1.2140833139419556\n",
      "      vf_explained_var: 0.998724639415741\n",
      "      vf_loss: 1.2049927711486816\n",
      "    sample_time_ms: 20047.025\n",
      "    update_time_ms: 8.27\n",
      "  iterations_since_restore: 893\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.97663935479169\n",
      "    rl_1: 105.11976287656884\n",
      "  time_since_restore: 21012.818690538406\n",
      "  time_this_iter_s: 23.338855981826782\n",
      "  time_total_s: 21012.818690538406\n",
      "  timestamp: 1550814456\n",
      "  timesteps_since_restore: 8930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8930000\n",
      "  training_iteration: 893\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21012 s, 893 iter, 8930000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-48-00\n",
      "  done: false\n",
      "  episode_len_mean: 90.52727272727273\n",
      "  episode_reward_max: 211.5469640210507\n",
      "  episode_reward_mean: 173.23717608747233\n",
      "  episode_reward_min: 138.20847516121754\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 95034\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.458\n",
      "    load_time_ms: 2.586\n",
      "    num_steps_sampled: 8940000\n",
      "    num_steps_trained: 8940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6614269018173218\n",
      "      kl: 0.03346507251262665\n",
      "      policy_loss: 0.0031054371502250433\n",
      "      total_loss: 1.055748462677002\n",
      "      vf_explained_var: 0.9976229667663574\n",
      "      vf_loss: 1.052642822265625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.468210369348526\n",
      "      kl: 0.014284067787230015\n",
      "      policy_loss: 0.0023179450072348118\n",
      "      total_loss: 1.0878180265426636\n",
      "      vf_explained_var: 0.99888014793396\n",
      "      vf_loss: 1.0855001211166382\n",
      "    sample_time_ms: 20034.965\n",
      "    update_time_ms: 7.851\n",
      "  iterations_since_restore: 894\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.64404799830578\n",
      "    rl_1: 106.59312808916656\n",
      "  time_since_restore: 21036.048533916473\n",
      "  time_this_iter_s: 23.229843378067017\n",
      "  time_total_s: 21036.048533916473\n",
      "  timestamp: 1550814480\n",
      "  timesteps_since_restore: 8940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8940000\n",
      "  training_iteration: 894\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21036 s, 894 iter, 8940000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-48-23\n",
      "  done: false\n",
      "  episode_len_mean: 90.04504504504504\n",
      "  episode_reward_max: 217.92179219667855\n",
      "  episode_reward_mean: 174.85111286635728\n",
      "  episode_reward_min: 142.33045691068017\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 95145\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.642\n",
      "    load_time_ms: 2.533\n",
      "    num_steps_sampled: 8950000\n",
      "    num_steps_trained: 8950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7121616005897522\n",
      "      kl: 0.022890079766511917\n",
      "      policy_loss: 0.0020627945195883512\n",
      "      total_loss: 1.1085224151611328\n",
      "      vf_explained_var: 0.9975720047950745\n",
      "      vf_loss: 1.1064597368240356\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5072348117828369\n",
      "      kl: 0.018800321966409683\n",
      "      policy_loss: 0.005281091667711735\n",
      "      total_loss: 0.9409130811691284\n",
      "      vf_explained_var: 0.9990293979644775\n",
      "      vf_loss: 0.9356318712234497\n",
      "    sample_time_ms: 20010.407\n",
      "    update_time_ms: 7.845\n",
      "  iterations_since_restore: 895\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.2117013557848\n",
      "    rl_1: 105.63941151057243\n",
      "  time_since_restore: 21059.42299103737\n",
      "  time_this_iter_s: 23.374457120895386\n",
      "  time_total_s: 21059.42299103737\n",
      "  timestamp: 1550814503\n",
      "  timesteps_since_restore: 8950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8950000\n",
      "  training_iteration: 895\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21059 s, 895 iter, 8950000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-48-46\n",
      "  done: false\n",
      "  episode_len_mean: 89.23893805309734\n",
      "  episode_reward_max: 212.9280695421901\n",
      "  episode_reward_mean: 172.77645855407323\n",
      "  episode_reward_min: 142.37138254800652\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 95258\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.348\n",
      "    load_time_ms: 2.41\n",
      "    num_steps_sampled: 8960000\n",
      "    num_steps_trained: 8960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6859464049339294\n",
      "      kl: 0.03544681891798973\n",
      "      policy_loss: 0.0032052379101514816\n",
      "      total_loss: 1.0945076942443848\n",
      "      vf_explained_var: 0.9975734353065491\n",
      "      vf_loss: 1.0913023948669434\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.41734009981155396\n",
      "      kl: 0.01468223612755537\n",
      "      policy_loss: 0.001505861640907824\n",
      "      total_loss: 1.0609647035598755\n",
      "      vf_explained_var: 0.9988523721694946\n",
      "      vf_loss: 1.05945885181427\n",
      "    sample_time_ms: 20007.379\n",
      "    update_time_ms: 7.762\n",
      "  iterations_since_restore: 896\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.36019872985811\n",
      "    rl_1: 103.41625982421515\n",
      "  time_since_restore: 21082.72927570343\n",
      "  time_this_iter_s: 23.3062846660614\n",
      "  time_total_s: 21082.72927570343\n",
      "  timestamp: 1550814526\n",
      "  timesteps_since_restore: 8960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8960000\n",
      "  training_iteration: 896\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21082 s, 896 iter, 8960000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-49-10\n",
      "  done: false\n",
      "  episode_len_mean: 90.58181818181818\n",
      "  episode_reward_max: 217.07210631170008\n",
      "  episode_reward_mean: 173.09827157866562\n",
      "  episode_reward_min: 138.61546199465317\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 95368\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.656\n",
      "    load_time_ms: 2.293\n",
      "    num_steps_sampled: 8970000\n",
      "    num_steps_trained: 8970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7180362343788147\n",
      "      kl: 0.7749321460723877\n",
      "      policy_loss: 0.035467665642499924\n",
      "      total_loss: 0.8900198340415955\n",
      "      vf_explained_var: 0.997991144657135\n",
      "      vf_loss: 0.854552149772644\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5588967800140381\n",
      "      kl: 0.020525682717561722\n",
      "      policy_loss: 0.0005522033898159862\n",
      "      total_loss: 0.8477528095245361\n",
      "      vf_explained_var: 0.9991379976272583\n",
      "      vf_loss: 0.8472006320953369\n",
      "    sample_time_ms: 20045.258\n",
      "    update_time_ms: 7.592\n",
      "  iterations_since_restore: 897\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.05952288429957\n",
      "    rl_1: 106.03874869436606\n",
      "  time_since_restore: 21105.872662305832\n",
      "  time_this_iter_s: 23.143386602401733\n",
      "  time_total_s: 21105.872662305832\n",
      "  timestamp: 1550814550\n",
      "  timesteps_since_restore: 8970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8970000\n",
      "  training_iteration: 897\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21105 s, 897 iter, 8970000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-49-33\n",
      "  done: false\n",
      "  episode_len_mean: 89.93693693693693\n",
      "  episode_reward_max: 209.65903409692572\n",
      "  episode_reward_mean: 172.60460363748987\n",
      "  episode_reward_min: 137.98609133556116\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 95479\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.705\n",
      "    load_time_ms: 2.279\n",
      "    num_steps_sampled: 8980000\n",
      "    num_steps_trained: 8980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.724043607711792\n",
      "      kl: 0.0333930142223835\n",
      "      policy_loss: -0.0002708508400246501\n",
      "      total_loss: 0.9880937337875366\n",
      "      vf_explained_var: 0.9976534843444824\n",
      "      vf_loss: 0.9883645176887512\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4953646957874298\n",
      "      kl: 0.01649235561490059\n",
      "      policy_loss: 0.0008240489405579865\n",
      "      total_loss: 1.0187400579452515\n",
      "      vf_explained_var: 0.9989407062530518\n",
      "      vf_loss: 1.017915964126587\n",
      "    sample_time_ms: 20043.037\n",
      "    update_time_ms: 7.802\n",
      "  iterations_since_restore: 898\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.53630544295159\n",
      "    rl_1: 105.06829819453824\n",
      "  time_since_restore: 21128.909394979477\n",
      "  time_this_iter_s: 23.03673267364502\n",
      "  time_total_s: 21128.909394979477\n",
      "  timestamp: 1550814573\n",
      "  timesteps_since_restore: 8980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8980000\n",
      "  training_iteration: 898\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21128 s, 898 iter, 8980000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-49-56\n",
      "  done: false\n",
      "  episode_len_mean: 89.47321428571429\n",
      "  episode_reward_max: 212.2528706932361\n",
      "  episode_reward_mean: 175.54849149223622\n",
      "  episode_reward_min: 141.26982271200802\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 95591\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.573\n",
      "    load_time_ms: 2.281\n",
      "    num_steps_sampled: 8990000\n",
      "    num_steps_trained: 8990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7334171533584595\n",
      "      kl: 0.0542253702878952\n",
      "      policy_loss: 0.006948391441255808\n",
      "      total_loss: 0.9479437470436096\n",
      "      vf_explained_var: 0.9978596568107605\n",
      "      vf_loss: 0.9409953355789185\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4465189576148987\n",
      "      kl: 0.01881212741136551\n",
      "      policy_loss: 0.00394248403608799\n",
      "      total_loss: 1.1336369514465332\n",
      "      vf_explained_var: 0.9988449215888977\n",
      "      vf_loss: 1.1296943426132202\n",
      "    sample_time_ms: 20015.322\n",
      "    update_time_ms: 7.923\n",
      "  iterations_since_restore: 899\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.68395143891476\n",
      "    rl_1: 105.86454005332143\n",
      "  time_since_restore: 21152.147859811783\n",
      "  time_this_iter_s: 23.238464832305908\n",
      "  time_total_s: 21152.147859811783\n",
      "  timestamp: 1550814596\n",
      "  timesteps_since_restore: 8990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 8990000\n",
      "  training_iteration: 899\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21152 s, 899 iter, 8990000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-50-19\n",
      "  done: false\n",
      "  episode_len_mean: 90.74545454545455\n",
      "  episode_reward_max: 214.19334018832052\n",
      "  episode_reward_mean: 174.2729290173771\n",
      "  episode_reward_min: 138.52569962123798\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 95701\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.364\n",
      "    load_time_ms: 2.232\n",
      "    num_steps_sampled: 9000000\n",
      "    num_steps_trained: 9000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7364770174026489\n",
      "      kl: 0.038428209722042084\n",
      "      policy_loss: 0.002137688221409917\n",
      "      total_loss: 1.007325530052185\n",
      "      vf_explained_var: 0.997779369354248\n",
      "      vf_loss: 1.005187749862671\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4779367744922638\n",
      "      kl: 0.01689249277114868\n",
      "      policy_loss: 0.0011214487021788955\n",
      "      total_loss: 0.9448228478431702\n",
      "      vf_explained_var: 0.9990377426147461\n",
      "      vf_loss: 0.943701446056366\n",
      "    sample_time_ms: 19998.598\n",
      "    update_time_ms: 7.537\n",
      "  iterations_since_restore: 900\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.91812581060931\n",
      "    rl_1: 106.35480320676777\n",
      "  time_since_restore: 21174.773193359375\n",
      "  time_this_iter_s: 22.625333547592163\n",
      "  time_total_s: 21174.773193359375\n",
      "  timestamp: 1550814619\n",
      "  timesteps_since_restore: 9000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9000000\n",
      "  training_iteration: 900\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21174 s, 900 iter, 9000000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-50-42\n",
      "  done: false\n",
      "  episode_len_mean: 90.55454545454545\n",
      "  episode_reward_max: 217.8965801928841\n",
      "  episode_reward_mean: 175.64641262594583\n",
      "  episode_reward_min: 141.35265471346665\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 95811\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.676\n",
      "    load_time_ms: 2.22\n",
      "    num_steps_sampled: 9010000\n",
      "    num_steps_trained: 9010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7131229639053345\n",
      "      kl: 0.039646923542022705\n",
      "      policy_loss: 0.005149281118065119\n",
      "      total_loss: 0.9066575169563293\n",
      "      vf_explained_var: 0.9979931712150574\n",
      "      vf_loss: 0.9015080332756042\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.40580859780311584\n",
      "      kl: 0.01770753227174282\n",
      "      policy_loss: -0.0017511293990537524\n",
      "      total_loss: 0.905664324760437\n",
      "      vf_explained_var: 0.9990757703781128\n",
      "      vf_loss: 0.9074153304100037\n",
      "    sample_time_ms: 20030.589\n",
      "    update_time_ms: 7.257\n",
      "  iterations_since_restore: 901\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.57355757661803\n",
      "    rl_1: 107.07285504932781\n",
      "  time_since_restore: 21198.460510253906\n",
      "  time_this_iter_s: 23.68731689453125\n",
      "  time_total_s: 21198.460510253906\n",
      "  timestamp: 1550814642\n",
      "  timesteps_since_restore: 9010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9010000\n",
      "  training_iteration: 901\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21198 s, 901 iter, 9010000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-51-06\n",
      "  done: false\n",
      "  episode_len_mean: 90.07207207207207\n",
      "  episode_reward_max: 218.2778567576633\n",
      "  episode_reward_mean: 172.6799025354458\n",
      "  episode_reward_min: 137.88763071360546\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 95922\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.719\n",
      "    load_time_ms: 2.208\n",
      "    num_steps_sampled: 9020000\n",
      "    num_steps_trained: 9020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6962454319000244\n",
      "      kl: 0.5390344262123108\n",
      "      policy_loss: 0.03641325235366821\n",
      "      total_loss: 1.1467922925949097\n",
      "      vf_explained_var: 0.9974356293678284\n",
      "      vf_loss: 1.1103790998458862\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5036689043045044\n",
      "      kl: 0.04512099549174309\n",
      "      policy_loss: 0.013352799229323864\n",
      "      total_loss: 1.1254916191101074\n",
      "      vf_explained_var: 0.9988295435905457\n",
      "      vf_loss: 1.1121388673782349\n",
      "    sample_time_ms: 20076.076\n",
      "    update_time_ms: 7.215\n",
      "  iterations_since_restore: 902\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.2524194120256\n",
      "    rl_1: 105.42748312342023\n",
      "  time_since_restore: 21221.775196313858\n",
      "  time_this_iter_s: 23.314686059951782\n",
      "  time_total_s: 21221.775196313858\n",
      "  timestamp: 1550814666\n",
      "  timesteps_since_restore: 9020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9020000\n",
      "  training_iteration: 902\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21221 s, 902 iter, 9020000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-51-29\n",
      "  done: false\n",
      "  episode_len_mean: 89.58928571428571\n",
      "  episode_reward_max: 215.71520319975895\n",
      "  episode_reward_mean: 175.40867701502268\n",
      "  episode_reward_min: 134.79955616391425\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 96034\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.437\n",
      "    load_time_ms: 2.172\n",
      "    num_steps_sampled: 9030000\n",
      "    num_steps_trained: 9030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7384009957313538\n",
      "      kl: 0.03466964513063431\n",
      "      policy_loss: 0.007073087617754936\n",
      "      total_loss: 0.9725416898727417\n",
      "      vf_explained_var: 0.9979538321495056\n",
      "      vf_loss: 0.96546870470047\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4419367015361786\n",
      "      kl: 0.024788323789834976\n",
      "      policy_loss: 0.007478313520550728\n",
      "      total_loss: 0.9008134603500366\n",
      "      vf_explained_var: 0.9990543723106384\n",
      "      vf_loss: 0.8933352828025818\n",
      "    sample_time_ms: 20068.566\n",
      "    update_time_ms: 6.923\n",
      "  iterations_since_restore: 903\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.08968943962194\n",
      "    rl_1: 104.31898757540073\n",
      "  time_since_restore: 21245.013882637024\n",
      "  time_this_iter_s: 23.238686323165894\n",
      "  time_total_s: 21245.013882637024\n",
      "  timestamp: 1550814689\n",
      "  timesteps_since_restore: 9030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9030000\n",
      "  training_iteration: 903\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21245 s, 903 iter, 9030000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-51-53\n",
      "  done: false\n",
      "  episode_len_mean: 90.42342342342343\n",
      "  episode_reward_max: 207.9763036479626\n",
      "  episode_reward_mean: 172.61775846194078\n",
      "  episode_reward_min: 137.75464508561004\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 96145\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3121.557\n",
      "    load_time_ms: 2.125\n",
      "    num_steps_sampled: 9040000\n",
      "    num_steps_trained: 9040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6608186960220337\n",
      "      kl: 0.04184264317154884\n",
      "      policy_loss: 0.0021780680399388075\n",
      "      total_loss: 1.1123485565185547\n",
      "      vf_explained_var: 0.9973086714744568\n",
      "      vf_loss: 1.110170602798462\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5257485508918762\n",
      "      kl: 0.032650869339704514\n",
      "      policy_loss: 0.005508818663656712\n",
      "      total_loss: 1.2602930068969727\n",
      "      vf_explained_var: 0.998730480670929\n",
      "      vf_loss: 1.254784345626831\n",
      "    sample_time_ms: 20085.932\n",
      "    update_time_ms: 7.199\n",
      "  iterations_since_restore: 904\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.04886336589675\n",
      "    rl_1: 106.56889509604393\n",
      "  time_since_restore: 21268.392047166824\n",
      "  time_this_iter_s: 23.378164529800415\n",
      "  time_total_s: 21268.392047166824\n",
      "  timestamp: 1550814713\n",
      "  timesteps_since_restore: 9040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9040000\n",
      "  training_iteration: 904\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21268 s, 904 iter, 9040000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-52-16\n",
      "  done: false\n",
      "  episode_len_mean: 89.78378378378379\n",
      "  episode_reward_max: 218.74194303213116\n",
      "  episode_reward_mean: 174.72055547734834\n",
      "  episode_reward_min: 134.96071641451778\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 96256\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3118.632\n",
      "    load_time_ms: 2.165\n",
      "    num_steps_sampled: 9050000\n",
      "    num_steps_trained: 9050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.733126699924469\n",
      "      kl: 0.04322052001953125\n",
      "      policy_loss: 0.006493557710200548\n",
      "      total_loss: 1.036692500114441\n",
      "      vf_explained_var: 0.9977421760559082\n",
      "      vf_loss: 1.0301990509033203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5087960362434387\n",
      "      kl: 0.01727793551981449\n",
      "      policy_loss: 0.004211845807731152\n",
      "      total_loss: 0.9770739674568176\n",
      "      vf_explained_var: 0.9989916682243347\n",
      "      vf_loss: 0.9728621244430542\n",
      "    sample_time_ms: 20074.697\n",
      "    update_time_ms: 7.201\n",
      "  iterations_since_restore: 905\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.0104856727144\n",
      "    rl_1: 105.71006980463396\n",
      "  time_since_restore: 21291.625413417816\n",
      "  time_this_iter_s: 23.23336625099182\n",
      "  time_total_s: 21291.625413417816\n",
      "  timestamp: 1550814736\n",
      "  timesteps_since_restore: 9050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9050000\n",
      "  training_iteration: 905\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21291 s, 905 iter, 9050000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-52-39\n",
      "  done: false\n",
      "  episode_len_mean: 90.24324324324324\n",
      "  episode_reward_max: 214.75065617489275\n",
      "  episode_reward_mean: 171.85412695684892\n",
      "  episode_reward_min: 137.17480432847591\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 96367\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.449\n",
      "    load_time_ms: 2.171\n",
      "    num_steps_sampled: 9060000\n",
      "    num_steps_trained: 9060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6696388721466064\n",
      "      kl: 0.03240169212222099\n",
      "      policy_loss: 0.0030801724642515182\n",
      "      total_loss: 1.039724349975586\n",
      "      vf_explained_var: 0.99762362241745\n",
      "      vf_loss: 1.0366439819335938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.43615320324897766\n",
      "      kl: 0.01338238175958395\n",
      "      policy_loss: -0.0010123676620423794\n",
      "      total_loss: 1.01111900806427\n",
      "      vf_explained_var: 0.9989261031150818\n",
      "      vf_loss: 1.0121313333511353\n",
      "    sample_time_ms: 20055.658\n",
      "    update_time_ms: 7.255\n",
      "  iterations_since_restore: 906\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.70184716664546\n",
      "    rl_1: 104.15227979020342\n",
      "  time_since_restore: 21314.818024873734\n",
      "  time_this_iter_s: 23.19261145591736\n",
      "  time_total_s: 21314.818024873734\n",
      "  timestamp: 1550814759\n",
      "  timesteps_since_restore: 9060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9060000\n",
      "  training_iteration: 906\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21314 s, 906 iter, 9060000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-53-02\n",
      "  done: false\n",
      "  episode_len_mean: 89.63063063063063\n",
      "  episode_reward_max: 220.22226557650458\n",
      "  episode_reward_mean: 174.3670972844756\n",
      "  episode_reward_min: 135.44033727297\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 96478\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.235\n",
      "    load_time_ms: 2.18\n",
      "    num_steps_sampled: 9070000\n",
      "    num_steps_trained: 9070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6955093145370483\n",
      "      kl: 0.03433012217283249\n",
      "      policy_loss: 0.0005092871142551303\n",
      "      total_loss: 0.9679875373840332\n",
      "      vf_explained_var: 0.9978761076927185\n",
      "      vf_loss: 0.967478334903717\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4535782039165497\n",
      "      kl: 0.022988295182585716\n",
      "      policy_loss: 0.0035455108154565096\n",
      "      total_loss: 1.176296591758728\n",
      "      vf_explained_var: 0.9987697005271912\n",
      "      vf_loss: 1.1727511882781982\n",
      "    sample_time_ms: 20049.294\n",
      "    update_time_ms: 7.208\n",
      "  iterations_since_restore: 907\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.34755117603189\n",
      "    rl_1: 105.01954610844366\n",
      "  time_since_restore: 21337.914353847504\n",
      "  time_this_iter_s: 23.09632897377014\n",
      "  time_total_s: 21337.914353847504\n",
      "  timestamp: 1550814782\n",
      "  timesteps_since_restore: 9070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9070000\n",
      "  training_iteration: 907\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21337 s, 907 iter, 9070000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-53-26\n",
      "  done: false\n",
      "  episode_len_mean: 89.72321428571429\n",
      "  episode_reward_max: 220.16805866797114\n",
      "  episode_reward_mean: 175.45285863915157\n",
      "  episode_reward_min: 140.32161718313242\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 96590\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.291\n",
      "    load_time_ms: 2.188\n",
      "    num_steps_sampled: 9080000\n",
      "    num_steps_trained: 9080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6968167424201965\n",
      "      kl: 0.04324724152684212\n",
      "      policy_loss: 0.005178986582905054\n",
      "      total_loss: 1.0212292671203613\n",
      "      vf_explained_var: 0.9977290034294128\n",
      "      vf_loss: 1.0160502195358276\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.41637754440307617\n",
      "      kl: 0.049015291035175323\n",
      "      policy_loss: 0.013838276267051697\n",
      "      total_loss: 1.1537772417068481\n",
      "      vf_explained_var: 0.9988426566123962\n",
      "      vf_loss: 1.1399389505386353\n",
      "    sample_time_ms: 20133.541\n",
      "    update_time_ms: 6.93\n",
      "  iterations_since_restore: 908\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.55073714031217\n",
      "    rl_1: 105.9021214988394\n",
      "  time_since_restore: 21361.75147509575\n",
      "  time_this_iter_s: 23.83712124824524\n",
      "  time_total_s: 21361.75147509575\n",
      "  timestamp: 1550814806\n",
      "  timesteps_since_restore: 9080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9080000\n",
      "  training_iteration: 908\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21361 s, 908 iter, 9080000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-53-50\n",
      "  done: false\n",
      "  episode_len_mean: 89.67857142857143\n",
      "  episode_reward_max: 215.45641416653245\n",
      "  episode_reward_mean: 166.54206387868777\n",
      "  episode_reward_min: -149.53972833064108\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 96702\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.708\n",
      "    load_time_ms: 2.211\n",
      "    num_steps_sampled: 9090000\n",
      "    num_steps_trained: 9090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6315945386886597\n",
      "      kl: 0.05713475123047829\n",
      "      policy_loss: -0.0019341760780662298\n",
      "      total_loss: 22.62550163269043\n",
      "      vf_explained_var: 0.9556078910827637\n",
      "      vf_loss: 22.627437591552734\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35678789019584656\n",
      "      kl: 0.013707759790122509\n",
      "      policy_loss: -0.0024930487852543592\n",
      "      total_loss: 37.705692291259766\n",
      "      vf_explained_var: 0.9646878242492676\n",
      "      vf_loss: 37.708187103271484\n",
      "    sample_time_ms: 20144.577\n",
      "    update_time_ms: 6.884\n",
      "  iterations_since_restore: 909\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.23235209546098\n",
      "    rl_1: 101.30971178322677\n",
      "  time_since_restore: 21385.275992393494\n",
      "  time_this_iter_s: 23.52451729774475\n",
      "  time_total_s: 21385.275992393494\n",
      "  timestamp: 1550814830\n",
      "  timesteps_since_restore: 9090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9090000\n",
      "  training_iteration: 909\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21385 s, 909 iter, 9090000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-54-13\n",
      "  done: false\n",
      "  episode_len_mean: 89.13392857142857\n",
      "  episode_reward_max: 214.41374656827838\n",
      "  episode_reward_mean: 173.0878647147853\n",
      "  episode_reward_min: 139.01506180995122\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 96814\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.499\n",
      "    load_time_ms: 2.205\n",
      "    num_steps_sampled: 9100000\n",
      "    num_steps_trained: 9100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6903122067451477\n",
      "      kl: 0.20512259006500244\n",
      "      policy_loss: 0.00010818128066603094\n",
      "      total_loss: 1.0044869184494019\n",
      "      vf_explained_var: 0.9976485967636108\n",
      "      vf_loss: 1.0043787956237793\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.38850080966949463\n",
      "      kl: 0.017918596044182777\n",
      "      policy_loss: 0.0035344308707863092\n",
      "      total_loss: 1.1650798320770264\n",
      "      vf_explained_var: 0.9987602233886719\n",
      "      vf_loss: 1.1615455150604248\n",
      "    sample_time_ms: 20168.147\n",
      "    update_time_ms: 7.209\n",
      "  iterations_since_restore: 910\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.40662487707216\n",
      "    rl_1: 104.68123983771314\n",
      "  time_since_restore: 21408.16796708107\n",
      "  time_this_iter_s: 22.891974687576294\n",
      "  time_total_s: 21408.16796708107\n",
      "  timestamp: 1550814853\n",
      "  timesteps_since_restore: 9100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9100000\n",
      "  training_iteration: 910\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21408 s, 910 iter, 9100000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-54-36\n",
      "  done: false\n",
      "  episode_len_mean: 90.26363636363637\n",
      "  episode_reward_max: 211.10522055017756\n",
      "  episode_reward_mean: 173.27646300456686\n",
      "  episode_reward_min: 137.3469104054867\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 96924\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.578\n",
      "    load_time_ms: 2.209\n",
      "    num_steps_sampled: 9110000\n",
      "    num_steps_trained: 9110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7226846218109131\n",
      "      kl: 0.028359517455101013\n",
      "      policy_loss: 0.0022402657195925713\n",
      "      total_loss: 0.9023930430412292\n",
      "      vf_explained_var: 0.9979655742645264\n",
      "      vf_loss: 0.9001529216766357\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4018409550189972\n",
      "      kl: 0.03552238270640373\n",
      "      policy_loss: 0.010164176113903522\n",
      "      total_loss: 0.7628985643386841\n",
      "      vf_explained_var: 0.9991959929466248\n",
      "      vf_loss: 0.7527344822883606\n",
      "    sample_time_ms: 20108.703\n",
      "    update_time_ms: 7.258\n",
      "  iterations_since_restore: 911\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.86271267056168\n",
      "    rl_1: 104.4137503340052\n",
      "  time_since_restore: 21431.204750299454\n",
      "  time_this_iter_s: 23.03678321838379\n",
      "  time_total_s: 21431.204750299454\n",
      "  timestamp: 1550814876\n",
      "  timesteps_since_restore: 9110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9110000\n",
      "  training_iteration: 911\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21431 s, 911 iter, 9110000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-54-59\n",
      "  done: false\n",
      "  episode_len_mean: 89.41964285714286\n",
      "  episode_reward_max: 217.5970306820051\n",
      "  episode_reward_mean: 169.5479084395894\n",
      "  episode_reward_min: -151.6058101686342\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 97036\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.954\n",
      "    load_time_ms: 2.189\n",
      "    num_steps_sampled: 9120000\n",
      "    num_steps_trained: 9120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.680293619632721\n",
      "      kl: 0.016017146408557892\n",
      "      policy_loss: -0.004919226747006178\n",
      "      total_loss: 24.093093872070312\n",
      "      vf_explained_var: 0.9550433158874512\n",
      "      vf_loss: 24.098011016845703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.38880422711372375\n",
      "      kl: 0.008052299730479717\n",
      "      policy_loss: -0.00022774093667976558\n",
      "      total_loss: 40.37271499633789\n",
      "      vf_explained_var: 0.9616314768791199\n",
      "      vf_loss: 40.37294006347656\n",
      "    sample_time_ms: 20131.944\n",
      "    update_time_ms: 7.296\n",
      "  iterations_since_restore: 912\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.69857522913546\n",
      "    rl_1: 102.84933321045392\n",
      "  time_since_restore: 21454.77898168564\n",
      "  time_this_iter_s: 23.574231386184692\n",
      "  time_total_s: 21454.77898168564\n",
      "  timestamp: 1550814899\n",
      "  timesteps_since_restore: 9120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9120000\n",
      "  training_iteration: 912\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21454 s, 912 iter, 9120000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-55-22\n",
      "  done: false\n",
      "  episode_len_mean: 90.00892857142857\n",
      "  episode_reward_max: 219.6056570032209\n",
      "  episode_reward_mean: 171.47606633713707\n",
      "  episode_reward_min: 135.93205497772192\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 97148\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3162.138\n",
      "    load_time_ms: 2.227\n",
      "    num_steps_sampled: 9130000\n",
      "    num_steps_trained: 9130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.682184636592865\n",
      "      kl: 0.02827654778957367\n",
      "      policy_loss: 0.004080761689692736\n",
      "      total_loss: 1.1179654598236084\n",
      "      vf_explained_var: 0.9973874092102051\n",
      "      vf_loss: 1.1138845682144165\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3911524713039398\n",
      "      kl: 0.03649884834885597\n",
      "      policy_loss: 0.010783765465021133\n",
      "      total_loss: 1.0280873775482178\n",
      "      vf_explained_var: 0.9989050030708313\n",
      "      vf_loss: 1.0173035860061646\n",
      "    sample_time_ms: 20066.164\n",
      "    update_time_ms: 7.305\n",
      "  iterations_since_restore: 913\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.25972664881415\n",
      "    rl_1: 104.2163396883229\n",
      "  time_since_restore: 21477.57221698761\n",
      "  time_this_iter_s: 22.793235301971436\n",
      "  time_total_s: 21477.57221698761\n",
      "  timestamp: 1550814922\n",
      "  timesteps_since_restore: 9130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9130000\n",
      "  training_iteration: 913\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21477 s, 913 iter, 9130000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-55-45\n",
      "  done: false\n",
      "  episode_len_mean: 90.27272727272727\n",
      "  episode_reward_max: 218.7539309537334\n",
      "  episode_reward_mean: 171.1788983008828\n",
      "  episode_reward_min: 137.033518922446\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 97258\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3159.207\n",
      "    load_time_ms: 2.208\n",
      "    num_steps_sampled: 9140000\n",
      "    num_steps_trained: 9140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6716429591178894\n",
      "      kl: 0.03248655050992966\n",
      "      policy_loss: 0.005075626540929079\n",
      "      total_loss: 1.0045207738876343\n",
      "      vf_explained_var: 0.9976351857185364\n",
      "      vf_loss: 0.9994451999664307\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.39809370040893555\n",
      "      kl: 0.015992432832717896\n",
      "      policy_loss: 0.004627277608960867\n",
      "      total_loss: 0.9770511984825134\n",
      "      vf_explained_var: 0.9989827275276184\n",
      "      vf_loss: 0.9724241495132446\n",
      "    sample_time_ms: 20024.535\n",
      "    update_time_ms: 7.174\n",
      "  iterations_since_restore: 914\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.26607232119777\n",
      "    rl_1: 104.912825979685\n",
      "  time_since_restore: 21500.50107884407\n",
      "  time_this_iter_s: 22.92886185646057\n",
      "  time_total_s: 21500.50107884407\n",
      "  timestamp: 1550814945\n",
      "  timesteps_since_restore: 9140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9140000\n",
      "  training_iteration: 914\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21500 s, 914 iter, 9140000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-56-09\n",
      "  done: false\n",
      "  episode_len_mean: 89.90090090090091\n",
      "  episode_reward_max: 219.76051226880267\n",
      "  episode_reward_mean: 170.5919083921555\n",
      "  episode_reward_min: 137.65445917564887\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 97369\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3159.048\n",
      "    load_time_ms: 2.152\n",
      "    num_steps_sampled: 9150000\n",
      "    num_steps_trained: 9150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6580333113670349\n",
      "      kl: 0.030019909143447876\n",
      "      policy_loss: -0.001354491920210421\n",
      "      total_loss: 0.8798618912696838\n",
      "      vf_explained_var: 0.9978722929954529\n",
      "      vf_loss: 0.881216287612915\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3940620422363281\n",
      "      kl: 0.0193412397056818\n",
      "      policy_loss: -0.003643311792984605\n",
      "      total_loss: 1.070888638496399\n",
      "      vf_explained_var: 0.9988663792610168\n",
      "      vf_loss: 1.0745317935943604\n",
      "    sample_time_ms: 20049.861\n",
      "    update_time_ms: 7.234\n",
      "  iterations_since_restore: 915\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.6158436214791\n",
      "    rl_1: 104.97606477067636\n",
      "  time_since_restore: 21523.985973596573\n",
      "  time_this_iter_s: 23.48489475250244\n",
      "  time_total_s: 21523.985973596573\n",
      "  timestamp: 1550814969\n",
      "  timesteps_since_restore: 9150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9150000\n",
      "  training_iteration: 915\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21523 s, 915 iter, 9150000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-56-32\n",
      "  done: false\n",
      "  episode_len_mean: 89.1875\n",
      "  episode_reward_max: 214.60251506301051\n",
      "  episode_reward_mean: 175.25771694262022\n",
      "  episode_reward_min: 143.23524410002517\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 97481\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.306\n",
      "    load_time_ms: 2.141\n",
      "    num_steps_sampled: 9160000\n",
      "    num_steps_trained: 9160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7135040163993835\n",
      "      kl: 0.03674965351819992\n",
      "      policy_loss: 0.006487320642918348\n",
      "      total_loss: 0.9184419512748718\n",
      "      vf_explained_var: 0.997965395450592\n",
      "      vf_loss: 0.9119546413421631\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3783661723136902\n",
      "      kl: 0.01901061460375786\n",
      "      policy_loss: 0.0025312125217169523\n",
      "      total_loss: 1.0365782976150513\n",
      "      vf_explained_var: 0.9989250302314758\n",
      "      vf_loss: 1.03404700756073\n",
      "    sample_time_ms: 20062.901\n",
      "    update_time_ms: 7.153\n",
      "  iterations_since_restore: 916\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.22757421479763\n",
      "    rl_1: 105.03014272782262\n",
      "  time_since_restore: 21547.180899381638\n",
      "  time_this_iter_s: 23.194925785064697\n",
      "  time_total_s: 21547.180899381638\n",
      "  timestamp: 1550814992\n",
      "  timesteps_since_restore: 9160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9160000\n",
      "  training_iteration: 916\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21547 s, 916 iter, 9160000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-56-55\n",
      "  done: false\n",
      "  episode_len_mean: 90.52252252252252\n",
      "  episode_reward_max: 217.28630515598232\n",
      "  episode_reward_mean: 173.06562091206501\n",
      "  episode_reward_min: 140.92989966817478\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 97592\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.659\n",
      "    load_time_ms: 2.164\n",
      "    num_steps_sampled: 9170000\n",
      "    num_steps_trained: 9170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6812657117843628\n",
      "      kl: 0.04795641824603081\n",
      "      policy_loss: 0.00535013061016798\n",
      "      total_loss: 0.859070897102356\n",
      "      vf_explained_var: 0.997992992401123\n",
      "      vf_loss: 0.8537207245826721\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.38995444774627686\n",
      "      kl: 0.015353400260210037\n",
      "      policy_loss: 0.0030343206599354744\n",
      "      total_loss: 1.0354816913604736\n",
      "      vf_explained_var: 0.9989362955093384\n",
      "      vf_loss: 1.0324474573135376\n",
      "    sample_time_ms: 20060.484\n",
      "    update_time_ms: 7.436\n",
      "  iterations_since_restore: 917\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.13726133825986\n",
      "    rl_1: 106.92835957380511\n",
      "  time_since_restore: 21570.259169101715\n",
      "  time_this_iter_s: 23.078269720077515\n",
      "  time_total_s: 21570.259169101715\n",
      "  timestamp: 1550815015\n",
      "  timesteps_since_restore: 9170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9170000\n",
      "  training_iteration: 917\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21570 s, 917 iter, 9170000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 89.54954954954955\n",
      "  episode_reward_max: 211.4108630985249\n",
      "  episode_reward_mean: 168.95474289662653\n",
      "  episode_reward_min: -152.03643673296557\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 97703\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.673\n",
      "    load_time_ms: 2.167\n",
      "    num_steps_sampled: 9180000\n",
      "    num_steps_trained: 9180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6944844126701355\n",
      "      kl: 0.02157285064458847\n",
      "      policy_loss: -0.0016450384864583611\n",
      "      total_loss: 24.08517074584961\n",
      "      vf_explained_var: 0.9561207890510559\n",
      "      vf_loss: 24.08681869506836\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3609384000301361\n",
      "      kl: 0.017155170440673828\n",
      "      policy_loss: -0.00471659516915679\n",
      "      total_loss: 40.95397186279297\n",
      "      vf_explained_var: 0.9605669975280762\n",
      "      vf_loss: 40.95868682861328\n",
      "    sample_time_ms: 19961.31\n",
      "    update_time_ms: 7.852\n",
      "  iterations_since_restore: 918\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.32811246294119\n",
      "    rl_1: 102.62663043368529\n",
      "  time_since_restore: 21593.091107606888\n",
      "  time_this_iter_s: 22.83193850517273\n",
      "  time_total_s: 21593.091107606888\n",
      "  timestamp: 1550815038\n",
      "  timesteps_since_restore: 9180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9180000\n",
      "  training_iteration: 918\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21593 s, 918 iter, 9180000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-57-41\n",
      "  done: false\n",
      "  episode_len_mean: 89.6875\n",
      "  episode_reward_max: 216.0872996737033\n",
      "  episode_reward_mean: 171.40342280289374\n",
      "  episode_reward_min: 139.06419927893387\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 97815\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.338\n",
      "    load_time_ms: 2.146\n",
      "    num_steps_sampled: 9190000\n",
      "    num_steps_trained: 9190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6762998104095459\n",
      "      kl: 0.039693981409072876\n",
      "      policy_loss: 0.0039007251616567373\n",
      "      total_loss: 1.10511314868927\n",
      "      vf_explained_var: 0.9974388480186462\n",
      "      vf_loss: 1.1012123823165894\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.2966841459274292\n",
      "      kl: 0.021903280168771744\n",
      "      policy_loss: 0.002582836663350463\n",
      "      total_loss: 1.1368159055709839\n",
      "      vf_explained_var: 0.9987356662750244\n",
      "      vf_loss: 1.1342328786849976\n",
      "    sample_time_ms: 19952.122\n",
      "    update_time_ms: 7.771\n",
      "  iterations_since_restore: 919\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.02389173717987\n",
      "    rl_1: 103.37953106571383\n",
      "  time_since_restore: 21616.380665779114\n",
      "  time_this_iter_s: 23.289558172225952\n",
      "  time_total_s: 21616.380665779114\n",
      "  timestamp: 1550815061\n",
      "  timesteps_since_restore: 9190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9190000\n",
      "  training_iteration: 919\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21616 s, 919 iter, 9190000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-58-05\n",
      "  done: false\n",
      "  episode_len_mean: 89.57142857142857\n",
      "  episode_reward_max: 215.85846778304185\n",
      "  episode_reward_mean: 176.4264684118609\n",
      "  episode_reward_min: 139.35202652854846\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 97927\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.538\n",
      "    load_time_ms: 2.165\n",
      "    num_steps_sampled: 9200000\n",
      "    num_steps_trained: 9200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7676118612289429\n",
      "      kl: 0.025806402787566185\n",
      "      policy_loss: 0.001885516569018364\n",
      "      total_loss: 0.947525680065155\n",
      "      vf_explained_var: 0.9980120658874512\n",
      "      vf_loss: 0.9456400871276855\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4135793447494507\n",
      "      kl: 0.027942094951868057\n",
      "      policy_loss: 0.004171400330960751\n",
      "      total_loss: 0.9583567380905151\n",
      "      vf_explained_var: 0.9990102052688599\n",
      "      vf_loss: 0.9541853070259094\n",
      "    sample_time_ms: 20056.82\n",
      "    update_time_ms: 7.48\n",
      "  iterations_since_restore: 920\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.92359591792643\n",
      "    rl_1: 105.50287249393445\n",
      "  time_since_restore: 21640.30978870392\n",
      "  time_this_iter_s: 23.929122924804688\n",
      "  time_total_s: 21640.30978870392\n",
      "  timestamp: 1550815085\n",
      "  timesteps_since_restore: 9200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9200000\n",
      "  training_iteration: 920\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21640 s, 920 iter, 9200000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-58-28\n",
      "  done: false\n",
      "  episode_len_mean: 89.76576576576576\n",
      "  episode_reward_max: 220.73864725031686\n",
      "  episode_reward_mean: 173.07994344581144\n",
      "  episode_reward_min: 139.540034355141\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 98038\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.193\n",
      "    load_time_ms: 2.17\n",
      "    num_steps_sampled: 9210000\n",
      "    num_steps_trained: 9210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7192675471305847\n",
      "      kl: 0.032205503433942795\n",
      "      policy_loss: 0.0032963664270937443\n",
      "      total_loss: 1.005362629890442\n",
      "      vf_explained_var: 0.9976935982704163\n",
      "      vf_loss: 1.0020662546157837\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3845786452293396\n",
      "      kl: 0.05520240589976311\n",
      "      policy_loss: 0.0141974538564682\n",
      "      total_loss: 0.9896122813224792\n",
      "      vf_explained_var: 0.9989596605300903\n",
      "      vf_loss: 0.9754148721694946\n",
      "    sample_time_ms: 20068.118\n",
      "    update_time_ms: 7.509\n",
      "  iterations_since_restore: 921\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.27032386653822\n",
      "    rl_1: 104.80961957927323\n",
      "  time_since_restore: 21663.48505806923\n",
      "  time_this_iter_s: 23.17526936531067\n",
      "  time_total_s: 21663.48505806923\n",
      "  timestamp: 1550815108\n",
      "  timesteps_since_restore: 9210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9210000\n",
      "  training_iteration: 921\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21663 s, 921 iter, 9210000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-58-52\n",
      "  done: false\n",
      "  episode_len_mean: 89.625\n",
      "  episode_reward_max: 219.38878598166116\n",
      "  episode_reward_mean: 170.37252407193006\n",
      "  episode_reward_min: 137.06586991820458\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 98150\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.171\n",
      "    load_time_ms: 2.146\n",
      "    num_steps_sampled: 9220000\n",
      "    num_steps_trained: 9220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.694892406463623\n",
      "      kl: 0.030795993283391\n",
      "      policy_loss: -0.0005006674327887595\n",
      "      total_loss: 1.0302503108978271\n",
      "      vf_explained_var: 0.9975601434707642\n",
      "      vf_loss: 1.03075110912323\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35817262530326843\n",
      "      kl: 0.014656458050012589\n",
      "      policy_loss: 0.00022337422706186771\n",
      "      total_loss: 0.8979905843734741\n",
      "      vf_explained_var: 0.9990072250366211\n",
      "      vf_loss: 0.897767186164856\n",
      "    sample_time_ms: 20091.464\n",
      "    update_time_ms: 7.548\n",
      "  iterations_since_restore: 922\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.68002157240936\n",
      "    rl_1: 102.69250249952067\n",
      "  time_since_restore: 21687.352181196213\n",
      "  time_this_iter_s: 23.867123126983643\n",
      "  time_total_s: 21687.352181196213\n",
      "  timestamp: 1550815132\n",
      "  timesteps_since_restore: 9220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9220000\n",
      "  training_iteration: 922\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21687 s, 922 iter, 9220000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-59-16\n",
      "  done: false\n",
      "  episode_len_mean: 89.58035714285714\n",
      "  episode_reward_max: 217.35413154421929\n",
      "  episode_reward_mean: 175.040537458149\n",
      "  episode_reward_min: 138.79828619604214\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 98262\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3117.7\n",
      "    load_time_ms: 2.213\n",
      "    num_steps_sampled: 9230000\n",
      "    num_steps_trained: 9230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7375108003616333\n",
      "      kl: 0.03030570037662983\n",
      "      policy_loss: 0.003586461069062352\n",
      "      total_loss: 0.9815009832382202\n",
      "      vf_explained_var: 0.9979051351547241\n",
      "      vf_loss: 0.9779146909713745\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3261083960533142\n",
      "      kl: 0.0137917660176754\n",
      "      policy_loss: 0.0005232137627899647\n",
      "      total_loss: 0.9857147336006165\n",
      "      vf_explained_var: 0.9989588856697083\n",
      "      vf_loss: 0.9851914644241333\n",
      "    sample_time_ms: 20172.427\n",
      "    update_time_ms: 7.922\n",
      "  iterations_since_restore: 923\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.58323744682664\n",
      "    rl_1: 104.45730001132235\n",
      "  time_since_restore: 21710.755321502686\n",
      "  time_this_iter_s: 23.40314030647278\n",
      "  time_total_s: 21710.755321502686\n",
      "  timestamp: 1550815156\n",
      "  timesteps_since_restore: 9230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9230000\n",
      "  training_iteration: 923\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21710 s, 923 iter, 9230000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_06-59-39\n",
      "  done: false\n",
      "  episode_len_mean: 90.26126126126127\n",
      "  episode_reward_max: 213.39982299846812\n",
      "  episode_reward_mean: 171.34668131201695\n",
      "  episode_reward_min: 133.83869860184166\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 98373\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.675\n",
      "    load_time_ms: 2.34\n",
      "    num_steps_sampled: 9240000\n",
      "    num_steps_trained: 9240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.704247236251831\n",
      "      kl: 0.23922981321811676\n",
      "      policy_loss: 0.010530650615692139\n",
      "      total_loss: 0.9988664388656616\n",
      "      vf_explained_var: 0.9977133274078369\n",
      "      vf_loss: 0.9883358478546143\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3425431549549103\n",
      "      kl: 0.022815672680735588\n",
      "      policy_loss: 0.004137486219406128\n",
      "      total_loss: 1.1401073932647705\n",
      "      vf_explained_var: 0.9987903237342834\n",
      "      vf_loss: 1.1359697580337524\n",
      "    sample_time_ms: 20222.875\n",
      "    update_time_ms: 7.803\n",
      "  iterations_since_restore: 924\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.66320227538421\n",
      "    rl_1: 104.68347903663273\n",
      "  time_since_restore: 21734.24062514305\n",
      "  time_this_iter_s: 23.4853036403656\n",
      "  time_total_s: 21734.24062514305\n",
      "  timestamp: 1550815179\n",
      "  timesteps_since_restore: 9240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9240000\n",
      "  training_iteration: 924\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21734 s, 924 iter, 9240000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-00-03\n",
      "  done: false\n",
      "  episode_len_mean: 89.63963963963964\n",
      "  episode_reward_max: 220.19769830283377\n",
      "  episode_reward_mean: 171.35469505215593\n",
      "  episode_reward_min: 133.24585616963427\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 98484\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3123.676\n",
      "    load_time_ms: 2.337\n",
      "    num_steps_sampled: 9250000\n",
      "    num_steps_trained: 9250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7297329902648926\n",
      "      kl: 0.045421864837408066\n",
      "      policy_loss: 0.0036605708301067352\n",
      "      total_loss: 0.912976086139679\n",
      "      vf_explained_var: 0.9979428052902222\n",
      "      vf_loss: 0.9093154668807983\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.36604955792427063\n",
      "      kl: 0.01705976389348507\n",
      "      policy_loss: 0.004446034785360098\n",
      "      total_loss: 1.0391792058944702\n",
      "      vf_explained_var: 0.9988815188407898\n",
      "      vf_loss: 1.0347331762313843\n",
      "    sample_time_ms: 20213.438\n",
      "    update_time_ms: 7.525\n",
      "  iterations_since_restore: 925\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.85398739673569\n",
      "    rl_1: 103.50070765542026\n",
      "  time_since_restore: 21757.639048576355\n",
      "  time_this_iter_s: 23.398423433303833\n",
      "  time_total_s: 21757.639048576355\n",
      "  timestamp: 1550815203\n",
      "  timesteps_since_restore: 9250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9250000\n",
      "  training_iteration: 925\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21757 s, 925 iter, 9250000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-00-26\n",
      "  done: false\n",
      "  episode_len_mean: 89.87387387387388\n",
      "  episode_reward_max: 212.1303605608813\n",
      "  episode_reward_mean: 173.02343422114114\n",
      "  episode_reward_min: 140.28877993782925\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 98595\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3124.375\n",
      "    load_time_ms: 2.377\n",
      "    num_steps_sampled: 9260000\n",
      "    num_steps_trained: 9260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.708613395690918\n",
      "      kl: 0.026169519871473312\n",
      "      policy_loss: -0.0011286012595519423\n",
      "      total_loss: 1.0057202577590942\n",
      "      vf_explained_var: 0.9976821541786194\n",
      "      vf_loss: 1.0068488121032715\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3690035045146942\n",
      "      kl: 0.01710130088031292\n",
      "      policy_loss: 0.0027694590389728546\n",
      "      total_loss: 1.288335919380188\n",
      "      vf_explained_var: 0.9986578226089478\n",
      "      vf_loss: 1.2855663299560547\n",
      "    sample_time_ms: 20187.534\n",
      "    update_time_ms: 7.957\n",
      "  iterations_since_restore: 926\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.36232339317728\n",
      "    rl_1: 105.66111082796388\n",
      "  time_since_restore: 21780.586963891983\n",
      "  time_this_iter_s: 22.94791531562805\n",
      "  time_total_s: 21780.586963891983\n",
      "  timestamp: 1550815226\n",
      "  timesteps_since_restore: 9260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9260000\n",
      "  training_iteration: 926\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21780 s, 926 iter, 9260000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-00-50\n",
      "  done: false\n",
      "  episode_len_mean: 90.31531531531532\n",
      "  episode_reward_max: 219.390463318367\n",
      "  episode_reward_mean: 173.39390241791233\n",
      "  episode_reward_min: 137.39139979019325\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 98706\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.795\n",
      "    load_time_ms: 2.421\n",
      "    num_steps_sampled: 9270000\n",
      "    num_steps_trained: 9270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7231642603874207\n",
      "      kl: 0.031153446063399315\n",
      "      policy_loss: 0.005832834169268608\n",
      "      total_loss: 1.0460681915283203\n",
      "      vf_explained_var: 0.997724175453186\n",
      "      vf_loss: 1.0402355194091797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35700997710227966\n",
      "      kl: 0.017162684351205826\n",
      "      policy_loss: 0.0027117086574435234\n",
      "      total_loss: 1.1105806827545166\n",
      "      vf_explained_var: 0.9988402128219604\n",
      "      vf_loss: 1.107869029045105\n",
      "    sample_time_ms: 20265.337\n",
      "    update_time_ms: 7.874\n",
      "  iterations_since_restore: 927\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.50611477480163\n",
      "    rl_1: 104.88778764311066\n",
      "  time_since_restore: 21804.627865314484\n",
      "  time_this_iter_s: 24.04090142250061\n",
      "  time_total_s: 21804.627865314484\n",
      "  timestamp: 1550815250\n",
      "  timesteps_since_restore: 9270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9270000\n",
      "  training_iteration: 927\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21804 s, 927 iter, 9270000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-01-13\n",
      "  done: false\n",
      "  episode_len_mean: 89.31531531531532\n",
      "  episode_reward_max: 217.2629203983633\n",
      "  episode_reward_mean: 175.23454136510173\n",
      "  episode_reward_min: 140.32705869607588\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 98817\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.687\n",
      "    load_time_ms: 2.4\n",
      "    num_steps_sampled: 9280000\n",
      "    num_steps_trained: 9280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7752413749694824\n",
      "      kl: 0.07534237205982208\n",
      "      policy_loss: 0.013286464847624302\n",
      "      total_loss: 0.9212446808815002\n",
      "      vf_explained_var: 0.9979809522628784\n",
      "      vf_loss: 0.9079583287239075\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4182400405406952\n",
      "      kl: 0.009545840322971344\n",
      "      policy_loss: 0.0040066782385110855\n",
      "      total_loss: 1.0358117818832397\n",
      "      vf_explained_var: 0.9989163279533386\n",
      "      vf_loss: 1.0318050384521484\n",
      "    sample_time_ms: 20256.053\n",
      "    update_time_ms: 7.86\n",
      "  iterations_since_restore: 928\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.69792278900046\n",
      "    rl_1: 105.53661857610129\n",
      "  time_since_restore: 21827.363164186478\n",
      "  time_this_iter_s: 22.73529887199402\n",
      "  time_total_s: 21827.363164186478\n",
      "  timestamp: 1550815273\n",
      "  timesteps_since_restore: 9280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9280000\n",
      "  training_iteration: 928\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21827 s, 928 iter, 9280000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-01-36\n",
      "  done: false\n",
      "  episode_len_mean: 89.72321428571429\n",
      "  episode_reward_max: 216.82780113673675\n",
      "  episode_reward_mean: 174.3025279874964\n",
      "  episode_reward_min: 134.7666630791757\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 98929\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.077\n",
      "    load_time_ms: 2.457\n",
      "    num_steps_sampled: 9290000\n",
      "    num_steps_trained: 9290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7366255521774292\n",
      "      kl: 0.028679680079221725\n",
      "      policy_loss: 0.0008120748680084944\n",
      "      total_loss: 0.9219182133674622\n",
      "      vf_explained_var: 0.9979807734489441\n",
      "      vf_loss: 0.9211061000823975\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3396770656108856\n",
      "      kl: 0.017397107556462288\n",
      "      policy_loss: 0.004880239255726337\n",
      "      total_loss: 0.964579164981842\n",
      "      vf_explained_var: 0.9989942908287048\n",
      "      vf_loss: 0.959699273109436\n",
      "    sample_time_ms: 20249.013\n",
      "    update_time_ms: 7.894\n",
      "  iterations_since_restore: 929\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.46637117564725\n",
      "    rl_1: 104.83615681184912\n",
      "  time_since_restore: 21850.57845902443\n",
      "  time_this_iter_s: 23.21529483795166\n",
      "  time_total_s: 21850.57845902443\n",
      "  timestamp: 1550815296\n",
      "  timesteps_since_restore: 9290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9290000\n",
      "  training_iteration: 929\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21850 s, 929 iter, 9290000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-01-59\n",
      "  done: false\n",
      "  episode_len_mean: 89.70535714285714\n",
      "  episode_reward_max: 221.4722178642461\n",
      "  episode_reward_mean: 170.07980674043102\n",
      "  episode_reward_min: -149.79264420856447\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 99041\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.688\n",
      "    load_time_ms: 2.432\n",
      "    num_steps_sampled: 9300000\n",
      "    num_steps_trained: 9300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6833673715591431\n",
      "      kl: 0.0452534519135952\n",
      "      policy_loss: -0.011126845143735409\n",
      "      total_loss: 23.784109115600586\n",
      "      vf_explained_var: 0.9562843441963196\n",
      "      vf_loss: 23.79523468017578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3120804727077484\n",
      "      kl: 0.009996024891734123\n",
      "      policy_loss: -0.0022837077267467976\n",
      "      total_loss: 40.90913772583008\n",
      "      vf_explained_var: 0.9619957208633423\n",
      "      vf_loss: 40.91142272949219\n",
      "    sample_time_ms: 20190.822\n",
      "    update_time_ms: 7.945\n",
      "  iterations_since_restore: 930\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.38911078558957\n",
      "    rl_1: 103.69069595484146\n",
      "  time_since_restore: 21873.90352010727\n",
      "  time_this_iter_s: 23.325061082839966\n",
      "  time_total_s: 21873.90352010727\n",
      "  timestamp: 1550815319\n",
      "  timesteps_since_restore: 9300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9300000\n",
      "  training_iteration: 930\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21873 s, 930 iter, 9300000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-02-23\n",
      "  done: false\n",
      "  episode_len_mean: 89.8108108108108\n",
      "  episode_reward_max: 219.11571503161383\n",
      "  episode_reward_mean: 172.8058559341839\n",
      "  episode_reward_min: 137.5061833934797\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 99152\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.823\n",
      "    load_time_ms: 2.462\n",
      "    num_steps_sampled: 9310000\n",
      "    num_steps_trained: 9310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7404268980026245\n",
      "      kl: 0.03594132885336876\n",
      "      policy_loss: 0.00739080086350441\n",
      "      total_loss: 0.9451594948768616\n",
      "      vf_explained_var: 0.9978500008583069\n",
      "      vf_loss: 0.9377686977386475\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.42329269647598267\n",
      "      kl: 0.02697335183620453\n",
      "      policy_loss: 0.0036735564935952425\n",
      "      total_loss: 1.1925832033157349\n",
      "      vf_explained_var: 0.9987707734107971\n",
      "      vf_loss: 1.1889095306396484\n",
      "    sample_time_ms: 20202.829\n",
      "    update_time_ms: 7.931\n",
      "  iterations_since_restore: 931\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.32215670664348\n",
      "    rl_1: 105.48369922754044\n",
      "  time_since_restore: 21897.179198265076\n",
      "  time_this_iter_s: 23.275678157806396\n",
      "  time_total_s: 21897.179198265076\n",
      "  timestamp: 1550815343\n",
      "  timesteps_since_restore: 9310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9310000\n",
      "  training_iteration: 931\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21897 s, 931 iter, 9310000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-02-46\n",
      "  done: false\n",
      "  episode_len_mean: 89.58035714285714\n",
      "  episode_reward_max: 216.53212769603385\n",
      "  episode_reward_mean: 175.7236855519527\n",
      "  episode_reward_min: 139.81545074442613\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 99264\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.073\n",
      "    load_time_ms: 2.505\n",
      "    num_steps_sampled: 9320000\n",
      "    num_steps_trained: 9320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7688611149787903\n",
      "      kl: 0.03867287561297417\n",
      "      policy_loss: 0.002052914584055543\n",
      "      total_loss: 0.8955720067024231\n",
      "      vf_explained_var: 0.9980029463768005\n",
      "      vf_loss: 0.8935191035270691\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3914537727832794\n",
      "      kl: 0.010359888896346092\n",
      "      policy_loss: 0.0024333251640200615\n",
      "      total_loss: 1.0021406412124634\n",
      "      vf_explained_var: 0.9989662170410156\n",
      "      vf_loss: 0.9997072219848633\n",
      "    sample_time_ms: 20123.534\n",
      "    update_time_ms: 7.878\n",
      "  iterations_since_restore: 932\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.39391232999243\n",
      "    rl_1: 106.3297732219603\n",
      "  time_since_restore: 21920.202916622162\n",
      "  time_this_iter_s: 23.02371835708618\n",
      "  time_total_s: 21920.202916622162\n",
      "  timestamp: 1550815366\n",
      "  timesteps_since_restore: 9320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9320000\n",
      "  training_iteration: 932\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21920 s, 932 iter, 9320000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-03-09\n",
      "  done: false\n",
      "  episode_len_mean: 90.02702702702703\n",
      "  episode_reward_max: 220.40923210389113\n",
      "  episode_reward_mean: 173.54452612118743\n",
      "  episode_reward_min: 137.3633302138692\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 99375\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.735\n",
      "    load_time_ms: 2.42\n",
      "    num_steps_sampled: 9330000\n",
      "    num_steps_trained: 9330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7541185021400452\n",
      "      kl: 0.040693338960409164\n",
      "      policy_loss: 0.007328405510634184\n",
      "      total_loss: 0.9234032034873962\n",
      "      vf_explained_var: 0.9979379177093506\n",
      "      vf_loss: 0.916074812412262\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.38138842582702637\n",
      "      kl: 0.022491585463285446\n",
      "      policy_loss: -0.0002164706529583782\n",
      "      total_loss: 0.8903674483299255\n",
      "      vf_explained_var: 0.999038815498352\n",
      "      vf_loss: 0.8905838131904602\n",
      "    sample_time_ms: 20111.108\n",
      "    update_time_ms: 7.545\n",
      "  iterations_since_restore: 933\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.37521010397833\n",
      "    rl_1: 104.16931601720908\n",
      "  time_since_restore: 21943.481907606125\n",
      "  time_this_iter_s: 23.278990983963013\n",
      "  time_total_s: 21943.481907606125\n",
      "  timestamp: 1550815389\n",
      "  timesteps_since_restore: 9330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9330000\n",
      "  training_iteration: 933\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21943 s, 933 iter, 9330000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-03-32\n",
      "  done: false\n",
      "  episode_len_mean: 90.10810810810811\n",
      "  episode_reward_max: 217.1110971051033\n",
      "  episode_reward_mean: 172.1593886547614\n",
      "  episode_reward_min: 139.57732004157384\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 99486\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.115\n",
      "    load_time_ms: 2.323\n",
      "    num_steps_sampled: 9340000\n",
      "    num_steps_trained: 9340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7181649804115295\n",
      "      kl: 0.05713680386543274\n",
      "      policy_loss: 0.003941053990274668\n",
      "      total_loss: 1.0875602960586548\n",
      "      vf_explained_var: 0.9975036382675171\n",
      "      vf_loss: 1.0836193561553955\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4127295911312103\n",
      "      kl: 0.03122086077928543\n",
      "      policy_loss: 0.008777124807238579\n",
      "      total_loss: 1.052667260169983\n",
      "      vf_explained_var: 0.9988970756530762\n",
      "      vf_loss: 1.043890118598938\n",
      "    sample_time_ms: 20035.756\n",
      "    update_time_ms: 7.545\n",
      "  iterations_since_restore: 934\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.15872524531561\n",
      "    rl_1: 105.00066340944585\n",
      "  time_since_restore: 21966.203239440918\n",
      "  time_this_iter_s: 22.72133183479309\n",
      "  time_total_s: 21966.203239440918\n",
      "  timestamp: 1550815412\n",
      "  timesteps_since_restore: 9340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9340000\n",
      "  training_iteration: 934\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21966 s, 934 iter, 9340000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-03-55\n",
      "  done: false\n",
      "  episode_len_mean: 89.41071428571429\n",
      "  episode_reward_max: 220.22151181519655\n",
      "  episode_reward_mean: 171.6850313364642\n",
      "  episode_reward_min: 138.88575963396357\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 99598\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.388\n",
      "    load_time_ms: 2.358\n",
      "    num_steps_sampled: 9350000\n",
      "    num_steps_trained: 9350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7092666029930115\n",
      "      kl: 0.03858180716633797\n",
      "      policy_loss: 0.004062363412231207\n",
      "      total_loss: 1.016770839691162\n",
      "      vf_explained_var: 0.9976398944854736\n",
      "      vf_loss: 1.0127085447311401\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.41697385907173157\n",
      "      kl: 0.019761549308896065\n",
      "      policy_loss: 0.004036463797092438\n",
      "      total_loss: 1.2293193340301514\n",
      "      vf_explained_var: 0.9987046718597412\n",
      "      vf_loss: 1.2252827882766724\n",
      "    sample_time_ms: 19993.454\n",
      "    update_time_ms: 7.598\n",
      "  iterations_since_restore: 935\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.266752695466\n",
      "    rl_1: 104.41827864099818\n",
      "  time_since_restore: 21989.17918252945\n",
      "  time_this_iter_s: 22.975943088531494\n",
      "  time_total_s: 21989.17918252945\n",
      "  timestamp: 1550815435\n",
      "  timesteps_since_restore: 9350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9350000\n",
      "  training_iteration: 935\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 21989 s, 935 iter, 9350000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-04-17\n",
      "  done: false\n",
      "  episode_len_mean: 89.16964285714286\n",
      "  episode_reward_max: 210.46602962934983\n",
      "  episode_reward_mean: 171.27814505264064\n",
      "  episode_reward_min: 137.8917803255783\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 99710\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.931\n",
      "    load_time_ms: 2.33\n",
      "    num_steps_sampled: 9360000\n",
      "    num_steps_trained: 9360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7304345369338989\n",
      "      kl: 0.05763956904411316\n",
      "      policy_loss: 0.010268616490066051\n",
      "      total_loss: 0.9770364165306091\n",
      "      vf_explained_var: 0.9977102875709534\n",
      "      vf_loss: 0.9667678475379944\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.43147268891334534\n",
      "      kl: 0.014462275430560112\n",
      "      policy_loss: 0.004376672208309174\n",
      "      total_loss: 1.053508996963501\n",
      "      vf_explained_var: 0.9988546371459961\n",
      "      vf_loss: 1.0491323471069336\n",
      "    sample_time_ms: 19962.928\n",
      "    update_time_ms: 7.112\n",
      "  iterations_since_restore: 936\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.22528804249487\n",
      "    rl_1: 103.05285701014577\n",
      "  time_since_restore: 22011.811736106873\n",
      "  time_this_iter_s: 22.632553577423096\n",
      "  time_total_s: 22011.811736106873\n",
      "  timestamp: 1550815457\n",
      "  timesteps_since_restore: 9360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9360000\n",
      "  training_iteration: 936\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22011 s, 936 iter, 9360000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-04-41\n",
      "  done: false\n",
      "  episode_len_mean: 90.38738738738739\n",
      "  episode_reward_max: 212.07920481918717\n",
      "  episode_reward_mean: 172.2369549696596\n",
      "  episode_reward_min: 140.98569912862143\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 99821\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.156\n",
      "    load_time_ms: 2.262\n",
      "    num_steps_sampled: 9370000\n",
      "    num_steps_trained: 9370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7121992111206055\n",
      "      kl: 0.03528282046318054\n",
      "      policy_loss: 0.0019099205965176225\n",
      "      total_loss: 0.9812614321708679\n",
      "      vf_explained_var: 0.997609555721283\n",
      "      vf_loss: 0.9793513417243958\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4810256361961365\n",
      "      kl: 0.012706897221505642\n",
      "      policy_loss: 0.004749820567667484\n",
      "      total_loss: 1.0804061889648438\n",
      "      vf_explained_var: 0.9988952875137329\n",
      "      vf_loss: 1.0756564140319824\n",
      "    sample_time_ms: 19883.501\n",
      "    update_time_ms: 6.736\n",
      "  iterations_since_restore: 937\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.45157991583862\n",
      "    rl_1: 105.78537505382103\n",
      "  time_since_restore: 22034.995455026627\n",
      "  time_this_iter_s: 23.18371891975403\n",
      "  time_total_s: 22034.995455026627\n",
      "  timestamp: 1550815481\n",
      "  timesteps_since_restore: 9370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9370000\n",
      "  training_iteration: 937\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22034 s, 937 iter, 9370000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-05-04\n",
      "  done: false\n",
      "  episode_len_mean: 89.1891891891892\n",
      "  episode_reward_max: 219.7888312455733\n",
      "  episode_reward_mean: 173.2313145047545\n",
      "  episode_reward_min: 139.1981318518351\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 99932\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.866\n",
      "    load_time_ms: 2.284\n",
      "    num_steps_sampled: 9380000\n",
      "    num_steps_trained: 9380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7435865998268127\n",
      "      kl: 0.16130559146404266\n",
      "      policy_loss: 0.017432155087590218\n",
      "      total_loss: 0.9072297215461731\n",
      "      vf_explained_var: 0.9979532957077026\n",
      "      vf_loss: 0.8897976279258728\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4406546652317047\n",
      "      kl: 0.013582536950707436\n",
      "      policy_loss: 0.0020769357215613127\n",
      "      total_loss: 0.8764933943748474\n",
      "      vf_explained_var: 0.9990618824958801\n",
      "      vf_loss: 0.8744164705276489\n",
      "    sample_time_ms: 19940.789\n",
      "    update_time_ms: 6.463\n",
      "  iterations_since_restore: 938\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.56609166084735\n",
      "    rl_1: 103.66522284390723\n",
      "  time_since_restore: 22058.31828069687\n",
      "  time_this_iter_s: 23.32282567024231\n",
      "  time_total_s: 22058.31828069687\n",
      "  timestamp: 1550815504\n",
      "  timesteps_since_restore: 9380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9380000\n",
      "  training_iteration: 938\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22058 s, 938 iter, 9380000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-05-27\n",
      "  done: false\n",
      "  episode_len_mean: 90.14285714285714\n",
      "  episode_reward_max: 214.88712111666175\n",
      "  episode_reward_mean: 172.9580642908145\n",
      "  episode_reward_min: 136.63620261457783\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 100044\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.327\n",
      "    load_time_ms: 2.218\n",
      "    num_steps_sampled: 9390000\n",
      "    num_steps_trained: 9390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7255195379257202\n",
      "      kl: 0.02858741208910942\n",
      "      policy_loss: -0.0017275023274123669\n",
      "      total_loss: 1.0042775869369507\n",
      "      vf_explained_var: 0.99776691198349\n",
      "      vf_loss: 1.006005048751831\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.47895348072052\n",
      "      kl: 0.025147633627057076\n",
      "      policy_loss: 0.004158931318670511\n",
      "      total_loss: 1.0133261680603027\n",
      "      vf_explained_var: 0.9989390969276428\n",
      "      vf_loss: 1.009167194366455\n",
      "    sample_time_ms: 19954.573\n",
      "    update_time_ms: 6.468\n",
      "  iterations_since_restore: 939\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.8607725082533\n",
      "    rl_1: 105.09729178256119\n",
      "  time_since_restore: 22081.684176921844\n",
      "  time_this_iter_s: 23.365896224975586\n",
      "  time_total_s: 22081.684176921844\n",
      "  timestamp: 1550815527\n",
      "  timesteps_since_restore: 9390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9390000\n",
      "  training_iteration: 939\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22081 s, 939 iter, 9390000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-05-51\n",
      "  done: false\n",
      "  episode_len_mean: 90.09009009009009\n",
      "  episode_reward_max: 216.15560923822133\n",
      "  episode_reward_mean: 172.72405972091738\n",
      "  episode_reward_min: 139.4447079029593\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 100155\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.861\n",
      "    load_time_ms: 2.256\n",
      "    num_steps_sampled: 9400000\n",
      "    num_steps_trained: 9400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6847039461135864\n",
      "      kl: 0.028594855219125748\n",
      "      policy_loss: 0.001390133867971599\n",
      "      total_loss: 0.8986203074455261\n",
      "      vf_explained_var: 0.9979143142700195\n",
      "      vf_loss: 0.8972300887107849\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3849029242992401\n",
      "      kl: 0.0197751484811306\n",
      "      policy_loss: -0.0005378426285460591\n",
      "      total_loss: 1.134095549583435\n",
      "      vf_explained_var: 0.9988180994987488\n",
      "      vf_loss: 1.1346334218978882\n",
      "    sample_time_ms: 19936.043\n",
      "    update_time_ms: 6.448\n",
      "  iterations_since_restore: 940\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.71043777898127\n",
      "    rl_1: 105.01362194193612\n",
      "  time_since_restore: 22104.99878358841\n",
      "  time_this_iter_s: 23.31460666656494\n",
      "  time_total_s: 22104.99878358841\n",
      "  timestamp: 1550815551\n",
      "  timesteps_since_restore: 9400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9400000\n",
      "  training_iteration: 940\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22104 s, 940 iter, 9400000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-06-14\n",
      "  done: false\n",
      "  episode_len_mean: 88.27433628318585\n",
      "  episode_reward_max: 221.4364840761427\n",
      "  episode_reward_mean: 170.4655205072512\n",
      "  episode_reward_min: -153.69868738381473\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 100268\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3149.35\n",
      "    load_time_ms: 2.316\n",
      "    num_steps_sampled: 9410000\n",
      "    num_steps_trained: 9410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7190430760383606\n",
      "      kl: 0.0347716361284256\n",
      "      policy_loss: -0.0050051514990627766\n",
      "      total_loss: 24.79616355895996\n",
      "      vf_explained_var: 0.9588974118232727\n",
      "      vf_loss: 24.80116844177246\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3782391846179962\n",
      "      kl: 0.023797277361154556\n",
      "      policy_loss: -0.000975400791503489\n",
      "      total_loss: 40.579833984375\n",
      "      vf_explained_var: 0.9610849022865295\n",
      "      vf_loss: 40.58080291748047\n",
      "    sample_time_ms: 19929.973\n",
      "    update_time_ms: 6.422\n",
      "  iterations_since_restore: 941\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.24254617114032\n",
      "    rl_1: 101.22297433611088\n",
      "  time_since_restore: 22128.241787672043\n",
      "  time_this_iter_s: 23.243004083633423\n",
      "  time_total_s: 22128.241787672043\n",
      "  timestamp: 1550815574\n",
      "  timesteps_since_restore: 9410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9410000\n",
      "  training_iteration: 941\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22128 s, 941 iter, 9410000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-06-37\n",
      "  done: false\n",
      "  episode_len_mean: 90.71559633027523\n",
      "  episode_reward_max: 213.1380826271631\n",
      "  episode_reward_mean: 173.7474535455017\n",
      "  episode_reward_min: 137.33508470869586\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 100377\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.443\n",
      "    load_time_ms: 2.279\n",
      "    num_steps_sampled: 9420000\n",
      "    num_steps_trained: 9420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6572308540344238\n",
      "      kl: 0.035066865384578705\n",
      "      policy_loss: -0.0007565054693259299\n",
      "      total_loss: 1.1252374649047852\n",
      "      vf_explained_var: 0.9973832964897156\n",
      "      vf_loss: 1.1259942054748535\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3327935039997101\n",
      "      kl: 0.01962628960609436\n",
      "      policy_loss: 0.002882407745346427\n",
      "      total_loss: 1.1153624057769775\n",
      "      vf_explained_var: 0.9988224506378174\n",
      "      vf_loss: 1.1124799251556396\n",
      "    sample_time_ms: 19911.614\n",
      "    update_time_ms: 6.9\n",
      "  iterations_since_restore: 942\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.08703109716778\n",
      "    rl_1: 105.66042244833392\n",
      "  time_since_restore: 22151.071190595627\n",
      "  time_this_iter_s: 22.829402923583984\n",
      "  time_total_s: 22151.071190595627\n",
      "  timestamp: 1550815597\n",
      "  timesteps_since_restore: 9420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9420000\n",
      "  training_iteration: 942\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22151 s, 942 iter, 9420000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-07-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.22321428571429\n",
      "  episode_reward_max: 217.54673778173557\n",
      "  episode_reward_mean: 173.1733986664076\n",
      "  episode_reward_min: 139.4420746462494\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 100489\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3150.607\n",
      "    load_time_ms: 2.31\n",
      "    num_steps_sampled: 9430000\n",
      "    num_steps_trained: 9430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6796835660934448\n",
      "      kl: 0.038215670734643936\n",
      "      policy_loss: 0.005122716072946787\n",
      "      total_loss: 0.9256596565246582\n",
      "      vf_explained_var: 0.9979526996612549\n",
      "      vf_loss: 0.920536994934082\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.37499552965164185\n",
      "      kl: 0.034288190305233\n",
      "      policy_loss: 0.009140538983047009\n",
      "      total_loss: 0.9995151162147522\n",
      "      vf_explained_var: 0.9989538788795471\n",
      "      vf_loss: 0.9903746247291565\n",
      "    sample_time_ms: 19963.42\n",
      "    update_time_ms: 7.072\n",
      "  iterations_since_restore: 943\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.1439756575085\n",
      "    rl_1: 105.02942300889906\n",
      "  time_since_restore: 22174.896122694016\n",
      "  time_this_iter_s: 23.824932098388672\n",
      "  time_total_s: 22174.896122694016\n",
      "  timestamp: 1550815621\n",
      "  timesteps_since_restore: 9430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9430000\n",
      "  training_iteration: 943\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22174 s, 943 iter, 9430000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-07-24\n",
      "  done: false\n",
      "  episode_len_mean: 89.65765765765765\n",
      "  episode_reward_max: 217.88798382186067\n",
      "  episode_reward_mean: 174.4236335339648\n",
      "  episode_reward_min: 134.97723521906724\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 100600\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3146.937\n",
      "    load_time_ms: 2.345\n",
      "    num_steps_sampled: 9440000\n",
      "    num_steps_trained: 9440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7052463889122009\n",
      "      kl: 0.022334598004817963\n",
      "      policy_loss: -0.0011389334686100483\n",
      "      total_loss: 1.0253592729568481\n",
      "      vf_explained_var: 0.9976813197135925\n",
      "      vf_loss: 1.0264981985092163\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.43683019280433655\n",
      "      kl: 0.01598239317536354\n",
      "      policy_loss: 0.0030716771725565195\n",
      "      total_loss: 1.0723994970321655\n",
      "      vf_explained_var: 0.9988788962364197\n",
      "      vf_loss: 1.0693278312683105\n",
      "    sample_time_ms: 19988.392\n",
      "    update_time_ms: 7.082\n",
      "  iterations_since_restore: 944\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.79203600510404\n",
      "    rl_1: 105.6315975288607\n",
      "  time_since_restore: 22197.83294224739\n",
      "  time_this_iter_s: 22.936819553375244\n",
      "  time_total_s: 22197.83294224739\n",
      "  timestamp: 1550815644\n",
      "  timesteps_since_restore: 9440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9440000\n",
      "  training_iteration: 944\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22197 s, 944 iter, 9440000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-07-47\n",
      "  done: false\n",
      "  episode_len_mean: 90.33333333333333\n",
      "  episode_reward_max: 219.02046223332752\n",
      "  episode_reward_mean: 173.8791540925772\n",
      "  episode_reward_min: 138.76292444185296\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 100711\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.808\n",
      "    load_time_ms: 2.32\n",
      "    num_steps_sampled: 9450000\n",
      "    num_steps_trained: 9450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6724333167076111\n",
      "      kl: 0.08338326960802078\n",
      "      policy_loss: 0.01107343751937151\n",
      "      total_loss: 0.980370819568634\n",
      "      vf_explained_var: 0.9978482127189636\n",
      "      vf_loss: 0.9692973494529724\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3743390440940857\n",
      "      kl: 0.020044397562742233\n",
      "      policy_loss: -0.002361154882237315\n",
      "      total_loss: 0.9612188935279846\n",
      "      vf_explained_var: 0.9989880323410034\n",
      "      vf_loss: 0.9635801315307617\n",
      "    sample_time_ms: 19980.702\n",
      "    update_time_ms: 7.146\n",
      "  iterations_since_restore: 945\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.72648259796631\n",
      "    rl_1: 105.15267149461086\n",
      "  time_since_restore: 22220.744881868362\n",
      "  time_this_iter_s: 22.91193962097168\n",
      "  time_total_s: 22220.744881868362\n",
      "  timestamp: 1550815667\n",
      "  timesteps_since_restore: 9450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9450000\n",
      "  training_iteration: 945\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22220 s, 945 iter, 9450000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-08-10\n",
      "  done: false\n",
      "  episode_len_mean: 89.94594594594595\n",
      "  episode_reward_max: 221.3604823735312\n",
      "  episode_reward_mean: 172.95567363300736\n",
      "  episode_reward_min: 142.2825197774926\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 100822\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3147.586\n",
      "    load_time_ms: 2.347\n",
      "    num_steps_sampled: 9460000\n",
      "    num_steps_trained: 9460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6930143237113953\n",
      "      kl: 0.045621030032634735\n",
      "      policy_loss: 0.005199735052883625\n",
      "      total_loss: 0.9319038391113281\n",
      "      vf_explained_var: 0.9979252815246582\n",
      "      vf_loss: 0.9267042875289917\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.43426406383514404\n",
      "      kl: 0.029692264273762703\n",
      "      policy_loss: 0.006898961961269379\n",
      "      total_loss: 1.0372538566589355\n",
      "      vf_explained_var: 0.9989364743232727\n",
      "      vf_loss: 1.0303548574447632\n",
      "    sample_time_ms: 20021.074\n",
      "    update_time_ms: 7.286\n",
      "  iterations_since_restore: 946\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.45694174610594\n",
      "    rl_1: 105.49873188690144\n",
      "  time_since_restore: 22243.7814514637\n",
      "  time_this_iter_s: 23.036569595336914\n",
      "  time_total_s: 22243.7814514637\n",
      "  timestamp: 1550815690\n",
      "  timesteps_since_restore: 9460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9460000\n",
      "  training_iteration: 946\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22243 s, 946 iter, 9460000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-08-33\n",
      "  done: false\n",
      "  episode_len_mean: 89.35714285714286\n",
      "  episode_reward_max: 216.40980624941216\n",
      "  episode_reward_mean: 174.2392261669673\n",
      "  episode_reward_min: 141.19401607133074\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 100934\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.335\n",
      "    load_time_ms: 2.334\n",
      "    num_steps_sampled: 9470000\n",
      "    num_steps_trained: 9470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7422910332679749\n",
      "      kl: 0.03017091192305088\n",
      "      policy_loss: 0.0012716071214526892\n",
      "      total_loss: 0.9429057240486145\n",
      "      vf_explained_var: 0.9979957342147827\n",
      "      vf_loss: 0.9416338801383972\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4451560378074646\n",
      "      kl: 1.5305355787277222\n",
      "      policy_loss: 0.056206610053777695\n",
      "      total_loss: 0.9061207175254822\n",
      "      vf_explained_var: 0.9990795850753784\n",
      "      vf_loss: 0.8499141335487366\n",
      "    sample_time_ms: 20026.009\n",
      "    update_time_ms: 7.358\n",
      "  iterations_since_restore: 947\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.75025594050042\n",
      "    rl_1: 103.48897022646688\n",
      "  time_since_restore: 22266.913599729538\n",
      "  time_this_iter_s: 23.132148265838623\n",
      "  time_total_s: 22266.913599729538\n",
      "  timestamp: 1550815713\n",
      "  timesteps_since_restore: 9470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9470000\n",
      "  training_iteration: 947\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22266 s, 947 iter, 9470000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-08-56\n",
      "  done: false\n",
      "  episode_len_mean: 88.97321428571429\n",
      "  episode_reward_max: 218.04236055018467\n",
      "  episode_reward_mean: 170.6785120608037\n",
      "  episode_reward_min: 138.28195360883439\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 101046\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.979\n",
      "    load_time_ms: 2.348\n",
      "    num_steps_sampled: 9480000\n",
      "    num_steps_trained: 9480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6951254606246948\n",
      "      kl: 0.032953377813100815\n",
      "      policy_loss: 0.002547377021983266\n",
      "      total_loss: 1.0601561069488525\n",
      "      vf_explained_var: 0.9975889921188354\n",
      "      vf_loss: 1.057608962059021\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4809260666370392\n",
      "      kl: 0.026334410533308983\n",
      "      policy_loss: 0.006906303111463785\n",
      "      total_loss: 1.167823314666748\n",
      "      vf_explained_var: 0.9987425208091736\n",
      "      vf_loss: 1.1609171628952026\n",
      "    sample_time_ms: 20031.645\n",
      "    update_time_ms: 7.241\n",
      "  iterations_since_restore: 948\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.68293483641273\n",
      "    rl_1: 102.995577224391\n",
      "  time_since_restore: 22290.24988102913\n",
      "  time_this_iter_s: 23.336281299591064\n",
      "  time_total_s: 22290.24988102913\n",
      "  timestamp: 1550815736\n",
      "  timesteps_since_restore: 9480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9480000\n",
      "  training_iteration: 948\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22290 s, 948 iter, 9480000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-09-19\n",
      "  done: false\n",
      "  episode_len_mean: 89.30973451327434\n",
      "  episode_reward_max: 215.95541356780492\n",
      "  episode_reward_mean: 172.66480221464934\n",
      "  episode_reward_min: 136.86338010670275\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 101159\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.543\n",
      "    load_time_ms: 2.363\n",
      "    num_steps_sampled: 9490000\n",
      "    num_steps_trained: 9490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7189238667488098\n",
      "      kl: 0.11322063952684402\n",
      "      policy_loss: 0.01126489695161581\n",
      "      total_loss: 0.8903630375862122\n",
      "      vf_explained_var: 0.9979995489120483\n",
      "      vf_loss: 0.8790981769561768\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.430176705121994\n",
      "      kl: 0.023160701617598534\n",
      "      policy_loss: 0.010507189668715\n",
      "      total_loss: 0.9646459817886353\n",
      "      vf_explained_var: 0.9989941716194153\n",
      "      vf_loss: 0.9541389346122742\n",
      "    sample_time_ms: 19979.823\n",
      "    update_time_ms: 7.516\n",
      "  iterations_since_restore: 949\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.57737136150843\n",
      "    rl_1: 104.0874308531409\n",
      "  time_since_restore: 22313.22757935524\n",
      "  time_this_iter_s: 22.97769832611084\n",
      "  time_total_s: 22313.22757935524\n",
      "  timestamp: 1550815759\n",
      "  timesteps_since_restore: 9490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9490000\n",
      "  training_iteration: 949\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22313 s, 949 iter, 9490000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-09-43\n",
      "  done: false\n",
      "  episode_len_mean: 89.7909090909091\n",
      "  episode_reward_max: 216.97031583841078\n",
      "  episode_reward_mean: 171.62611971597363\n",
      "  episode_reward_min: -149.1599609532459\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 101269\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.256\n",
      "    load_time_ms: 2.375\n",
      "    num_steps_sampled: 9500000\n",
      "    num_steps_trained: 9500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7343094944953918\n",
      "      kl: 0.043330319225788116\n",
      "      policy_loss: -0.011356105096638203\n",
      "      total_loss: 22.12352180480957\n",
      "      vf_explained_var: 0.963266134262085\n",
      "      vf_loss: 22.134878158569336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4754467010498047\n",
      "      kl: 0.010110737755894661\n",
      "      policy_loss: -0.0011245270725339651\n",
      "      total_loss: 38.654361724853516\n",
      "      vf_explained_var: 0.9647840857505798\n",
      "      vf_loss: 38.655487060546875\n",
      "    sample_time_ms: 19964.069\n",
      "    update_time_ms: 7.759\n",
      "  iterations_since_restore: 950\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.64943626598073\n",
      "    rl_1: 104.97668344999285\n",
      "  time_since_restore: 22336.235553264618\n",
      "  time_this_iter_s: 23.00797390937805\n",
      "  time_total_s: 22336.235553264618\n",
      "  timestamp: 1550815783\n",
      "  timesteps_since_restore: 9500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9500000\n",
      "  training_iteration: 950\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22336 s, 950 iter, 9500000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-10-06\n",
      "  done: false\n",
      "  episode_len_mean: 89.02631578947368\n",
      "  episode_reward_max: 222.26946813996375\n",
      "  episode_reward_mean: 175.02011873266483\n",
      "  episode_reward_min: 140.1134970904617\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 101383\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.86\n",
      "    load_time_ms: 2.348\n",
      "    num_steps_sampled: 9510000\n",
      "    num_steps_trained: 9510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7154898643493652\n",
      "      kl: 0.03894134238362312\n",
      "      policy_loss: 0.004125731997191906\n",
      "      total_loss: 0.9037396311759949\n",
      "      vf_explained_var: 0.9980518817901611\n",
      "      vf_loss: 0.8996140956878662\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.38170433044433594\n",
      "      kl: 0.013347606174647808\n",
      "      policy_loss: 0.00019279295520391315\n",
      "      total_loss: 0.9917108416557312\n",
      "      vf_explained_var: 0.9989327788352966\n",
      "      vf_loss: 0.9915180802345276\n",
      "    sample_time_ms: 19968.974\n",
      "    update_time_ms: 7.934\n",
      "  iterations_since_restore: 951\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.92952666259384\n",
      "    rl_1: 104.09059207007098\n",
      "  time_since_restore: 22359.51376247406\n",
      "  time_this_iter_s: 23.27820920944214\n",
      "  time_total_s: 22359.51376247406\n",
      "  timestamp: 1550815806\n",
      "  timesteps_since_restore: 9510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9510000\n",
      "  training_iteration: 951\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22359 s, 951 iter, 9510000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-10-29\n",
      "  done: false\n",
      "  episode_len_mean: 89.87272727272727\n",
      "  episode_reward_max: 210.70068930083417\n",
      "  episode_reward_mean: 173.9350595885513\n",
      "  episode_reward_min: 139.63431793281356\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 101493\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.067\n",
      "    load_time_ms: 2.428\n",
      "    num_steps_sampled: 9520000\n",
      "    num_steps_trained: 9520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6666658520698547\n",
      "      kl: 0.04311233013868332\n",
      "      policy_loss: 0.005400612019002438\n",
      "      total_loss: 0.8992904424667358\n",
      "      vf_explained_var: 0.9979115724563599\n",
      "      vf_loss: 0.8938899040222168\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.39217016100883484\n",
      "      kl: 0.03415748476982117\n",
      "      policy_loss: 0.009074293076992035\n",
      "      total_loss: 1.0965086221694946\n",
      "      vf_explained_var: 0.9988595247268677\n",
      "      vf_loss: 1.0874345302581787\n",
      "    sample_time_ms: 20014.888\n",
      "    update_time_ms: 7.358\n",
      "  iterations_since_restore: 952\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.04686740466519\n",
      "    rl_1: 105.88819218388609\n",
      "  time_since_restore: 22382.830551624298\n",
      "  time_this_iter_s: 23.316789150238037\n",
      "  time_total_s: 22382.830551624298\n",
      "  timestamp: 1550815829\n",
      "  timesteps_since_restore: 9520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9520000\n",
      "  training_iteration: 952\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22382 s, 952 iter, 9520000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-10-52\n",
      "  done: false\n",
      "  episode_len_mean: 90.43243243243244\n",
      "  episode_reward_max: 216.99066825562184\n",
      "  episode_reward_mean: 174.42954573366475\n",
      "  episode_reward_min: 140.06548748240348\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 101604\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.333\n",
      "    load_time_ms: 2.371\n",
      "    num_steps_sampled: 9530000\n",
      "    num_steps_trained: 9530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6859514713287354\n",
      "      kl: 0.023291606456041336\n",
      "      policy_loss: 0.000822238449472934\n",
      "      total_loss: 0.8315133452415466\n",
      "      vf_explained_var: 0.9980999827384949\n",
      "      vf_loss: 0.8306910991668701\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.38649123907089233\n",
      "      kl: 0.04018052667379379\n",
      "      policy_loss: 0.011181389912962914\n",
      "      total_loss: 0.9993510842323303\n",
      "      vf_explained_var: 0.9989779591560364\n",
      "      vf_loss: 0.9881697297096252\n",
      "    sample_time_ms: 19958.698\n",
      "    update_time_ms: 7.511\n",
      "  iterations_since_restore: 953\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.84921449937369\n",
      "    rl_1: 106.58033123429105\n",
      "  time_since_restore: 22406.0625705719\n",
      "  time_this_iter_s: 23.23201894760132\n",
      "  time_total_s: 22406.0625705719\n",
      "  timestamp: 1550815852\n",
      "  timesteps_since_restore: 9530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9530000\n",
      "  training_iteration: 953\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22406 s, 953 iter, 9530000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-11-15\n",
      "  done: false\n",
      "  episode_len_mean: 88.98230088495575\n",
      "  episode_reward_max: 220.47498170630908\n",
      "  episode_reward_mean: 172.30561360465973\n",
      "  episode_reward_min: 140.17138476618385\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 101717\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.778\n",
      "    load_time_ms: 2.35\n",
      "    num_steps_sampled: 9540000\n",
      "    num_steps_trained: 9540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7000581622123718\n",
      "      kl: 0.029880136251449585\n",
      "      policy_loss: 0.00469845999032259\n",
      "      total_loss: 0.9736484885215759\n",
      "      vf_explained_var: 0.9978550672531128\n",
      "      vf_loss: 0.9689498543739319\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.35528135299682617\n",
      "      kl: 0.028960704803466797\n",
      "      policy_loss: 0.0033598088193684816\n",
      "      total_loss: 1.0065677165985107\n",
      "      vf_explained_var: 0.9989109635353088\n",
      "      vf_loss: 1.003208041191101\n",
      "    sample_time_ms: 19918.299\n",
      "    update_time_ms: 7.58\n",
      "  iterations_since_restore: 954\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.6311183270808\n",
      "    rl_1: 102.6744952775789\n",
      "  time_since_restore: 22428.62833762169\n",
      "  time_this_iter_s: 22.56576704978943\n",
      "  time_total_s: 22428.62833762169\n",
      "  timestamp: 1550815875\n",
      "  timesteps_since_restore: 9540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9540000\n",
      "  training_iteration: 954\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22428 s, 954 iter, 9540000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-11-38\n",
      "  done: false\n",
      "  episode_len_mean: 89.13513513513513\n",
      "  episode_reward_max: 219.31720539652108\n",
      "  episode_reward_mean: 173.76379354522192\n",
      "  episode_reward_min: 138.5180651411349\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 101828\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.493\n",
      "    load_time_ms: 2.336\n",
      "    num_steps_sampled: 9550000\n",
      "    num_steps_trained: 9550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7206405997276306\n",
      "      kl: 9.651677131652832\n",
      "      policy_loss: 0.09054666012525558\n",
      "      total_loss: 1.0311983823776245\n",
      "      vf_explained_var: 0.9978920221328735\n",
      "      vf_loss: 0.940651535987854\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4089871644973755\n",
      "      kl: 0.0435544028878212\n",
      "      policy_loss: 0.005380492191761732\n",
      "      total_loss: 1.1462113857269287\n",
      "      vf_explained_var: 0.9987869262695312\n",
      "      vf_loss: 1.1408309936523438\n",
      "    sample_time_ms: 19949.462\n",
      "    update_time_ms: 7.494\n",
      "  iterations_since_restore: 955\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.85703861074005\n",
      "    rl_1: 104.90675493448192\n",
      "  time_since_restore: 22451.835325479507\n",
      "  time_this_iter_s: 23.206987857818604\n",
      "  time_total_s: 22451.835325479507\n",
      "  timestamp: 1550815898\n",
      "  timesteps_since_restore: 9550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9550000\n",
      "  training_iteration: 955\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22451 s, 955 iter, 9550000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-12-01\n",
      "  done: false\n",
      "  episode_len_mean: 89.00884955752213\n",
      "  episode_reward_max: 221.4866052618873\n",
      "  episode_reward_mean: 175.78162445910309\n",
      "  episode_reward_min: 142.97275067356608\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 101941\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.654\n",
      "    load_time_ms: 2.339\n",
      "    num_steps_sampled: 9560000\n",
      "    num_steps_trained: 9560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7285304069519043\n",
      "      kl: 0.026072034612298012\n",
      "      policy_loss: 0.0009031465742737055\n",
      "      total_loss: 0.9604727625846863\n",
      "      vf_explained_var: 0.9978565573692322\n",
      "      vf_loss: 0.95956951379776\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3785375952720642\n",
      "      kl: 0.02015371434390545\n",
      "      policy_loss: 0.002245575888082385\n",
      "      total_loss: 1.0579622983932495\n",
      "      vf_explained_var: 0.9988835453987122\n",
      "      vf_loss: 1.055716872215271\n",
      "    sample_time_ms: 19915.411\n",
      "    update_time_ms: 7.459\n",
      "  iterations_since_restore: 956\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.55000390056586\n",
      "    rl_1: 105.23162055853723\n",
      "  time_since_restore: 22474.571299791336\n",
      "  time_this_iter_s: 22.735974311828613\n",
      "  time_total_s: 22474.571299791336\n",
      "  timestamp: 1550815921\n",
      "  timesteps_since_restore: 9560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9560000\n",
      "  training_iteration: 956\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22474 s, 956 iter, 9560000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-12-24\n",
      "  done: false\n",
      "  episode_len_mean: 89.00892857142857\n",
      "  episode_reward_max: 222.53494455860954\n",
      "  episode_reward_mean: 174.1810497823764\n",
      "  episode_reward_min: 139.6590909590775\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 102053\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.936\n",
      "    load_time_ms: 2.35\n",
      "    num_steps_sampled: 9570000\n",
      "    num_steps_trained: 9570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.737796425819397\n",
      "      kl: 0.02733381651341915\n",
      "      policy_loss: 0.0028795339167118073\n",
      "      total_loss: 0.9521951079368591\n",
      "      vf_explained_var: 0.99795001745224\n",
      "      vf_loss: 0.94931560754776\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.45519882440567017\n",
      "      kl: 0.03422475606203079\n",
      "      policy_loss: 0.005555284675210714\n",
      "      total_loss: 1.0751980543136597\n",
      "      vf_explained_var: 0.9988635182380676\n",
      "      vf_loss: 1.0696427822113037\n",
      "    sample_time_ms: 19933.359\n",
      "    update_time_ms: 7.498\n",
      "  iterations_since_restore: 957\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.62474034001778\n",
      "    rl_1: 104.5563094423586\n",
      "  time_since_restore: 22497.846190690994\n",
      "  time_this_iter_s: 23.274890899658203\n",
      "  time_total_s: 22497.846190690994\n",
      "  timestamp: 1550815944\n",
      "  timesteps_since_restore: 9570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9570000\n",
      "  training_iteration: 957\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22497 s, 957 iter, 9570000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-12-47\n",
      "  done: false\n",
      "  episode_len_mean: 88.61946902654867\n",
      "  episode_reward_max: 221.62320955882458\n",
      "  episode_reward_mean: 174.5082199580352\n",
      "  episode_reward_min: 138.34435834055733\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 102166\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.598\n",
      "    load_time_ms: 2.32\n",
      "    num_steps_sampled: 9580000\n",
      "    num_steps_trained: 9580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.728823184967041\n",
      "      kl: 0.03371914103627205\n",
      "      policy_loss: 0.004212795756757259\n",
      "      total_loss: 0.8611024618148804\n",
      "      vf_explained_var: 0.9981626868247986\n",
      "      vf_loss: 0.8568898439407349\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4070858657360077\n",
      "      kl: 0.03372655808925629\n",
      "      policy_loss: 0.003594758687540889\n",
      "      total_loss: 0.9474689960479736\n",
      "      vf_explained_var: 0.9989975094795227\n",
      "      vf_loss: 0.9438742399215698\n",
      "    sample_time_ms: 19884.153\n",
      "    update_time_ms: 8.036\n",
      "  iterations_since_restore: 958\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.64925138528451\n",
      "    rl_1: 103.8589685727507\n",
      "  time_since_restore: 22520.720121383667\n",
      "  time_this_iter_s: 22.87393069267273\n",
      "  time_total_s: 22520.720121383667\n",
      "  timestamp: 1550815967\n",
      "  timesteps_since_restore: 9580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9580000\n",
      "  training_iteration: 958\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22520 s, 958 iter, 9580000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-13-11\n",
      "  done: false\n",
      "  episode_len_mean: 89.8108108108108\n",
      "  episode_reward_max: 212.8500633628257\n",
      "  episode_reward_mean: 175.45603223783687\n",
      "  episode_reward_min: 141.28082158014283\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 102277\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3120.953\n",
      "    load_time_ms: 2.318\n",
      "    num_steps_sampled: 9590000\n",
      "    num_steps_trained: 9590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7077836394309998\n",
      "      kl: 0.03188621625304222\n",
      "      policy_loss: 0.004050005692988634\n",
      "      total_loss: 0.7580033540725708\n",
      "      vf_explained_var: 0.9983624219894409\n",
      "      vf_loss: 0.7539533376693726\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.39281606674194336\n",
      "      kl: 0.016696617007255554\n",
      "      policy_loss: 0.0005664096679538488\n",
      "      total_loss: 0.9703651070594788\n",
      "      vf_explained_var: 0.9989684820175171\n",
      "      vf_loss: 0.969798743724823\n",
      "    sample_time_ms: 19918.047\n",
      "    update_time_ms: 7.701\n",
      "  iterations_since_restore: 959\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.21672579198935\n",
      "    rl_1: 105.2393064458475\n",
      "  time_since_restore: 22543.894463777542\n",
      "  time_this_iter_s: 23.174342393875122\n",
      "  time_total_s: 22543.894463777542\n",
      "  timestamp: 1550815991\n",
      "  timesteps_since_restore: 9590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9590000\n",
      "  training_iteration: 959\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22543 s, 959 iter, 9590000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-13-34\n",
      "  done: false\n",
      "  episode_len_mean: 89.57142857142857\n",
      "  episode_reward_max: 212.99534527361394\n",
      "  episode_reward_mean: 176.81482732112778\n",
      "  episode_reward_min: 141.52527668851965\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 102389\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3120.927\n",
      "    load_time_ms: 2.268\n",
      "    num_steps_sampled: 9600000\n",
      "    num_steps_trained: 9600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7459509968757629\n",
      "      kl: 0.028316160663962364\n",
      "      policy_loss: 0.00020423397654667497\n",
      "      total_loss: 0.9840964078903198\n",
      "      vf_explained_var: 0.9978834986686707\n",
      "      vf_loss: 0.9838920831680298\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4540310502052307\n",
      "      kl: 0.019833015277981758\n",
      "      policy_loss: 0.0021176685113459826\n",
      "      total_loss: 0.9947160482406616\n",
      "      vf_explained_var: 0.9989712834358215\n",
      "      vf_loss: 0.9925985336303711\n",
      "    sample_time_ms: 19927.593\n",
      "    update_time_ms: 7.773\n",
      "  iterations_since_restore: 960\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 70.40954147220111\n",
      "    rl_1: 106.40528584892665\n",
      "  time_since_restore: 22566.99517941475\n",
      "  time_this_iter_s: 23.10071563720703\n",
      "  time_total_s: 22566.99517941475\n",
      "  timestamp: 1550816014\n",
      "  timesteps_since_restore: 9600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9600000\n",
      "  training_iteration: 960\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22566 s, 960 iter, 9600000 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-13-57\n",
      "  done: false\n",
      "  episode_len_mean: 88.36283185840708\n",
      "  episode_reward_max: 222.07274365257925\n",
      "  episode_reward_mean: 170.42414362256167\n",
      "  episode_reward_min: -148.92640632904232\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 102502\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3122.495\n",
      "    load_time_ms: 2.237\n",
      "    num_steps_sampled: 9610000\n",
      "    num_steps_trained: 9610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7240290641784668\n",
      "      kl: 0.045411255210638046\n",
      "      policy_loss: -0.0013934909366071224\n",
      "      total_loss: 24.18781089782715\n",
      "      vf_explained_var: 0.9575201272964478\n",
      "      vf_loss: 24.189205169677734\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4636250436306\n",
      "      kl: 0.009129156358540058\n",
      "      policy_loss: -0.0015999467577785254\n",
      "      total_loss: 40.76424789428711\n",
      "      vf_explained_var: 0.9611169099807739\n",
      "      vf_loss: 40.76585006713867\n",
      "    sample_time_ms: 19914.594\n",
      "    update_time_ms: 7.574\n",
      "  iterations_since_restore: 961\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.19724365657639\n",
      "    rl_1: 102.22689996598528\n",
      "  time_since_restore: 22590.1558675766\n",
      "  time_this_iter_s: 23.160688161849976\n",
      "  time_total_s: 22590.1558675766\n",
      "  timestamp: 1550816037\n",
      "  timesteps_since_restore: 9610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9610000\n",
      "  training_iteration: 961\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22590 s, 961 iter, 9610000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-14-21\n",
      "  done: false\n",
      "  episode_len_mean: 89.44642857142857\n",
      "  episode_reward_max: 221.6688423475278\n",
      "  episode_reward_mean: 176.03219217298798\n",
      "  episode_reward_min: 144.71841387356577\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 102614\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.609\n",
      "    load_time_ms: 2.225\n",
      "    num_steps_sampled: 9620000\n",
      "    num_steps_trained: 9620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6756076812744141\n",
      "      kl: 0.4927249848842621\n",
      "      policy_loss: 0.025372415781021118\n",
      "      total_loss: 0.8370872139930725\n",
      "      vf_explained_var: 0.9982953071594238\n",
      "      vf_loss: 0.8117148280143738\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3485078513622284\n",
      "      kl: 0.01542586088180542\n",
      "      policy_loss: -0.002323327586054802\n",
      "      total_loss: 0.8381820917129517\n",
      "      vf_explained_var: 0.9990977048873901\n",
      "      vf_loss: 0.8405054807662964\n",
      "    sample_time_ms: 19936.142\n",
      "    update_time_ms: 7.661\n",
      "  iterations_since_restore: 962\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.89612028209982\n",
      "    rl_1: 104.13607189088812\n",
      "  time_since_restore: 22613.8473842144\n",
      "  time_this_iter_s: 23.691516637802124\n",
      "  time_total_s: 22613.8473842144\n",
      "  timestamp: 1550816061\n",
      "  timesteps_since_restore: 9620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9620000\n",
      "  training_iteration: 962\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22613 s, 962 iter, 9620000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-14-44\n",
      "  done: false\n",
      "  episode_len_mean: 88.99107142857143\n",
      "  episode_reward_max: 217.4230256090873\n",
      "  episode_reward_mean: 169.34755500304968\n",
      "  episode_reward_min: -156.23095974177306\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 102726\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.872\n",
      "    load_time_ms: 2.231\n",
      "    num_steps_sampled: 9630000\n",
      "    num_steps_trained: 9630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6619041562080383\n",
      "      kl: 0.014295756816864014\n",
      "      policy_loss: -0.0024560822639614344\n",
      "      total_loss: 24.03372573852539\n",
      "      vf_explained_var: 0.9544246196746826\n",
      "      vf_loss: 24.03618049621582\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.45834067463874817\n",
      "      kl: 0.008968493901193142\n",
      "      policy_loss: -0.0006151962443254888\n",
      "      total_loss: 39.62480545043945\n",
      "      vf_explained_var: 0.9618015885353088\n",
      "      vf_loss: 39.62541580200195\n",
      "    sample_time_ms: 19969.376\n",
      "    update_time_ms: 7.395\n",
      "  iterations_since_restore: 963\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.37376327241262\n",
      "    rl_1: 102.97379173063709\n",
      "  time_since_restore: 22637.433190107346\n",
      "  time_this_iter_s: 23.585805892944336\n",
      "  time_total_s: 22637.433190107346\n",
      "  timestamp: 1550816084\n",
      "  timesteps_since_restore: 9630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9630000\n",
      "  training_iteration: 963\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22637 s, 963 iter, 9630000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-15-08\n",
      "  done: false\n",
      "  episode_len_mean: 89.30088495575221\n",
      "  episode_reward_max: 217.42901372139684\n",
      "  episode_reward_mean: 175.46725946312296\n",
      "  episode_reward_min: 141.0430876172369\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 102839\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.759\n",
      "    load_time_ms: 2.193\n",
      "    num_steps_sampled: 9640000\n",
      "    num_steps_trained: 9640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.712514340877533\n",
      "      kl: 0.037794359028339386\n",
      "      policy_loss: 0.00939872208982706\n",
      "      total_loss: 1.0895369052886963\n",
      "      vf_explained_var: 0.9976762533187866\n",
      "      vf_loss: 1.0801383256912231\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.45872223377227783\n",
      "      kl: 0.02779827080667019\n",
      "      policy_loss: 0.005006825551390648\n",
      "      total_loss: 0.9554235935211182\n",
      "      vf_explained_var: 0.9990049600601196\n",
      "      vf_loss: 0.9504168629646301\n",
      "    sample_time_ms: 20042.0\n",
      "    update_time_ms: 7.312\n",
      "  iterations_since_restore: 964\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.74618292874361\n",
      "    rl_1: 105.72107653437939\n",
      "  time_since_restore: 22660.704432964325\n",
      "  time_this_iter_s: 23.27124285697937\n",
      "  time_total_s: 22660.704432964325\n",
      "  timestamp: 1550816108\n",
      "  timesteps_since_restore: 9640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9640000\n",
      "  training_iteration: 964\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22660 s, 964 iter, 9640000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-15-31\n",
      "  done: false\n",
      "  episode_len_mean: 89.04464285714286\n",
      "  episode_reward_max: 216.83154893179906\n",
      "  episode_reward_mean: 170.98015103440318\n",
      "  episode_reward_min: -155.6829073779354\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 102951\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.795\n",
      "    load_time_ms: 2.216\n",
      "    num_steps_sampled: 9650000\n",
      "    num_steps_trained: 9650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6860938668251038\n",
      "      kl: 0.02561785839498043\n",
      "      policy_loss: 0.00041383475763723254\n",
      "      total_loss: 25.076066970825195\n",
      "      vf_explained_var: 0.9576191306114197\n",
      "      vf_loss: 25.075653076171875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4476012587547302\n",
      "      kl: 2.9565176963806152\n",
      "      policy_loss: 0.010702974162995815\n",
      "      total_loss: 41.69637680053711\n",
      "      vf_explained_var: 0.9610467553138733\n",
      "      vf_loss: 41.68567657470703\n",
      "    sample_time_ms: 20034.432\n",
      "    update_time_ms: 7.272\n",
      "  iterations_since_restore: 965\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.62272492281748\n",
      "    rl_1: 103.3574261115857\n",
      "  time_since_restore: 22683.888333320618\n",
      "  time_this_iter_s: 23.183900356292725\n",
      "  time_total_s: 22683.888333320618\n",
      "  timestamp: 1550816131\n",
      "  timesteps_since_restore: 9650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9650000\n",
      "  training_iteration: 965\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22683 s, 965 iter, 9650000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-15-55\n",
      "  done: false\n",
      "  episode_len_mean: 89.375\n",
      "  episode_reward_max: 217.597991425573\n",
      "  episode_reward_mean: 175.68783230993586\n",
      "  episode_reward_min: 137.58164839765024\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 103063\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3141.052\n",
      "    load_time_ms: 2.235\n",
      "    num_steps_sampled: 9660000\n",
      "    num_steps_trained: 9660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6872299909591675\n",
      "      kl: 0.03825836256146431\n",
      "      policy_loss: 0.003888117615133524\n",
      "      total_loss: 1.0805176496505737\n",
      "      vf_explained_var: 0.9976083636283875\n",
      "      vf_loss: 1.0766295194625854\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4612850844860077\n",
      "      kl: 0.013134835287928581\n",
      "      policy_loss: 0.0005597261479124427\n",
      "      total_loss: 1.0221710205078125\n",
      "      vf_explained_var: 0.9989320635795593\n",
      "      vf_loss: 1.0216110944747925\n",
      "    sample_time_ms: 20129.585\n",
      "    update_time_ms: 7.183\n",
      "  iterations_since_restore: 966\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.18763578609365\n",
      "    rl_1: 106.5001965238422\n",
      "  time_since_restore: 22707.54983997345\n",
      "  time_this_iter_s: 23.66150665283203\n",
      "  time_total_s: 22707.54983997345\n",
      "  timestamp: 1550816155\n",
      "  timesteps_since_restore: 9660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9660000\n",
      "  training_iteration: 966\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22707 s, 966 iter, 9660000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-16-18\n",
      "  done: false\n",
      "  episode_len_mean: 88.20353982300885\n",
      "  episode_reward_max: 217.48649061595668\n",
      "  episode_reward_mean: 176.23312246430777\n",
      "  episode_reward_min: 141.10496803276254\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 103176\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3143.13\n",
      "    load_time_ms: 2.275\n",
      "    num_steps_sampled: 9670000\n",
      "    num_steps_trained: 9670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6973955035209656\n",
      "      kl: 0.026111479848623276\n",
      "      policy_loss: 0.0012178876204416156\n",
      "      total_loss: 0.866517961025238\n",
      "      vf_explained_var: 0.9981496930122375\n",
      "      vf_loss: 0.8652999997138977\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3720797300338745\n",
      "      kl: 0.025658268481492996\n",
      "      policy_loss: 0.007687835488468409\n",
      "      total_loss: 0.8797781467437744\n",
      "      vf_explained_var: 0.9990896582603455\n",
      "      vf_loss: 0.8720902800559998\n",
      "    sample_time_ms: 20118.62\n",
      "    update_time_ms: 7.463\n",
      "  iterations_since_restore: 967\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 72.23515828741495\n",
      "    rl_1: 103.99796417689281\n",
      "  time_since_restore: 22730.737703561783\n",
      "  time_this_iter_s: 23.18786358833313\n",
      "  time_total_s: 22730.737703561783\n",
      "  timestamp: 1550816178\n",
      "  timesteps_since_restore: 9670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9670000\n",
      "  training_iteration: 967\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22730 s, 967 iter, 9670000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-16-41\n",
      "  done: false\n",
      "  episode_len_mean: 89.24107142857143\n",
      "  episode_reward_max: 218.05889814265967\n",
      "  episode_reward_mean: 174.41450472554416\n",
      "  episode_reward_min: 140.92256837079182\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 103288\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.363\n",
      "    load_time_ms: 2.315\n",
      "    num_steps_sampled: 9680000\n",
      "    num_steps_trained: 9680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7119156718254089\n",
      "      kl: 0.024672280997037888\n",
      "      policy_loss: -0.00020123034482821822\n",
      "      total_loss: 1.0528167486190796\n",
      "      vf_explained_var: 0.9977369904518127\n",
      "      vf_loss: 1.0530179738998413\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.43090131878852844\n",
      "      kl: 0.03499239683151245\n",
      "      policy_loss: 0.011284613981842995\n",
      "      total_loss: 0.9061275124549866\n",
      "      vf_explained_var: 0.9990777373313904\n",
      "      vf_loss: 0.894842803478241\n",
      "    sample_time_ms: 20183.735\n",
      "    update_time_ms: 6.938\n",
      "  iterations_since_restore: 968\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.3464759608725\n",
      "    rl_1: 105.06802876467165\n",
      "  time_since_restore: 22754.31279683113\n",
      "  time_this_iter_s: 23.575093269348145\n",
      "  time_total_s: 22754.31279683113\n",
      "  timestamp: 1550816201\n",
      "  timesteps_since_restore: 9680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9680000\n",
      "  training_iteration: 968\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22754 s, 968 iter, 9680000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-17-05\n",
      "  done: false\n",
      "  episode_len_mean: 88.66071428571429\n",
      "  episode_reward_max: 218.64227708467806\n",
      "  episode_reward_mean: 170.4058036550391\n",
      "  episode_reward_min: -151.8159633420468\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 103400\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3145.464\n",
      "    load_time_ms: 2.359\n",
      "    num_steps_sampled: 9690000\n",
      "    num_steps_trained: 9690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6244705319404602\n",
      "      kl: 0.019975990056991577\n",
      "      policy_loss: -0.0033868520986288786\n",
      "      total_loss: 20.451322555541992\n",
      "      vf_explained_var: 0.9653258919715881\n",
      "      vf_loss: 20.4547061920166\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3024270832538605\n",
      "      kl: 0.10240975767374039\n",
      "      policy_loss: -0.01094952505081892\n",
      "      total_loss: 36.88365936279297\n",
      "      vf_explained_var: 0.967174232006073\n",
      "      vf_loss: 36.89459991455078\n",
      "    sample_time_ms: 20211.447\n",
      "    update_time_ms: 7.016\n",
      "  iterations_since_restore: 969\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.05312658709052\n",
      "    rl_1: 102.35267706794858\n",
      "  time_since_restore: 22777.73695921898\n",
      "  time_this_iter_s: 23.4241623878479\n",
      "  time_total_s: 22777.73695921898\n",
      "  timestamp: 1550816225\n",
      "  timesteps_since_restore: 9690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9690000\n",
      "  training_iteration: 969\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22777 s, 969 iter, 9690000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-17-28\n",
      "  done: false\n",
      "  episode_len_mean: 88.58771929824562\n",
      "  episode_reward_max: 213.56813476722436\n",
      "  episode_reward_mean: 170.91902781967826\n",
      "  episode_reward_min: 142.34331440690673\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 103514\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.246\n",
      "    load_time_ms: 2.433\n",
      "    num_steps_sampled: 9700000\n",
      "    num_steps_trained: 9700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6667861938476562\n",
      "      kl: 0.025461550801992416\n",
      "      policy_loss: 0.0006003379821777344\n",
      "      total_loss: 1.2244787216186523\n",
      "      vf_explained_var: 0.9972241520881653\n",
      "      vf_loss: 1.223878264427185\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.39626529812812805\n",
      "      kl: 0.014169448055326939\n",
      "      policy_loss: 0.0024154577404260635\n",
      "      total_loss: 1.0482038259506226\n",
      "      vf_explained_var: 0.9988524317741394\n",
      "      vf_loss: 1.045788288116455\n",
      "    sample_time_ms: 20186.139\n",
      "    update_time_ms: 6.749\n",
      "  iterations_since_restore: 970\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.1612185078241\n",
      "    rl_1: 102.75780931185415\n",
      "  time_since_restore: 22800.573581933975\n",
      "  time_this_iter_s: 22.836622714996338\n",
      "  time_total_s: 22800.573581933975\n",
      "  timestamp: 1550816248\n",
      "  timesteps_since_restore: 9700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9700000\n",
      "  training_iteration: 970\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22800 s, 970 iter, 9700000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-17-51\n",
      "  done: false\n",
      "  episode_len_mean: 88.32743362831859\n",
      "  episode_reward_max: 218.44276281054675\n",
      "  episode_reward_mean: 176.0042911747655\n",
      "  episode_reward_min: 139.30039080599877\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 103627\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3161.291\n",
      "    load_time_ms: 2.396\n",
      "    num_steps_sampled: 9710000\n",
      "    num_steps_trained: 9710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7250819802284241\n",
      "      kl: 0.023069672286510468\n",
      "      policy_loss: 0.0017217883141711354\n",
      "      total_loss: 1.134934425354004\n",
      "      vf_explained_var: 0.997628927230835\n",
      "      vf_loss: 1.1332128047943115\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4443419873714447\n",
      "      kl: 0.011893186718225479\n",
      "      policy_loss: 0.0001514524919912219\n",
      "      total_loss: 0.9946256875991821\n",
      "      vf_explained_var: 0.9989209175109863\n",
      "      vf_loss: 0.9944742918014526\n",
      "    sample_time_ms: 20183.997\n",
      "    update_time_ms: 6.936\n",
      "  iterations_since_restore: 971\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.66996067109439\n",
      "    rl_1: 104.33433050367113\n",
      "  time_since_restore: 22823.882568359375\n",
      "  time_this_iter_s: 23.30898642539978\n",
      "  time_total_s: 22823.882568359375\n",
      "  timestamp: 1550816271\n",
      "  timesteps_since_restore: 9710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9710000\n",
      "  training_iteration: 971\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22823 s, 971 iter, 9710000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-18-14\n",
      "  done: false\n",
      "  episode_len_mean: 89.34821428571429\n",
      "  episode_reward_max: 219.07726056666348\n",
      "  episode_reward_mean: 175.9047266110478\n",
      "  episode_reward_min: 138.86875742417723\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 103739\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.923\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 9720000\n",
      "    num_steps_trained: 9720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6746135354042053\n",
      "      kl: 0.03547514230012894\n",
      "      policy_loss: 0.0007940074428915977\n",
      "      total_loss: 1.0641530752182007\n",
      "      vf_explained_var: 0.9978025555610657\n",
      "      vf_loss: 1.0633589029312134\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.40549522638320923\n",
      "      kl: 0.05069008097052574\n",
      "      policy_loss: 0.009785532020032406\n",
      "      total_loss: 1.1744781732559204\n",
      "      vf_explained_var: 0.9988038539886475\n",
      "      vf_loss: 1.1646928787231445\n",
      "    sample_time_ms: 20149.607\n",
      "    update_time_ms: 6.866\n",
      "  iterations_since_restore: 972\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.89094897626562\n",
      "    rl_1: 106.0137776347822\n",
      "  time_since_restore: 22847.046459436417\n",
      "  time_this_iter_s: 23.163891077041626\n",
      "  time_total_s: 22847.046459436417\n",
      "  timestamp: 1550816294\n",
      "  timesteps_since_restore: 9720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9720000\n",
      "  training_iteration: 972\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22847 s, 972 iter, 9720000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-18-38\n",
      "  done: false\n",
      "  episode_len_mean: 88.79464285714286\n",
      "  episode_reward_max: 219.12080449662523\n",
      "  episode_reward_mean: 173.18690461784666\n",
      "  episode_reward_min: 142.30692070072817\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 103851\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3142.081\n",
      "    load_time_ms: 2.382\n",
      "    num_steps_sampled: 9730000\n",
      "    num_steps_trained: 9730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7113646268844604\n",
      "      kl: 0.03699414059519768\n",
      "      policy_loss: 0.0019251849735155702\n",
      "      total_loss: 0.98712158203125\n",
      "      vf_explained_var: 0.9978355765342712\n",
      "      vf_loss: 0.9851963520050049\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.46622976660728455\n",
      "      kl: 0.02187197282910347\n",
      "      policy_loss: 0.010538259521126747\n",
      "      total_loss: 1.192739486694336\n",
      "      vf_explained_var: 0.9987536668777466\n",
      "      vf_loss: 1.1822011470794678\n",
      "    sample_time_ms: 20110.924\n",
      "    update_time_ms: 6.758\n",
      "  iterations_since_restore: 973\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.6219955435838\n",
      "    rl_1: 104.56490907426281\n",
      "  time_since_restore: 22870.239507436752\n",
      "  time_this_iter_s: 23.193048000335693\n",
      "  time_total_s: 22870.239507436752\n",
      "  timestamp: 1550816318\n",
      "  timesteps_since_restore: 9730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9730000\n",
      "  training_iteration: 973\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22870 s, 973 iter, 9730000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-19-01\n",
      "  done: false\n",
      "  episode_len_mean: 88.72566371681415\n",
      "  episode_reward_max: 215.0120764657098\n",
      "  episode_reward_mean: 177.84887150735403\n",
      "  episode_reward_min: 143.92959607626238\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 103964\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3144.147\n",
      "    load_time_ms: 2.423\n",
      "    num_steps_sampled: 9740000\n",
      "    num_steps_trained: 9740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7383340001106262\n",
      "      kl: 0.03153001517057419\n",
      "      policy_loss: 0.0016572471940889955\n",
      "      total_loss: 1.024786114692688\n",
      "      vf_explained_var: 0.9978706240653992\n",
      "      vf_loss: 1.023128867149353\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.45920515060424805\n",
      "      kl: 0.03122798539698124\n",
      "      policy_loss: 0.003644517157226801\n",
      "      total_loss: 1.008506417274475\n",
      "      vf_explained_var: 0.9989688396453857\n",
      "      vf_loss: 1.0048621892929077\n",
      "    sample_time_ms: 20134.053\n",
      "    update_time_ms: 6.845\n",
      "  iterations_since_restore: 974\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.66856906668552\n",
      "    rl_1: 106.18030244066848\n",
      "  time_since_restore: 22893.765476465225\n",
      "  time_this_iter_s: 23.5259690284729\n",
      "  time_total_s: 22893.765476465225\n",
      "  timestamp: 1550816341\n",
      "  timesteps_since_restore: 9740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9740000\n",
      "  training_iteration: 974\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22893 s, 974 iter, 9740000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-19-24\n",
      "  done: false\n",
      "  episode_len_mean: 88.15789473684211\n",
      "  episode_reward_max: 222.3585348376351\n",
      "  episode_reward_mean: 172.7185947855199\n",
      "  episode_reward_min: 139.20943846759582\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 104078\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3139.584\n",
      "    load_time_ms: 2.416\n",
      "    num_steps_sampled: 9750000\n",
      "    num_steps_trained: 9750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.68463534116745\n",
      "      kl: 0.028995037078857422\n",
      "      policy_loss: 0.0023553892970085144\n",
      "      total_loss: 1.0897282361984253\n",
      "      vf_explained_var: 0.9976977705955505\n",
      "      vf_loss: 1.0873726606369019\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.43878188729286194\n",
      "      kl: 0.021720362827181816\n",
      "      policy_loss: 0.0021172831766307354\n",
      "      total_loss: 1.0431036949157715\n",
      "      vf_explained_var: 0.9988995790481567\n",
      "      vf_loss: 1.04098641872406\n",
      "    sample_time_ms: 20141.45\n",
      "    update_time_ms: 6.819\n",
      "  iterations_since_restore: 975\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.97222734963722\n",
      "    rl_1: 102.74636743588266\n",
      "  time_since_restore: 22916.97398662567\n",
      "  time_this_iter_s: 23.208510160446167\n",
      "  time_total_s: 22916.97398662567\n",
      "  timestamp: 1550816364\n",
      "  timesteps_since_restore: 9750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9750000\n",
      "  training_iteration: 975\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22916 s, 975 iter, 9750000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-19-47\n",
      "  done: false\n",
      "  episode_len_mean: 88.46428571428571\n",
      "  episode_reward_max: 215.46600562433878\n",
      "  episode_reward_mean: 171.32683773033304\n",
      "  episode_reward_min: 140.9849816020252\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 104190\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3140.945\n",
      "    load_time_ms: 2.365\n",
      "    num_steps_sampled: 9760000\n",
      "    num_steps_trained: 9760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6817671656608582\n",
      "      kl: 0.03520619124174118\n",
      "      policy_loss: 0.00220666010864079\n",
      "      total_loss: 1.0200954675674438\n",
      "      vf_explained_var: 0.9976533055305481\n",
      "      vf_loss: 1.0178886651992798\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.46955400705337524\n",
      "      kl: 0.02482878975570202\n",
      "      policy_loss: 0.0022033522836863995\n",
      "      total_loss: 1.0908899307250977\n",
      "      vf_explained_var: 0.9988113045692444\n",
      "      vf_loss: 1.088686466217041\n",
      "    sample_time_ms: 20045.116\n",
      "    update_time_ms: 6.827\n",
      "  iterations_since_restore: 976\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.0391346054457\n",
      "    rl_1: 103.28770312488734\n",
      "  time_since_restore: 22939.6842648983\n",
      "  time_this_iter_s: 22.710278272628784\n",
      "  time_total_s: 22939.6842648983\n",
      "  timestamp: 1550816387\n",
      "  timesteps_since_restore: 9760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9760000\n",
      "  training_iteration: 976\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22939 s, 976 iter, 9760000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-20-11\n",
      "  done: false\n",
      "  episode_len_mean: 88.14035087719299\n",
      "  episode_reward_max: 221.57199784223263\n",
      "  episode_reward_mean: 167.90885180365515\n",
      "  episode_reward_min: -157.66217738815607\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 104304\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.853\n",
      "    load_time_ms: 2.404\n",
      "    num_steps_sampled: 9770000\n",
      "    num_steps_trained: 9770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6584448218345642\n",
      "      kl: 0.027680177241563797\n",
      "      policy_loss: 0.002635739278048277\n",
      "      total_loss: 40.08645248413086\n",
      "      vf_explained_var: 0.9387933611869812\n",
      "      vf_loss: 40.08382034301758\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.45399489998817444\n",
      "      kl: 7.873802661895752\n",
      "      policy_loss: 0.012645021080970764\n",
      "      total_loss: 72.5559310913086\n",
      "      vf_explained_var: 0.9399940371513367\n",
      "      vf_loss: 72.54328918457031\n",
      "    sample_time_ms: 20079.43\n",
      "    update_time_ms: 7.18\n",
      "  iterations_since_restore: 977\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.09289778446197\n",
      "    rl_1: 101.81595401919319\n",
      "  time_since_restore: 22963.18987774849\n",
      "  time_this_iter_s: 23.50561285018921\n",
      "  time_total_s: 22963.18987774849\n",
      "  timestamp: 1550816411\n",
      "  timesteps_since_restore: 9770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9770000\n",
      "  training_iteration: 977\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22963 s, 977 iter, 9770000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-20-34\n",
      "  done: false\n",
      "  episode_len_mean: 88.45132743362832\n",
      "  episode_reward_max: 216.5712348978502\n",
      "  episode_reward_mean: 172.89263933788658\n",
      "  episode_reward_min: 134.80406972538375\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 104417\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.39\n",
      "    load_time_ms: 2.49\n",
      "    num_steps_sampled: 9780000\n",
      "    num_steps_trained: 9780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6665418148040771\n",
      "      kl: 0.036814648658037186\n",
      "      policy_loss: 0.0034018165897578\n",
      "      total_loss: 1.0225050449371338\n",
      "      vf_explained_var: 0.9976781010627747\n",
      "      vf_loss: 1.0191031694412231\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4527159333229065\n",
      "      kl: 0.011232273653149605\n",
      "      policy_loss: 0.0002489876060280949\n",
      "      total_loss: 1.0622109174728394\n",
      "      vf_explained_var: 0.998911440372467\n",
      "      vf_loss: 1.0619617700576782\n",
      "    sample_time_ms: 20054.705\n",
      "    update_time_ms: 7.149\n",
      "  iterations_since_restore: 978\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.54163489703156\n",
      "    rl_1: 104.35100444085506\n",
      "  time_since_restore: 22986.461631774902\n",
      "  time_this_iter_s: 23.271754026412964\n",
      "  time_total_s: 22986.461631774902\n",
      "  timestamp: 1550816434\n",
      "  timesteps_since_restore: 9780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9780000\n",
      "  training_iteration: 978\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 22986 s, 978 iter, 9780000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-20-58\n",
      "  done: false\n",
      "  episode_len_mean: 88.75\n",
      "  episode_reward_max: 216.62672294332197\n",
      "  episode_reward_mean: 175.26577038080683\n",
      "  episode_reward_min: 141.7418024158759\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 104529\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.59\n",
      "    load_time_ms: 2.514\n",
      "    num_steps_sampled: 9790000\n",
      "    num_steps_trained: 9790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.709532618522644\n",
      "      kl: 7.878839015960693\n",
      "      policy_loss: 0.041990578174591064\n",
      "      total_loss: 1.0157980918884277\n",
      "      vf_explained_var: 0.997934877872467\n",
      "      vf_loss: 0.9738075137138367\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.48161911964416504\n",
      "      kl: 0.02705875039100647\n",
      "      policy_loss: 0.009192869998514652\n",
      "      total_loss: 0.9714656472206116\n",
      "      vf_explained_var: 0.9990084767341614\n",
      "      vf_loss: 0.9622727632522583\n",
      "    sample_time_ms: 20080.345\n",
      "    update_time_ms: 7.651\n",
      "  iterations_since_restore: 979\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.30546179268386\n",
      "    rl_1: 105.96030858812296\n",
      "  time_since_restore: 23010.150388002396\n",
      "  time_this_iter_s: 23.688756227493286\n",
      "  time_total_s: 23010.150388002396\n",
      "  timestamp: 1550816458\n",
      "  timesteps_since_restore: 9790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9790000\n",
      "  training_iteration: 979\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23010 s, 979 iter, 9790000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-21-21\n",
      "  done: false\n",
      "  episode_len_mean: 89.00877192982456\n",
      "  episode_reward_max: 219.92691782574195\n",
      "  episode_reward_mean: 172.35301989271431\n",
      "  episode_reward_min: 134.57191434726897\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 104643\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3148.575\n",
      "    load_time_ms: 2.532\n",
      "    num_steps_sampled: 9800000\n",
      "    num_steps_trained: 9800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6927513480186462\n",
      "      kl: 0.021913006901741028\n",
      "      policy_loss: 0.0011805612593889236\n",
      "      total_loss: 0.9300307035446167\n",
      "      vf_explained_var: 0.9979276657104492\n",
      "      vf_loss: 0.9288502335548401\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5302572250366211\n",
      "      kl: 0.0233631432056427\n",
      "      policy_loss: 0.005356703884899616\n",
      "      total_loss: 0.9597576856613159\n",
      "      vf_explained_var: 0.999017596244812\n",
      "      vf_loss: 0.954400897026062\n",
      "    sample_time_ms: 20085.96\n",
      "    update_time_ms: 7.715\n",
      "  iterations_since_restore: 980\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.3635281296223\n",
      "    rl_1: 105.98949176309205\n",
      "  time_since_restore: 23033.20225572586\n",
      "  time_this_iter_s: 23.051867723464966\n",
      "  time_total_s: 23033.20225572586\n",
      "  timestamp: 1550816481\n",
      "  timesteps_since_restore: 9800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9800000\n",
      "  training_iteration: 980\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23033 s, 980 iter, 9800000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-21-44\n",
      "  done: false\n",
      "  episode_len_mean: 89.1891891891892\n",
      "  episode_reward_max: 213.3621373499814\n",
      "  episode_reward_mean: 175.06655583812446\n",
      "  episode_reward_min: 139.77264285495826\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 104754\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3131.294\n",
      "    load_time_ms: 2.604\n",
      "    num_steps_sampled: 9810000\n",
      "    num_steps_trained: 9810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6910169124603271\n",
      "      kl: 0.02031911537051201\n",
      "      policy_loss: 0.0012787693412974477\n",
      "      total_loss: 0.8367988467216492\n",
      "      vf_explained_var: 0.9981001019477844\n",
      "      vf_loss: 0.8355200290679932\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5021799802780151\n",
      "      kl: 0.020717527717351913\n",
      "      policy_loss: 0.004659694153815508\n",
      "      total_loss: 0.9399589896202087\n",
      "      vf_explained_var: 0.999075174331665\n",
      "      vf_loss: 0.9352992177009583\n",
      "    sample_time_ms: 20103.825\n",
      "    update_time_ms: 7.589\n",
      "  iterations_since_restore: 981\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.62561641286045\n",
      "    rl_1: 107.44093942526398\n",
      "  time_since_restore: 23056.519154548645\n",
      "  time_this_iter_s: 23.316898822784424\n",
      "  time_total_s: 23056.519154548645\n",
      "  timestamp: 1550816504\n",
      "  timesteps_since_restore: 9810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9810000\n",
      "  training_iteration: 981\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23056 s, 981 iter, 9810000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-22-08\n",
      "  done: false\n",
      "  episode_len_mean: 88.14035087719299\n",
      "  episode_reward_max: 220.26108595787932\n",
      "  episode_reward_mean: 170.38746776990945\n",
      "  episode_reward_min: -155.36126135739408\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 104868\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.899\n",
      "    load_time_ms: 2.568\n",
      "    num_steps_sampled: 9820000\n",
      "    num_steps_trained: 9820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7099252939224243\n",
      "      kl: 0.018764678388834\n",
      "      policy_loss: 3.476507845334709e-05\n",
      "      total_loss: 25.332889556884766\n",
      "      vf_explained_var: 0.955031156539917\n",
      "      vf_loss: 25.332855224609375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4648090898990631\n",
      "      kl: 0.020307445898652077\n",
      "      policy_loss: 0.0004929350106976926\n",
      "      total_loss: 39.566368103027344\n",
      "      vf_explained_var: 0.9637956023216248\n",
      "      vf_loss: 39.56587600708008\n",
      "    sample_time_ms: 20118.385\n",
      "    update_time_ms: 7.597\n",
      "  iterations_since_restore: 982\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.83180577755572\n",
      "    rl_1: 103.55566199235375\n",
      "  time_since_restore: 23079.80332994461\n",
      "  time_this_iter_s: 23.284175395965576\n",
      "  time_total_s: 23079.80332994461\n",
      "  timestamp: 1550816528\n",
      "  timesteps_since_restore: 9820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9820000\n",
      "  training_iteration: 982\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23079 s, 982 iter, 9820000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-22-31\n",
      "  done: false\n",
      "  episode_len_mean: 89.08035714285714\n",
      "  episode_reward_max: 216.0935895463884\n",
      "  episode_reward_mean: 172.85615091333148\n",
      "  episode_reward_min: 138.96630426924173\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 104980\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3128.22\n",
      "    load_time_ms: 2.606\n",
      "    num_steps_sampled: 9830000\n",
      "    num_steps_trained: 9830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6821407675743103\n",
      "      kl: 0.06629560887813568\n",
      "      policy_loss: 0.00930345244705677\n",
      "      total_loss: 1.0822513103485107\n",
      "      vf_explained_var: 0.9974624514579773\n",
      "      vf_loss: 1.0729478597640991\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4789571166038513\n",
      "      kl: 0.01498548686504364\n",
      "      policy_loss: -0.000350540824001655\n",
      "      total_loss: 0.9226281642913818\n",
      "      vf_explained_var: 0.9990460276603699\n",
      "      vf_loss: 0.9229786992073059\n",
      "    sample_time_ms: 20125.012\n",
      "    update_time_ms: 7.769\n",
      "  iterations_since_restore: 983\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.96673957403853\n",
      "    rl_1: 106.88941133929295\n",
      "  time_since_restore: 23103.05709171295\n",
      "  time_this_iter_s: 23.253761768341064\n",
      "  time_total_s: 23103.05709171295\n",
      "  timestamp: 1550816551\n",
      "  timesteps_since_restore: 9830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9830000\n",
      "  training_iteration: 983\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23103 s, 983 iter, 9830000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-22-55\n",
      "  done: false\n",
      "  episode_len_mean: 88.74107142857143\n",
      "  episode_reward_max: 217.30260931269177\n",
      "  episode_reward_mean: 173.98254297407155\n",
      "  episode_reward_min: 141.87754802672922\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 105092\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.14\n",
      "    load_time_ms: 2.595\n",
      "    num_steps_sampled: 9840000\n",
      "    num_steps_trained: 9840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7145262360572815\n",
      "      kl: 0.040981076657772064\n",
      "      policy_loss: 0.005753775592893362\n",
      "      total_loss: 0.9565883278846741\n",
      "      vf_explained_var: 0.9978129267692566\n",
      "      vf_loss: 0.9508343935012817\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.495908260345459\n",
      "      kl: 0.023802349343895912\n",
      "      policy_loss: 0.0018518182914704084\n",
      "      total_loss: 0.8897163271903992\n",
      "      vf_explained_var: 0.9990915060043335\n",
      "      vf_loss: 0.8878645300865173\n",
      "    sample_time_ms: 20151.402\n",
      "    update_time_ms: 7.861\n",
      "  iterations_since_restore: 984\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.89441494356217\n",
      "    rl_1: 106.08812803050937\n",
      "  time_since_restore: 23126.82666015625\n",
      "  time_this_iter_s: 23.76956844329834\n",
      "  time_total_s: 23126.82666015625\n",
      "  timestamp: 1550816575\n",
      "  timesteps_since_restore: 9840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9840000\n",
      "  training_iteration: 984\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23126 s, 984 iter, 9840000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-23-19\n",
      "  done: false\n",
      "  episode_len_mean: 88.72566371681415\n",
      "  episode_reward_max: 219.15082119139663\n",
      "  episode_reward_mean: 166.0422927508677\n",
      "  episode_reward_min: -155.67650538992325\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 105205\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3127.128\n",
      "    load_time_ms: 2.598\n",
      "    num_steps_sampled: 9850000\n",
      "    num_steps_trained: 9850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6548426747322083\n",
      "      kl: 0.010501096956431866\n",
      "      policy_loss: -0.0025811451487243176\n",
      "      total_loss: 43.37664794921875\n",
      "      vf_explained_var: 0.9238803386688232\n",
      "      vf_loss: 43.37923049926758\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4531060755252838\n",
      "      kl: 0.038101255893707275\n",
      "      policy_loss: -0.0007331891101785004\n",
      "      total_loss: 76.57290649414062\n",
      "      vf_explained_var: 0.9302577376365662\n",
      "      vf_loss: 76.57364654541016\n",
      "    sample_time_ms: 20220.147\n",
      "    update_time_ms: 8.205\n",
      "  iterations_since_restore: 985\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.28794192274604\n",
      "    rl_1: 102.75435082812167\n",
      "  time_since_restore: 23150.739208698273\n",
      "  time_this_iter_s: 23.912548542022705\n",
      "  time_total_s: 23150.739208698273\n",
      "  timestamp: 1550816599\n",
      "  timesteps_since_restore: 9850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9850000\n",
      "  training_iteration: 985\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23150 s, 985 iter, 9850000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-23-42\n",
      "  done: false\n",
      "  episode_len_mean: 88.39823008849558\n",
      "  episode_reward_max: 212.89936649083532\n",
      "  episode_reward_mean: 166.41585678826812\n",
      "  episode_reward_min: -149.89518201201292\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 105318\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3126.892\n",
      "    load_time_ms: 2.588\n",
      "    num_steps_sampled: 9860000\n",
      "    num_steps_trained: 9860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.683049738407135\n",
      "      kl: 0.018500659614801407\n",
      "      policy_loss: -0.0044862013310194016\n",
      "      total_loss: 33.31394577026367\n",
      "      vf_explained_var: 0.9421358108520508\n",
      "      vf_loss: 33.31843948364258\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5543412566184998\n",
      "      kl: 0.04836670309305191\n",
      "      policy_loss: 0.002620177110657096\n",
      "      total_loss: 62.478477478027344\n",
      "      vf_explained_var: 0.9434657692909241\n",
      "      vf_loss: 62.47585678100586\n",
      "    sample_time_ms: 20265.337\n",
      "    update_time_ms: 8.396\n",
      "  iterations_since_restore: 986\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.781464909416215\n",
      "    rl_1: 103.63439187885186\n",
      "  time_since_restore: 23173.899746418\n",
      "  time_this_iter_s: 23.160537719726562\n",
      "  time_total_s: 23173.899746418\n",
      "  timestamp: 1550816622\n",
      "  timesteps_since_restore: 9860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9860000\n",
      "  training_iteration: 986\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23173 s, 986 iter, 9860000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-24-05\n",
      "  done: false\n",
      "  episode_len_mean: 88.94690265486726\n",
      "  episode_reward_max: 214.00021130033318\n",
      "  episode_reward_mean: 174.67286715685418\n",
      "  episode_reward_min: 136.2827253663598\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 105431\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3129.915\n",
      "    load_time_ms: 2.536\n",
      "    num_steps_sampled: 9870000\n",
      "    num_steps_trained: 9870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6971097588539124\n",
      "      kl: 0.04694018140435219\n",
      "      policy_loss: 0.006402728613466024\n",
      "      total_loss: 1.3177329301834106\n",
      "      vf_explained_var: 0.9971621632575989\n",
      "      vf_loss: 1.3113304376602173\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.4261092245578766\n",
      "      kl: 0.014776411466300488\n",
      "      policy_loss: 0.0007317760027945042\n",
      "      total_loss: 1.046789526939392\n",
      "      vf_explained_var: 0.9988784790039062\n",
      "      vf_loss: 1.0460578203201294\n",
      "    sample_time_ms: 20250.027\n",
      "    update_time_ms: 7.897\n",
      "  iterations_since_restore: 987\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.3294908908846\n",
      "    rl_1: 105.34337626596957\n",
      "  time_since_restore: 23197.276377677917\n",
      "  time_this_iter_s: 23.376631259918213\n",
      "  time_total_s: 23197.276377677917\n",
      "  timestamp: 1550816645\n",
      "  timesteps_since_restore: 9870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9870000\n",
      "  training_iteration: 987\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23197 s, 987 iter, 9870000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-24-28\n",
      "  done: false\n",
      "  episode_len_mean: 88.6283185840708\n",
      "  episode_reward_max: 214.67431131483497\n",
      "  episode_reward_mean: 172.0342289825187\n",
      "  episode_reward_min: 145.0762421766126\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 105544\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3130.988\n",
      "    load_time_ms: 2.469\n",
      "    num_steps_sampled: 9880000\n",
      "    num_steps_trained: 9880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6531057953834534\n",
      "      kl: 0.0256674662232399\n",
      "      policy_loss: 0.00308376201428473\n",
      "      total_loss: 1.455016851425171\n",
      "      vf_explained_var: 0.9966055154800415\n",
      "      vf_loss: 1.45193350315094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3762907385826111\n",
      "      kl: 0.022389234974980354\n",
      "      policy_loss: 0.0035598392132669687\n",
      "      total_loss: 1.0750234127044678\n",
      "      vf_explained_var: 0.9988653659820557\n",
      "      vf_loss: 1.0714634656906128\n",
      "    sample_time_ms: 20238.952\n",
      "    update_time_ms: 7.809\n",
      "  iterations_since_restore: 988\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.35369193454103\n",
      "    rl_1: 104.68053704797768\n",
      "  time_since_restore: 23220.449677228928\n",
      "  time_this_iter_s: 23.173299551010132\n",
      "  time_total_s: 23220.449677228928\n",
      "  timestamp: 1550816668\n",
      "  timesteps_since_restore: 9880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9880000\n",
      "  training_iteration: 988\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23220 s, 988 iter, 9880000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-24-52\n",
      "  done: false\n",
      "  episode_len_mean: 88.95535714285714\n",
      "  episode_reward_max: 218.6480198332354\n",
      "  episode_reward_mean: 174.08457426208074\n",
      "  episode_reward_min: 135.21890969516284\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 105656\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3133.373\n",
      "    load_time_ms: 2.389\n",
      "    num_steps_sampled: 9890000\n",
      "    num_steps_trained: 9890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.725611686706543\n",
      "      kl: 0.029841724783182144\n",
      "      policy_loss: 0.0033964579924941063\n",
      "      total_loss: 0.952187180519104\n",
      "      vf_explained_var: 0.9978641867637634\n",
      "      vf_loss: 0.9487905502319336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.5063135623931885\n",
      "      kl: 0.014340672641992569\n",
      "      policy_loss: 0.003281204728409648\n",
      "      total_loss: 0.7778855562210083\n",
      "      vf_explained_var: 0.9991897344589233\n",
      "      vf_loss: 0.7746042609214783\n",
      "    sample_time_ms: 20262.606\n",
      "    update_time_ms: 7.62\n",
      "  iterations_since_restore: 989\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.55306486501301\n",
      "    rl_1: 105.53150939706772\n",
      "  time_since_restore: 23244.396951198578\n",
      "  time_this_iter_s: 23.94727396965027\n",
      "  time_total_s: 23244.396951198578\n",
      "  timestamp: 1550816692\n",
      "  timesteps_since_restore: 9890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9890000\n",
      "  training_iteration: 989\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23244 s, 989 iter, 9890000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-25-16\n",
      "  done: false\n",
      "  episode_len_mean: 87.79824561403508\n",
      "  episode_reward_max: 220.51451930363538\n",
      "  episode_reward_mean: 173.54931023102603\n",
      "  episode_reward_min: 140.49670728590198\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 105770\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3119.179\n",
      "    load_time_ms: 2.374\n",
      "    num_steps_sampled: 9900000\n",
      "    num_steps_trained: 9900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.7044300436973572\n",
      "      kl: 0.0344371497631073\n",
      "      policy_loss: 0.0049403090961277485\n",
      "      total_loss: 1.086039662361145\n",
      "      vf_explained_var: 0.997564435005188\n",
      "      vf_loss: 1.0810993909835815\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.43127793073654175\n",
      "      kl: 0.015622413717210293\n",
      "      policy_loss: 0.005218385718762875\n",
      "      total_loss: 0.8656336665153503\n",
      "      vf_explained_var: 0.9990838766098022\n",
      "      vf_loss: 0.8604152202606201\n",
      "    sample_time_ms: 20327.564\n",
      "    update_time_ms: 7.764\n",
      "  iterations_since_restore: 990\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 69.35099780545355\n",
      "    rl_1: 104.19831242557251\n",
      "  time_since_restore: 23267.957357883453\n",
      "  time_this_iter_s: 23.56040668487549\n",
      "  time_total_s: 23267.957357883453\n",
      "  timestamp: 1550816716\n",
      "  timesteps_since_restore: 9900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9900000\n",
      "  training_iteration: 990\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23267 s, 990 iter, 9900000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-25-40\n",
      "  done: false\n",
      "  episode_len_mean: 87.80701754385964\n",
      "  episode_reward_max: 216.13807611975776\n",
      "  episode_reward_mean: 166.810401126216\n",
      "  episode_reward_min: -154.02958994813326\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 105884\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3118.783\n",
      "    load_time_ms: 2.399\n",
      "    num_steps_sampled: 9910000\n",
      "    num_steps_trained: 9910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6529290080070496\n",
      "      kl: 0.020502613857388496\n",
      "      policy_loss: -0.0030870353803038597\n",
      "      total_loss: 13.847663879394531\n",
      "      vf_explained_var: 0.9746983051300049\n",
      "      vf_loss: 13.850753784179688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.40143224596977234\n",
      "      kl: 0.04811765253543854\n",
      "      policy_loss: -0.002099381759762764\n",
      "      total_loss: 25.223608016967773\n",
      "      vf_explained_var: 0.9757412672042847\n",
      "      vf_loss: 25.225709915161133\n",
      "    sample_time_ms: 20362.649\n",
      "    update_time_ms: 7.943\n",
      "  iterations_since_restore: 991\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.88966420400895\n",
      "    rl_1: 100.92073692220704\n",
      "  time_since_restore: 23291.620277404785\n",
      "  time_this_iter_s: 23.662919521331787\n",
      "  time_total_s: 23291.620277404785\n",
      "  timestamp: 1550816740\n",
      "  timesteps_since_restore: 9910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9910000\n",
      "  training_iteration: 991\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23291 s, 991 iter, 9910000 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-26-04\n",
      "  done: false\n",
      "  episode_len_mean: 88.46017699115045\n",
      "  episode_reward_max: 222.44451628752893\n",
      "  episode_reward_mean: 168.52203997837827\n",
      "  episode_reward_min: -150.4770592384404\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 105997\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3121.242\n",
      "    load_time_ms: 2.45\n",
      "    num_steps_sampled: 9920000\n",
      "    num_steps_trained: 9920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6501637697219849\n",
      "      kl: 0.02941332384943962\n",
      "      policy_loss: -0.0060664573684334755\n",
      "      total_loss: 20.94257926940918\n",
      "      vf_explained_var: 0.9611726403236389\n",
      "      vf_loss: 20.94864845275879\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3399398624897003\n",
      "      kl: 0.01415269635617733\n",
      "      policy_loss: -0.001837593619711697\n",
      "      total_loss: 36.401214599609375\n",
      "      vf_explained_var: 0.965259850025177\n",
      "      vf_loss: 36.40305709838867\n",
      "    sample_time_ms: 20412.856\n",
      "    update_time_ms: 8.113\n",
      "  iterations_since_restore: 992\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.43255790780078\n",
      "    rl_1: 103.08948207057745\n",
      "  time_since_restore: 23315.43489074707\n",
      "  time_this_iter_s: 23.814613342285156\n",
      "  time_total_s: 23315.43489074707\n",
      "  timestamp: 1550816764\n",
      "  timesteps_since_restore: 9920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9920000\n",
      "  training_iteration: 992\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23315 s, 992 iter, 9920000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-26-27\n",
      "  done: false\n",
      "  episode_len_mean: 88.1858407079646\n",
      "  episode_reward_max: 220.7265955695936\n",
      "  episode_reward_mean: 171.2260349164686\n",
      "  episode_reward_min: -148.39933013454356\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 106110\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3119.97\n",
      "    load_time_ms: 2.435\n",
      "    num_steps_sampled: 9930000\n",
      "    num_steps_trained: 9930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.693457305431366\n",
      "      kl: 0.022124115377664566\n",
      "      policy_loss: -0.0002359816717216745\n",
      "      total_loss: 16.62830924987793\n",
      "      vf_explained_var: 0.9717564582824707\n",
      "      vf_loss: 16.628543853759766\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3371753394603729\n",
      "      kl: 0.01907840371131897\n",
      "      policy_loss: -0.005808583460748196\n",
      "      total_loss: 29.27692413330078\n",
      "      vf_explained_var: 0.9726427793502808\n",
      "      vf_loss: 29.282730102539062\n",
      "    sample_time_ms: 20430.486\n",
      "    update_time_ms: 7.945\n",
      "  iterations_since_restore: 993\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.60609699911537\n",
      "    rl_1: 103.61993791735323\n",
      "  time_since_restore: 23338.84804725647\n",
      "  time_this_iter_s: 23.413156509399414\n",
      "  time_total_s: 23338.84804725647\n",
      "  timestamp: 1550816787\n",
      "  timesteps_since_restore: 9930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9930000\n",
      "  training_iteration: 993\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23338 s, 993 iter, 9930000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-26-51\n",
      "  done: false\n",
      "  episode_len_mean: 89.3125\n",
      "  episode_reward_max: 219.0023029709487\n",
      "  episode_reward_mean: 171.99626707561103\n",
      "  episode_reward_min: 136.31454052027485\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 106222\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3137.593\n",
      "    load_time_ms: 2.4\n",
      "    num_steps_sampled: 9940000\n",
      "    num_steps_trained: 9940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6407942771911621\n",
      "      kl: 0.051428716629743576\n",
      "      policy_loss: 0.004846872296184301\n",
      "      total_loss: 1.2411469221115112\n",
      "      vf_explained_var: 0.9971502423286438\n",
      "      vf_loss: 1.236299991607666\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.259776771068573\n",
      "      kl: 0.016409857198596\n",
      "      policy_loss: 0.00286336662247777\n",
      "      total_loss: 1.066909670829773\n",
      "      vf_explained_var: 0.9988874793052673\n",
      "      vf_loss: 1.0640462636947632\n",
      "    sample_time_ms: 20429.028\n",
      "    update_time_ms: 8.091\n",
      "  iterations_since_restore: 994\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.11492212684769\n",
      "    rl_1: 104.8813449487633\n",
      "  time_since_restore: 23362.778455018997\n",
      "  time_this_iter_s: 23.930407762527466\n",
      "  time_total_s: 23362.778455018997\n",
      "  timestamp: 1550816811\n",
      "  timesteps_since_restore: 9940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9940000\n",
      "  training_iteration: 994\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23362 s, 994 iter, 9940000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-27-14\n",
      "  done: false\n",
      "  episode_len_mean: 88.54867256637168\n",
      "  episode_reward_max: 218.4291441224203\n",
      "  episode_reward_mean: 172.25061893527476\n",
      "  episode_reward_min: 138.9276787990522\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 106335\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3138.589\n",
      "    load_time_ms: 2.409\n",
      "    num_steps_sampled: 9950000\n",
      "    num_steps_trained: 9950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6861922144889832\n",
      "      kl: 0.026977520436048508\n",
      "      policy_loss: 0.0005824969848617911\n",
      "      total_loss: 1.2648481130599976\n",
      "      vf_explained_var: 0.997155487537384\n",
      "      vf_loss: 1.264266014099121\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.33292439579963684\n",
      "      kl: 0.01677277311682701\n",
      "      policy_loss: 0.0032765890937298536\n",
      "      total_loss: 1.131717324256897\n",
      "      vf_explained_var: 0.9988061785697937\n",
      "      vf_loss: 1.1284408569335938\n",
      "    sample_time_ms: 20332.602\n",
      "    update_time_ms: 7.753\n",
      "  iterations_since_restore: 995\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.72730350757085\n",
      "    rl_1: 104.52331542770388\n",
      "  time_since_restore: 23385.733770608902\n",
      "  time_this_iter_s: 22.955315589904785\n",
      "  time_total_s: 23385.733770608902\n",
      "  timestamp: 1550816834\n",
      "  timesteps_since_restore: 9950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9950000\n",
      "  training_iteration: 995\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23385 s, 995 iter, 9950000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-27-37\n",
      "  done: false\n",
      "  episode_len_mean: 90.16216216216216\n",
      "  episode_reward_max: 216.43125114627435\n",
      "  episode_reward_mean: 172.82076385416661\n",
      "  episode_reward_min: 138.56776402566734\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 106446\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.999\n",
      "    load_time_ms: 2.459\n",
      "    num_steps_sampled: 9960000\n",
      "    num_steps_trained: 9960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6543017029762268\n",
      "      kl: 0.026372913271188736\n",
      "      policy_loss: 0.0034087079111486673\n",
      "      total_loss: 0.922622561454773\n",
      "      vf_explained_var: 0.997816801071167\n",
      "      vf_loss: 0.9192138910293579\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.31675174832344055\n",
      "      kl: 0.028902143239974976\n",
      "      policy_loss: 0.007306172512471676\n",
      "      total_loss: 0.8313753008842468\n",
      "      vf_explained_var: 0.9991709589958191\n",
      "      vf_loss: 0.8240692019462585\n",
      "    sample_time_ms: 20323.561\n",
      "    update_time_ms: 7.637\n",
      "  iterations_since_restore: 996\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.61473321900206\n",
      "    rl_1: 107.20603063516458\n",
      "  time_since_restore: 23408.778841018677\n",
      "  time_this_iter_s: 23.04507040977478\n",
      "  time_total_s: 23408.778841018677\n",
      "  timestamp: 1550816857\n",
      "  timesteps_since_restore: 9960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9960000\n",
      "  training_iteration: 996\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23408 s, 996 iter, 9960000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-28-00\n",
      "  done: false\n",
      "  episode_len_mean: 89.04464285714286\n",
      "  episode_reward_max: 219.26473070098413\n",
      "  episode_reward_mean: 172.58642341564197\n",
      "  episode_reward_min: 136.377583836241\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 106558\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3135.695\n",
      "    load_time_ms: 2.462\n",
      "    num_steps_sampled: 9970000\n",
      "    num_steps_trained: 9970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6653083562850952\n",
      "      kl: 0.02454107627272606\n",
      "      policy_loss: -0.0006705865380354226\n",
      "      total_loss: 1.0551989078521729\n",
      "      vf_explained_var: 0.9976544380187988\n",
      "      vf_loss: 1.0558695793151855\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3035517632961273\n",
      "      kl: 0.012061516754329205\n",
      "      policy_loss: -0.00013231638877186924\n",
      "      total_loss: 0.9709405303001404\n",
      "      vf_explained_var: 0.9989808797836304\n",
      "      vf_loss: 0.9710727334022522\n",
      "    sample_time_ms: 20295.289\n",
      "    update_time_ms: 7.455\n",
      "  iterations_since_restore: 997\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.65458480033753\n",
      "    rl_1: 104.93183861530441\n",
      "  time_since_restore: 23431.866756677628\n",
      "  time_this_iter_s: 23.087915658950806\n",
      "  time_total_s: 23431.866756677628\n",
      "  timestamp: 1550816880\n",
      "  timesteps_since_restore: 9970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9970000\n",
      "  training_iteration: 997\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23431 s, 997 iter, 9970000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-28-24\n",
      "  done: false\n",
      "  episode_len_mean: 88.96460176991151\n",
      "  episode_reward_max: 220.62949934357536\n",
      "  episode_reward_mean: 173.17375608286633\n",
      "  episode_reward_min: 138.28830795237343\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 106671\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.877\n",
      "    load_time_ms: 2.547\n",
      "    num_steps_sampled: 9980000\n",
      "    num_steps_trained: 9980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6833663582801819\n",
      "      kl: 0.031143631786108017\n",
      "      policy_loss: -0.0014128910843282938\n",
      "      total_loss: 0.9294028878211975\n",
      "      vf_explained_var: 0.9979057312011719\n",
      "      vf_loss: 0.9308158755302429\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.3136354088783264\n",
      "      kl: 0.025846486911177635\n",
      "      policy_loss: 0.0052941348403692245\n",
      "      total_loss: 0.9295098781585693\n",
      "      vf_explained_var: 0.9990164041519165\n",
      "      vf_loss: 0.9242156744003296\n",
      "    sample_time_ms: 20339.933\n",
      "    update_time_ms: 7.589\n",
      "  iterations_since_restore: 998\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 68.18502978933327\n",
      "    rl_1: 104.98872629353306\n",
      "  time_since_restore: 23455.48338699341\n",
      "  time_this_iter_s: 23.61663031578064\n",
      "  time_total_s: 23455.48338699341\n",
      "  timestamp: 1550816904\n",
      "  timesteps_since_restore: 9980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9980000\n",
      "  training_iteration: 998\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23455 s, 998 iter, 9980000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-28-48\n",
      "  done: false\n",
      "  episode_len_mean: 89.14285714285714\n",
      "  episode_reward_max: 221.18651797051615\n",
      "  episode_reward_mean: 172.1765239288805\n",
      "  episode_reward_min: 138.12275635378512\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 106783\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3134.865\n",
      "    load_time_ms: 2.551\n",
      "    num_steps_sampled: 9990000\n",
      "    num_steps_trained: 9990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6740927696228027\n",
      "      kl: 0.03232496231794357\n",
      "      policy_loss: 0.0007204855792224407\n",
      "      total_loss: 1.0431305170059204\n",
      "      vf_explained_var: 0.9975396990776062\n",
      "      vf_loss: 1.042409896850586\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.30029603838920593\n",
      "      kl: 0.006378035992383957\n",
      "      policy_loss: 0.0010407964000478387\n",
      "      total_loss: 0.8828417658805847\n",
      "      vf_explained_var: 0.9990890622138977\n",
      "      vf_loss: 0.8818009495735168\n",
      "    sample_time_ms: 20319.807\n",
      "    update_time_ms: 7.205\n",
      "  iterations_since_restore: 999\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.97729111248306\n",
      "    rl_1: 105.19923281639743\n",
      "  time_since_restore: 23479.22245979309\n",
      "  time_this_iter_s: 23.739072799682617\n",
      "  time_total_s: 23479.22245979309\n",
      "  timestamp: 1550816928\n",
      "  timesteps_since_restore: 9990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 9990000\n",
      "  training_iteration: 999\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=9795], 23479 s, 999 iter, 9990000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-02-22_07-29-11\n",
      "  done: true\n",
      "  episode_len_mean: 88.12389380530973\n",
      "  episode_reward_max: 216.21244233528506\n",
      "  episode_reward_mean: 170.08159574685854\n",
      "  episode_reward_min: -151.20701492835047\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 106896\n",
      "  experiment_id: e9a0a10898fb45ea8bdfb380c7f1a8bb\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3132.1\n",
      "    load_time_ms: 2.481\n",
      "    num_steps_sampled: 10000000\n",
      "    num_steps_trained: 10000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.6723099946975708\n",
      "      kl: 0.01799936592578888\n",
      "      policy_loss: -0.005848770495504141\n",
      "      total_loss: 20.13812255859375\n",
      "      vf_explained_var: 0.9659734964370728\n",
      "      vf_loss: 20.143970489501953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: -0.17793896794319153\n",
      "      kl: 0.04294731840491295\n",
      "      policy_loss: -0.004171544220298529\n",
      "      total_loss: 37.48474884033203\n",
      "      vf_explained_var: 0.9649003744125366\n",
      "      vf_loss: 37.48892593383789\n",
      "    sample_time_ms: 20286.795\n",
      "    update_time_ms: 6.901\n",
      "  iterations_since_restore: 1000\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 9795\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.74295791158642\n",
      "    rl_1: 102.33863783527215\n",
      "  time_since_restore: 23502.42000222206\n",
      "  time_this_iter_s: 23.197542428970337\n",
      "  time_total_s: 23502.42000222206\n",
      "  timestamp: 1550816951\n",
      "  timesteps_since_restore: 10000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 10000000\n",
      "  training_iteration: 1000\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "TERMINATED trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tTERMINATED [pid=9795], 23502 s, 1000 iter, 10000000 ts, 170 rew\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 9.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "TERMINATED trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tTERMINATED [pid=9795], 23502 s, 1000 iter, 10000000 ts, 170 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,  # RL algorithm to run\n",
    "        \"env\": gym_name,  # environment name generated earlier\n",
    "        \"config\": {  # configuration params (must match \"run\" value)\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1000,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow_2)",
   "language": "python",
   "name": "flow_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
