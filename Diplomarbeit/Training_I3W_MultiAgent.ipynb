{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING I3W\n",
    "\n",
    "\n",
    "# A) Create Envorinment, Vehicles etc\n",
    "\n",
    "### General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scenarios:\n",
      "['Scenario', 'BayBridgeScenario', 'BayBridgeTollScenario', 'BottleneckScenario', 'Figure8Scenario', 'SimpleGridScenario', 'HighwayScenario', 'LoopScenario', 'MergeScenario', 'TwoLoopsOneMergingScenario', 'MultiLoopScenario', 'IntersectionScenarioTW']\n",
      "\n",
      "Available environments:\n",
      "['MultiEnv', 'MultiAgentAccelEnv', 'MultiWaveAttenuationPOEnv', 'MultiAgentIntersectionEnv', 'MultiAgentTeamSpiritIntersectionEnv']\n"
     ]
    }
   ],
   "source": [
    "# Define horizon as a variable to ensure consistent use across notebook (length of one rollout)\n",
    "HORIZON=103\n",
    "\n",
    "# name of the experiment\n",
    "experiment_name = \"IntersectionExample\"\n",
    "\n",
    "# scenario class\n",
    "import flow.scenarios as scenarios\n",
    "print(\"Available scenarios:\")\n",
    "print(scenarios.__all__)\n",
    "scenario_name = \"IntersectionTWScenario\"\n",
    "\n",
    "# environment class\n",
    "import flow.multiagent_envs as flowenvs\n",
    "print(\"\\nAvailable environments:\")\n",
    "print(flowenvs.__all__)\n",
    "env_name = \"MultiAgentIntersectionEnv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "from flow.scenarios.intersection import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "additionalNetParams = {\n",
    "            \"edge_length\": 40,\n",
    "            \"lanes\": 1,\n",
    "            \"speed_limit\": 30\n",
    "        }\n",
    "\n",
    "net_params = NetParams( no_internal_links=False,                  #default: True   !! damit Kreuzungen nicht Ã¼berspr. werden\n",
    "                        inflows=None,                             #default: None\n",
    "                        osm_path=None,                            #default: None\n",
    "                        netfile=None,                             #default: None\n",
    "                        additional_params=additionalNetParams     #default: None   !!\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InitialConfig Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "\n",
    "initial_config = InitialConfig( shuffle=True,                            #default: False         !!\n",
    "                                spacing=\"custom\",                        #default: \"uniform\"     !!\n",
    "                                min_gap=10,                              #default: 0\n",
    "                                perturbation=29.99,                      #default: 0.0            !!        \n",
    "                                x0=0,                                    #default: 0\n",
    "                                bunching=0,                              #default: 0\n",
    "                                lanes_distribution=float(\"inf\"),         #default: float(\"inf\")\n",
    "                                edges_distribution=\"all\",                #default: \"all\"\n",
    "                                additional_params=None )                 #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams( port = None,                  #default: None\n",
    "                          sim_step=0.1,                 #default: 0.1\n",
    "                          emission_path=None,           #default: None\n",
    "                          lateral_resolution=None,      #default: None\n",
    "                          no_step_log=True,             #default: True\n",
    "                          render=False,                 #default: False\n",
    "                          save_render=False,            #default: False\n",
    "                          sight_radius=25,              #default: 25\n",
    "                          show_radius=False,            #default: False\n",
    "                          pxpm=2,                       #default: 2\n",
    "                          overtake_right=False,         #default: False    \n",
    "                          seed=None,                    #default: None\n",
    "                          restart_instance=False,       #default: False\n",
    "                          print_warnings=True,          #default: True\n",
    "                          teleport_time=-1,             #default: -1\n",
    "                          num_clients=1,                #default: 1\n",
    "                          sumo_binary=None )            #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "additionalEnvParams = {\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 3,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 3,\n",
    "        \"target_velocity\": 30\n",
    "    }\n",
    "\n",
    "env_params = EnvParams( additional_params=additionalEnvParams, #default: None    !!\n",
    "                        horizon=HORIZON,                       #default: 500     !!\n",
    "                        warmup_steps=0,                        #default: 0       \n",
    "                        sims_per_step=1,                       #default: 1\n",
    "                        evaluate=False )                       #default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# import vehicles dynamics models\n",
    "#from flow.controllers import SumoCarFollowingController\n",
    "from flow.controllers import ContinuousRouter\n",
    "#from flow.controllers.lane_change_controllers import SumoLaneChangeController\n",
    "from flow.controllers.lane_change_controllers import StaticLaneChanger\n",
    "from flow.controllers import RLController\n",
    "from flow.core.params import SumoLaneChangeParams\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "from random import *\n",
    "\n",
    "vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RL-Agent controlled vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car following parameters, default: None\n",
    "cf_parameter = SumoCarFollowingParams(\n",
    "                speed_mode=\"aggressive\")\n",
    "# lane change parameters, default: None\n",
    "lc_parameter =  None\n",
    "\n",
    "vehicles.add( # name of the vehicle\n",
    "                veh_id = \"rl\",\n",
    "              # acceleration controller, default: (SumoCarFollowingController, {})\n",
    "                acceleration_controller=(RLController, {}),\n",
    "              # lane_change_controller, default: (SumoLaneChangeController, {})\n",
    "                lane_change_controller=(StaticLaneChanger,{}),\n",
    "              # routing controller, default: None\n",
    "                routing_controller=(ContinuousRouter, {}),\n",
    "              # initial speed, default: 0\n",
    "                initial_speed=0,\n",
    "              # number of vehicles, default: 1 \n",
    "                num_vehicles=2,\n",
    "                \n",
    "                car_following_params=cf_parameter\n",
    "              # speed mode, default: \"right_of_way\"\n",
    "                #speed_mode=\"aggressive\",\n",
    "              # lane change mode, default: \"no_lat_collide\"\n",
    "                #lane_change_mode=\"aggressive\", \n",
    "              # car following parameter, default: None\n",
    "                #sumo_car_following_params=cf_parameter,\n",
    "              # lane change parameter, default: None\n",
    "                #sumo_lc_params=lc_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict( # name of the experiment\n",
    "                      exp_tag=experiment_name,\n",
    "                    # name of the flow environment the experiment is running on\n",
    "                      env_name=env_name,\n",
    "                    # name of the scenario class the experiment uses\n",
    "                      scenario=scenario_name,\n",
    "                    # simulator that is used by the experiment\n",
    "                      simulator='traci',\n",
    "                    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "                      sim=sumo_params,\n",
    "                    # environment related parameters (see flow.core.params.EnvParams)\n",
    "                      env=env_params,\n",
    "                    # network-related parameters (see flow.core.params.NetParams and\n",
    "                    # the scenario's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "                      net=net_params,\n",
    "                    # vehicles to be placed in the network at the start of a rollout \n",
    "                    # (see flow.core.vehicles.Vehicles)\n",
    "                      veh=vehicles,\n",
    "                   # (optional) parameters affecting the positioning of vehicles upon \n",
    "                   # initialization/reset (see flow.core.params.InitialConfig)\n",
    "                      initial=initial_config\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-03-11_15-39-55_18336/logs.\n",
      "Waiting for redis server at 127.0.0.1:43604 to respond...\n",
      "Waiting for redis server at 127.0.0.1:54079 to respond...\n",
      "Starting the Plasma object store with 6.5546633210000005 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=30f0302ce2f7f65d3a1489cfbc57b40dfb05c4335bfcd93a\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.16.123.117',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-03-11_15-39-55_18336/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-03-11_15-39-55_18336/sockets/raylet'],\n",
       " 'redis_address': '172.16.123.117:43604',\n",
       " 'webui_url': 'http://localhost:8889/notebooks/ray_ui.ipynb?token=30f0302ce2f7f65d3a1489cfbc57b40dfb05c4335bfcd93a'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "\n",
    "ray.init(redirect_output=True, num_cpus=N_CPUS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [64, 32]})  # size of hidden layers in network\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "#config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "#config[\"sample_batch_size\"] = config[\"train_batch_size\"]/config[\"num_workers\"] # 200 default, trotzdem zu hoch?\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 58479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.849015537022204\n",
      "25.000549626928986\n"
     ]
    }
   ],
   "source": [
    "# multi agent policy mapping\n",
    "test_env = create_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "\n",
    "def gen_policy():\n",
    "    return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "# Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "policy_graphs = {'rl_0': gen_policy(), 'rl_1': gen_policy()}\n",
    "    \n",
    "def policy_mapping_fn(agent_id):\n",
    "    return agent_id\n",
    "\n",
    "config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn)\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "\n",
      "Created LogSyncer for /home/thorsten/ray_results/IntersectionExample/PPO_MultiAgentIntersectionEnv-v0_0_2019-03-11_15-39-58ssrs95mm -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-40-41\n",
      "  done: false\n",
      "  episode_len_mean: 103.0\n",
      "  episode_reward_max: 20.473525119472047\n",
      "  episode_reward_mean: 10.040975470023351\n",
      "  episode_reward_min: 4.446865024276959\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2139.944\n",
      "    load_time_ms: 150.256\n",
      "    num_steps_sampled: 2200\n",
      "    num_steps_trained: 2200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4206384420394897\n",
      "      kl: 0.0005904324352741241\n",
      "      policy_loss: -0.0017360685160383582\n",
      "      total_loss: 2.081641435623169\n",
      "      vf_explained_var: 0.017633242532610893\n",
      "      vf_loss: 2.083259105682373\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.425036907196045\n",
      "      kl: 0.0008647365029901266\n",
      "      policy_loss: -0.007769505959004164\n",
      "      total_loss: 1.6638065576553345\n",
      "      vf_explained_var: 0.028704022988677025\n",
      "      vf_loss: 1.6714030504226685\n",
      "    sample_time_ms: 6659.411\n",
      "    update_time_ms: 1812.473\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 5.177939108125488\n",
      "    rl_1: 4.863036361897863\n",
      "  time_since_restore: 10.89127516746521\n",
      "  time_this_iter_s: 10.89127516746521\n",
      "  time_total_s: 10.89127516746521\n",
      "  timestamp: 1552315241\n",
      "  timesteps_since_restore: 2200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 2200\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 10 s, 1 iter, 2200 ts, 10 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-40-47\n",
      "  done: false\n",
      "  episode_len_mean: 103.0\n",
      "  episode_reward_max: 20.473525119472047\n",
      "  episode_reward_mean: 9.246309025083324\n",
      "  episode_reward_min: 2.5088519950107915\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 42\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1574.432\n",
      "    load_time_ms: 76.417\n",
      "    num_steps_sampled: 4400\n",
      "    num_steps_trained: 4400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4235475063323975\n",
      "      kl: 0.0005411548190750182\n",
      "      policy_loss: -0.002789067104458809\n",
      "      total_loss: 1.515466332435608\n",
      "      vf_explained_var: 0.07004907727241516\n",
      "      vf_loss: 1.518201231956482\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4200468063354492\n",
      "      kl: 0.0008357089827768505\n",
      "      policy_loss: -0.00885737407952547\n",
      "      total_loss: 1.3082363605499268\n",
      "      vf_explained_var: 0.1240660697221756\n",
      "      vf_loss: 1.3170102834701538\n",
      "    sample_time_ms: 6190.475\n",
      "    update_time_ms: 911.276\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 4.868514862999906\n",
      "    rl_1: 4.377794162083417\n",
      "  time_since_restore: 17.643560886383057\n",
      "  time_this_iter_s: 6.752285718917847\n",
      "  time_total_s: 17.643560886383057\n",
      "  timestamp: 1552315247\n",
      "  timesteps_since_restore: 4400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 4400\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 17 s, 2 iter, 4400 ts, 9.25 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-40-54\n",
      "  done: false\n",
      "  episode_len_mean: 103.0\n",
      "  episode_reward_max: 27.587906167502293\n",
      "  episode_reward_mean: 10.398013676005604\n",
      "  episode_reward_min: 2.5088519950107915\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 64\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1301.758\n",
      "    load_time_ms: 51.826\n",
      "    num_steps_sampled: 6600\n",
      "    num_steps_trained: 6600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4192205667495728\n",
      "      kl: 0.0017009753501042724\n",
      "      policy_loss: -0.0072219460271298885\n",
      "      total_loss: 2.614722967147827\n",
      "      vf_explained_var: 0.15532903373241425\n",
      "      vf_loss: 2.6218600273132324\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.418267846107483\n",
      "      kl: 0.0018744569970294833\n",
      "      policy_loss: -0.005475211888551712\n",
      "      total_loss: 3.7039902210235596\n",
      "      vf_explained_var: 0.2527581751346588\n",
      "      vf_loss: 3.709371566772461\n",
      "    sample_time_ms: 6159.069\n",
      "    update_time_ms: 609.504\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 5.14550613310848\n",
      "    rl_1: 5.252507542897126\n",
      "  time_since_restore: 24.51634407043457\n",
      "  time_this_iter_s: 6.872783184051514\n",
      "  time_total_s: 24.51634407043457\n",
      "  timestamp: 1552315254\n",
      "  timesteps_since_restore: 6600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 6600\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 24 s, 3 iter, 6600 ts, 10.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-41-01\n",
      "  done: false\n",
      "  episode_len_mean: 103.0\n",
      "  episode_reward_max: 27.587906167502293\n",
      "  episode_reward_mean: 10.841822933393281\n",
      "  episode_reward_min: 2.5088519950107915\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 84\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1187.462\n",
      "    load_time_ms: 39.579\n",
      "    num_steps_sampled: 8800\n",
      "    num_steps_trained: 8800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.426375389099121\n",
      "      kl: 0.000641524326056242\n",
      "      policy_loss: -0.0014881337992846966\n",
      "      total_loss: 4.082107067108154\n",
      "      vf_explained_var: 0.22349925339221954\n",
      "      vf_loss: 4.0835795402526855\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4203195571899414\n",
      "      kl: 0.0007470595883205533\n",
      "      policy_loss: -0.0008107358007691801\n",
      "      total_loss: 2.6592020988464355\n",
      "      vf_explained_var: 0.32933127880096436\n",
      "      vf_loss: 2.659994125366211\n",
      "    sample_time_ms: 6120.504\n",
      "    update_time_ms: 458.616\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 5.504129411736757\n",
      "    rl_1: 5.337693521656526\n",
      "  time_since_restore: 31.386723279953003\n",
      "  time_this_iter_s: 6.870379209518433\n",
      "  time_total_s: 31.386723279953003\n",
      "  timestamp: 1552315261\n",
      "  timesteps_since_restore: 8800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 8800\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 31 s, 4 iter, 8800 ts, 10.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-41-10\n",
      "  done: false\n",
      "  episode_len_mean: 103.0\n",
      "  episode_reward_max: 33.8636255653574\n",
      "  episode_reward_mean: 10.943074297584197\n",
      "  episode_reward_min: 2.5088519950107915\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 106\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1142.806\n",
      "    load_time_ms: 32.316\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4244626760482788\n",
      "      kl: 0.0010611612815409899\n",
      "      policy_loss: -0.003908729180693626\n",
      "      total_loss: 3.588604211807251\n",
      "      vf_explained_var: 0.33883801102638245\n",
      "      vf_loss: 3.5924999713897705\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4246826171875\n",
      "      kl: 0.0010549579747021198\n",
      "      policy_loss: -0.005975959822535515\n",
      "      total_loss: 3.2014319896698\n",
      "      vf_explained_var: 0.42145609855651855\n",
      "      vf_loss: 3.207394599914551\n",
      "    sample_time_ms: 6471.834\n",
      "    update_time_ms: 368.723\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 5.548206475337231\n",
      "    rl_1: 5.394867822246965\n",
      "  time_since_restore: 40.253467082977295\n",
      "  time_this_iter_s: 8.866743803024292\n",
      "  time_total_s: 40.253467082977295\n",
      "  timestamp: 1552315270\n",
      "  timesteps_since_restore: 11000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 5\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 40 s, 5 iter, 11000 ts, 10.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-41-17\n",
      "  done: false\n",
      "  episode_len_mean: 103.0\n",
      "  episode_reward_max: 34.60625055541311\n",
      "  episode_reward_mean: 12.050355552273963\n",
      "  episode_reward_min: 2.69685118657009\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 128\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1087.486\n",
      "    load_time_ms: 27.249\n",
      "    num_steps_sampled: 13200\n",
      "    num_steps_trained: 13200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4203909635543823\n",
      "      kl: 0.0008975878590717912\n",
      "      policy_loss: -0.003517513396218419\n",
      "      total_loss: 4.247462749481201\n",
      "      vf_explained_var: 0.446717768907547\n",
      "      vf_loss: 4.250974655151367\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.42567777633667\n",
      "      kl: 0.0007857457967475057\n",
      "      policy_loss: -0.0047262501902878284\n",
      "      total_loss: 3.704587459564209\n",
      "      vf_explained_var: 0.40742209553718567\n",
      "      vf_loss: 3.709308385848999\n",
      "    sample_time_ms: 6343.879\n",
      "    update_time_ms: 308.71\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 6.251212130212105\n",
      "    rl_1: 5.799143422061857\n",
      "  time_since_restore: 46.79167604446411\n",
      "  time_this_iter_s: 6.538208961486816\n",
      "  time_total_s: 46.79167604446411\n",
      "  timestamp: 1552315277\n",
      "  timesteps_since_restore: 13200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 13200\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 46 s, 6 iter, 13200 ts, 12.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-41-26\n",
      "  done: false\n",
      "  episode_len_mean: 103.0\n",
      "  episode_reward_max: 36.140420041262225\n",
      "  episode_reward_mean: 13.33871600878711\n",
      "  episode_reward_min: 2.6446190952897837\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 148\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1110.622\n",
      "    load_time_ms: 24.05\n",
      "    num_steps_sampled: 15400\n",
      "    num_steps_trained: 15400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4217857122421265\n",
      "      kl: 0.0007889208500273526\n",
      "      policy_loss: -0.0018204827792942524\n",
      "      total_loss: 4.35127067565918\n",
      "      vf_explained_var: 0.5140545964241028\n",
      "      vf_loss: 4.35308837890625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4110350608825684\n",
      "      kl: 0.008795827627182007\n",
      "      policy_loss: -0.015250377357006073\n",
      "      total_loss: 8.030129432678223\n",
      "      vf_explained_var: 0.4138002097606659\n",
      "      vf_loss: 8.045352935791016\n",
      "    sample_time_ms: 6571.628\n",
      "    update_time_ms: 266.472\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 6.638420281468026\n",
      "    rl_1: 6.700295727319086\n",
      "  time_since_restore: 56.010693311691284\n",
      "  time_this_iter_s: 9.219017267227173\n",
      "  time_total_s: 56.010693311691284\n",
      "  timestamp: 1552315286\n",
      "  timesteps_since_restore: 15400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 15400\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 56 s, 7 iter, 15400 ts, 13.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-41-33\n",
      "  done: false\n",
      "  episode_len_mean: 103.0\n",
      "  episode_reward_max: 43.67223018225759\n",
      "  episode_reward_mean: 14.419530808084408\n",
      "  episode_reward_min: 2.4599815605381488\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 170\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1091.874\n",
      "    load_time_ms: 21.295\n",
      "    num_steps_sampled: 17600\n",
      "    num_steps_trained: 17600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4252063035964966\n",
      "      kl: 0.00035493113682605326\n",
      "      policy_loss: -0.0069091045297682285\n",
      "      total_loss: 3.635831832885742\n",
      "      vf_explained_var: 0.6517190337181091\n",
      "      vf_loss: 3.642740249633789\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4207968711853027\n",
      "      kl: 0.002028420101851225\n",
      "      policy_loss: -0.003570539178326726\n",
      "      total_loss: 8.185934066772461\n",
      "      vf_explained_var: 0.4977821409702301\n",
      "      vf_loss: 8.18950080871582\n",
      "    sample_time_ms: 6537.499\n",
      "    update_time_ms: 234.492\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 7.073119448408122\n",
      "    rl_1: 7.346411359676287\n",
      "  time_since_restore: 63.295047760009766\n",
      "  time_this_iter_s: 7.2843544483184814\n",
      "  time_total_s: 63.295047760009766\n",
      "  timestamp: 1552315293\n",
      "  timesteps_since_restore: 17600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 17600\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 63 s, 8 iter, 17600 ts, 14.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-41-40\n",
      "  done: false\n",
      "  episode_len_mean: 103.0\n",
      "  episode_reward_max: 43.67223018225759\n",
      "  episode_reward_mean: 14.587481909235073\n",
      "  episode_reward_min: 2.4599815605381488\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 192\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1071.981\n",
      "    load_time_ms: 19.16\n",
      "    num_steps_sampled: 19800\n",
      "    num_steps_trained: 19800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4236435890197754\n",
      "      kl: 0.0028319754637777805\n",
      "      policy_loss: -0.007997063919901848\n",
      "      total_loss: 3.8843631744384766\n",
      "      vf_explained_var: 0.5246787071228027\n",
      "      vf_loss: 3.8923583030700684\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4288043975830078\n",
      "      kl: 0.0019430480897426605\n",
      "      policy_loss: -0.008219851180911064\n",
      "      total_loss: 5.068671703338623\n",
      "      vf_explained_var: 0.5495071411132812\n",
      "      vf_loss: 5.076890468597412\n",
      "    sample_time_ms: 6474.941\n",
      "    update_time_ms: 209.494\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 7.110145182910915\n",
      "    rl_1: 7.4773367263241575\n",
      "  time_since_restore: 70.20367288589478\n",
      "  time_this_iter_s: 6.90862512588501\n",
      "  time_total_s: 70.20367288589478\n",
      "  timestamp: 1552315300\n",
      "  timesteps_since_restore: 19800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 19800\n",
      "  training_iteration: 9\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 70 s, 9 iter, 19800 ts, 14.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-41-50\n",
      "  done: false\n",
      "  episode_len_mean: 103.0\n",
      "  episode_reward_max: 47.49019103099962\n",
      "  episode_reward_mean: 15.807334070455195\n",
      "  episode_reward_min: 2.4599815605381488\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 212\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1094.976\n",
      "    load_time_ms: 17.513\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4063326120376587\n",
      "      kl: 0.0051465872675180435\n",
      "      policy_loss: -0.007830937393009663\n",
      "      total_loss: 5.856719017028809\n",
      "      vf_explained_var: 0.4241020083427429\n",
      "      vf_loss: 5.864548206329346\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.425553560256958\n",
      "      kl: 0.002159722149372101\n",
      "      policy_loss: -0.002527463948354125\n",
      "      total_loss: 10.531951904296875\n",
      "      vf_explained_var: 0.5049683451652527\n",
      "      vf_loss: 10.534478187561035\n",
      "    sample_time_ms: 6642.68\n",
      "    update_time_ms: 189.554\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 7.617094674693743\n",
      "    rl_1: 8.190239395761452\n",
      "  time_since_restore: 79.68155455589294\n",
      "  time_this_iter_s: 9.477881669998169\n",
      "  time_total_s: 79.68155455589294\n",
      "  timestamp: 1552315310\n",
      "  timesteps_since_restore: 22000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 79 s, 10 iter, 22000 ts, 15.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-41-58\n",
      "  done: false\n",
      "  episode_len_mean: 103.0\n",
      "  episode_reward_max: 47.49019103099962\n",
      "  episode_reward_mean: 16.90791013894173\n",
      "  episode_reward_min: 2.4599815605381488\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 234\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 999.993\n",
      "    load_time_ms: 2.935\n",
      "    num_steps_sampled: 24200\n",
      "    num_steps_trained: 24200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.412846326828003\n",
      "      kl: 0.006881370674818754\n",
      "      policy_loss: -0.012098778039216995\n",
      "      total_loss: 5.161352634429932\n",
      "      vf_explained_var: 0.3927139341831207\n",
      "      vf_loss: 5.173450469970703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4193952083587646\n",
      "      kl: 0.0020650208462029696\n",
      "      policy_loss: -0.0050638169050216675\n",
      "      total_loss: 9.032858848571777\n",
      "      vf_explained_var: 0.5252642631530762\n",
      "      vf_loss: 9.037921905517578\n",
      "    sample_time_ms: 6709.562\n",
      "    update_time_ms: 9.349\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 7.91709666422865\n",
      "    rl_1: 8.990813474713077\n",
      "  time_since_restore: 88.2297887802124\n",
      "  time_this_iter_s: 8.548234224319458\n",
      "  time_total_s: 88.2297887802124\n",
      "  timestamp: 1552315318\n",
      "  timesteps_since_restore: 24200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 24200\n",
      "  training_iteration: 11\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 88 s, 11 iter, 24200 ts, 16.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-42-08\n",
      "  done: false\n",
      "  episode_len_mean: 102.87\n",
      "  episode_reward_max: 55.37569067973517\n",
      "  episode_reward_mean: 13.561858178703124\n",
      "  episode_reward_min: -158.92797707252288\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 256\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1127.815\n",
      "    load_time_ms: 2.893\n",
      "    num_steps_sampled: 26400\n",
      "    num_steps_trained: 26400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4011545181274414\n",
      "      kl: 0.0004820407775696367\n",
      "      policy_loss: -0.004816994071006775\n",
      "      total_loss: 187.5013427734375\n",
      "      vf_explained_var: 0.03470437601208687\n",
      "      vf_loss: 187.50616455078125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4133671522140503\n",
      "      kl: 0.0012600277550518513\n",
      "      policy_loss: -0.0021087289787828922\n",
      "      total_loss: 195.08482360839844\n",
      "      vf_explained_var: -0.001989995827898383\n",
      "      vf_loss: 195.0869140625\n",
      "    sample_time_ms: 6875.69\n",
      "    update_time_ms: 8.969\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 6.102834784864922\n",
      "    rl_1: 7.4590233938382\n",
      "  time_since_restore: 97.92877221107483\n",
      "  time_this_iter_s: 9.698983430862427\n",
      "  time_total_s: 97.92877221107483\n",
      "  timestamp: 1552315328\n",
      "  timesteps_since_restore: 26400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 26400\n",
      "  training_iteration: 12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 97 s, 12 iter, 26400 ts, 13.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-42-25\n",
      "  done: false\n",
      "  episode_len_mean: 102.87\n",
      "  episode_reward_max: 55.37569067973517\n",
      "  episode_reward_mean: 14.889859075479755\n",
      "  episode_reward_min: -158.92797707252288\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 276\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1248.166\n",
      "    load_time_ms: 2.926\n",
      "    num_steps_sampled: 28600\n",
      "    num_steps_trained: 28600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.406599760055542\n",
      "      kl: 0.0005470230244100094\n",
      "      policy_loss: -0.0032755183055996895\n",
      "      total_loss: 10.241495132446289\n",
      "      vf_explained_var: 0.12061856687068939\n",
      "      vf_loss: 10.244771003723145\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4106838703155518\n",
      "      kl: 0.005786443594843149\n",
      "      policy_loss: -0.008602075278759003\n",
      "      total_loss: 14.31163501739502\n",
      "      vf_explained_var: 0.4283607602119446\n",
      "      vf_loss: 14.320235252380371\n",
      "    sample_time_ms: 7776.552\n",
      "    update_time_ms: 10.588\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 6.900281159569492\n",
      "    rl_1: 7.989577915910262\n",
      "  time_since_restore: 115.03938841819763\n",
      "  time_this_iter_s: 17.110616207122803\n",
      "  time_total_s: 115.03938841819763\n",
      "  timestamp: 1552315345\n",
      "  timesteps_since_restore: 28600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 28600\n",
      "  training_iteration: 13\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 115 s, 13 iter, 28600 ts, 14.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-42-40\n",
      "  done: false\n",
      "  episode_len_mean: 102.77\n",
      "  episode_reward_max: 55.37569067973517\n",
      "  episode_reward_mean: 12.602099049369249\n",
      "  episode_reward_min: -158.92797707252288\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 298\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1290.098\n",
      "    load_time_ms: 2.98\n",
      "    num_steps_sampled: 30800\n",
      "    num_steps_trained: 30800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4247939586639404\n",
      "      kl: 0.0012764902785420418\n",
      "      policy_loss: 0.0001357960281893611\n",
      "      total_loss: 110.79852294921875\n",
      "      vf_explained_var: 0.05740984529256821\n",
      "      vf_loss: 110.79840087890625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4112348556518555\n",
      "      kl: 0.0024720653891563416\n",
      "      policy_loss: -0.0017884369008243084\n",
      "      total_loss: 121.79431915283203\n",
      "      vf_explained_var: 0.02365066111087799\n",
      "      vf_loss: 121.79612731933594\n",
      "    sample_time_ms: 8499.172\n",
      "    update_time_ms: 11.182\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 5.2866680494724845\n",
      "    rl_1: 7.315430999896762\n",
      "  time_since_restore: 129.56583786010742\n",
      "  time_this_iter_s: 14.52644944190979\n",
      "  time_total_s: 129.56583786010742\n",
      "  timestamp: 1552315360\n",
      "  timesteps_since_restore: 30800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 30800\n",
      "  training_iteration: 14\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 129 s, 14 iter, 30800 ts, 12.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-42-49\n",
      "  done: false\n",
      "  episode_len_mean: 102.44\n",
      "  episode_reward_max: 55.49541427057067\n",
      "  episode_reward_mean: 9.63875273597274\n",
      "  episode_reward_min: -164.71235867701722\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 320\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1274.816\n",
      "    load_time_ms: 2.974\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4215896129608154\n",
      "      kl: 0.001090708072297275\n",
      "      policy_loss: -0.0007822535699233413\n",
      "      total_loss: 113.8405532836914\n",
      "      vf_explained_var: 0.060870297253131866\n",
      "      vf_loss: 113.8413314819336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4195692539215088\n",
      "      kl: 0.0006841563154011965\n",
      "      policy_loss: 0.0006857295520603657\n",
      "      total_loss: 115.74685668945312\n",
      "      vf_explained_var: 0.05816301330924034\n",
      "      vf_loss: 115.74616241455078\n",
      "    sample_time_ms: 8473.253\n",
      "    update_time_ms: 11.484\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 3.8874408745317535\n",
      "    rl_1: 5.751311861440988\n",
      "  time_since_restore: 138.0214626789093\n",
      "  time_this_iter_s: 8.45562481880188\n",
      "  time_total_s: 138.0214626789093\n",
      "  timestamp: 1552315369\n",
      "  timesteps_since_restore: 33000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 15\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 138 s, 15 iter, 33000 ts, 9.64 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-42-55\n",
      "  done: false\n",
      "  episode_len_mean: 102.54\n",
      "  episode_reward_max: 55.49541427057067\n",
      "  episode_reward_mean: 11.514131767555863\n",
      "  episode_reward_min: -164.71235867701722\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 342\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1264.752\n",
      "    load_time_ms: 2.982\n",
      "    num_steps_sampled: 35200\n",
      "    num_steps_trained: 35200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4131132364273071\n",
      "      kl: 0.0008469563908874989\n",
      "      policy_loss: -0.00037775846431031823\n",
      "      total_loss: 68.34081268310547\n",
      "      vf_explained_var: 0.1351822018623352\n",
      "      vf_loss: 68.34119415283203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4261445999145508\n",
      "      kl: 0.0019680457189679146\n",
      "      policy_loss: -0.0028283868450671434\n",
      "      total_loss: 69.67900848388672\n",
      "      vf_explained_var: 0.05670648813247681\n",
      "      vf_loss: 69.68183898925781\n",
      "    sample_time_ms: 8427.845\n",
      "    update_time_ms: 11.368\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 4.948304613146588\n",
      "    rl_1: 6.565827154409277\n",
      "  time_since_restore: 143.99962902069092\n",
      "  time_this_iter_s: 5.978166341781616\n",
      "  time_total_s: 143.99962902069092\n",
      "  timestamp: 1552315375\n",
      "  timesteps_since_restore: 35200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 35200\n",
      "  training_iteration: 16\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 143 s, 16 iter, 35200 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-43-02\n",
      "  done: false\n",
      "  episode_len_mean: 102.5\n",
      "  episode_reward_max: 62.18640583172605\n",
      "  episode_reward_mean: 11.507900480497419\n",
      "  episode_reward_min: -164.71235867701722\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 362\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1279.253\n",
      "    load_time_ms: 2.805\n",
      "    num_steps_sampled: 37400\n",
      "    num_steps_trained: 37400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4197362661361694\n",
      "      kl: 0.0014283295022323728\n",
      "      policy_loss: -0.001850709319114685\n",
      "      total_loss: 68.56779479980469\n",
      "      vf_explained_var: 0.1433631330728531\n",
      "      vf_loss: 68.56964111328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.430059552192688\n",
      "      kl: 0.0007770160445943475\n",
      "      policy_loss: -0.001902249758131802\n",
      "      total_loss: 70.4121322631836\n",
      "      vf_explained_var: 0.10683204233646393\n",
      "      vf_loss: 70.4140396118164\n",
      "    sample_time_ms: 8229.678\n",
      "    update_time_ms: 10.873\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 5.3951173333556595\n",
      "    rl_1: 6.11278314714176\n",
      "  time_since_restore: 151.3740541934967\n",
      "  time_this_iter_s: 7.374425172805786\n",
      "  time_total_s: 151.3740541934967\n",
      "  timestamp: 1552315382\n",
      "  timesteps_since_restore: 37400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 37400\n",
      "  training_iteration: 17\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 151 s, 17 iter, 37400 ts, 11.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-43-11\n",
      "  done: false\n",
      "  episode_len_mean: 102.5\n",
      "  episode_reward_max: 62.18640583172605\n",
      "  episode_reward_mean: 12.828824081026871\n",
      "  episode_reward_min: -164.71235867701722\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 384\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1288.914\n",
      "    load_time_ms: 2.874\n",
      "    num_steps_sampled: 39600\n",
      "    num_steps_trained: 39600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4073543548583984\n",
      "      kl: 0.0011585579486563802\n",
      "      policy_loss: 0.0003391407954040915\n",
      "      total_loss: 70.3208999633789\n",
      "      vf_explained_var: 0.07418761402368546\n",
      "      vf_loss: 70.32056427001953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4331514835357666\n",
      "      kl: 0.0016641065012663603\n",
      "      policy_loss: -0.0027107212226837873\n",
      "      total_loss: 78.5523910522461\n",
      "      vf_explained_var: 0.035010967403650284\n",
      "      vf_loss: 78.55510711669922\n",
      "    sample_time_ms: 8416.554\n",
      "    update_time_ms: 12.826\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 5.603262355255652\n",
      "    rl_1: 7.22556172577122\n",
      "  time_since_restore: 160.6463487148285\n",
      "  time_this_iter_s: 9.272294521331787\n",
      "  time_total_s: 160.6463487148285\n",
      "  timestamp: 1552315391\n",
      "  timesteps_since_restore: 39600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 39600\n",
      "  training_iteration: 18\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 160 s, 18 iter, 39600 ts, 12.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-43-18\n",
      "  done: false\n",
      "  episode_len_mean: 102.73\n",
      "  episode_reward_max: 62.18640583172605\n",
      "  episode_reward_mean: 17.29126745798605\n",
      "  episode_reward_min: -160.54216454788255\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 406\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1295.501\n",
      "    load_time_ms: 2.917\n",
      "    num_steps_sampled: 41800\n",
      "    num_steps_trained: 41800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4242969751358032\n",
      "      kl: 0.001288839615881443\n",
      "      policy_loss: -0.002988477936014533\n",
      "      total_loss: 7.245151042938232\n",
      "      vf_explained_var: 0.19328255951404572\n",
      "      vf_loss: 7.248139381408691\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.439121961593628\n",
      "      kl: 0.0013101771473884583\n",
      "      policy_loss: -0.0033958316780626774\n",
      "      total_loss: 11.027087211608887\n",
      "      vf_explained_var: 0.2823996841907501\n",
      "      vf_loss: 11.03048324584961\n",
      "    sample_time_ms: 8384.115\n",
      "    update_time_ms: 13.678\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 8.06111506764178\n",
      "    rl_1: 9.230152390344271\n",
      "  time_since_restore: 167.30553889274597\n",
      "  time_this_iter_s: 6.6591901779174805\n",
      "  time_total_s: 167.30553889274597\n",
      "  timestamp: 1552315398\n",
      "  timesteps_since_restore: 41800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 41800\n",
      "  training_iteration: 19\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 167 s, 19 iter, 41800 ts, 17.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-43-24\n",
      "  done: false\n",
      "  episode_len_mean: 102.74\n",
      "  episode_reward_max: 62.18640583172605\n",
      "  episode_reward_mean: 16.172823571421098\n",
      "  episode_reward_min: -160.54216454788255\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 427\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1246.459\n",
      "    load_time_ms: 2.864\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4268198013305664\n",
      "      kl: 0.001855549169704318\n",
      "      policy_loss: -0.002835640450939536\n",
      "      total_loss: 61.93123245239258\n",
      "      vf_explained_var: 0.14224816858768463\n",
      "      vf_loss: 61.93406677246094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4363653659820557\n",
      "      kl: 0.00031576649053022265\n",
      "      policy_loss: -0.005400134716182947\n",
      "      total_loss: 69.1628189086914\n",
      "      vf_explained_var: 0.15847057104110718\n",
      "      vf_loss: 69.1682357788086\n",
      "    sample_time_ms: 8092.338\n",
      "    update_time_ms: 13.731\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 7.5228714875993425\n",
      "    rl_1: 8.649952083821752\n",
      "  time_since_restore: 173.37549328804016\n",
      "  time_this_iter_s: 6.0699543952941895\n",
      "  time_total_s: 173.37549328804016\n",
      "  timestamp: 1552315404\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 20\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 173 s, 20 iter, 44000 ts, 16.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 102.77\n",
      "  episode_reward_max: 60.00710102877987\n",
      "  episode_reward_mean: 15.942328348328349\n",
      "  episode_reward_min: -160.54216454788255\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 448\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1254.16\n",
      "    load_time_ms: 2.715\n",
      "    num_steps_sampled: 46200\n",
      "    num_steps_trained: 46200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4141582250595093\n",
      "      kl: 0.0008042327826842666\n",
      "      policy_loss: -0.004180629272013903\n",
      "      total_loss: 57.51081085205078\n",
      "      vf_explained_var: 0.21721957623958588\n",
      "      vf_loss: 57.51498794555664\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4422261714935303\n",
      "      kl: 0.00014256931899581105\n",
      "      policy_loss: 0.0011373955057933927\n",
      "      total_loss: 55.91965103149414\n",
      "      vf_explained_var: 0.09088128805160522\n",
      "      vf_loss: 55.918514251708984\n",
      "    sample_time_ms: 8163.805\n",
      "    update_time_ms: 13.421\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 7.820481929044483\n",
      "    rl_1: 8.121846419283864\n",
      "  time_since_restore: 182.7113003730774\n",
      "  time_this_iter_s: 9.335807085037231\n",
      "  time_total_s: 182.7113003730774\n",
      "  timestamp: 1552315414\n",
      "  timesteps_since_restore: 46200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 46200\n",
      "  training_iteration: 21\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 182 s, 21 iter, 46200 ts, 15.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-43-43\n",
      "  done: false\n",
      "  episode_len_mean: 102.84\n",
      "  episode_reward_max: 60.00710102877987\n",
      "  episode_reward_mean: 18.867333080777584\n",
      "  episode_reward_min: -153.27128312405716\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 470\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1173.59\n",
      "    load_time_ms: 2.806\n",
      "    num_steps_sampled: 48400\n",
      "    num_steps_trained: 48400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4221677780151367\n",
      "      kl: 0.0011249263770878315\n",
      "      policy_loss: -0.0003487608046270907\n",
      "      total_loss: 5.938742637634277\n",
      "      vf_explained_var: 0.40588077902793884\n",
      "      vf_loss: 5.939091205596924\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.428149700164795\n",
      "      kl: 0.004361405968666077\n",
      "      policy_loss: -0.006463808938860893\n",
      "      total_loss: 10.271641731262207\n",
      "      vf_explained_var: 0.3003665506839752\n",
      "      vf_loss: 10.278105735778809\n",
      "    sample_time_ms: 8176.392\n",
      "    update_time_ms: 14.172\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 9.518221931096816\n",
      "    rl_1: 9.349111149680768\n",
      "  time_since_restore: 191.73092794418335\n",
      "  time_this_iter_s: 9.019627571105957\n",
      "  time_total_s: 191.73092794418335\n",
      "  timestamp: 1552315423\n",
      "  timesteps_since_restore: 48400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 48400\n",
      "  training_iteration: 22\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 191 s, 22 iter, 48400 ts, 18.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 102.84\n",
      "  episode_reward_max: 60.00710102877987\n",
      "  episode_reward_mean: 21.1538763515523\n",
      "  episode_reward_min: -153.27128312405716\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 491\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1069.071\n",
      "    load_time_ms: 2.942\n",
      "    num_steps_sampled: 50600\n",
      "    num_steps_trained: 50600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3886438608169556\n",
      "      kl: 0.0004268349730409682\n",
      "      policy_loss: -0.0027050161734223366\n",
      "      total_loss: 12.863483428955078\n",
      "      vf_explained_var: 0.4263761639595032\n",
      "      vf_loss: 12.866188049316406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4290639162063599\n",
      "      kl: 0.0004819711612071842\n",
      "      policy_loss: 0.0035507292486727238\n",
      "      total_loss: 11.473151206970215\n",
      "      vf_explained_var: 0.41307762265205383\n",
      "      vf_loss: 11.469600677490234\n",
      "    sample_time_ms: 7397.509\n",
      "    update_time_ms: 12.989\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 10.6231840172588\n",
      "    rl_1: 10.530692334293503\n",
      "  time_since_restore: 199.98781967163086\n",
      "  time_this_iter_s: 8.25689172744751\n",
      "  time_total_s: 199.98781967163086\n",
      "  timestamp: 1552315431\n",
      "  timesteps_since_restore: 50600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 50600\n",
      "  training_iteration: 23\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 199 s, 23 iter, 50600 ts, 21.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-44-00\n",
      "  done: false\n",
      "  episode_len_mean: 102.8\n",
      "  episode_reward_max: 58.366397868965166\n",
      "  episode_reward_mean: 20.573075491508188\n",
      "  episode_reward_min: -153.27128312405716\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 512\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1023.988\n",
      "    load_time_ms: 2.824\n",
      "    num_steps_sampled: 52800\n",
      "    num_steps_trained: 52800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3899524211883545\n",
      "      kl: 0.0005834848270751536\n",
      "      policy_loss: -0.0006460095755755901\n",
      "      total_loss: 39.957820892333984\n",
      "      vf_explained_var: 0.3750126361846924\n",
      "      vf_loss: 39.95846939086914\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4181894063949585\n",
      "      kl: 0.0012220325879752636\n",
      "      policy_loss: -0.001437799888662994\n",
      "      total_loss: 45.935272216796875\n",
      "      vf_explained_var: 0.13582843542099\n",
      "      vf_loss: 45.93671417236328\n",
      "    sample_time_ms: 6839.196\n",
      "    update_time_ms: 12.602\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 10.158953990736569\n",
      "    rl_1: 10.414121500771621\n",
      "  time_since_restore: 208.46906757354736\n",
      "  time_this_iter_s: 8.481247901916504\n",
      "  time_total_s: 208.46906757354736\n",
      "  timestamp: 1552315440\n",
      "  timesteps_since_restore: 52800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 52800\n",
      "  training_iteration: 24\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 208 s, 24 iter, 52800 ts, 20.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-44-06\n",
      "  done: false\n",
      "  episode_len_mean: 102.85\n",
      "  episode_reward_max: 58.366397868965166\n",
      "  episode_reward_mean: 21.36546524901523\n",
      "  episode_reward_min: -156.88082968425294\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 534\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1024.348\n",
      "    load_time_ms: 2.811\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4077293872833252\n",
      "      kl: 0.0007547828136011958\n",
      "      policy_loss: 0.0004799317684955895\n",
      "      total_loss: 65.59380340576172\n",
      "      vf_explained_var: 0.1136305034160614\n",
      "      vf_loss: 65.59332275390625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4349457025527954\n",
      "      kl: 0.0006138428580015898\n",
      "      policy_loss: 5.42840534762945e-05\n",
      "      total_loss: 68.12325286865234\n",
      "      vf_explained_var: 0.17185446619987488\n",
      "      vf_loss: 68.12319946289062\n",
      "    sample_time_ms: 6619.068\n",
      "    update_time_ms: 12.212\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 10.617566748789027\n",
      "    rl_1: 10.747898500226206\n",
      "  time_since_restore: 214.7242305278778\n",
      "  time_this_iter_s: 6.255162954330444\n",
      "  time_total_s: 214.7242305278778\n",
      "  timestamp: 1552315446\n",
      "  timesteps_since_restore: 55000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 25\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 214 s, 25 iter, 55000 ts, 21.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 102.82\n",
      "  episode_reward_max: 58.366397868965166\n",
      "  episode_reward_mean: 22.70634669117329\n",
      "  episode_reward_min: -156.88082968425294\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 556\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1034.642\n",
      "    load_time_ms: 2.881\n",
      "    num_steps_sampled: 57200\n",
      "    num_steps_trained: 57200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.395024299621582\n",
      "      kl: 0.000486410892335698\n",
      "      policy_loss: 0.00035643138107843697\n",
      "      total_loss: 73.17047882080078\n",
      "      vf_explained_var: 0.17857354879379272\n",
      "      vf_loss: 73.17012786865234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4286303520202637\n",
      "      kl: 0.0006945134955458343\n",
      "      policy_loss: 0.00027795881032943726\n",
      "      total_loss: 71.83717346191406\n",
      "      vf_explained_var: 0.13082106411457062\n",
      "      vf_loss: 71.8368911743164\n",
      "    sample_time_ms: 6706.099\n",
      "    update_time_ms: 12.504\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 11.457507835010885\n",
      "    rl_1: 11.248838856162404\n",
      "  time_since_restore: 221.68109965324402\n",
      "  time_this_iter_s: 6.956869125366211\n",
      "  time_total_s: 221.68109965324402\n",
      "  timestamp: 1552315453\n",
      "  timesteps_since_restore: 57200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 57200\n",
      "  training_iteration: 26\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 221 s, 26 iter, 57200 ts, 22.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-44-20\n",
      "  done: false\n",
      "  episode_len_mean: 102.82\n",
      "  episode_reward_max: 66.45964128909621\n",
      "  episode_reward_mean: 24.38238518654409\n",
      "  episode_reward_min: -156.88082968425294\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 576\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 978.07\n",
      "    load_time_ms: 2.897\n",
      "    num_steps_sampled: 59400\n",
      "    num_steps_trained: 59400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9802322831784522e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3818882703781128\n",
      "      kl: 0.0023247776553034782\n",
      "      policy_loss: -0.0033643385395407677\n",
      "      total_loss: 12.70424747467041\n",
      "      vf_explained_var: 0.527888834476471\n",
      "      vf_loss: 12.707611083984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9802322831784522e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.421656608581543\n",
      "      kl: 0.00453559635207057\n",
      "      policy_loss: -0.009482578374445438\n",
      "      total_loss: 13.603251457214355\n",
      "      vf_explained_var: 0.2010749876499176\n",
      "      vf_loss: 13.6127347946167\n",
      "    sample_time_ms: 6687.91\n",
      "    update_time_ms: 12.51\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 12.418189214255724\n",
      "    rl_1: 11.964195972288369\n",
      "  time_since_restore: 228.30557346343994\n",
      "  time_this_iter_s: 6.624473810195923\n",
      "  time_total_s: 228.30557346343994\n",
      "  timestamp: 1552315460\n",
      "  timesteps_since_restore: 59400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 59400\n",
      "  training_iteration: 27\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 228 s, 27 iter, 59400 ts, 24.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-44-27\n",
      "  done: false\n",
      "  episode_len_mean: 102.8\n",
      "  episode_reward_max: 78.7976745187454\n",
      "  episode_reward_mean: 24.835511341422894\n",
      "  episode_reward_min: -156.88082968425294\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 598\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 950.364\n",
      "    load_time_ms: 2.913\n",
      "    num_steps_sampled: 61600\n",
      "    num_steps_trained: 61600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3671072721481323\n",
      "      kl: 0.005595427006483078\n",
      "      policy_loss: -0.006983013823628426\n",
      "      total_loss: 75.13684844970703\n",
      "      vf_explained_var: 0.2242903709411621\n",
      "      vf_loss: 75.14383697509766\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4156440496444702\n",
      "      kl: 0.000635328353382647\n",
      "      policy_loss: -0.0014121714048087597\n",
      "      total_loss: 69.33502197265625\n",
      "      vf_explained_var: 0.13681375980377197\n",
      "      vf_loss: 69.33642578125\n",
      "    sample_time_ms: 6486.702\n",
      "    update_time_ms: 10.206\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 13.036448771716273\n",
      "    rl_1: 11.799062569706612\n",
      "  time_since_restore: 235.2611949443817\n",
      "  time_this_iter_s: 6.9556214809417725\n",
      "  time_total_s: 235.2611949443817\n",
      "  timestamp: 1552315467\n",
      "  timesteps_since_restore: 61600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 61600\n",
      "  training_iteration: 28\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 235 s, 28 iter, 61600 ts, 24.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-44-33\n",
      "  done: false\n",
      "  episode_len_mean: 102.92\n",
      "  episode_reward_max: 78.7976745187454\n",
      "  episode_reward_mean: 29.43199940325818\n",
      "  episode_reward_min: -150.2171392094341\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 620\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 942.812\n",
      "    load_time_ms: 2.95\n",
      "    num_steps_sampled: 63800\n",
      "    num_steps_trained: 63800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3632094860076904\n",
      "      kl: 0.0024987291544675827\n",
      "      policy_loss: -0.002580277156084776\n",
      "      total_loss: 83.52686309814453\n",
      "      vf_explained_var: 0.276842325925827\n",
      "      vf_loss: 83.5294418334961\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4230034351348877\n",
      "      kl: 0.0011286167427897453\n",
      "      policy_loss: -0.0006961213075555861\n",
      "      total_loss: 78.9054183959961\n",
      "      vf_explained_var: 0.05695958435535431\n",
      "      vf_loss: 78.90611267089844\n",
      "    sample_time_ms: 6460.754\n",
      "    update_time_ms: 9.052\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 15.790631103549696\n",
      "    rl_1: 13.641368299708486\n",
      "  time_since_restore: 241.57759761810303\n",
      "  time_this_iter_s: 6.3164026737213135\n",
      "  time_total_s: 241.57759761810303\n",
      "  timestamp: 1552315473\n",
      "  timesteps_since_restore: 63800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 63800\n",
      "  training_iteration: 29\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 241 s, 29 iter, 63800 ts, 29.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-44-43\n",
      "  done: false\n",
      "  episode_len_mean: 102.92\n",
      "  episode_reward_max: 78.7976745187454\n",
      "  episode_reward_mean: 29.169465157580053\n",
      "  episode_reward_min: -150.2171392094341\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 640\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 980.119\n",
      "    load_time_ms: 3.097\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.385297179222107\n",
      "      kl: 0.002854041289538145\n",
      "      policy_loss: -0.0051377080380916595\n",
      "      total_loss: 13.896014213562012\n",
      "      vf_explained_var: 0.47882965207099915\n",
      "      vf_loss: 13.901150703430176\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4365525245666504\n",
      "      kl: 0.0063928645104169846\n",
      "      policy_loss: -0.012422666884958744\n",
      "      total_loss: 12.909674644470215\n",
      "      vf_explained_var: 0.2846826910972595\n",
      "      vf_loss: 12.922097206115723\n",
      "    sample_time_ms: 6840.228\n",
      "    update_time_ms: 8.799\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 15.90173442726946\n",
      "    rl_1: 13.267730730310593\n",
      "  time_since_restore: 251.8264982700348\n",
      "  time_this_iter_s: 10.248900651931763\n",
      "  time_total_s: 251.8264982700348\n",
      "  timestamp: 1552315483\n",
      "  timesteps_since_restore: 66000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 30\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 251 s, 30 iter, 66000 ts, 29.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-44-51\n",
      "  done: false\n",
      "  episode_len_mean: 102.56\n",
      "  episode_reward_max: 78.7976745187454\n",
      "  episode_reward_mean: 25.555857308969557\n",
      "  episode_reward_min: -162.23332846149142\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 662\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 943.177\n",
      "    load_time_ms: 3.225\n",
      "    num_steps_sampled: 68200\n",
      "    num_steps_trained: 68200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3812744617462158\n",
      "      kl: 0.01441167388111353\n",
      "      policy_loss: -0.020225193351507187\n",
      "      total_loss: 210.1625518798828\n",
      "      vf_explained_var: 0.28246957063674927\n",
      "      vf_loss: 210.18275451660156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4242886304855347\n",
      "      kl: 0.001206871005706489\n",
      "      policy_loss: -0.0020758151076734066\n",
      "      total_loss: 238.6240692138672\n",
      "      vf_explained_var: 0.18957477807998657\n",
      "      vf_loss: 238.6261444091797\n",
      "    sample_time_ms: 6688.497\n",
      "    update_time_ms: 10.892\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 13.259687619845698\n",
      "    rl_1: 12.296169689123863\n",
      "  time_since_restore: 259.2955141067505\n",
      "  time_this_iter_s: 7.469015836715698\n",
      "  time_total_s: 259.2955141067505\n",
      "  timestamp: 1552315491\n",
      "  timesteps_since_restore: 68200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 68200\n",
      "  training_iteration: 31\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 259 s, 31 iter, 68200 ts, 25.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-44-57\n",
      "  done: false\n",
      "  episode_len_mean: 101.98\n",
      "  episode_reward_max: 78.7976745187454\n",
      "  episode_reward_mean: 20.835320517855912\n",
      "  episode_reward_min: -162.23332846149142\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 684\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 869.808\n",
      "    load_time_ms: 3.242\n",
      "    num_steps_sampled: 70400\n",
      "    num_steps_trained: 70400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3619974851608276\n",
      "      kl: 0.0002974304370582104\n",
      "      policy_loss: -9.393165964866057e-05\n",
      "      total_loss: 107.57164001464844\n",
      "      vf_explained_var: 0.340941458940506\n",
      "      vf_loss: 107.57173919677734\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.313225884932663e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4259655475616455\n",
      "      kl: 0.001689978875219822\n",
      "      policy_loss: -0.0006478638970293105\n",
      "      total_loss: 107.64056396484375\n",
      "      vf_explained_var: 0.19651927053928375\n",
      "      vf_loss: 107.64120483398438\n",
      "    sample_time_ms: 6525.998\n",
      "    update_time_ms: 10.573\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 10.767740064809434\n",
      "    rl_1: 10.06758045304648\n",
      "  time_since_restore: 265.95108437538147\n",
      "  time_this_iter_s: 6.6555702686309814\n",
      "  time_total_s: 265.95108437538147\n",
      "  timestamp: 1552315497\n",
      "  timesteps_since_restore: 70400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 70400\n",
      "  training_iteration: 32\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 265 s, 32 iter, 70400 ts, 20.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-45-04\n",
      "  done: false\n",
      "  episode_len_mean: 101.65\n",
      "  episode_reward_max: 76.45347433644453\n",
      "  episode_reward_mean: 18.42044822682825\n",
      "  episode_reward_min: -162.23332846149142\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 706\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 854.089\n",
      "    load_time_ms: 2.998\n",
      "    num_steps_sampled: 72600\n",
      "    num_steps_trained: 72600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.313225884932663e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.375611424446106\n",
      "      kl: 0.006194610148668289\n",
      "      policy_loss: -0.006781982257962227\n",
      "      total_loss: 138.7724151611328\n",
      "      vf_explained_var: 0.3366210162639618\n",
      "      vf_loss: 138.77919006347656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4152110815048218\n",
      "      kl: 0.0009923134930431843\n",
      "      policy_loss: -0.0007290507201105356\n",
      "      total_loss: 151.68792724609375\n",
      "      vf_explained_var: 0.21927857398986816\n",
      "      vf_loss: 151.68865966796875\n",
      "    sample_time_ms: 6349.562\n",
      "    update_time_ms: 10.214\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 8.62399519093175\n",
      "    rl_1: 9.7964530358965\n",
      "  time_since_restore: 272.2781219482422\n",
      "  time_this_iter_s: 6.327037572860718\n",
      "  time_total_s: 272.2781219482422\n",
      "  timestamp: 1552315504\n",
      "  timesteps_since_restore: 72600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 72600\n",
      "  training_iteration: 33\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 272 s, 33 iter, 72600 ts, 18.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-45-12\n",
      "  done: false\n",
      "  episode_len_mean: 101.65\n",
      "  episode_reward_max: 76.45347433644453\n",
      "  episode_reward_mean: 20.34147089912723\n",
      "  episode_reward_min: -162.23332846149142\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 728\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 855.189\n",
      "    load_time_ms: 2.985\n",
      "    num_steps_sampled: 74800\n",
      "    num_steps_trained: 74800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3553928136825562\n",
      "      kl: 0.004539292771369219\n",
      "      policy_loss: -0.0023045616690069437\n",
      "      total_loss: 12.396358489990234\n",
      "      vf_explained_var: 0.6303235292434692\n",
      "      vf_loss: 12.398663520812988\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.41352117061615\n",
      "      kl: 0.0020762006752192974\n",
      "      policy_loss: -0.005104417912662029\n",
      "      total_loss: 15.608705520629883\n",
      "      vf_explained_var: 0.30927783250808716\n",
      "      vf_loss: 15.613808631896973\n",
      "    sample_time_ms: 6322.754\n",
      "    update_time_ms: 10.069\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 8.542337856944018\n",
      "    rl_1: 11.799133042183215\n",
      "  time_since_restore: 280.50048303604126\n",
      "  time_this_iter_s: 8.222361087799072\n",
      "  time_total_s: 280.50048303604126\n",
      "  timestamp: 1552315512\n",
      "  timesteps_since_restore: 74800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 74800\n",
      "  training_iteration: 34\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 280 s, 34 iter, 74800 ts, 20.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-45-18\n",
      "  done: false\n",
      "  episode_len_mean: 101.8\n",
      "  episode_reward_max: 76.45347433644453\n",
      "  episode_reward_mean: 23.27306213825827\n",
      "  episode_reward_min: -161.1169182775248\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 749\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 846.744\n",
      "    load_time_ms: 3.015\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3771058320999146\n",
      "      kl: 0.0021300730295479298\n",
      "      policy_loss: -0.000330275041051209\n",
      "      total_loss: 71.36372375488281\n",
      "      vf_explained_var: 0.3552153408527374\n",
      "      vf_loss: 71.36405944824219\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4063713550567627\n",
      "      kl: 0.0030419290997087955\n",
      "      policy_loss: -0.0028295342344790697\n",
      "      total_loss: 87.86565399169922\n",
      "      vf_explained_var: 0.21036574244499207\n",
      "      vf_loss: 87.86846923828125\n",
      "    sample_time_ms: 6328.583\n",
      "    update_time_ms: 9.915\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 10.025543608740161\n",
      "    rl_1: 13.247518529518112\n",
      "  time_since_restore: 286.72657465934753\n",
      "  time_this_iter_s: 6.226091623306274\n",
      "  time_total_s: 286.72657465934753\n",
      "  timestamp: 1552315518\n",
      "  timesteps_since_restore: 77000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 35\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 286 s, 35 iter, 77000 ts, 23.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-45-24\n",
      "  done: false\n",
      "  episode_len_mean: 102.27\n",
      "  episode_reward_max: 76.45347433644453\n",
      "  episode_reward_mean: 33.986621429250796\n",
      "  episode_reward_min: -156.69496026532374\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 770\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 839.941\n",
      "    load_time_ms: 2.961\n",
      "    num_steps_sampled: 79200\n",
      "    num_steps_trained: 79200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.365723729133606\n",
      "      kl: 0.0035460125654935837\n",
      "      policy_loss: -0.004534920211881399\n",
      "      total_loss: 16.50130844116211\n",
      "      vf_explained_var: 0.5595335960388184\n",
      "      vf_loss: 16.505842208862305\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4118908643722534\n",
      "      kl: 0.002745373174548149\n",
      "      policy_loss: -0.0058904606848955154\n",
      "      total_loss: 29.739660263061523\n",
      "      vf_explained_var: 0.26153919100761414\n",
      "      vf_loss: 29.745548248291016\n",
      "    sample_time_ms: 6201.963\n",
      "    update_time_ms: 9.512\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 15.216621398687307\n",
      "    rl_1: 18.770000030563487\n",
      "  time_since_restore: 292.34489250183105\n",
      "  time_this_iter_s: 5.6183178424835205\n",
      "  time_total_s: 292.34489250183105\n",
      "  timestamp: 1552315524\n",
      "  timesteps_since_restore: 79200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 79200\n",
      "  training_iteration: 36\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 292 s, 36 iter, 79200 ts, 34 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-45-32\n",
      "  done: false\n",
      "  episode_len_mean: 102.48\n",
      "  episode_reward_max: 76.45347433644453\n",
      "  episode_reward_mean: 37.44866417514562\n",
      "  episode_reward_min: -156.69496026532374\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 792\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 837.676\n",
      "    load_time_ms: 2.943\n",
      "    num_steps_sampled: 81400\n",
      "    num_steps_trained: 81400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3758240938186646\n",
      "      kl: 0.001737997867166996\n",
      "      policy_loss: -0.004457787610590458\n",
      "      total_loss: 86.79319763183594\n",
      "      vf_explained_var: 0.3055746555328369\n",
      "      vf_loss: 86.79766082763672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9103830890414573e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.414546251296997\n",
      "      kl: 0.005054139066487551\n",
      "      policy_loss: -0.010006404481828213\n",
      "      total_loss: 91.42070007324219\n",
      "      vf_explained_var: 0.14411982893943787\n",
      "      vf_loss: 91.43070983886719\n",
      "    sample_time_ms: 6296.428\n",
      "    update_time_ms: 9.368\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.665881820151025\n",
      "    rl_1: 20.78278235499459\n",
      "  time_since_restore: 299.89235520362854\n",
      "  time_this_iter_s: 7.547462701797485\n",
      "  time_total_s: 299.89235520362854\n",
      "  timestamp: 1552315532\n",
      "  timesteps_since_restore: 81400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 81400\n",
      "  training_iteration: 37\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 299 s, 37 iter, 81400 ts, 37.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-45-39\n",
      "  done: false\n",
      "  episode_len_mean: 102.75\n",
      "  episode_reward_max: 98.44716095493446\n",
      "  episode_reward_mean: 40.8056549800417\n",
      "  episode_reward_min: -152.69682392539534\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 813\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 839.292\n",
      "    load_time_ms: 2.965\n",
      "    num_steps_sampled: 83600\n",
      "    num_steps_trained: 83600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9103830890414573e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.369110107421875\n",
      "      kl: 0.00038490653969347477\n",
      "      policy_loss: -0.004241127520799637\n",
      "      total_loss: 111.98650360107422\n",
      "      vf_explained_var: 0.28978896141052246\n",
      "      vf_loss: 111.99075317382812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4089664220809937\n",
      "      kl: 0.002406210405752063\n",
      "      policy_loss: -0.0009441327420063317\n",
      "      total_loss: 121.14371490478516\n",
      "      vf_explained_var: 0.13217349350452423\n",
      "      vf_loss: 121.14466094970703\n",
      "    sample_time_ms: 6324.609\n",
      "    update_time_ms: 9.426\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 19.07401329171603\n",
      "    rl_1: 21.73164168832567\n",
      "  time_since_restore: 307.14815521240234\n",
      "  time_this_iter_s: 7.255800008773804\n",
      "  time_total_s: 307.14815521240234\n",
      "  timestamp: 1552315539\n",
      "  timesteps_since_restore: 83600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 83600\n",
      "  training_iteration: 38\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 307 s, 38 iter, 83600 ts, 40.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 102.75\n",
      "  episode_reward_max: 98.44716095493446\n",
      "  episode_reward_mean: 44.36389620067841\n",
      "  episode_reward_min: -152.69682392539534\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 835\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 834.767\n",
      "    load_time_ms: 2.929\n",
      "    num_steps_sampled: 85800\n",
      "    num_steps_trained: 85800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3722275495529175\n",
      "      kl: 0.001125388778746128\n",
      "      policy_loss: -0.005316566210240126\n",
      "      total_loss: 20.223783493041992\n",
      "      vf_explained_var: 0.5264986753463745\n",
      "      vf_loss: 20.229101181030273\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3955731391906738\n",
      "      kl: 0.004062078893184662\n",
      "      policy_loss: -0.007729766890406609\n",
      "      total_loss: 50.944095611572266\n",
      "      vf_explained_var: 0.16749869287014008\n",
      "      vf_loss: 50.95181655883789\n",
      "    sample_time_ms: 6364.796\n",
      "    update_time_ms: 9.56\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 20.4228154282876\n",
      "    rl_1: 23.941080772390816\n",
      "  time_since_restore: 313.82091665267944\n",
      "  time_this_iter_s: 6.6727614402771\n",
      "  time_total_s: 313.82091665267944\n",
      "  timestamp: 1552315546\n",
      "  timesteps_since_restore: 85800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 85800\n",
      "  training_iteration: 39\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 313 s, 39 iter, 85800 ts, 44.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-45-55\n",
      "  done: false\n",
      "  episode_len_mean: 102.79\n",
      "  episode_reward_max: 98.44716095493446\n",
      "  episode_reward_mean: 47.74255968073939\n",
      "  episode_reward_min: -152.69682392539534\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 856\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 875.814\n",
      "    load_time_ms: 3.104\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3805408477783203\n",
      "      kl: 0.0015909959329292178\n",
      "      policy_loss: 0.0008897623629309237\n",
      "      total_loss: 19.95072364807129\n",
      "      vf_explained_var: 0.39224106073379517\n",
      "      vf_loss: 19.94983673095703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3790117502212524\n",
      "      kl: 0.001890859566628933\n",
      "      policy_loss: -0.0009215890313498676\n",
      "      total_loss: 69.28245544433594\n",
      "      vf_explained_var: 0.09650986641645432\n",
      "      vf_loss: 69.28337860107422\n",
      "    sample_time_ms: 6259.469\n",
      "    update_time_ms: 9.54\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 21.9095944407664\n",
      "    rl_1: 25.832965239972985\n",
      "  time_since_restore: 323.421826839447\n",
      "  time_this_iter_s: 9.600910186767578\n",
      "  time_total_s: 323.421826839447\n",
      "  timestamp: 1552315555\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 40\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 323 s, 40 iter, 88000 ts, 47.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-46-03\n",
      "  done: false\n",
      "  episode_len_mean: 102.73\n",
      "  episode_reward_max: 98.44716095493446\n",
      "  episode_reward_mean: 47.914330696414474\n",
      "  episode_reward_min: -152.69682392539534\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 877\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 885.408\n",
      "    load_time_ms: 2.905\n",
      "    num_steps_sampled: 90200\n",
      "    num_steps_trained: 90200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.374731421470642\n",
      "      kl: 0.0034799259155988693\n",
      "      policy_loss: -0.0020906224381178617\n",
      "      total_loss: 85.13365936279297\n",
      "      vf_explained_var: 0.33579474687576294\n",
      "      vf_loss: 85.1357421875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8189894306509108e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.383345603942871\n",
      "      kl: 0.005167943425476551\n",
      "      policy_loss: -0.004329564981162548\n",
      "      total_loss: 107.97027587890625\n",
      "      vf_explained_var: 0.16538745164871216\n",
      "      vf_loss: 107.97460174560547\n",
      "    sample_time_ms: 6280.045\n",
      "    update_time_ms: 7.655\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.211087436120287\n",
      "    rl_1: 25.70324326029419\n",
      "  time_since_restore: 331.170533657074\n",
      "  time_this_iter_s: 7.748706817626953\n",
      "  time_total_s: 331.170533657074\n",
      "  timestamp: 1552315563\n",
      "  timesteps_since_restore: 90200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 90200\n",
      "  training_iteration: 41\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 331 s, 41 iter, 90200 ts, 47.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-46-10\n",
      "  done: false\n",
      "  episode_len_mean: 102.47\n",
      "  episode_reward_max: 98.44716095493446\n",
      "  episode_reward_mean: 50.003393790505996\n",
      "  episode_reward_min: -143.95604418608013\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 899\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 893.222\n",
      "    load_time_ms: 2.922\n",
      "    num_steps_sampled: 92400\n",
      "    num_steps_trained: 92400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8189894306509108e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.378173589706421\n",
      "      kl: 0.011441124603152275\n",
      "      policy_loss: -0.012713920325040817\n",
      "      total_loss: 180.8798065185547\n",
      "      vf_explained_var: 0.3112216293811798\n",
      "      vf_loss: 180.89251708984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3659508228302002\n",
      "      kl: 0.002461227821186185\n",
      "      policy_loss: -0.0032385149970650673\n",
      "      total_loss: 230.49989318847656\n",
      "      vf_explained_var: 0.21457889676094055\n",
      "      vf_loss: 230.50314331054688\n",
      "    sample_time_ms: 6313.055\n",
      "    update_time_ms: 7.405\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 21.85393774868524\n",
      "    rl_1: 28.149456041820763\n",
      "  time_since_restore: 338.2389466762543\n",
      "  time_this_iter_s: 7.068413019180298\n",
      "  time_total_s: 338.2389466762543\n",
      "  timestamp: 1552315570\n",
      "  timesteps_since_restore: 92400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 92400\n",
      "  training_iteration: 42\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 338 s, 42 iter, 92400 ts, 50 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-46-20\n",
      "  done: false\n",
      "  episode_len_mean: 102.55\n",
      "  episode_reward_max: 93.94217142364342\n",
      "  episode_reward_mean: 53.555635224818104\n",
      "  episode_reward_min: -143.95604418608013\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 921\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 898.025\n",
      "    load_time_ms: 2.95\n",
      "    num_steps_sampled: 94600\n",
      "    num_steps_trained: 94600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8189894306509108e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3867398500442505\n",
      "      kl: 0.003494050120934844\n",
      "      policy_loss: -0.006174447014927864\n",
      "      total_loss: 14.525579452514648\n",
      "      vf_explained_var: 0.6413537263870239\n",
      "      vf_loss: 14.531753540039062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3564844131469727\n",
      "      kl: 0.003920809365808964\n",
      "      policy_loss: -0.00407000258564949\n",
      "      total_loss: 72.80744934082031\n",
      "      vf_explained_var: 0.13599592447280884\n",
      "      vf_loss: 72.81153106689453\n",
      "    sample_time_ms: 6635.407\n",
      "    update_time_ms: 8.021\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 21.66652256673532\n",
      "    rl_1: 31.889112658082794\n",
      "  time_since_restore: 347.8451671600342\n",
      "  time_this_iter_s: 9.606220483779907\n",
      "  time_total_s: 347.8451671600342\n",
      "  timestamp: 1552315580\n",
      "  timesteps_since_restore: 94600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 94600\n",
      "  training_iteration: 43\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 347 s, 43 iter, 94600 ts, 53.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 102.04\n",
      "  episode_reward_max: 96.71597531614779\n",
      "  episode_reward_mean: 48.53442073924518\n",
      "  episode_reward_min: -160.67461602609353\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 942\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 896.079\n",
      "    load_time_ms: 2.992\n",
      "    num_steps_sampled: 96800\n",
      "    num_steps_trained: 96800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3695839643478394\n",
      "      kl: 0.003491318551823497\n",
      "      policy_loss: -0.000904797634575516\n",
      "      total_loss: 199.16806030273438\n",
      "      vf_explained_var: 0.32108935713768005\n",
      "      vf_loss: 199.1689453125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3567076921463013\n",
      "      kl: 0.0038765519857406616\n",
      "      policy_loss: -0.003985016606748104\n",
      "      total_loss: 231.77593994140625\n",
      "      vf_explained_var: 0.16888633370399475\n",
      "      vf_loss: 231.77992248535156\n",
      "    sample_time_ms: 6578.102\n",
      "    update_time_ms: 8.24\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 19.49638486994273\n",
      "    rl_1: 29.038035869302444\n",
      "  time_since_restore: 355.4792642593384\n",
      "  time_this_iter_s: 7.634097099304199\n",
      "  time_total_s: 355.4792642593384\n",
      "  timestamp: 1552315588\n",
      "  timesteps_since_restore: 96800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 96800\n",
      "  training_iteration: 44\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 355 s, 44 iter, 96800 ts, 48.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-46-34\n",
      "  done: false\n",
      "  episode_len_mean: 102.04\n",
      "  episode_reward_max: 116.53629364089227\n",
      "  episode_reward_mean: 50.20086316862097\n",
      "  episode_reward_min: -160.67461602609353\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 964\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 897.093\n",
      "    load_time_ms: 2.93\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3786206245422363\n",
      "      kl: 0.0009402100113220513\n",
      "      policy_loss: -0.006504802033305168\n",
      "      total_loss: 22.40303611755371\n",
      "      vf_explained_var: 0.5447376370429993\n",
      "      vf_loss: 22.409542083740234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1368683941568192e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3609126806259155\n",
      "      kl: 0.001613192493095994\n",
      "      policy_loss: -0.00089441635645926\n",
      "      total_loss: 77.21517181396484\n",
      "      vf_explained_var: 0.11921229213476181\n",
      "      vf_loss: 77.216064453125\n",
      "    sample_time_ms: 6566.023\n",
      "    update_time_ms: 8.444\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 19.92298038029353\n",
      "    rl_1: 30.277882788327428\n",
      "  time_since_restore: 361.59654092788696\n",
      "  time_this_iter_s: 6.117276668548584\n",
      "  time_total_s: 361.59654092788696\n",
      "  timestamp: 1552315594\n",
      "  timesteps_since_restore: 99000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 45\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 361 s, 45 iter, 99000 ts, 50.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-46-40\n",
      "  done: false\n",
      "  episode_len_mean: 102.16\n",
      "  episode_reward_max: 116.53629364089227\n",
      "  episode_reward_mean: 56.06464782156155\n",
      "  episode_reward_min: -160.67461602609353\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 985\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 919.695\n",
      "    load_time_ms: 2.904\n",
      "    num_steps_sampled: 101200\n",
      "    num_steps_trained: 101200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3936586380004883\n",
      "      kl: 0.002373667433857918\n",
      "      policy_loss: -0.004891814664006233\n",
      "      total_loss: 25.40845489501953\n",
      "      vf_explained_var: 0.6157236695289612\n",
      "      vf_loss: 25.413347244262695\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3389326333999634\n",
      "      kl: 0.0009195870370604098\n",
      "      policy_loss: 0.0022550274152308702\n",
      "      total_loss: 101.2724838256836\n",
      "      vf_explained_var: 0.15486007928848267\n",
      "      vf_loss: 101.27021789550781\n",
      "    sample_time_ms: 6637.376\n",
      "    update_time_ms: 8.589\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 21.788770818099735\n",
      "    rl_1: 34.275877003461815\n",
      "  time_since_restore: 368.1558401584625\n",
      "  time_this_iter_s: 6.5592992305755615\n",
      "  time_total_s: 368.1558401584625\n",
      "  timestamp: 1552315600\n",
      "  timesteps_since_restore: 101200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 101200\n",
      "  training_iteration: 46\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 368 s, 46 iter, 101200 ts, 56.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-46-50\n",
      "  done: false\n",
      "  episode_len_mean: 102.49\n",
      "  episode_reward_max: 116.53629364089227\n",
      "  episode_reward_mean: 61.543194502568106\n",
      "  episode_reward_min: -160.67461602609353\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1006\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 993.408\n",
      "    load_time_ms: 2.982\n",
      "    num_steps_sampled: 103400\n",
      "    num_steps_trained: 103400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1368683941568192e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3939464092254639\n",
      "      kl: 0.001368899829685688\n",
      "      policy_loss: 0.0005863593542017043\n",
      "      total_loss: 30.396818161010742\n",
      "      vf_explained_var: 0.5937097668647766\n",
      "      vf_loss: 30.39623260498047\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.842170985392048e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3509786128997803\n",
      "      kl: 0.0031514596194028854\n",
      "      policy_loss: -0.0026217226404696703\n",
      "      total_loss: 80.53406524658203\n",
      "      vf_explained_var: 0.10400684922933578\n",
      "      vf_loss: 80.53668212890625\n",
      "    sample_time_ms: 6733.712\n",
      "    update_time_ms: 8.613\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.33759780191164\n",
      "    rl_1: 37.205596700656464\n",
      "  time_since_restore: 377.4123396873474\n",
      "  time_this_iter_s: 9.256499528884888\n",
      "  time_total_s: 377.4123396873474\n",
      "  timestamp: 1552315610\n",
      "  timesteps_since_restore: 103400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 103400\n",
      "  training_iteration: 47\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 377 s, 47 iter, 103400 ts, 61.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-46-58\n",
      "  done: false\n",
      "  episode_len_mean: 102.77\n",
      "  episode_reward_max: 116.53629364089227\n",
      "  episode_reward_mean: 62.118870691221765\n",
      "  episode_reward_min: -148.9985836892733\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1028\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1001.681\n",
      "    load_time_ms: 2.957\n",
      "    num_steps_sampled: 105600\n",
      "    num_steps_trained: 105600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.385325312614441\n",
      "      kl: 0.0016682506538927555\n",
      "      policy_loss: -0.0007702208822593093\n",
      "      total_loss: 136.2122802734375\n",
      "      vf_explained_var: 0.37482568621635437\n",
      "      vf_loss: 136.21304321289062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.421085492696024e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3370859622955322\n",
      "      kl: 0.0013936202740296721\n",
      "      policy_loss: -0.0009288202854804695\n",
      "      total_loss: 205.5730743408203\n",
      "      vf_explained_var: 0.15466050803661346\n",
      "      vf_loss: 205.57400512695312\n",
      "    sample_time_ms: 6865.884\n",
      "    update_time_ms: 9.123\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.01299560642169\n",
      "    rl_1: 37.10587508480008\n",
      "  time_since_restore: 386.0810532569885\n",
      "  time_this_iter_s: 8.668713569641113\n",
      "  time_total_s: 386.0810532569885\n",
      "  timestamp: 1552315618\n",
      "  timesteps_since_restore: 105600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 105600\n",
      "  training_iteration: 48\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 386 s, 48 iter, 105600 ts, 62.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-47-06\n",
      "  done: false\n",
      "  episode_len_mean: 102.92\n",
      "  episode_reward_max: 102.42384559727483\n",
      "  episode_reward_mean: 68.59957717065421\n",
      "  episode_reward_min: -127.25869950293482\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1049\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 996.995\n",
      "    load_time_ms: 3.069\n",
      "    num_steps_sampled: 107800\n",
      "    num_steps_trained: 107800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.842170985392048e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3775229454040527\n",
      "      kl: 0.002559981308877468\n",
      "      policy_loss: -0.005680664908140898\n",
      "      total_loss: 24.3765811920166\n",
      "      vf_explained_var: 0.6840489506721497\n",
      "      vf_loss: 24.382261276245117\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.333676815032959\n",
      "      kl: 0.00154778640717268\n",
      "      policy_loss: -0.00373885128647089\n",
      "      total_loss: 138.12489318847656\n",
      "      vf_explained_var: 0.11071348190307617\n",
      "      vf_loss: 138.12864685058594\n",
      "    sample_time_ms: 6993.163\n",
      "    update_time_ms: 9.766\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.071863694929263\n",
      "    rl_1: 43.527713475724966\n",
      "  time_since_restore: 393.9870502948761\n",
      "  time_this_iter_s: 7.905997037887573\n",
      "  time_total_s: 393.9870502948761\n",
      "  timestamp: 1552315626\n",
      "  timesteps_since_restore: 107800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 107800\n",
      "  training_iteration: 49\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 393 s, 49 iter, 107800 ts, 68.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-47-13\n",
      "  done: false\n",
      "  episode_len_mean: 102.92\n",
      "  episode_reward_max: 108.93552095474004\n",
      "  episode_reward_mean: 71.5013607335509\n",
      "  episode_reward_min: -127.25869950293482\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1071\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 958.565\n",
      "    load_time_ms: 2.809\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.421085492696024e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.380571961402893\n",
      "      kl: 0.0011150395730510354\n",
      "      policy_loss: -0.0006647999980486929\n",
      "      total_loss: 38.569034576416016\n",
      "      vf_explained_var: 0.5936959385871887\n",
      "      vf_loss: 38.569698333740234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.55271373174006e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3419413566589355\n",
      "      kl: 0.004901418462395668\n",
      "      policy_loss: -0.006301542278379202\n",
      "      total_loss: 139.22059631347656\n",
      "      vf_explained_var: 0.1437995880842209\n",
      "      vf_loss: 139.22689819335938\n",
      "    sample_time_ms: 6731.396\n",
      "    update_time_ms: 9.67\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.21771691888466\n",
      "    rl_1: 45.28364381466623\n",
      "  time_since_restore: 400.57683181762695\n",
      "  time_this_iter_s: 6.5897815227508545\n",
      "  time_total_s: 400.57683181762695\n",
      "  timestamp: 1552315633\n",
      "  timesteps_since_restore: 110000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 50\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 400 s, 50 iter, 110000 ts, 71.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-47-20\n",
      "  done: false\n",
      "  episode_len_mean: 102.83\n",
      "  episode_reward_max: 108.93552095474004\n",
      "  episode_reward_mean: 72.29606770061518\n",
      "  episode_reward_min: -127.25869950293482\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1092\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 935.798\n",
      "    load_time_ms: 2.862\n",
      "    num_steps_sampled: 112200\n",
      "    num_steps_trained: 112200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3819501399993896\n",
      "      kl: 0.002004914917051792\n",
      "      policy_loss: -0.0018652856815606356\n",
      "      total_loss: 96.21573638916016\n",
      "      vf_explained_var: 0.4065879285335541\n",
      "      vf_loss: 96.21759033203125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.77635686587003e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3271936178207397\n",
      "      kl: 0.0008615124388597906\n",
      "      policy_loss: -0.004484072793275118\n",
      "      total_loss: 187.27459716796875\n",
      "      vf_explained_var: 0.16480077803134918\n",
      "      vf_loss: 187.2790985107422\n",
      "    sample_time_ms: 6632.385\n",
      "    update_time_ms: 9.729\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 27.144507260079482\n",
      "    rl_1: 45.1515604405357\n",
      "  time_since_restore: 407.1098210811615\n",
      "  time_this_iter_s: 6.532989263534546\n",
      "  time_total_s: 407.1098210811615\n",
      "  timestamp: 1552315640\n",
      "  timesteps_since_restore: 112200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 112200\n",
      "  training_iteration: 51\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 407 s, 51 iter, 112200 ts, 72.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-47-26\n",
      "  done: false\n",
      "  episode_len_mean: 102.59\n",
      "  episode_reward_max: 108.93552095474004\n",
      "  episode_reward_mean: 69.3096249715133\n",
      "  episode_reward_min: -137.0786099596445\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1114\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 933.981\n",
      "    load_time_ms: 2.726\n",
      "    num_steps_sampled: 114400\n",
      "    num_steps_trained: 114400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.55271373174006e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3780590295791626\n",
      "      kl: 0.006549334153532982\n",
      "      policy_loss: -0.007490434218198061\n",
      "      total_loss: 209.74435424804688\n",
      "      vf_explained_var: 0.3273541033267975\n",
      "      vf_loss: 209.7518310546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.88178432935015e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.347240924835205\n",
      "      kl: 0.0011209890944883227\n",
      "      policy_loss: -0.001484400243498385\n",
      "      total_loss: 273.9070129394531\n",
      "      vf_explained_var: 0.17657166719436646\n",
      "      vf_loss: 273.90850830078125\n",
      "    sample_time_ms: 6553.391\n",
      "    update_time_ms: 9.753\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.667766804837733\n",
      "    rl_1: 43.641858166675576\n",
      "  time_since_restore: 413.360552072525\n",
      "  time_this_iter_s: 6.250730991363525\n",
      "  time_total_s: 413.360552072525\n",
      "  timestamp: 1552315646\n",
      "  timesteps_since_restore: 114400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 114400\n",
      "  training_iteration: 52\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 413 s, 52 iter, 114400 ts, 69.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-47-34\n",
      "  done: false\n",
      "  episode_len_mean: 102.4\n",
      "  episode_reward_max: 110.22888811754929\n",
      "  episode_reward_mean: 69.98141802148047\n",
      "  episode_reward_min: -137.0786099596445\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1136\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 937.12\n",
      "    load_time_ms: 2.729\n",
      "    num_steps_sampled: 116600\n",
      "    num_steps_trained: 116600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.77635686587003e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.384621500968933\n",
      "      kl: 0.002628365531563759\n",
      "      policy_loss: -0.0039031729102134705\n",
      "      total_loss: 201.99423217773438\n",
      "      vf_explained_var: 0.3603609800338745\n",
      "      vf_loss: 201.9981231689453\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3408524990081787\n",
      "      kl: 0.0003652065934147686\n",
      "      policy_loss: -0.0021258946508169174\n",
      "      total_loss: 316.7016906738281\n",
      "      vf_explained_var: 0.17638717591762543\n",
      "      vf_loss: 316.703857421875\n",
      "    sample_time_ms: 6399.665\n",
      "    update_time_ms: 9.369\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.774126291313205\n",
      "    rl_1: 44.207291730167256\n",
      "  time_since_restore: 421.4564127922058\n",
      "  time_this_iter_s: 8.095860719680786\n",
      "  time_total_s: 421.4564127922058\n",
      "  timestamp: 1552315654\n",
      "  timesteps_since_restore: 116600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 116600\n",
      "  training_iteration: 53\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 421 s, 53 iter, 116600 ts, 70 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-47-42\n",
      "  done: false\n",
      "  episode_len_mean: 101.96\n",
      "  episode_reward_max: 117.52134028654802\n",
      "  episode_reward_mean: 67.53679807739981\n",
      "  episode_reward_min: -160.8371282295389\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1157\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 938.179\n",
      "    load_time_ms: 2.769\n",
      "    num_steps_sampled: 118800\n",
      "    num_steps_trained: 118800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.88178432935015e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3971664905548096\n",
      "      kl: 0.004811114631593227\n",
      "      policy_loss: -0.0038874242454767227\n",
      "      total_loss: 166.98667907714844\n",
      "      vf_explained_var: 0.39264586567878723\n",
      "      vf_loss: 166.99057006835938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2204460823375376e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3246972560882568\n",
      "      kl: 0.006409239023923874\n",
      "      policy_loss: -0.006407151930034161\n",
      "      total_loss: 250.3096923828125\n",
      "      vf_explained_var: 0.1804431974887848\n",
      "      vf_loss: 250.3160858154297\n",
      "    sample_time_ms: 6457.217\n",
      "    update_time_ms: 9.371\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.400453860312997\n",
      "    rl_1: 41.136344217086815\n",
      "  time_since_restore: 429.6759240627289\n",
      "  time_this_iter_s: 8.219511270523071\n",
      "  time_total_s: 429.6759240627289\n",
      "  timestamp: 1552315662\n",
      "  timesteps_since_restore: 118800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 118800\n",
      "  training_iteration: 54\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 429 s, 54 iter, 118800 ts, 67.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-47-48\n",
      "  done: false\n",
      "  episode_len_mean: 101.93\n",
      "  episode_reward_max: 117.52134028654802\n",
      "  episode_reward_mean: 69.21033588026425\n",
      "  episode_reward_min: -160.8371282295389\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1179\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 946.168\n",
      "    load_time_ms: 2.739\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4010310173034668\n",
      "      kl: 0.001192743657156825\n",
      "      policy_loss: -0.006892018485814333\n",
      "      total_loss: 91.93435668945312\n",
      "      vf_explained_var: 0.549750804901123\n",
      "      vf_loss: 91.94124603271484\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1102230411687688e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3179749250411987\n",
      "      kl: 0.0020267306827008724\n",
      "      policy_loss: -0.0002644478518050164\n",
      "      total_loss: 183.19183349609375\n",
      "      vf_explained_var: 0.22021490335464478\n",
      "      vf_loss: 183.19212341308594\n",
      "    sample_time_ms: 6424.27\n",
      "    update_time_ms: 9.34\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.37428967546133\n",
      "    rl_1: 43.83604620480293\n",
      "  time_since_restore: 435.54376769065857\n",
      "  time_this_iter_s: 5.8678436279296875\n",
      "  time_total_s: 435.54376769065857\n",
      "  timestamp: 1552315668\n",
      "  timesteps_since_restore: 121000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 55\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 435 s, 55 iter, 121000 ts, 69.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-47-55\n",
      "  done: false\n",
      "  episode_len_mean: 101.15\n",
      "  episode_reward_max: 117.52134028654802\n",
      "  episode_reward_mean: 61.6705146930444\n",
      "  episode_reward_min: -160.8371282295389\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1200\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 922.785\n",
      "    load_time_ms: 2.796\n",
      "    num_steps_sampled: 123200\n",
      "    num_steps_trained: 123200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2204460823375376e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4080716371536255\n",
      "      kl: 0.001019554678350687\n",
      "      policy_loss: -0.002465869765728712\n",
      "      total_loss: 235.42906188964844\n",
      "      vf_explained_var: 0.36206790804862976\n",
      "      vf_loss: 235.4315185546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.551115205843844e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3191567659378052\n",
      "      kl: 0.0028930536936968565\n",
      "      policy_loss: -0.00011869651643792167\n",
      "      total_loss: 319.326416015625\n",
      "      vf_explained_var: 0.17316219210624695\n",
      "      vf_loss: 319.3265380859375\n",
      "    sample_time_ms: 6416.756\n",
      "    update_time_ms: 9.338\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.807273067742244\n",
      "    rl_1: 38.86324162530216\n",
      "  time_since_restore: 441.7931649684906\n",
      "  time_this_iter_s: 6.249397277832031\n",
      "  time_total_s: 441.7931649684906\n",
      "  timestamp: 1552315675\n",
      "  timesteps_since_restore: 123200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 123200\n",
      "  training_iteration: 56\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 441 s, 56 iter, 123200 ts, 61.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-48-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.9\n",
      "  episode_reward_max: 117.52134028654802\n",
      "  episode_reward_mean: 64.0863414319813\n",
      "  episode_reward_min: -160.8371282295389\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1223\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 843.369\n",
      "    load_time_ms: 2.609\n",
      "    num_steps_sampled: 125400\n",
      "    num_steps_trained: 125400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1102230411687688e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3824224472045898\n",
      "      kl: 0.003277502255514264\n",
      "      policy_loss: -0.00413086824119091\n",
      "      total_loss: 261.564697265625\n",
      "      vf_explained_var: 0.3824574947357178\n",
      "      vf_loss: 261.5688171386719\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.775557602921922e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3324742317199707\n",
      "      kl: 0.0056362999603152275\n",
      "      policy_loss: -0.0037718210369348526\n",
      "      total_loss: 315.1944274902344\n",
      "      vf_explained_var: 0.2790501117706299\n",
      "      vf_loss: 315.1982421875\n",
      "    sample_time_ms: 6149.923\n",
      "    update_time_ms: 9.364\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.611184804775284\n",
      "    rl_1: 39.475156627206005\n",
      "  time_since_restore: 447.5757999420166\n",
      "  time_this_iter_s: 5.782634973526001\n",
      "  time_total_s: 447.5757999420166\n",
      "  timestamp: 1552315680\n",
      "  timesteps_since_restore: 125400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 125400\n",
      "  training_iteration: 57\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 447 s, 57 iter, 125400 ts, 64.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-48-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.37\n",
      "  episode_reward_max: 116.75436156954243\n",
      "  episode_reward_mean: 58.78700013793307\n",
      "  episode_reward_min: -162.73696315732786\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1245\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 837.463\n",
      "    load_time_ms: 2.549\n",
      "    num_steps_sampled: 127600\n",
      "    num_steps_trained: 127600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.551115205843844e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3977795839309692\n",
      "      kl: 0.0018476133700460196\n",
      "      policy_loss: -0.0025945864617824554\n",
      "      total_loss: 209.58309936523438\n",
      "      vf_explained_var: 0.4369891285896301\n",
      "      vf_loss: 209.58570861816406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.387778801460961e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3308476209640503\n",
      "      kl: 0.0022464385256171227\n",
      "      policy_loss: -0.005814654286950827\n",
      "      total_loss: 291.47198486328125\n",
      "      vf_explained_var: 0.26430028676986694\n",
      "      vf_loss: 291.477783203125\n",
      "    sample_time_ms: 6036.711\n",
      "    update_time_ms: 8.723\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 20.68418982609814\n",
      "    rl_1: 38.10281031183493\n",
      "  time_since_restore: 455.0416271686554\n",
      "  time_this_iter_s: 7.465827226638794\n",
      "  time_total_s: 455.0416271686554\n",
      "  timestamp: 1552315688\n",
      "  timesteps_since_restore: 127600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 127600\n",
      "  training_iteration: 58\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 455 s, 58 iter, 127600 ts, 58.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-48-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.78\n",
      "  episode_reward_max: 116.75436156954243\n",
      "  episode_reward_mean: 60.72312129453829\n",
      "  episode_reward_min: -162.73696315732786\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1266\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 836.937\n",
      "    load_time_ms: 2.401\n",
      "    num_steps_sampled: 129800\n",
      "    num_steps_trained: 129800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.775557602921922e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3934454917907715\n",
      "      kl: 0.002662454964593053\n",
      "      policy_loss: -0.000585599394980818\n",
      "      total_loss: 89.48026275634766\n",
      "      vf_explained_var: 0.5339718461036682\n",
      "      vf_loss: 89.48085021972656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.938894007304805e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.326550841331482\n",
      "      kl: 0.0017161256400868297\n",
      "      policy_loss: -0.003837733529508114\n",
      "      total_loss: 175.05152893066406\n",
      "      vf_explained_var: 0.3169173300266266\n",
      "      vf_loss: 175.0553741455078\n",
      "    sample_time_ms: 6017.714\n",
      "    update_time_ms: 8.029\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 20.95429173397962\n",
      "    rl_1: 39.76882956055866\n",
      "  time_since_restore: 462.7429220676422\n",
      "  time_this_iter_s: 7.701294898986816\n",
      "  time_total_s: 462.7429220676422\n",
      "  timestamp: 1552315696\n",
      "  timesteps_since_restore: 129800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 129800\n",
      "  training_iteration: 59\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 462 s, 59 iter, 129800 ts, 60.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-48-22\n",
      "  done: false\n",
      "  episode_len_mean: 101.34\n",
      "  episode_reward_max: 116.75436156954243\n",
      "  episode_reward_mean: 63.6466404756071\n",
      "  episode_reward_min: -162.73696315732786\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1287\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 790.566\n",
      "    load_time_ms: 2.328\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.387778801460961e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.404231071472168\n",
      "      kl: 0.0006642249645665288\n",
      "      policy_loss: -0.0009590970003046095\n",
      "      total_loss: 136.61622619628906\n",
      "      vf_explained_var: 0.5170556306838989\n",
      "      vf_loss: 136.6171875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.4694470036524025e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3060617446899414\n",
      "      kl: 0.0045805382542312145\n",
      "      policy_loss: -0.006940081715583801\n",
      "      total_loss: 215.39114379882812\n",
      "      vf_explained_var: 0.3793671429157257\n",
      "      vf_loss: 215.3980712890625\n",
      "    sample_time_ms: 6014.041\n",
      "    update_time_ms: 8.303\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 20.955817140459946\n",
      "    rl_1: 42.69082333514715\n",
      "  time_since_restore: 468.8336889743805\n",
      "  time_this_iter_s: 6.090766906738281\n",
      "  time_total_s: 468.8336889743805\n",
      "  timestamp: 1552315702\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 60\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 468 s, 60 iter, 132000 ts, 63.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-48-29\n",
      "  done: false\n",
      "  episode_len_mean: 101.38\n",
      "  episode_reward_max: 116.75436156954243\n",
      "  episode_reward_mean: 61.97940593832325\n",
      "  episode_reward_min: -162.73696315732786\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1309\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 808.71\n",
      "    load_time_ms: 2.24\n",
      "    num_steps_sampled: 134200\n",
      "    num_steps_trained: 134200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.938894007304805e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4044438600540161\n",
      "      kl: 0.005044896621257067\n",
      "      policy_loss: -0.007056592497974634\n",
      "      total_loss: 204.4497833251953\n",
      "      vf_explained_var: 0.45834410190582275\n",
      "      vf_loss: 204.45684814453125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3066962957382202\n",
      "      kl: 0.00021447788458317518\n",
      "      policy_loss: -0.0007520879153162241\n",
      "      total_loss: 282.3316955566406\n",
      "      vf_explained_var: 0.3368337154388428\n",
      "      vf_loss: 282.33245849609375\n",
      "    sample_time_ms: 6043.243\n",
      "    update_time_ms: 7.993\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 19.99485726599198\n",
      "    rl_1: 41.98454867233126\n",
      "  time_since_restore: 475.83406376838684\n",
      "  time_this_iter_s: 7.000374794006348\n",
      "  time_total_s: 475.83406376838684\n",
      "  timestamp: 1552315709\n",
      "  timesteps_since_restore: 134200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 134200\n",
      "  training_iteration: 61\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 475 s, 61 iter, 134200 ts, 62 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-48-36\n",
      "  done: false\n",
      "  episode_len_mean: 101.78\n",
      "  episode_reward_max: 116.75436156954243\n",
      "  episode_reward_mean: 64.47828288476701\n",
      "  episode_reward_min: -162.73696315732786\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1331\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 802.488\n",
      "    load_time_ms: 2.285\n",
      "    num_steps_sampled: 136400\n",
      "    num_steps_trained: 136400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.4694470036524025e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4067456722259521\n",
      "      kl: 0.006924677640199661\n",
      "      policy_loss: -0.0071211643517017365\n",
      "      total_loss: 82.88177490234375\n",
      "      vf_explained_var: 0.5276812314987183\n",
      "      vf_loss: 82.88888549804688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.297308087348938\n",
      "      kl: 0.0020255553536117077\n",
      "      policy_loss: -0.002455819398164749\n",
      "      total_loss: 155.1527099609375\n",
      "      vf_explained_var: 0.3053317368030548\n",
      "      vf_loss: 155.1551513671875\n",
      "    sample_time_ms: 6136.42\n",
      "    update_time_ms: 7.925\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 21.851801789355093\n",
      "    rl_1: 42.62648109541191\n",
      "  time_since_restore: 482.95410919189453\n",
      "  time_this_iter_s: 7.12004542350769\n",
      "  time_total_s: 482.95410919189453\n",
      "  timestamp: 1552315716\n",
      "  timesteps_since_restore: 136400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 136400\n",
      "  training_iteration: 62\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 482 s, 62 iter, 136400 ts, 64.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-48-42\n",
      "  done: false\n",
      "  episode_len_mean: 102.38\n",
      "  episode_reward_max: 115.54756501421521\n",
      "  episode_reward_mean: 70.31038840591059\n",
      "  episode_reward_min: -150.73009011305865\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1352\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 798.176\n",
      "    load_time_ms: 2.285\n",
      "    num_steps_sampled: 138600\n",
      "    num_steps_trained: 138600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7347235018262012e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4083000421524048\n",
      "      kl: 0.0005016335635446012\n",
      "      policy_loss: -0.002136233961209655\n",
      "      total_loss: 27.764339447021484\n",
      "      vf_explained_var: 0.7374944090843201\n",
      "      vf_loss: 27.766475677490234\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.336808754565503e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2828795909881592\n",
      "      kl: 0.00443872669711709\n",
      "      policy_loss: -0.0016554519534111023\n",
      "      total_loss: 124.85613250732422\n",
      "      vf_explained_var: 0.1665368378162384\n",
      "      vf_loss: 124.85779571533203\n",
      "    sample_time_ms: 5967.266\n",
      "    update_time_ms: 7.783\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.687298638597664\n",
      "    rl_1: 46.62308976731291\n",
      "  time_since_restore: 489.31559109687805\n",
      "  time_this_iter_s: 6.3614819049835205\n",
      "  time_total_s: 489.31559109687805\n",
      "  timestamp: 1552315722\n",
      "  timesteps_since_restore: 138600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 138600\n",
      "  training_iteration: 63\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 489 s, 63 iter, 138600 ts, 70.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-48-49\n",
      "  done: false\n",
      "  episode_len_mean: 102.41\n",
      "  episode_reward_max: 118.38508296868442\n",
      "  episode_reward_mean: 73.02265942665775\n",
      "  episode_reward_min: -150.73009011305865\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1373\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 802.473\n",
      "    load_time_ms: 2.208\n",
      "    num_steps_sampled: 140800\n",
      "    num_steps_trained: 140800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.673617509131006e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3907885551452637\n",
      "      kl: 0.003379630157724023\n",
      "      policy_loss: -0.006559035275131464\n",
      "      total_loss: 35.28371810913086\n",
      "      vf_explained_var: 0.6806323528289795\n",
      "      vf_loss: 35.290279388427734\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.1684043772827515e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2664746046066284\n",
      "      kl: 0.003531424328684807\n",
      "      policy_loss: -0.007631953340023756\n",
      "      total_loss: 127.483154296875\n",
      "      vf_explained_var: 0.17850841581821442\n",
      "      vf_loss: 127.49079132080078\n",
      "    sample_time_ms: 5801.207\n",
      "    update_time_ms: 7.679\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.107500703389924\n",
      "    rl_1: 46.91515872326782\n",
      "  time_since_restore: 495.91644835472107\n",
      "  time_this_iter_s: 6.600857257843018\n",
      "  time_total_s: 495.91644835472107\n",
      "  timestamp: 1552315729\n",
      "  timesteps_since_restore: 140800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 140800\n",
      "  training_iteration: 64\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 495 s, 64 iter, 140800 ts, 73 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-48-55\n",
      "  done: false\n",
      "  episode_len_mean: 102.61\n",
      "  episode_reward_max: 118.38508296868442\n",
      "  episode_reward_mean: 78.44263543600829\n",
      "  episode_reward_min: -150.73009011305865\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1395\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 796.108\n",
      "    load_time_ms: 2.159\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.336808754565503e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.375795602798462\n",
      "      kl: 0.004631166812032461\n",
      "      policy_loss: -0.0035513078328222036\n",
      "      total_loss: 88.40113067626953\n",
      "      vf_explained_var: 0.4743059277534485\n",
      "      vf_loss: 88.4046859741211\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0842021886413758e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2813384532928467\n",
      "      kl: 0.0025084943044930696\n",
      "      policy_loss: -0.004341150633990765\n",
      "      total_loss: 148.27639770507812\n",
      "      vf_explained_var: 0.209574356675148\n",
      "      vf_loss: 148.28073120117188\n",
      "    sample_time_ms: 5842.918\n",
      "    update_time_ms: 7.547\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.991699441055417\n",
      "    rl_1: 49.45093599495288\n",
      "  time_since_restore: 502.13406205177307\n",
      "  time_this_iter_s: 6.217613697052002\n",
      "  time_total_s: 502.13406205177307\n",
      "  timestamp: 1552315735\n",
      "  timesteps_since_restore: 143000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 65\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 502 s, 65 iter, 143000 ts, 78.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-49-01\n",
      "  done: false\n",
      "  episode_len_mean: 102.95\n",
      "  episode_reward_max: 118.38508296868442\n",
      "  episode_reward_mean: 82.6751257009954\n",
      "  episode_reward_min: -134.39917623865813\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1416\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 797.829\n",
      "    load_time_ms: 2.192\n",
      "    num_steps_sampled: 145200\n",
      "    num_steps_trained: 145200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.1684043772827515e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3559379577636719\n",
      "      kl: 0.006359677296131849\n",
      "      policy_loss: -0.0020796428434550762\n",
      "      total_loss: 96.67365264892578\n",
      "      vf_explained_var: 0.506732165813446\n",
      "      vf_loss: 96.67572784423828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.421010943206879e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3037689924240112\n",
      "      kl: 0.006343628745526075\n",
      "      policy_loss: -0.0059690335765480995\n",
      "      total_loss: 153.96678161621094\n",
      "      vf_explained_var: 0.29684439301490784\n",
      "      vf_loss: 153.97274780273438\n",
      "    sample_time_ms: 5823.044\n",
      "    update_time_ms: 7.571\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.5830193953045\n",
      "    rl_1: 51.09210630569091\n",
      "  time_since_restore: 508.2045273780823\n",
      "  time_this_iter_s: 6.070465326309204\n",
      "  time_total_s: 508.2045273780823\n",
      "  timestamp: 1552315741\n",
      "  timesteps_since_restore: 145200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 145200\n",
      "  training_iteration: 66\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 508 s, 66 iter, 145200 ts, 82.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-49-10\n",
      "  done: false\n",
      "  episode_len_mean: 102.49\n",
      "  episode_reward_max: 118.38508296868442\n",
      "  episode_reward_mean: 76.21178092406001\n",
      "  episode_reward_min: -163.1478785198502\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1438\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 805.655\n",
      "    load_time_ms: 2.32\n",
      "    num_steps_sampled: 147400\n",
      "    num_steps_trained: 147400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842021886413758e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.367327094078064\n",
      "      kl: 0.007582902908325195\n",
      "      policy_loss: -0.010718115605413914\n",
      "      total_loss: 159.0674591064453\n",
      "      vf_explained_var: 0.4216887354850769\n",
      "      vf_loss: 159.07815551757812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.7105054716034394e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.267697811126709\n",
      "      kl: 0.00011533142969710752\n",
      "      policy_loss: -0.0014412534656003118\n",
      "      total_loss: 261.24652099609375\n",
      "      vf_explained_var: 0.26548314094543457\n",
      "      vf_loss: 261.2479553222656\n",
      "    sample_time_ms: 6124.716\n",
      "    update_time_ms: 7.496\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.16947132966593\n",
      "    rl_1: 47.042309594394084\n",
      "  time_since_restore: 517.0841403007507\n",
      "  time_this_iter_s: 8.879612922668457\n",
      "  time_total_s: 517.0841403007507\n",
      "  timestamp: 1552315750\n",
      "  timesteps_since_restore: 147400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 147400\n",
      "  training_iteration: 67\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 517 s, 67 iter, 147400 ts, 76.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-49-17\n",
      "  done: false\n",
      "  episode_len_mean: 102.19\n",
      "  episode_reward_max: 119.46783972086463\n",
      "  episode_reward_mean: 75.11302763492152\n",
      "  episode_reward_min: -163.1478785198502\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1460\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 805.527\n",
      "    load_time_ms: 2.415\n",
      "    num_steps_sampled: 149600\n",
      "    num_steps_trained: 149600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.421010943206879e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3600413799285889\n",
      "      kl: 0.004816669505089521\n",
      "      policy_loss: -0.002526896772906184\n",
      "      total_loss: 91.52228546142578\n",
      "      vf_explained_var: 0.4960556626319885\n",
      "      vf_loss: 91.52481842041016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.3552527358017197e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2745238542556763\n",
      "      kl: 0.005921041127294302\n",
      "      policy_loss: -0.004366076085716486\n",
      "      total_loss: 171.76136779785156\n",
      "      vf_explained_var: 0.2024955451488495\n",
      "      vf_loss: 171.7657470703125\n",
      "    sample_time_ms: 5988.26\n",
      "    update_time_ms: 7.616\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.37972619375786\n",
      "    rl_1: 45.73330144116368\n",
      "  time_since_restore: 523.1867847442627\n",
      "  time_this_iter_s: 6.102644443511963\n",
      "  time_total_s: 523.1867847442627\n",
      "  timestamp: 1552315757\n",
      "  timesteps_since_restore: 149600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 149600\n",
      "  training_iteration: 68\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 523 s, 68 iter, 149600 ts, 75.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-49-26\n",
      "  done: false\n",
      "  episode_len_mean: 102.05\n",
      "  episode_reward_max: 119.46783972086463\n",
      "  episode_reward_mean: 72.35481360424646\n",
      "  episode_reward_min: -163.1478785198502\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1481\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 838.486\n",
      "    load_time_ms: 2.692\n",
      "    num_steps_sampled: 151800\n",
      "    num_steps_trained: 151800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.7105054716034394e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3534585237503052\n",
      "      kl: 0.006755320355296135\n",
      "      policy_loss: -0.006491772830486298\n",
      "      total_loss: 90.60169219970703\n",
      "      vf_explained_var: 0.5422555804252625\n",
      "      vf_loss: 90.60818481445312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.776263679008599e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2433054447174072\n",
      "      kl: 0.005309343338012695\n",
      "      policy_loss: -0.0015697154449298978\n",
      "      total_loss: 199.0511932373047\n",
      "      vf_explained_var: 0.29262423515319824\n",
      "      vf_loss: 199.05276489257812\n",
      "    sample_time_ms: 6076.191\n",
      "    update_time_ms: 7.605\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.897148125591524\n",
      "    rl_1: 45.457665478654924\n",
      "  time_since_restore: 532.1029012203217\n",
      "  time_this_iter_s: 8.91611647605896\n",
      "  time_total_s: 532.1029012203217\n",
      "  timestamp: 1552315766\n",
      "  timesteps_since_restore: 151800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 151800\n",
      "  training_iteration: 69\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 532 s, 69 iter, 151800 ts, 72.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-49-33\n",
      "  done: false\n",
      "  episode_len_mean: 101.8\n",
      "  episode_reward_max: 119.46783972086463\n",
      "  episode_reward_mean: 70.92109371800555\n",
      "  episode_reward_min: -163.1478785198502\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1503\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 860.861\n",
      "    load_time_ms: 2.812\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3552527358017197e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3600672483444214\n",
      "      kl: 0.009381260722875595\n",
      "      policy_loss: -0.0068247150629758835\n",
      "      total_loss: 166.18594360351562\n",
      "      vf_explained_var: 0.4240739047527313\n",
      "      vf_loss: 166.19276428222656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.3881318395042993e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2779502868652344\n",
      "      kl: 0.008363221772015095\n",
      "      policy_loss: -0.010535980574786663\n",
      "      total_loss: 223.36495971679688\n",
      "      vf_explained_var: 0.22418348491191864\n",
      "      vf_loss: 223.37551879882812\n",
      "    sample_time_ms: 6200.947\n",
      "    update_time_ms: 7.716\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.852265099532374\n",
      "    rl_1: 45.068828618473155\n",
      "  time_since_restore: 539.6693727970123\n",
      "  time_this_iter_s: 7.566471576690674\n",
      "  time_total_s: 539.6693727970123\n",
      "  timestamp: 1552315773\n",
      "  timesteps_since_restore: 154000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 70\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 539 s, 70 iter, 154000 ts, 70.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-49-41\n",
      "  done: false\n",
      "  episode_len_mean: 101.24\n",
      "  episode_reward_max: 119.46783972086463\n",
      "  episode_reward_mean: 66.48584665786542\n",
      "  episode_reward_min: -163.1478785198502\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1525\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 859.843\n",
      "    load_time_ms: 2.913\n",
      "    num_steps_sampled: 156200\n",
      "    num_steps_trained: 156200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.776263679008599e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.359902024269104\n",
      "      kl: 0.005083613097667694\n",
      "      policy_loss: -0.0022402587346732616\n",
      "      total_loss: 250.85816955566406\n",
      "      vf_explained_var: 0.44406718015670776\n",
      "      vf_loss: 250.86041259765625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.6940659197521496e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2857437133789062\n",
      "      kl: 0.003352230880409479\n",
      "      policy_loss: -0.00034103760845027864\n",
      "      total_loss: 309.87786865234375\n",
      "      vf_explained_var: 0.3425147831439972\n",
      "      vf_loss: 309.878173828125\n",
      "    sample_time_ms: 6318.634\n",
      "    update_time_ms: 8.642\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.221208840729002\n",
      "    rl_1: 43.26463781713642\n",
      "  time_since_restore: 547.8460311889648\n",
      "  time_this_iter_s: 8.176658391952515\n",
      "  time_total_s: 547.8460311889648\n",
      "  timestamp: 1552315781\n",
      "  timesteps_since_restore: 156200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 156200\n",
      "  training_iteration: 71\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 547 s, 71 iter, 156200 ts, 66.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-49-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.89\n",
      "  episode_reward_max: 116.97803103655126\n",
      "  episode_reward_mean: 62.49566344776931\n",
      "  episode_reward_min: -160.1741724859164\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1547\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 927.46\n",
      "    load_time_ms: 3.051\n",
      "    num_steps_sampled: 158400\n",
      "    num_steps_trained: 158400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.3881318395042993e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.362969160079956\n",
      "      kl: 0.0009832007344812155\n",
      "      policy_loss: -0.004725409206002951\n",
      "      total_loss: 239.039794921875\n",
      "      vf_explained_var: 0.47694721817970276\n",
      "      vf_loss: 239.0445098876953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.470329598760748e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.300517201423645\n",
      "      kl: 0.0031971808057278395\n",
      "      policy_loss: -0.005260416306555271\n",
      "      total_loss: 303.8688049316406\n",
      "      vf_explained_var: 0.41612929105758667\n",
      "      vf_loss: 303.87408447265625\n",
      "    sample_time_ms: 6370.16\n",
      "    update_time_ms: 8.565\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 21.430946160334802\n",
      "    rl_1: 41.06471728743451\n",
      "  time_since_restore: 556.1662821769714\n",
      "  time_this_iter_s: 8.320250988006592\n",
      "  time_total_s: 556.1662821769714\n",
      "  timestamp: 1552315790\n",
      "  timesteps_since_restore: 158400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 158400\n",
      "  training_iteration: 72\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 556 s, 72 iter, 158400 ts, 62.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-49-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.85\n",
      "  episode_reward_max: 116.97803103655126\n",
      "  episode_reward_mean: 64.03187503705202\n",
      "  episode_reward_min: -160.1741724859164\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1569\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 940.499\n",
      "    load_time_ms: 3.141\n",
      "    num_steps_sampled: 160600\n",
      "    num_steps_trained: 160600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6940659197521496e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3563123941421509\n",
      "      kl: 0.00654276879504323\n",
      "      policy_loss: -0.0038869474083185196\n",
      "      total_loss: 105.8760757446289\n",
      "      vf_explained_var: 0.5477299690246582\n",
      "      vf_loss: 105.87996673583984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.235164799380374e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3015892505645752\n",
      "      kl: 0.003537242067977786\n",
      "      policy_loss: -0.00298745883628726\n",
      "      total_loss: 165.68841552734375\n",
      "      vf_explained_var: 0.23450955748558044\n",
      "      vf_loss: 165.69140625\n",
      "    sample_time_ms: 6529.335\n",
      "    update_time_ms: 9.102\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.701243865484894\n",
      "    rl_1: 41.33063117156713\n",
      "  time_since_restore: 564.2563569545746\n",
      "  time_this_iter_s: 8.09007477760315\n",
      "  time_total_s: 564.2563569545746\n",
      "  timestamp: 1552315798\n",
      "  timesteps_since_restore: 160600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 160600\n",
      "  training_iteration: 73\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 564 s, 73 iter, 160600 ts, 64 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-50-05\n",
      "  done: false\n",
      "  episode_len_mean: 101.16\n",
      "  episode_reward_max: 116.97803103655126\n",
      "  episode_reward_mean: 69.66707097016112\n",
      "  episode_reward_min: -153.1166431676664\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1589\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 941.927\n",
      "    load_time_ms: 3.193\n",
      "    num_steps_sampled: 162800\n",
      "    num_steps_trained: 162800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.470329598760748e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3423759937286377\n",
      "      kl: 0.00767109077423811\n",
      "      policy_loss: -0.005922486539930105\n",
      "      total_loss: 48.13690948486328\n",
      "      vf_explained_var: 0.7172014713287354\n",
      "      vf_loss: 48.14283752441406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.117582399690187e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.283756971359253\n",
      "      kl: 0.009320701472461224\n",
      "      policy_loss: -0.008322150446474552\n",
      "      total_loss: 170.46580505371094\n",
      "      vf_explained_var: 0.22163042426109314\n",
      "      vf_loss: 170.47412109375\n",
      "    sample_time_ms: 6557.778\n",
      "    update_time_ms: 9.211\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.95592572717149\n",
      "    rl_1: 45.71114524298963\n",
      "  time_since_restore: 571.1580231189728\n",
      "  time_this_iter_s: 6.901666164398193\n",
      "  time_total_s: 571.1580231189728\n",
      "  timestamp: 1552315805\n",
      "  timesteps_since_restore: 162800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 162800\n",
      "  training_iteration: 74\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 571 s, 74 iter, 162800 ts, 69.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-50-11\n",
      "  done: false\n",
      "  episode_len_mean: 101.38\n",
      "  episode_reward_max: 133.78899390907068\n",
      "  episode_reward_mean: 75.51180438813883\n",
      "  episode_reward_min: -153.1166431676664\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1611\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 944.117\n",
      "    load_time_ms: 3.252\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.235164799380374e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3202283382415771\n",
      "      kl: 0.0034622319508343935\n",
      "      policy_loss: -0.0008284243522211909\n",
      "      total_loss: 165.8035888671875\n",
      "      vf_explained_var: 0.5286526679992676\n",
      "      vf_loss: 165.80441284179688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3596882820129395\n",
      "      kl: 0.004297970328480005\n",
      "      policy_loss: -0.006750402040779591\n",
      "      total_loss: 231.97767639160156\n",
      "      vf_explained_var: 0.22743827104568481\n",
      "      vf_loss: 231.98443603515625\n",
      "    sample_time_ms: 6569.272\n",
      "    update_time_ms: 9.214\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 27.515014530174575\n",
      "    rl_1: 47.99678985796427\n",
      "  time_since_restore: 577.5137205123901\n",
      "  time_this_iter_s: 6.355697393417358\n",
      "  time_total_s: 577.5137205123901\n",
      "  timestamp: 1552315811\n",
      "  timesteps_since_restore: 165000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 75\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 577 s, 75 iter, 165000 ts, 75.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 101.45\n",
      "  episode_reward_max: 133.78899390907068\n",
      "  episode_reward_mean: 78.5004548760441\n",
      "  episode_reward_min: -153.1166431676664\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1633\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 951.912\n",
      "    load_time_ms: 3.305\n",
      "    num_steps_sampled: 167200\n",
      "    num_steps_trained: 167200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.117582399690187e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3415347337722778\n",
      "      kl: 0.0038474281318485737\n",
      "      policy_loss: -0.0028364912141114473\n",
      "      total_loss: 151.04222106933594\n",
      "      vf_explained_var: 0.48708483576774597\n",
      "      vf_loss: 151.0450439453125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3116341829299927\n",
      "      kl: 0.0010354699334129691\n",
      "      policy_loss: 0.00027542319730855525\n",
      "      total_loss: 258.77490234375\n",
      "      vf_explained_var: 0.22737188637256622\n",
      "      vf_loss: 258.7746276855469\n",
      "    sample_time_ms: 6600.03\n",
      "    update_time_ms: 9.064\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.759427269970626\n",
      "    rl_1: 49.74102760607347\n",
      "  time_since_restore: 583.9678418636322\n",
      "  time_this_iter_s: 6.454121351242065\n",
      "  time_total_s: 583.9678418636322\n",
      "  timestamp: 1552315818\n",
      "  timesteps_since_restore: 167200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 167200\n",
      "  training_iteration: 76\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 583 s, 76 iter, 167200 ts, 78.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-50-26\n",
      "  done: false\n",
      "  episode_len_mean: 102.07\n",
      "  episode_reward_max: 139.79768049301765\n",
      "  episode_reward_mean: 87.74725710952721\n",
      "  episode_reward_min: -149.59888981775495\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1655\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 952.513\n",
      "    load_time_ms: 3.202\n",
      "    num_steps_sampled: 169400\n",
      "    num_steps_trained: 169400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0587911998450935e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.361802101135254\n",
      "      kl: 0.0007515358738601208\n",
      "      policy_loss: -0.0017211107769981027\n",
      "      total_loss: 147.1157989501953\n",
      "      vf_explained_var: 0.4807749390602112\n",
      "      vf_loss: 147.11752319335938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.646977999612734e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3122520446777344\n",
      "      kl: 0.005605023354291916\n",
      "      policy_loss: -0.003029076848179102\n",
      "      total_loss: 282.06512451171875\n",
      "      vf_explained_var: 0.2744145095348358\n",
      "      vf_loss: 282.0681457519531\n",
      "    sample_time_ms: 6509.893\n",
      "    update_time_ms: 9.107\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.52371237856915\n",
      "    rl_1: 55.22354473095809\n",
      "  time_since_restore: 591.9510807991028\n",
      "  time_this_iter_s: 7.983238935470581\n",
      "  time_total_s: 591.9510807991028\n",
      "  timestamp: 1552315826\n",
      "  timesteps_since_restore: 169400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 169400\n",
      "  training_iteration: 77\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 591 s, 77 iter, 169400 ts, 87.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-50-34\n",
      "  done: false\n",
      "  episode_len_mean: 101.44\n",
      "  episode_reward_max: 139.79768049301765\n",
      "  episode_reward_mean: 84.19237979771322\n",
      "  episode_reward_min: -163.27029613688\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1677\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 953.687\n",
      "    load_time_ms: 3.152\n",
      "    num_steps_sampled: 171600\n",
      "    num_steps_trained: 171600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.293955999225468e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3593031167984009\n",
      "      kl: 0.0033095208927989006\n",
      "      policy_loss: -0.010884009301662445\n",
      "      total_loss: 179.676025390625\n",
      "      vf_explained_var: 0.43108031153678894\n",
      "      vf_loss: 179.68690490722656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.323488999806367e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3370567560195923\n",
      "      kl: 0.0028298499528318644\n",
      "      policy_loss: -0.0020475457422435284\n",
      "      total_loss: 318.59149169921875\n",
      "      vf_explained_var: 0.10535289347171783\n",
      "      vf_loss: 318.5935363769531\n",
      "    sample_time_ms: 6688.668\n",
      "    update_time_ms: 9.187\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.509223839860756\n",
      "    rl_1: 53.68315595785245\n",
      "  time_since_restore: 599.8534286022186\n",
      "  time_this_iter_s: 7.902347803115845\n",
      "  time_total_s: 599.8534286022186\n",
      "  timestamp: 1552315834\n",
      "  timesteps_since_restore: 171600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 171600\n",
      "  training_iteration: 78\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 599 s, 78 iter, 171600 ts, 84.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-50-40\n",
      "  done: false\n",
      "  episode_len_mean: 101.21\n",
      "  episode_reward_max: 139.79768049301765\n",
      "  episode_reward_mean: 84.43271341079415\n",
      "  episode_reward_min: -163.27029613688\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1699\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 922.534\n",
      "    load_time_ms: 2.962\n",
      "    num_steps_sampled: 173800\n",
      "    num_steps_trained: 173800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.646977999612734e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.382402777671814\n",
      "      kl: 0.0006212817970663309\n",
      "      policy_loss: -0.001939672278240323\n",
      "      total_loss: 103.42615509033203\n",
      "      vf_explained_var: 0.5800137519836426\n",
      "      vf_loss: 103.42808532714844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.617444999031835e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3290821313858032\n",
      "      kl: 0.0025034132413566113\n",
      "      policy_loss: -0.005869914311915636\n",
      "      total_loss: 280.5065612792969\n",
      "      vf_explained_var: 0.1380237638950348\n",
      "      vf_loss: 280.512451171875\n",
      "    sample_time_ms: 6446.016\n",
      "    update_time_ms: 9.252\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.70206325932453\n",
      "    rl_1: 55.7306501514696\n",
      "  time_since_restore: 606.0266196727753\n",
      "  time_this_iter_s: 6.173191070556641\n",
      "  time_total_s: 606.0266196727753\n",
      "  timestamp: 1552315840\n",
      "  timesteps_since_restore: 173800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 173800\n",
      "  training_iteration: 79\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 606 s, 79 iter, 173800 ts, 84.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-50-50\n",
      "  done: false\n",
      "  episode_len_mean: 101.34\n",
      "  episode_reward_max: 140.11516037014593\n",
      "  episode_reward_mean: 87.23879091836177\n",
      "  episode_reward_min: -163.27029613688\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1720\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 944.245\n",
      "    load_time_ms: 2.934\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.323488999806367e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3549443483352661\n",
      "      kl: 0.0061983815394341946\n",
      "      policy_loss: -0.008554980158805847\n",
      "      total_loss: 103.11883544921875\n",
      "      vf_explained_var: 0.5864095091819763\n",
      "      vf_loss: 103.12739562988281\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.3087224995159173e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3335294723510742\n",
      "      kl: 0.006660297513008118\n",
      "      policy_loss: -0.005850866436958313\n",
      "      total_loss: 284.5208435058594\n",
      "      vf_explained_var: 0.2502578794956207\n",
      "      vf_loss: 284.5267333984375\n",
      "    sample_time_ms: 6635.309\n",
      "    update_time_ms: 8.904\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.22257463637811\n",
      "    rl_1: 61.016216281983674\n",
      "  time_since_restore: 615.7012014389038\n",
      "  time_this_iter_s: 9.67458176612854\n",
      "  time_total_s: 615.7012014389038\n",
      "  timestamp: 1552315850\n",
      "  timesteps_since_restore: 176000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 80\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 615 s, 80 iter, 176000 ts, 87.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-50-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.98\n",
      "  episode_reward_max: 141.1804597758346\n",
      "  episode_reward_mean: 86.45131063385661\n",
      "  episode_reward_min: -164.12589388874073\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1742\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 967.258\n",
      "    load_time_ms: 2.99\n",
      "    num_steps_sampled: 178200\n",
      "    num_steps_trained: 178200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.617444999031835e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3257735967636108\n",
      "      kl: 0.002846567425876856\n",
      "      policy_loss: -0.0036063846200704575\n",
      "      total_loss: 220.28500366210938\n",
      "      vf_explained_var: 0.462974488735199\n",
      "      vf_loss: 220.28860473632812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.6543612497579586e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3617945909500122\n",
      "      kl: 0.007385361008346081\n",
      "      policy_loss: -0.003783831372857094\n",
      "      total_loss: 409.32147216796875\n",
      "      vf_explained_var: 0.1790723204612732\n",
      "      vf_loss: 409.32525634765625\n",
      "    sample_time_ms: 6574.692\n",
      "    update_time_ms: 8.51\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 27.13515318050382\n",
      "    rl_1: 59.31615745335278\n",
      "  time_since_restore: 623.5022249221802\n",
      "  time_this_iter_s: 7.801023483276367\n",
      "  time_total_s: 623.5022249221802\n",
      "  timestamp: 1552315858\n",
      "  timesteps_since_restore: 178200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 178200\n",
      "  training_iteration: 81\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 623 s, 81 iter, 178200 ts, 86.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-51-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.36\n",
      "  episode_reward_max: 144.2570561762953\n",
      "  episode_reward_mean: 81.0315350715557\n",
      "  episode_reward_min: -164.12589388874073\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1764\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 904.45\n",
      "    load_time_ms: 2.924\n",
      "    num_steps_sampled: 180400\n",
      "    num_steps_trained: 180400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.3087224995159173e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3507331609725952\n",
      "      kl: 0.0037039632443338633\n",
      "      policy_loss: -0.004372359719127417\n",
      "      total_loss: 227.41566467285156\n",
      "      vf_explained_var: 0.48328280448913574\n",
      "      vf_loss: 227.4200439453125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.271806248789793e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3537448644638062\n",
      "      kl: 0.0012701429659500718\n",
      "      policy_loss: -0.0004859601613134146\n",
      "      total_loss: 402.27142333984375\n",
      "      vf_explained_var: 0.2603696882724762\n",
      "      vf_loss: 402.2719421386719\n",
      "    sample_time_ms: 6476.703\n",
      "    update_time_ms: 8.89\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.374604248376535\n",
      "    rl_1: 57.65693082317914\n",
      "  time_since_restore: 630.2103786468506\n",
      "  time_this_iter_s: 6.70815372467041\n",
      "  time_total_s: 630.2103786468506\n",
      "  timestamp: 1552315864\n",
      "  timesteps_since_restore: 180400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 180400\n",
      "  training_iteration: 82\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 630 s, 82 iter, 180400 ts, 81 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-51-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.99\n",
      "  episode_reward_max: 145.68272465285912\n",
      "  episode_reward_mean: 91.00125840010496\n",
      "  episode_reward_min: -164.12589388874073\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1786\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 890.656\n",
      "    load_time_ms: 2.836\n",
      "    num_steps_sampled: 182600\n",
      "    num_steps_trained: 182600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6543612497579586e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3004131317138672\n",
      "      kl: 0.002698812633752823\n",
      "      policy_loss: -0.0033493167720735073\n",
      "      total_loss: 48.44464111328125\n",
      "      vf_explained_var: 0.6564598679542542\n",
      "      vf_loss: 48.44799041748047\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.1359031243948966e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3851802349090576\n",
      "      kl: 0.0015271742595359683\n",
      "      policy_loss: -0.004655951634049416\n",
      "      total_loss: 182.4386444091797\n",
      "      vf_explained_var: 0.33255860209465027\n",
      "      vf_loss: 182.44329833984375\n",
      "    sample_time_ms: 6355.463\n",
      "    update_time_ms: 8.262\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.338283433552743\n",
      "    rl_1: 61.66297496655219\n",
      "  time_since_restore: 636.9413001537323\n",
      "  time_this_iter_s: 6.730921506881714\n",
      "  time_total_s: 636.9413001537323\n",
      "  timestamp: 1552315871\n",
      "  timesteps_since_restore: 182600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 182600\n",
      "  training_iteration: 83\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 636 s, 83 iter, 182600 ts, 91 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-51-18\n",
      "  done: false\n",
      "  episode_len_mean: 101.08\n",
      "  episode_reward_max: 145.68272465285912\n",
      "  episode_reward_mean: 93.30255511081519\n",
      "  episode_reward_min: -164.12589388874073\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1808\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 892.003\n",
      "    load_time_ms: 2.826\n",
      "    num_steps_sampled: 184800\n",
      "    num_steps_trained: 184800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.271806248789793e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3024470806121826\n",
      "      kl: 0.00467478483915329\n",
      "      policy_loss: -0.0029069690499454737\n",
      "      total_loss: 176.29437255859375\n",
      "      vf_explained_var: 0.48227450251579285\n",
      "      vf_loss: 176.29727172851562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.0679515621974483e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3904136419296265\n",
      "      kl: 0.0012318697990849614\n",
      "      policy_loss: -0.001153356395661831\n",
      "      total_loss: 285.679443359375\n",
      "      vf_explained_var: 0.30161723494529724\n",
      "      vf_loss: 285.6805725097656\n",
      "    sample_time_ms: 6308.11\n",
      "    update_time_ms: 8.041\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.10019399053933\n",
      "    rl_1: 60.20236112027585\n",
      "  time_since_restore: 643.3805689811707\n",
      "  time_this_iter_s: 6.4392688274383545\n",
      "  time_total_s: 643.3805689811707\n",
      "  timestamp: 1552315878\n",
      "  timesteps_since_restore: 184800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 184800\n",
      "  training_iteration: 84\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 643 s, 84 iter, 184800 ts, 93.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-51-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.61\n",
      "  episode_reward_max: 145.68272465285912\n",
      "  episode_reward_mean: 88.03267394749984\n",
      "  episode_reward_min: -166.57108657434938\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1830\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 888.675\n",
      "    load_time_ms: 2.902\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.1359031243948966e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3478929996490479\n",
      "      kl: 0.003478762460872531\n",
      "      policy_loss: -0.002440081909298897\n",
      "      total_loss: 199.04478454589844\n",
      "      vf_explained_var: 0.5258268713951111\n",
      "      vf_loss: 199.04721069335938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0339757810987241e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3672453165054321\n",
      "      kl: 0.007130035664886236\n",
      "      policy_loss: -0.010631702840328217\n",
      "      total_loss: 398.470947265625\n",
      "      vf_explained_var: 0.2245045304298401\n",
      "      vf_loss: 398.4815368652344\n",
      "    sample_time_ms: 6290.353\n",
      "    update_time_ms: 8.048\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.42596634879825\n",
      "    rl_1: 57.606707598701576\n",
      "  time_since_restore: 649.5259606838226\n",
      "  time_this_iter_s: 6.1453917026519775\n",
      "  time_total_s: 649.5259606838226\n",
      "  timestamp: 1552315884\n",
      "  timesteps_since_restore: 187000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 85\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 649 s, 85 iter, 187000 ts, 88 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-51-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.8\n",
      "  episode_reward_max: 151.5364750476102\n",
      "  episode_reward_mean: 91.60926514342056\n",
      "  episode_reward_min: -166.57108657434938\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1852\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 883.168\n",
      "    load_time_ms: 2.833\n",
      "    num_steps_sampled: 189200\n",
      "    num_steps_trained: 189200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.0679515621974483e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3433606624603271\n",
      "      kl: 0.0029176624957472086\n",
      "      policy_loss: 7.159876258810982e-05\n",
      "      total_loss: 191.6156768798828\n",
      "      vf_explained_var: 0.38847485184669495\n",
      "      vf_loss: 191.6156005859375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.169878905493621e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3549389839172363\n",
      "      kl: 0.0008850180893205106\n",
      "      policy_loss: -0.000837376865092665\n",
      "      total_loss: 389.8581237792969\n",
      "      vf_explained_var: 0.17266111075878143\n",
      "      vf_loss: 389.8589782714844\n",
      "    sample_time_ms: 6270.188\n",
      "    update_time_ms: 8.612\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.511379057542353\n",
      "    rl_1: 61.097886085878216\n",
      "  time_since_restore: 655.7278861999512\n",
      "  time_this_iter_s: 6.20192551612854\n",
      "  time_total_s: 655.7278861999512\n",
      "  timestamp: 1552315890\n",
      "  timesteps_since_restore: 189200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 189200\n",
      "  training_iteration: 86\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 655 s, 86 iter, 189200 ts, 91.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-51-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.97\n",
      "  episode_reward_max: 161.14466040639635\n",
      "  episode_reward_mean: 95.02325837500946\n",
      "  episode_reward_min: -166.57108657434938\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1873\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 875.15\n",
      "    load_time_ms: 2.871\n",
      "    num_steps_sampled: 191400\n",
      "    num_steps_trained: 191400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0339757810987241e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3282270431518555\n",
      "      kl: 0.005598060786724091\n",
      "      policy_loss: -0.0033134985715150833\n",
      "      total_loss: 115.59930419921875\n",
      "      vf_explained_var: 0.5986548662185669\n",
      "      vf_loss: 115.60261535644531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.5849394527468104e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.359467625617981\n",
      "      kl: 0.0034041483886539936\n",
      "      policy_loss: -0.0016337585402652621\n",
      "      total_loss: 339.9368896484375\n",
      "      vf_explained_var: 0.17028960585594177\n",
      "      vf_loss: 339.9385070800781\n",
      "    sample_time_ms: 6072.865\n",
      "    update_time_ms: 8.799\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.965256235263777\n",
      "    rl_1: 63.05800213974571\n",
      "  time_since_restore: 661.6587240695953\n",
      "  time_this_iter_s: 5.930837869644165\n",
      "  time_total_s: 661.6587240695953\n",
      "  timestamp: 1552315896\n",
      "  timesteps_since_restore: 191400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 191400\n",
      "  training_iteration: 87\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 661 s, 87 iter, 191400 ts, 95 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-51-43\n",
      "  done: false\n",
      "  episode_len_mean: 99.56\n",
      "  episode_reward_max: 161.14466040639635\n",
      "  episode_reward_mean: 82.84794227558561\n",
      "  episode_reward_min: -166.57108657434938\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1895\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 873.882\n",
      "    load_time_ms: 2.902\n",
      "    num_steps_sampled: 193600\n",
      "    num_steps_trained: 193600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.169878905493621e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3075430393218994\n",
      "      kl: 0.002682853490114212\n",
      "      policy_loss: -0.0019036330049857497\n",
      "      total_loss: 345.69830322265625\n",
      "      vf_explained_var: 0.4144885540008545\n",
      "      vf_loss: 345.7001647949219\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2924697263734052e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3674590587615967\n",
      "      kl: 0.0012385048903524876\n",
      "      policy_loss: -0.0014262479962781072\n",
      "      total_loss: 521.2794799804688\n",
      "      vf_explained_var: 0.2589971423149109\n",
      "      vf_loss: 521.2808837890625\n",
      "    sample_time_ms: 6016.904\n",
      "    update_time_ms: 8.683\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.039670579675043\n",
      "    rl_1: 57.80827169591058\n",
      "  time_since_restore: 668.9880170822144\n",
      "  time_this_iter_s: 7.3292930126190186\n",
      "  time_total_s: 668.9880170822144\n",
      "  timestamp: 1552315903\n",
      "  timesteps_since_restore: 193600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 193600\n",
      "  training_iteration: 88\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 668 s, 88 iter, 193600 ts, 82.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-51-50\n",
      "  done: false\n",
      "  episode_len_mean: 99.96\n",
      "  episode_reward_max: 161.14466040639635\n",
      "  episode_reward_mean: 91.4818248910522\n",
      "  episode_reward_min: -166.57108657434938\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1917\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 867.022\n",
      "    load_time_ms: 2.863\n",
      "    num_steps_sampled: 195800\n",
      "    num_steps_trained: 195800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5849394527468104e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.313219666481018\n",
      "      kl: 0.003231574082747102\n",
      "      policy_loss: -0.0031042834743857384\n",
      "      total_loss: 100.77066802978516\n",
      "      vf_explained_var: 0.6243856549263\n",
      "      vf_loss: 100.77377319335938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.462348631867026e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3635454177856445\n",
      "      kl: 0.005055082030594349\n",
      "      policy_loss: -0.0029627587646245956\n",
      "      total_loss: 307.9819641113281\n",
      "      vf_explained_var: 0.2140788733959198\n",
      "      vf_loss: 307.98492431640625\n",
      "    sample_time_ms: 6057.745\n",
      "    update_time_ms: 8.743\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 27.009264664503206\n",
      "    rl_1: 64.47256022654899\n",
      "  time_since_restore: 675.5002226829529\n",
      "  time_this_iter_s: 6.512205600738525\n",
      "  time_total_s: 675.5002226829529\n",
      "  timestamp: 1552315910\n",
      "  timesteps_since_restore: 195800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 195800\n",
      "  training_iteration: 89\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 675 s, 89 iter, 195800 ts, 91.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-51-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.7\n",
      "  episode_reward_max: 161.14466040639635\n",
      "  episode_reward_mean: 100.28680227789039\n",
      "  episode_reward_min: -164.56762076491836\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1939\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 823.408\n",
      "    load_time_ms: 2.832\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2924697263734052e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.285119891166687\n",
      "      kl: 0.00522854458540678\n",
      "      policy_loss: -0.005101319868117571\n",
      "      total_loss: 41.60811996459961\n",
      "      vf_explained_var: 0.7600353956222534\n",
      "      vf_loss: 41.613216400146484\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.231174315933513e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.364824652671814\n",
      "      kl: 0.004320200998336077\n",
      "      policy_loss: -0.005389906466007233\n",
      "      total_loss: 255.64508056640625\n",
      "      vf_explained_var: 0.19580377638339996\n",
      "      vf_loss: 255.65048217773438\n",
      "    sample_time_ms: 5780.843\n",
      "    update_time_ms: 9.141\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.627352267928373\n",
      "    rl_1: 70.659450009962\n",
      "  time_since_restore: 681.9699368476868\n",
      "  time_this_iter_s: 6.469714164733887\n",
      "  time_total_s: 681.9699368476868\n",
      "  timestamp: 1552315917\n",
      "  timesteps_since_restore: 198000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 90\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 681 s, 90 iter, 198000 ts, 100 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-52-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.98\n",
      "  episode_reward_max: 165.4643081658311\n",
      "  episode_reward_mean: 103.33702667290002\n",
      "  episode_reward_min: -164.56762076491836\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1960\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 784.816\n",
      "    load_time_ms: 2.713\n",
      "    num_steps_sampled: 200200\n",
      "    num_steps_trained: 200200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.462348631867026e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3150527477264404\n",
      "      kl: 0.004497305024415255\n",
      "      policy_loss: -0.003917388617992401\n",
      "      total_loss: 54.84326934814453\n",
      "      vf_explained_var: 0.7379021048545837\n",
      "      vf_loss: 54.847190856933594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.6155871579667565e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3653336763381958\n",
      "      kl: 0.009248640388250351\n",
      "      policy_loss: -0.006150954868644476\n",
      "      total_loss: 265.4598083496094\n",
      "      vf_explained_var: 0.26261526346206665\n",
      "      vf_loss: 265.46600341796875\n",
      "    sample_time_ms: 5671.194\n",
      "    update_time_ms: 8.596\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.534320504807216\n",
      "    rl_1: 71.80270616809281\n",
      "  time_since_restore: 688.2764806747437\n",
      "  time_this_iter_s: 6.306543827056885\n",
      "  time_total_s: 688.2764806747437\n",
      "  timestamp: 1552315923\n",
      "  timesteps_since_restore: 200200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 200200\n",
      "  training_iteration: 91\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 688 s, 91 iter, 200200 ts, 103 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-52-09\n",
      "  done: false\n",
      "  episode_len_mean: 101.21\n",
      "  episode_reward_max: 165.4643081658311\n",
      "  episode_reward_mean: 108.76913348138007\n",
      "  episode_reward_min: -164.56762076491836\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1981\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 777.876\n",
      "    load_time_ms: 2.657\n",
      "    num_steps_sampled: 202400\n",
      "    num_steps_trained: 202400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.231174315933513e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2805109024047852\n",
      "      kl: 0.006321473512798548\n",
      "      policy_loss: -0.011726504191756248\n",
      "      total_loss: 101.72840881347656\n",
      "      vf_explained_var: 0.6925767064094543\n",
      "      vf_loss: 101.74014282226562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.077935789833782e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3733891248703003\n",
      "      kl: 0.0021409180480986834\n",
      "      policy_loss: -0.000732258427888155\n",
      "      total_loss: 395.5853576660156\n",
      "      vf_explained_var: 0.2102135270833969\n",
      "      vf_loss: 395.5860595703125\n",
      "    sample_time_ms: 5589.976\n",
      "    update_time_ms: 8.234\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.98862146390414\n",
      "    rl_1: 74.78051201747593\n",
      "  time_since_restore: 694.099217414856\n",
      "  time_this_iter_s: 5.822736740112305\n",
      "  time_total_s: 694.099217414856\n",
      "  timestamp: 1552315929\n",
      "  timesteps_since_restore: 202400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 202400\n",
      "  training_iteration: 92\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 694 s, 92 iter, 202400 ts, 109 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-52-15\n",
      "  done: false\n",
      "  episode_len_mean: 102.49\n",
      "  episode_reward_max: 165.4643081658311\n",
      "  episode_reward_mean: 121.58432462358938\n",
      "  episode_reward_min: -134.50474562022964\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2003\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 771.98\n",
      "    load_time_ms: 2.799\n",
      "    num_steps_sampled: 204600\n",
      "    num_steps_trained: 204600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6155871579667565e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.305036187171936\n",
      "      kl: 0.00409797765314579\n",
      "      policy_loss: -0.002313473727554083\n",
      "      total_loss: 63.49164962768555\n",
      "      vf_explained_var: 0.7351688742637634\n",
      "      vf_loss: 63.493961334228516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.038967894916891e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.392403244972229\n",
      "      kl: 0.002883882960304618\n",
      "      policy_loss: -0.003069395199418068\n",
      "      total_loss: 270.671142578125\n",
      "      vf_explained_var: 0.2982008159160614\n",
      "      vf_loss: 270.6742248535156\n",
      "    sample_time_ms: 5528.31\n",
      "    update_time_ms: 8.339\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.36839386278343\n",
      "    rl_1: 82.21593076080597\n",
      "  time_since_restore: 700.157913684845\n",
      "  time_this_iter_s: 6.058696269989014\n",
      "  time_total_s: 700.157913684845\n",
      "  timestamp: 1552315935\n",
      "  timesteps_since_restore: 204600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 204600\n",
      "  training_iteration: 93\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 700 s, 93 iter, 204600 ts, 122 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-52-21\n",
      "  done: false\n",
      "  episode_len_mean: 101.8\n",
      "  episode_reward_max: 165.4643081658311\n",
      "  episode_reward_mean: 115.45304780209152\n",
      "  episode_reward_min: -150.47072813322245\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2025\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 757.068\n",
      "    load_time_ms: 2.861\n",
      "    num_steps_sampled: 206800\n",
      "    num_steps_trained: 206800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.077935789833782e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2917993068695068\n",
      "      kl: 0.006059270352125168\n",
      "      policy_loss: -0.007642397191375494\n",
      "      total_loss: 260.9629211425781\n",
      "      vf_explained_var: 0.5037222504615784\n",
      "      vf_loss: 260.9705505371094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.0194839474584456e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3774261474609375\n",
      "      kl: 0.0016066046664491296\n",
      "      policy_loss: -0.010101628489792347\n",
      "      total_loss: 483.97833251953125\n",
      "      vf_explained_var: 0.24824008345603943\n",
      "      vf_loss: 483.98846435546875\n",
      "    sample_time_ms: 5480.109\n",
      "    update_time_ms: 8.34\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.102331201802265\n",
      "    rl_1: 78.35071660028925\n",
      "  time_since_restore: 705.9659371376038\n",
      "  time_this_iter_s: 5.808023452758789\n",
      "  time_total_s: 705.9659371376038\n",
      "  timestamp: 1552315941\n",
      "  timesteps_since_restore: 206800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 206800\n",
      "  training_iteration: 94\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 705 s, 94 iter, 206800 ts, 115 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-52-27\n",
      "  done: false\n",
      "  episode_len_mean: 101.03\n",
      "  episode_reward_max: 166.03095336953754\n",
      "  episode_reward_mean: 111.80079027557747\n",
      "  episode_reward_min: -150.47072813322245\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2047\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 755.279\n",
      "    load_time_ms: 2.825\n",
      "    num_steps_sampled: 209000\n",
      "    num_steps_trained: 209000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.038967894916891e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2436350584030151\n",
      "      kl: 0.005122196860611439\n",
      "      policy_loss: -0.000374405033653602\n",
      "      total_loss: 212.73934936523438\n",
      "      vf_explained_var: 0.5695253014564514\n",
      "      vf_loss: 212.73971557617188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.0097419737292228e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.381045937538147\n",
      "      kl: 0.0027705058455467224\n",
      "      policy_loss: -0.0007044623489491642\n",
      "      total_loss: 383.45166015625\n",
      "      vf_explained_var: 0.4026302993297577\n",
      "      vf_loss: 383.4523620605469\n",
      "    sample_time_ms: 5464.588\n",
      "    update_time_ms: 8.508\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.24950234342616\n",
      "    rl_1: 74.5512879321513\n",
      "  time_since_restore: 711.9392683506012\n",
      "  time_this_iter_s: 5.9733312129974365\n",
      "  time_total_s: 711.9392683506012\n",
      "  timestamp: 1552315947\n",
      "  timesteps_since_restore: 209000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 209000\n",
      "  training_iteration: 95\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 711 s, 95 iter, 209000 ts, 112 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-52-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.13\n",
      "  episode_reward_max: 176.97303457284065\n",
      "  episode_reward_mean: 105.59256708870974\n",
      "  episode_reward_min: -150.47072813322245\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2070\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 751.094\n",
      "    load_time_ms: 2.841\n",
      "    num_steps_sampled: 211200\n",
      "    num_steps_trained: 211200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.0194839474584456e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.299872636795044\n",
      "      kl: 0.004550185985863209\n",
      "      policy_loss: -0.0035461371298879385\n",
      "      total_loss: 341.8276062011719\n",
      "      vf_explained_var: 0.4596431255340576\n",
      "      vf_loss: 341.8311767578125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.048709868646114e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3481682538986206\n",
      "      kl: 0.006358011160045862\n",
      "      policy_loss: -0.004555652383714914\n",
      "      total_loss: 583.62060546875\n",
      "      vf_explained_var: 0.32437095046043396\n",
      "      vf_loss: 583.6250610351562\n",
      "    sample_time_ms: 5410.45\n",
      "    update_time_ms: 8.066\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.37655724513557\n",
      "    rl_1: 70.21600984357417\n",
      "  time_since_restore: 717.5536296367645\n",
      "  time_this_iter_s: 5.61436128616333\n",
      "  time_total_s: 717.5536296367645\n",
      "  timestamp: 1552315952\n",
      "  timesteps_since_restore: 211200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 211200\n",
      "  training_iteration: 96\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 717 s, 96 iter, 211200 ts, 106 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-52-38\n",
      "  done: false\n",
      "  episode_len_mean: 99.1\n",
      "  episode_reward_max: 176.97303457284065\n",
      "  episode_reward_mean: 96.91833946371062\n",
      "  episode_reward_min: -150.47072813322245\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2092\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 748.766\n",
      "    load_time_ms: 2.873\n",
      "    num_steps_sampled: 213400\n",
      "    num_steps_trained: 213400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0097419737292228e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2977306842803955\n",
      "      kl: 0.0017257563304156065\n",
      "      policy_loss: -0.0028216198552399874\n",
      "      total_loss: 194.23609924316406\n",
      "      vf_explained_var: 0.49309802055358887\n",
      "      vf_loss: 194.23892211914062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.524354934323057e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3231652975082397\n",
      "      kl: 0.0062188031151890755\n",
      "      policy_loss: -0.0043375007808208466\n",
      "      total_loss: 431.197021484375\n",
      "      vf_explained_var: 0.22443634271621704\n",
      "      vf_loss: 431.20135498046875\n",
      "    sample_time_ms: 5421.008\n",
      "    update_time_ms: 8.082\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.10327023577336\n",
      "    rl_1: 65.81506922793724\n",
      "  time_since_restore: 723.5685946941376\n",
      "  time_this_iter_s: 6.014965057373047\n",
      "  time_total_s: 723.5685946941376\n",
      "  timestamp: 1552315958\n",
      "  timesteps_since_restore: 213400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 213400\n",
      "  training_iteration: 97\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 723 s, 97 iter, 213400 ts, 96.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-52-44\n",
      "  done: false\n",
      "  episode_len_mean: 99.9\n",
      "  episode_reward_max: 176.97303457284065\n",
      "  episode_reward_mean: 105.05045575498397\n",
      "  episode_reward_min: -145.59054098592608\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2114\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 741.335\n",
      "    load_time_ms: 2.862\n",
      "    num_steps_sampled: 215600\n",
      "    num_steps_trained: 215600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.048709868646114e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3277854919433594\n",
      "      kl: 0.0014542655553668737\n",
      "      policy_loss: -0.002202934119850397\n",
      "      total_loss: 49.42562484741211\n",
      "      vf_explained_var: 0.7973465919494629\n",
      "      vf_loss: 49.42782974243164\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2621774671615285e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3376942873001099\n",
      "      kl: 0.004597522784024477\n",
      "      policy_loss: -0.010941736400127411\n",
      "      total_loss: 240.2473907470703\n",
      "      vf_explained_var: 0.33641546964645386\n",
      "      vf_loss: 240.25833129882812\n",
      "    sample_time_ms: 5280.476\n",
      "    update_time_ms: 8.22\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.28798603836176\n",
      "    rl_1: 70.76246971662222\n",
      "  time_since_restore: 729.4187932014465\n",
      "  time_this_iter_s: 5.85019850730896\n",
      "  time_total_s: 729.4187932014465\n",
      "  timestamp: 1552315964\n",
      "  timesteps_since_restore: 215600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 215600\n",
      "  training_iteration: 98\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 729 s, 98 iter, 215600 ts, 105 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-52-50\n",
      "  done: false\n",
      "  episode_len_mean: 97.91\n",
      "  episode_reward_max: 176.97303457284065\n",
      "  episode_reward_mean: 89.78309582685242\n",
      "  episode_reward_min: -149.6321780526883\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2137\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 743.431\n",
      "    load_time_ms: 2.776\n",
      "    num_steps_sampled: 217800\n",
      "    num_steps_trained: 217800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.524354934323057e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3569749593734741\n",
      "      kl: 0.00163793854881078\n",
      "      policy_loss: -0.003082349430769682\n",
      "      total_loss: 489.6520080566406\n",
      "      vf_explained_var: 0.3650707006454468\n",
      "      vf_loss: 489.65509033203125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.3108873358076425e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3182930946350098\n",
      "      kl: 0.0068458132445812225\n",
      "      policy_loss: -0.005222639534622431\n",
      "      total_loss: 754.511962890625\n",
      "      vf_explained_var: 0.2645593583583832\n",
      "      vf_loss: 754.5171508789062\n",
      "    sample_time_ms: 5216.867\n",
      "    update_time_ms: 8.183\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.87666273539427\n",
      "    rl_1: 62.90643309145817\n",
      "  time_since_restore: 735.3138244152069\n",
      "  time_this_iter_s: 5.895031213760376\n",
      "  time_total_s: 735.3138244152069\n",
      "  timestamp: 1552315970\n",
      "  timesteps_since_restore: 217800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 217800\n",
      "  training_iteration: 99\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 735 s, 99 iter, 217800 ts, 89.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-52-56\n",
      "  done: false\n",
      "  episode_len_mean: 97.59\n",
      "  episode_reward_max: 175.29799474668172\n",
      "  episode_reward_mean: 87.44991649826554\n",
      "  episode_reward_min: -149.6321780526883\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2160\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 740.765\n",
      "    load_time_ms: 2.697\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2621774671615285e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2920277118682861\n",
      "      kl: 0.003311971202492714\n",
      "      policy_loss: -0.0010961368680000305\n",
      "      total_loss: 331.6173095703125\n",
      "      vf_explained_var: 0.4596019685268402\n",
      "      vf_loss: 331.618408203125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.1554436679038213e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.337336540222168\n",
      "      kl: 0.0033772531896829605\n",
      "      policy_loss: -0.0026887143030762672\n",
      "      total_loss: 519.99951171875\n",
      "      vf_explained_var: 0.34461936354637146\n",
      "      vf_loss: 520.0023193359375\n",
      "    sample_time_ms: 5179.697\n",
      "    update_time_ms: 7.887\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.701465341313355\n",
      "    rl_1: 62.74845115695218\n",
      "  time_since_restore: 741.3801205158234\n",
      "  time_this_iter_s: 6.066296100616455\n",
      "  time_total_s: 741.3801205158234\n",
      "  timestamp: 1552315976\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 100\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 741 s, 100 iter, 220000 ts, 87.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-53-03\n",
      "  done: false\n",
      "  episode_len_mean: 97.89\n",
      "  episode_reward_max: 175.9810312716478\n",
      "  episode_reward_mean: 91.90656236226513\n",
      "  episode_reward_min: -149.6321780526883\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2182\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 738.524\n",
      "    load_time_ms: 2.733\n",
      "    num_steps_sampled: 222200\n",
      "    num_steps_trained: 222200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.3108873358076425e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2673419713974\n",
      "      kl: 0.001644955831579864\n",
      "      policy_loss: -0.0004298932326491922\n",
      "      total_loss: 241.55801391601562\n",
      "      vf_explained_var: 0.4897680878639221\n",
      "      vf_loss: 241.55845642089844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5777218339519106e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3609014749526978\n",
      "      kl: 0.002932417904958129\n",
      "      policy_loss: -0.000682047160807997\n",
      "      total_loss: 414.8458557128906\n",
      "      vf_explained_var: 0.38807302713394165\n",
      "      vf_loss: 414.8465576171875\n",
      "    sample_time_ms: 5155.703\n",
      "    update_time_ms: 7.826\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.912064552458467\n",
      "    rl_1: 62.994497809806646\n",
      "  time_since_restore: 747.4256527423859\n",
      "  time_this_iter_s: 6.0455322265625\n",
      "  time_total_s: 747.4256527423859\n",
      "  timestamp: 1552315983\n",
      "  timesteps_since_restore: 222200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 222200\n",
      "  training_iteration: 101\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 747 s, 101 iter, 222200 ts, 91.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-53-09\n",
      "  done: false\n",
      "  episode_len_mean: 97.32\n",
      "  episode_reward_max: 183.20574533765446\n",
      "  episode_reward_mean: 87.16861067387222\n",
      "  episode_reward_min: -149.6321780526883\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2205\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 743.565\n",
      "    load_time_ms: 2.651\n",
      "    num_steps_sampled: 224400\n",
      "    num_steps_trained: 224400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.1554436679038213e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2987369298934937\n",
      "      kl: 0.004903748165816069\n",
      "      policy_loss: -0.003733990015462041\n",
      "      total_loss: 267.7657470703125\n",
      "      vf_explained_var: 0.4658416509628296\n",
      "      vf_loss: 267.7694396972656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.888609169759553e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3342814445495605\n",
      "      kl: 0.0011980911949649453\n",
      "      policy_loss: -0.00499339122325182\n",
      "      total_loss: 573.9430541992188\n",
      "      vf_explained_var: 0.30275559425354004\n",
      "      vf_loss: 573.9480590820312\n",
      "    sample_time_ms: 5173.829\n",
      "    update_time_ms: 7.948\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.84623786588083\n",
      "    rl_1: 60.32237280799141\n",
      "  time_since_restore: 753.482649564743\n",
      "  time_this_iter_s: 6.056996822357178\n",
      "  time_total_s: 753.482649564743\n",
      "  timestamp: 1552315989\n",
      "  timesteps_since_restore: 224400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 224400\n",
      "  training_iteration: 102\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 753 s, 102 iter, 224400 ts, 87.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-53-15\n",
      "  done: false\n",
      "  episode_len_mean: 97.83\n",
      "  episode_reward_max: 186.2996915842395\n",
      "  episode_reward_mean: 95.60005644738798\n",
      "  episode_reward_min: -145.33236495115816\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2226\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 751.818\n",
      "    load_time_ms: 2.485\n",
      "    num_steps_sampled: 226600\n",
      "    num_steps_trained: 226600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5777218339519106e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3083903789520264\n",
      "      kl: 0.006493000779300928\n",
      "      policy_loss: -0.011547907255589962\n",
      "      total_loss: 227.4235382080078\n",
      "      vf_explained_var: 0.5865403413772583\n",
      "      vf_loss: 227.43507385253906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.9443045848797766e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3294379711151123\n",
      "      kl: 0.006175328977406025\n",
      "      policy_loss: -0.002742873737588525\n",
      "      total_loss: 540.217529296875\n",
      "      vf_explained_var: 0.40262529253959656\n",
      "      vf_loss: 540.22021484375\n",
      "    sample_time_ms: 5163.158\n",
      "    update_time_ms: 7.859\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.126689833544788\n",
      "    rl_1: 65.47336661384318\n",
      "  time_since_restore: 759.5139803886414\n",
      "  time_this_iter_s: 6.031330823898315\n",
      "  time_total_s: 759.5139803886414\n",
      "  timestamp: 1552315995\n",
      "  timesteps_since_restore: 226600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 226600\n",
      "  training_iteration: 103\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 759 s, 103 iter, 226600 ts, 95.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-53-21\n",
      "  done: false\n",
      "  episode_len_mean: 98.9\n",
      "  episode_reward_max: 195.19829880345142\n",
      "  episode_reward_mean: 109.13883521654603\n",
      "  episode_reward_min: -144.13860763534183\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2249\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 758.141\n",
      "    load_time_ms: 2.404\n",
      "    num_steps_sampled: 228800\n",
      "    num_steps_trained: 228800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.888609169759553e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.285400152206421\n",
      "      kl: 0.0002567542251199484\n",
      "      policy_loss: -3.931890387320891e-05\n",
      "      total_loss: 281.5213623046875\n",
      "      vf_explained_var: 0.4861694872379303\n",
      "      vf_loss: 281.5213928222656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9721522924398883e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3343212604522705\n",
      "      kl: 0.008720985613763332\n",
      "      policy_loss: -0.0058907391503453255\n",
      "      total_loss: 524.5642700195312\n",
      "      vf_explained_var: 0.2855040729045868\n",
      "      vf_loss: 524.5701293945312\n",
      "    sample_time_ms: 5161.313\n",
      "    update_time_ms: 7.794\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.43951750032874\n",
      "    rl_1: 72.6993177162173\n",
      "  time_since_restore: 765.3640787601471\n",
      "  time_this_iter_s: 5.850098371505737\n",
      "  time_total_s: 765.3640787601471\n",
      "  timestamp: 1552316001\n",
      "  timesteps_since_restore: 228800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 228800\n",
      "  training_iteration: 104\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 765 s, 104 iter, 228800 ts, 109 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-53-27\n",
      "  done: false\n",
      "  episode_len_mean: 97.84\n",
      "  episode_reward_max: 196.50060415955866\n",
      "  episode_reward_mean: 108.3820174176103\n",
      "  episode_reward_min: -160.33116593322453\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2272\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 757.915\n",
      "    load_time_ms: 2.376\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.9443045848797766e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2978965044021606\n",
      "      kl: 0.004736135248094797\n",
      "      policy_loss: -0.001290210522711277\n",
      "      total_loss: 358.5191955566406\n",
      "      vf_explained_var: 0.5024223327636719\n",
      "      vf_loss: 358.5205078125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.860761462199441e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3209565877914429\n",
      "      kl: 0.006977160926908255\n",
      "      policy_loss: -0.003751227864995599\n",
      "      total_loss: 753.3945922851562\n",
      "      vf_explained_var: 0.28742876648902893\n",
      "      vf_loss: 753.3983154296875\n",
      "    sample_time_ms: 5149.492\n",
      "    update_time_ms: 7.719\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.651362555198574\n",
      "    rl_1: 74.73065486241173\n",
      "  time_since_restore: 771.2160098552704\n",
      "  time_this_iter_s: 5.851931095123291\n",
      "  time_total_s: 771.2160098552704\n",
      "  timestamp: 1552316007\n",
      "  timesteps_since_restore: 231000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 105\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 771 s, 105 iter, 231000 ts, 108 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-53-32\n",
      "  done: false\n",
      "  episode_len_mean: 97.6\n",
      "  episode_reward_max: 202.76801603715285\n",
      "  episode_reward_mean: 111.46860011662744\n",
      "  episode_reward_min: -160.33116593322453\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2294\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 764.857\n",
      "    load_time_ms: 2.301\n",
      "    num_steps_sampled: 233200\n",
      "    num_steps_trained: 233200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9721522924398883e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.284832239151001\n",
      "      kl: 0.0037362356670200825\n",
      "      policy_loss: -0.005152783822268248\n",
      "      total_loss: 270.69171142578125\n",
      "      vf_explained_var: 0.458959698677063\n",
      "      vf_loss: 270.6968688964844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.930380731099721e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2861599922180176\n",
      "      kl: 0.011013626120984554\n",
      "      policy_loss: -0.00846206583082676\n",
      "      total_loss: 607.232421875\n",
      "      vf_explained_var: 0.29148969054222107\n",
      "      vf_loss: 607.2409057617188\n",
      "    sample_time_ms: 5162.793\n",
      "    update_time_ms: 7.569\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.35437375407852\n",
      "    rl_1: 78.11422636254893\n",
      "  time_since_restore: 777.0304980278015\n",
      "  time_this_iter_s: 5.814488172531128\n",
      "  time_total_s: 777.0304980278015\n",
      "  timestamp: 1552316012\n",
      "  timesteps_since_restore: 233200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 233200\n",
      "  training_iteration: 106\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 777 s, 106 iter, 233200 ts, 111 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-53-38\n",
      "  done: false\n",
      "  episode_len_mean: 98.1\n",
      "  episode_reward_max: 202.76801603715285\n",
      "  episode_reward_mean: 119.05992191208335\n",
      "  episode_reward_min: -160.33116593322453\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2317\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 763.255\n",
      "    load_time_ms: 2.267\n",
      "    num_steps_sampled: 235400\n",
      "    num_steps_trained: 235400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.860761462199441e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3273437023162842\n",
      "      kl: 0.0011153906816616654\n",
      "      policy_loss: -0.004510132595896721\n",
      "      total_loss: 207.42330932617188\n",
      "      vf_explained_var: 0.6044026613235474\n",
      "      vf_loss: 207.42782592773438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.930380731099721e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2696701288223267\n",
      "      kl: 0.010936848819255829\n",
      "      policy_loss: -0.004636543337255716\n",
      "      total_loss: 545.7910766601562\n",
      "      vf_explained_var: 0.4066631495952606\n",
      "      vf_loss: 545.7957153320312\n",
      "    sample_time_ms: 5150.385\n",
      "    update_time_ms: 7.412\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.22025846948218\n",
      "    rl_1: 83.83966344260116\n",
      "  time_since_restore: 782.9013979434967\n",
      "  time_this_iter_s: 5.87089991569519\n",
      "  time_total_s: 782.9013979434967\n",
      "  timestamp: 1552316018\n",
      "  timesteps_since_restore: 235400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 235400\n",
      "  training_iteration: 107\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 782 s, 107 iter, 235400 ts, 119 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-53-45\n",
      "  done: false\n",
      "  episode_len_mean: 97.46\n",
      "  episode_reward_max: 202.76801603715285\n",
      "  episode_reward_mean: 114.11973690452459\n",
      "  episode_reward_min: -160.33116593322453\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2339\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 768.37\n",
      "    load_time_ms: 2.164\n",
      "    num_steps_sampled: 237600\n",
      "    num_steps_trained: 237600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.930380731099721e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3333079814910889\n",
      "      kl: 0.0037115246523171663\n",
      "      policy_loss: -0.0026082159020006657\n",
      "      total_loss: 234.3396453857422\n",
      "      vf_explained_var: 0.5651282072067261\n",
      "      vf_loss: 234.34225463867188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.930380731099721e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3022490739822388\n",
      "      kl: 0.007695361040532589\n",
      "      policy_loss: -0.006205129437148571\n",
      "      total_loss: 633.5099487304688\n",
      "      vf_explained_var: 0.37488678097724915\n",
      "      vf_loss: 633.5162353515625\n",
      "    sample_time_ms: 5206.381\n",
      "    update_time_ms: 7.182\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.272323336138115\n",
      "    rl_1: 81.84741356838647\n",
      "  time_since_restore: 789.3615076541901\n",
      "  time_this_iter_s: 6.460109710693359\n",
      "  time_total_s: 789.3615076541901\n",
      "  timestamp: 1552316025\n",
      "  timesteps_since_restore: 237600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 237600\n",
      "  training_iteration: 108\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 789 s, 108 iter, 237600 ts, 114 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-53-51\n",
      "  done: false\n",
      "  episode_len_mean: 97.86\n",
      "  episode_reward_max: 202.76801603715285\n",
      "  episode_reward_mean: 111.89332200009511\n",
      "  episode_reward_min: -161.33600366622062\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2362\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 771.787\n",
      "    load_time_ms: 2.359\n",
      "    num_steps_sampled: 239800\n",
      "    num_steps_trained: 239800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4651903655498604e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3576115369796753\n",
      "      kl: 0.0025504690129309893\n",
      "      policy_loss: -0.005273027811199427\n",
      "      total_loss: 359.19659423828125\n",
      "      vf_explained_var: 0.4493127465248108\n",
      "      vf_loss: 359.2018737792969\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.4651903655498604e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3133331537246704\n",
      "      kl: 0.006858633365482092\n",
      "      policy_loss: -0.006849389988929033\n",
      "      total_loss: 685.37890625\n",
      "      vf_explained_var: 0.3662065267562866\n",
      "      vf_loss: 685.3858032226562\n",
      "    sample_time_ms: 5253.508\n",
      "    update_time_ms: 7.538\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.360573435993516\n",
      "    rl_1: 82.53274856410161\n",
      "  time_since_restore: 795.7700302600861\n",
      "  time_this_iter_s: 6.408522605895996\n",
      "  time_total_s: 795.7700302600861\n",
      "  timestamp: 1552316031\n",
      "  timesteps_since_restore: 239800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 239800\n",
      "  training_iteration: 109\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 795 s, 109 iter, 239800 ts, 112 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-53-57\n",
      "  done: false\n",
      "  episode_len_mean: 99.54\n",
      "  episode_reward_max: 201.25246147981449\n",
      "  episode_reward_mean: 119.91908739673114\n",
      "  episode_reward_min: -161.33600366622062\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2383\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 775.973\n",
      "    load_time_ms: 2.497\n",
      "    num_steps_sampled: 242000\n",
      "    num_steps_trained: 242000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2325951827749302e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3864858150482178\n",
      "      kl: 0.002138093113899231\n",
      "      policy_loss: -0.0038701549638062716\n",
      "      total_loss: 86.49850463867188\n",
      "      vf_explained_var: 0.7169607281684875\n",
      "      vf_loss: 86.50238037109375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2325951827749302e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.290608525276184\n",
      "      kl: 0.007372585125267506\n",
      "      policy_loss: -0.00426815589889884\n",
      "      total_loss: 495.49310302734375\n",
      "      vf_explained_var: 0.47083204984664917\n",
      "      vf_loss: 495.4973449707031\n",
      "    sample_time_ms: 5242.057\n",
      "    update_time_ms: 7.38\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.221801345693635\n",
      "    rl_1: 88.69728605103752\n",
      "  time_since_restore: 801.7642493247986\n",
      "  time_this_iter_s: 5.994219064712524\n",
      "  time_total_s: 801.7642493247986\n",
      "  timestamp: 1552316037\n",
      "  timesteps_since_restore: 242000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 242000\n",
      "  training_iteration: 110\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 801 s, 110 iter, 242000 ts, 120 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-54-05\n",
      "  done: false\n",
      "  episode_len_mean: 98.56\n",
      "  episode_reward_max: 201.25246147981449\n",
      "  episode_reward_mean: 110.717719508679\n",
      "  episode_reward_min: -161.33600366622062\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2405\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 800.481\n",
      "    load_time_ms: 2.512\n",
      "    num_steps_sampled: 244200\n",
      "    num_steps_trained: 244200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.162975913874651e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.328354001045227\n",
      "      kl: 0.003316147020086646\n",
      "      policy_loss: -0.004059788770973682\n",
      "      total_loss: 244.0887451171875\n",
      "      vf_explained_var: 0.5793755650520325\n",
      "      vf_loss: 244.09283447265625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.162975913874651e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.30820894241333\n",
      "      kl: 0.0038134658243507147\n",
      "      policy_loss: -0.0013263558503240347\n",
      "      total_loss: 617.2122192382812\n",
      "      vf_explained_var: 0.338830828666687\n",
      "      vf_loss: 617.2135009765625\n",
      "    sample_time_ms: 5422.822\n",
      "    update_time_ms: 7.496\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 27.351865513772037\n",
      "    rl_1: 83.36585399490697\n",
      "  time_since_restore: 809.8645141124725\n",
      "  time_this_iter_s: 8.10026478767395\n",
      "  time_total_s: 809.8645141124725\n",
      "  timestamp: 1552316045\n",
      "  timesteps_since_restore: 244200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 244200\n",
      "  training_iteration: 111\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 809 s, 111 iter, 244200 ts, 111 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-54-12\n",
      "  done: false\n",
      "  episode_len_mean: 99.72\n",
      "  episode_reward_max: 205.3299676143463\n",
      "  episode_reward_mean: 121.61718202768216\n",
      "  episode_reward_min: -161.33600366622062\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2427\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 795.434\n",
      "    load_time_ms: 2.51\n",
      "    num_steps_sampled: 246400\n",
      "    num_steps_trained: 246400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.0814879569373254e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.315430760383606\n",
      "      kl: 0.0056351362727582455\n",
      "      policy_loss: -0.002727257087826729\n",
      "      total_loss: 126.16580200195312\n",
      "      vf_explained_var: 0.6904568076133728\n",
      "      vf_loss: 126.16854095458984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0814879569373254e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2995789051055908\n",
      "      kl: 0.011217961087822914\n",
      "      policy_loss: -0.004669256508350372\n",
      "      total_loss: 478.894775390625\n",
      "      vf_explained_var: 0.3711113929748535\n",
      "      vf_loss: 478.8994445800781\n",
      "    sample_time_ms: 5453.378\n",
      "    update_time_ms: 7.571\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.68045819542648\n",
      "    rl_1: 89.93672383225571\n",
      "  time_since_restore: 816.1746909618378\n",
      "  time_this_iter_s: 6.310176849365234\n",
      "  time_total_s: 816.1746909618378\n",
      "  timestamp: 1552316052\n",
      "  timesteps_since_restore: 246400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 246400\n",
      "  training_iteration: 112\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 816 s, 112 iter, 246400 ts, 122 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-54-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.11\n",
      "  episode_reward_max: 205.714147083263\n",
      "  episode_reward_mean: 131.35804217565945\n",
      "  episode_reward_min: -161.33600366622062\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2449\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 791.896\n",
      "    load_time_ms: 2.517\n",
      "    num_steps_sampled: 248600\n",
      "    num_steps_trained: 248600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5407439784686627e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3093128204345703\n",
      "      kl: 0.002654491690918803\n",
      "      policy_loss: -0.005433544982224703\n",
      "      total_loss: 105.45519256591797\n",
      "      vf_explained_var: 0.7303172945976257\n",
      "      vf_loss: 105.46062469482422\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.0814879569373254e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2848042249679565\n",
      "      kl: 0.008072919212281704\n",
      "      policy_loss: -0.0018749912269413471\n",
      "      total_loss: 547.344482421875\n",
      "      vf_explained_var: 0.23887598514556885\n",
      "      vf_loss: 547.3463745117188\n",
      "    sample_time_ms: 5452.303\n",
      "    update_time_ms: 7.707\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.62819693186048\n",
      "    rl_1: 95.72984524379898\n",
      "  time_since_restore: 822.1603333950043\n",
      "  time_this_iter_s: 5.985642433166504\n",
      "  time_total_s: 822.1603333950043\n",
      "  timestamp: 1552316058\n",
      "  timesteps_since_restore: 248600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 248600\n",
      "  training_iteration: 113\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 822 s, 113 iter, 248600 ts, 131 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.72\n",
      "  episode_reward_max: 207.6613034779724\n",
      "  episode_reward_mean: 140.33254215985235\n",
      "  episode_reward_min: -158.61467674461662\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2471\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 788.518\n",
      "    load_time_ms: 2.57\n",
      "    num_steps_sampled: 250800\n",
      "    num_steps_trained: 250800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.703719892343314e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3191555738449097\n",
      "      kl: 0.0018354854546487331\n",
      "      policy_loss: -0.0011672868859022856\n",
      "      total_loss: 205.3714141845703\n",
      "      vf_explained_var: 0.5973976254463196\n",
      "      vf_loss: 205.37258911132812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5407439784686627e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2756818532943726\n",
      "      kl: 0.018334457650780678\n",
      "      policy_loss: -0.014800860546529293\n",
      "      total_loss: 680.75439453125\n",
      "      vf_explained_var: 0.24458420276641846\n",
      "      vf_loss: 680.7691650390625\n",
      "    sample_time_ms: 5468.508\n",
      "    update_time_ms: 7.902\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.1974360566597\n",
      "    rl_1: 101.13510610319268\n",
      "  time_since_restore: 828.1411867141724\n",
      "  time_this_iter_s: 5.980853319168091\n",
      "  time_total_s: 828.1411867141724\n",
      "  timestamp: 1552316064\n",
      "  timesteps_since_restore: 250800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 250800\n",
      "  training_iteration: 114\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 828 s, 114 iter, 250800 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-54-30\n",
      "  done: false\n",
      "  episode_len_mean: 99.51\n",
      "  episode_reward_max: 207.6613034779724\n",
      "  episode_reward_mean: 132.950257219581\n",
      "  episode_reward_min: -162.5188357660861\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2494\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 791.811\n",
      "    load_time_ms: 2.539\n",
      "    num_steps_sampled: 253000\n",
      "    num_steps_trained: 253000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.851859946171657e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2781850099563599\n",
      "      kl: 0.0031837248243391514\n",
      "      policy_loss: -0.009462312795221806\n",
      "      total_loss: 372.7478332519531\n",
      "      vf_explained_var: 0.5169066190719604\n",
      "      vf_loss: 372.7572937011719\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5407439784686627e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3004193305969238\n",
      "      kl: 0.004124706611037254\n",
      "      policy_loss: -0.0007128383731469512\n",
      "      total_loss: 685.4676513671875\n",
      "      vf_explained_var: 0.3722129166126251\n",
      "      vf_loss: 685.4683837890625\n",
      "    sample_time_ms: 5469.343\n",
      "    update_time_ms: 7.817\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.87647819647347\n",
      "    rl_1: 95.07377902310755\n",
      "  time_since_restore: 834.0327627658844\n",
      "  time_this_iter_s: 5.891576051712036\n",
      "  time_total_s: 834.0327627658844\n",
      "  timestamp: 1552316070\n",
      "  timesteps_since_restore: 253000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 253000\n",
      "  training_iteration: 115\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 834 s, 115 iter, 253000 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-54-36\n",
      "  done: false\n",
      "  episode_len_mean: 99.65\n",
      "  episode_reward_max: 207.6613034779724\n",
      "  episode_reward_mean: 136.67958731927058\n",
      "  episode_reward_min: -162.5188357660861\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2516\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 783.637\n",
      "    load_time_ms: 2.527\n",
      "    num_steps_sampled: 255200\n",
      "    num_steps_trained: 255200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9259299730858284e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2809877395629883\n",
      "      kl: 0.005198115948587656\n",
      "      policy_loss: -0.002573598874732852\n",
      "      total_loss: 136.49478149414062\n",
      "      vf_explained_var: 0.6387879252433777\n",
      "      vf_loss: 136.49734497070312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.703719892343314e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2872934341430664\n",
      "      kl: 0.002242602640762925\n",
      "      policy_loss: -0.0021375457290560007\n",
      "      total_loss: 462.96978759765625\n",
      "      vf_explained_var: 0.21950556337833405\n",
      "      vf_loss: 462.971923828125\n",
      "    sample_time_ms: 5480.989\n",
      "    update_time_ms: 8.058\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.75073222822879\n",
      "    rl_1: 95.92885509104181\n",
      "  time_since_restore: 839.8837032318115\n",
      "  time_this_iter_s: 5.850940465927124\n",
      "  time_total_s: 839.8837032318115\n",
      "  timestamp: 1552316076\n",
      "  timesteps_since_restore: 255200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 255200\n",
      "  training_iteration: 116\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 839 s, 116 iter, 255200 ts, 137 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-54-42\n",
      "  done: false\n",
      "  episode_len_mean: 99.64\n",
      "  episode_reward_max: 207.6613034779724\n",
      "  episode_reward_mean: 136.561727215314\n",
      "  episode_reward_min: -162.5188357660861\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2538\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 787.454\n",
      "    load_time_ms: 2.459\n",
      "    num_steps_sampled: 257400\n",
      "    num_steps_trained: 257400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.629649865429142e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3321071863174438\n",
      "      kl: 0.0015784773277118802\n",
      "      policy_loss: -0.004262398462742567\n",
      "      total_loss: 102.7455062866211\n",
      "      vf_explained_var: 0.7321178913116455\n",
      "      vf_loss: 102.74976348876953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.851859946171657e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2693958282470703\n",
      "      kl: 0.008623339235782623\n",
      "      policy_loss: -0.01137972716242075\n",
      "      total_loss: 545.7261962890625\n",
      "      vf_explained_var: 0.31973710656166077\n",
      "      vf_loss: 545.7376098632812\n",
      "    sample_time_ms: 5471.902\n",
      "    update_time_ms: 7.99\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.339725671876614\n",
      "    rl_1: 96.22200154343736\n",
      "  time_since_restore: 845.7009708881378\n",
      "  time_this_iter_s: 5.817267656326294\n",
      "  time_total_s: 845.7009708881378\n",
      "  timestamp: 1552316082\n",
      "  timesteps_since_restore: 257400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 257400\n",
      "  training_iteration: 117\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 845 s, 117 iter, 257400 ts, 137 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-54-47\n",
      "  done: false\n",
      "  episode_len_mean: 98.67\n",
      "  episode_reward_max: 209.10599284379265\n",
      "  episode_reward_mean: 127.15052133827822\n",
      "  episode_reward_min: -162.5188357660861\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2560\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 780.61\n",
      "    load_time_ms: 2.449\n",
      "    num_steps_sampled: 259600\n",
      "    num_steps_trained: 259600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.814824932714571e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3102689981460571\n",
      "      kl: 0.006222154479473829\n",
      "      policy_loss: -0.0027754271868616343\n",
      "      total_loss: 325.66827392578125\n",
      "      vf_explained_var: 0.49288076162338257\n",
      "      vf_loss: 325.67108154296875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9259299730858284e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2878179550170898\n",
      "      kl: 0.007542560342699289\n",
      "      policy_loss: -0.004220247268676758\n",
      "      total_loss: 684.5543212890625\n",
      "      vf_explained_var: 0.28712761402130127\n",
      "      vf_loss: 684.55859375\n",
      "    sample_time_ms: 5407.359\n",
      "    update_time_ms: 8.115\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.947293256251044\n",
      "    rl_1: 92.20322808202722\n",
      "  time_since_restore: 851.4447226524353\n",
      "  time_this_iter_s: 5.743751764297485\n",
      "  time_total_s: 851.4447226524353\n",
      "  timestamp: 1552316087\n",
      "  timesteps_since_restore: 259600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 259600\n",
      "  training_iteration: 118\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 851 s, 118 iter, 259600 ts, 127 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-54-54\n",
      "  done: false\n",
      "  episode_len_mean: 99.36\n",
      "  episode_reward_max: 209.10599284379265\n",
      "  episode_reward_mean: 130.63724514267153\n",
      "  episode_reward_min: -153.7058817029452\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2582\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 775.631\n",
      "    load_time_ms: 2.265\n",
      "    num_steps_sampled: 261800\n",
      "    num_steps_trained: 261800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4074124663572855e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2925267219543457\n",
      "      kl: 0.003263711929321289\n",
      "      policy_loss: -0.0009662300581112504\n",
      "      total_loss: 174.17945861816406\n",
      "      vf_explained_var: 0.6210583448410034\n",
      "      vf_loss: 174.18043518066406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.629649865429142e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2940022945404053\n",
      "      kl: 0.007898692972958088\n",
      "      policy_loss: -0.003329135477542877\n",
      "      total_loss: 549.6602783203125\n",
      "      vf_explained_var: 0.3254620134830475\n",
      "      vf_loss: 549.6636352539062\n",
      "    sample_time_ms: 5436.243\n",
      "    update_time_ms: 7.799\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.93885031985291\n",
      "    rl_1: 93.69839482281867\n",
      "  time_since_restore: 858.0865321159363\n",
      "  time_this_iter_s: 6.641809463500977\n",
      "  time_total_s: 858.0865321159363\n",
      "  timestamp: 1552316094\n",
      "  timesteps_since_restore: 261800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 261800\n",
      "  training_iteration: 119\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 858 s, 119 iter, 261800 ts, 131 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-55-00\n",
      "  done: false\n",
      "  episode_len_mean: 99.5\n",
      "  episode_reward_max: 211.2676513815722\n",
      "  episode_reward_mean: 131.8890007458254\n",
      "  episode_reward_min: -153.7058817029452\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2604\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 779.051\n",
      "    load_time_ms: 2.213\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037062331786428e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.344802737236023\n",
      "      kl: 0.003838219679892063\n",
      "      policy_loss: -0.005248131696134806\n",
      "      total_loss: 143.11544799804688\n",
      "      vf_explained_var: 0.6404120326042175\n",
      "      vf_loss: 143.12071228027344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.814824932714571e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.253529667854309\n",
      "      kl: 0.004545511677861214\n",
      "      policy_loss: -0.00404973141849041\n",
      "      total_loss: 596.4271240234375\n",
      "      vf_explained_var: 0.3755705654621124\n",
      "      vf_loss: 596.43115234375\n",
      "    sample_time_ms: 5442.698\n",
      "    update_time_ms: 7.845\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.05566161867486\n",
      "    rl_1: 96.83333912715055\n",
      "  time_since_restore: 864.1798813343048\n",
      "  time_this_iter_s: 6.09334921836853\n",
      "  time_total_s: 864.1798813343048\n",
      "  timestamp: 1552316100\n",
      "  timesteps_since_restore: 264000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 120\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 864 s, 120 iter, 264000 ts, 132 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-55-07\n",
      "  done: false\n",
      "  episode_len_mean: 99.71\n",
      "  episode_reward_max: 211.2676513815722\n",
      "  episode_reward_mean: 134.78024699046045\n",
      "  episode_reward_min: -158.6584620394799\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2626\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 794.062\n",
      "    load_time_ms: 2.268\n",
      "    num_steps_sampled: 266200\n",
      "    num_steps_trained: 266200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531165893214e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3296382427215576\n",
      "      kl: 0.005289074033498764\n",
      "      policy_loss: -0.010836866684257984\n",
      "      total_loss: 131.47677612304688\n",
      "      vf_explained_var: 0.6715432405471802\n",
      "      vf_loss: 131.48760986328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.4074124663572855e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2975082397460938\n",
      "      kl: 0.005294318776577711\n",
      "      policy_loss: -0.0033013864886015654\n",
      "      total_loss: 492.4395751953125\n",
      "      vf_explained_var: 0.279237300157547\n",
      "      vf_loss: 492.4429016113281\n",
      "    sample_time_ms: 5324.484\n",
      "    update_time_ms: 7.951\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.02190454689358\n",
      "    rl_1: 98.75834244356689\n",
      "  time_since_restore: 871.249764919281\n",
      "  time_this_iter_s: 7.069883584976196\n",
      "  time_total_s: 871.249764919281\n",
      "  timestamp: 1552316107\n",
      "  timesteps_since_restore: 266200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 266200\n",
      "  training_iteration: 121\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 871 s, 121 iter, 266200 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-55-14\n",
      "  done: false\n",
      "  episode_len_mean: 98.28\n",
      "  episode_reward_max: 211.2676513815722\n",
      "  episode_reward_mean: 127.32844988453046\n",
      "  episode_reward_min: -158.6584620394799\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2649\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 815.032\n",
      "    load_time_ms: 2.318\n",
      "    num_steps_sampled: 268400\n",
      "    num_steps_trained: 268400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.009265582946607e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2714964151382446\n",
      "      kl: 0.010351482778787613\n",
      "      policy_loss: -0.003888728329911828\n",
      "      total_loss: 492.399169921875\n",
      "      vf_explained_var: 0.39550426602363586\n",
      "      vf_loss: 492.403076171875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2037062331786428e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3099764585494995\n",
      "      kl: 0.0052538467571139336\n",
      "      policy_loss: -0.005754489451646805\n",
      "      total_loss: 859.9310913085938\n",
      "      vf_explained_var: 0.16106340289115906\n",
      "      vf_loss: 859.9368286132812\n",
      "    sample_time_ms: 5292.461\n",
      "    update_time_ms: 7.755\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.07660465813672\n",
      "    rl_1: 94.25184522639374\n",
      "  time_since_restore: 877.4520900249481\n",
      "  time_this_iter_s: 6.202325105667114\n",
      "  time_total_s: 877.4520900249481\n",
      "  timestamp: 1552316114\n",
      "  timesteps_since_restore: 268400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 268400\n",
      "  training_iteration: 122\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 877 s, 122 iter, 268400 ts, 127 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-55-23\n",
      "  done: false\n",
      "  episode_len_mean: 97.83\n",
      "  episode_reward_max: 211.2676513815722\n",
      "  episode_reward_mean: 127.8803615926794\n",
      "  episode_reward_min: -158.6584620394799\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2672\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 819.356\n",
      "    load_time_ms: 2.396\n",
      "    num_steps_sampled: 270600\n",
      "    num_steps_trained: 270600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.009265582946607e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.316174030303955\n",
      "      kl: 0.005846525076776743\n",
      "      policy_loss: -0.00044680957216769457\n",
      "      total_loss: 316.338134765625\n",
      "      vf_explained_var: 0.4873303771018982\n",
      "      vf_loss: 316.338623046875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.018531165893214e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2797178030014038\n",
      "      kl: 0.004734471440315247\n",
      "      policy_loss: -0.00447510601952672\n",
      "      total_loss: 757.7659301757812\n",
      "      vf_explained_var: 0.17956629395484924\n",
      "      vf_loss: 757.7703247070312\n",
      "    sample_time_ms: 5620.399\n",
      "    update_time_ms: 8.922\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.433240541946034\n",
      "    rl_1: 93.44712105073337\n",
      "  time_since_restore: 886.7738637924194\n",
      "  time_this_iter_s: 9.321773767471313\n",
      "  time_total_s: 886.7738637924194\n",
      "  timestamp: 1552316123\n",
      "  timesteps_since_restore: 270600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 270600\n",
      "  training_iteration: 123\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 886 s, 123 iter, 270600 ts, 128 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-55-29\n",
      "  done: false\n",
      "  episode_len_mean: 96.19\n",
      "  episode_reward_max: 210.1846355543833\n",
      "  episode_reward_mean: 116.60598060415164\n",
      "  episode_reward_min: -158.6584620394799\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2696\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 822.721\n",
      "    load_time_ms: 2.396\n",
      "    num_steps_sampled: 272800\n",
      "    num_steps_trained: 272800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5046327914733034e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.330665111541748\n",
      "      kl: 0.003229525638744235\n",
      "      policy_loss: -0.0015724943950772285\n",
      "      total_loss: 399.7798156738281\n",
      "      vf_explained_var: 0.4706166088581085\n",
      "      vf_loss: 399.7814025878906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.009265582946607e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3041625022888184\n",
      "      kl: 0.004420319572091103\n",
      "      policy_loss: -0.000630641239695251\n",
      "      total_loss: 800.94775390625\n",
      "      vf_explained_var: 0.1851813644170761\n",
      "      vf_loss: 800.9483642578125\n",
      "    sample_time_ms: 5639.858\n",
      "    update_time_ms: 8.781\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.37826809115239\n",
      "    rl_1: 85.22771251299926\n",
      "  time_since_restore: 892.9820957183838\n",
      "  time_this_iter_s: 6.2082319259643555\n",
      "  time_total_s: 892.9820957183838\n",
      "  timestamp: 1552316129\n",
      "  timesteps_since_restore: 272800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 272800\n",
      "  training_iteration: 124\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 892 s, 124 iter, 272800 ts, 117 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-55-36\n",
      "  done: false\n",
      "  episode_len_mean: 97.1\n",
      "  episode_reward_max: 210.1846355543833\n",
      "  episode_reward_mean: 125.05024131706915\n",
      "  episode_reward_min: -156.7601679535291\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2717\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 817.084\n",
      "    load_time_ms: 2.343\n",
      "    num_steps_sampled: 275000\n",
      "    num_steps_trained: 275000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.523163957366517e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2889117002487183\n",
      "      kl: 0.0026530460454523563\n",
      "      policy_loss: -0.002741782460361719\n",
      "      total_loss: 111.88900756835938\n",
      "      vf_explained_var: 0.7874736785888672\n",
      "      vf_loss: 111.8917465209961\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5046327914733034e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3204107284545898\n",
      "      kl: 0.002025948604568839\n",
      "      policy_loss: -0.0009515495621599257\n",
      "      total_loss: 421.9981384277344\n",
      "      vf_explained_var: 0.38445979356765747\n",
      "      vf_loss: 421.9991149902344\n",
      "    sample_time_ms: 5680.87\n",
      "    update_time_ms: 8.93\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.7544796164195\n",
      "    rl_1: 89.29576170064965\n",
      "  time_since_restore: 899.2270681858063\n",
      "  time_this_iter_s: 6.244972467422485\n",
      "  time_total_s: 899.2270681858063\n",
      "  timestamp: 1552316136\n",
      "  timesteps_since_restore: 275000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 275000\n",
      "  training_iteration: 125\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 899 s, 125 iter, 275000 ts, 125 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-55-41\n",
      "  done: false\n",
      "  episode_len_mean: 96.75\n",
      "  episode_reward_max: 213.6769794595711\n",
      "  episode_reward_mean: 122.24728047989485\n",
      "  episode_reward_min: -156.7601679535291\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2739\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 818.172\n",
      "    load_time_ms: 2.345\n",
      "    num_steps_sampled: 277200\n",
      "    num_steps_trained: 277200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7615819786832586e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2562015056610107\n",
      "      kl: 0.0086195794865489\n",
      "      policy_loss: -0.007171825505793095\n",
      "      total_loss: 251.65469360351562\n",
      "      vf_explained_var: 0.6509261131286621\n",
      "      vf_loss: 251.66188049316406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.523163957366517e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3056167364120483\n",
      "      kl: 0.003238687291741371\n",
      "      policy_loss: -0.0057623921893537045\n",
      "      total_loss: 639.9879150390625\n",
      "      vf_explained_var: 0.4749487042427063\n",
      "      vf_loss: 639.99365234375\n",
      "    sample_time_ms: 5679.832\n",
      "    update_time_ms: 8.77\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.17863721088688\n",
      "    rl_1: 86.06864326900796\n",
      "  time_since_restore: 905.0787961483002\n",
      "  time_this_iter_s: 5.8517279624938965\n",
      "  time_total_s: 905.0787961483002\n",
      "  timestamp: 1552316141\n",
      "  timesteps_since_restore: 277200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 277200\n",
      "  training_iteration: 126\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 905 s, 126 iter, 277200 ts, 122 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-55-47\n",
      "  done: false\n",
      "  episode_len_mean: 98.55\n",
      "  episode_reward_max: 213.6769794595711\n",
      "  episode_reward_mean: 133.42503238150874\n",
      "  episode_reward_min: -156.64303386444976\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2761\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 813.795\n",
      "    load_time_ms: 2.495\n",
      "    num_steps_sampled: 279400\n",
      "    num_steps_trained: 279400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8807909893416293e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.321536898612976\n",
      "      kl: 0.00611504539847374\n",
      "      policy_loss: -0.0021481825970113277\n",
      "      total_loss: 298.7594909667969\n",
      "      vf_explained_var: 0.3958744704723358\n",
      "      vf_loss: 298.7616271972656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7615819786832586e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3242864608764648\n",
      "      kl: 0.003469492308795452\n",
      "      policy_loss: -0.005047711078077555\n",
      "      total_loss: 638.4683227539062\n",
      "      vf_explained_var: 0.20847810804843903\n",
      "      vf_loss: 638.473388671875\n",
      "    sample_time_ms: 5681.31\n",
      "    update_time_ms: 8.821\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.47557259054734\n",
      "    rl_1: 91.94945979096143\n",
      "  time_since_restore: 910.8692045211792\n",
      "  time_this_iter_s: 5.790408372879028\n",
      "  time_total_s: 910.8692045211792\n",
      "  timestamp: 1552316147\n",
      "  timesteps_since_restore: 279400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 279400\n",
      "  training_iteration: 127\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 910 s, 127 iter, 279400 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-55-53\n",
      "  done: false\n",
      "  episode_len_mean: 99.8\n",
      "  episode_reward_max: 213.6769794595711\n",
      "  episode_reward_mean: 144.00661642308006\n",
      "  episode_reward_min: -156.64303386444976\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2783\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 815.469\n",
      "    load_time_ms: 2.513\n",
      "    num_steps_sampled: 281600\n",
      "    num_steps_trained: 281600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.403954246058914e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3129631280899048\n",
      "      kl: 0.004881265107542276\n",
      "      policy_loss: -0.0072178104892373085\n",
      "      total_loss: 92.72339630126953\n",
      "      vf_explained_var: 0.78425532579422\n",
      "      vf_loss: 92.73060607910156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8807909893416293e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3179651498794556\n",
      "      kl: 0.0028872285038232803\n",
      "      policy_loss: 0.00042477052193135023\n",
      "      total_loss: 366.98480224609375\n",
      "      vf_explained_var: 0.37381476163864136\n",
      "      vf_loss: 366.98443603515625\n",
      "    sample_time_ms: 5671.025\n",
      "    update_time_ms: 9.002\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 47.30310931485972\n",
      "    rl_1: 96.70350710822031\n",
      "  time_since_restore: 916.5301027297974\n",
      "  time_this_iter_s: 5.660898208618164\n",
      "  time_total_s: 916.5301027297974\n",
      "  timestamp: 1552316153\n",
      "  timesteps_since_restore: 281600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 281600\n",
      "  training_iteration: 128\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 916 s, 128 iter, 281600 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-55-59\n",
      "  done: false\n",
      "  episode_len_mean: 99.02\n",
      "  episode_reward_max: 213.6769794595711\n",
      "  episode_reward_mean: 138.62422170371244\n",
      "  episode_reward_min: -154.2354409250256\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2807\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 817.55\n",
      "    load_time_ms: 2.595\n",
      "    num_steps_sampled: 283800\n",
      "    num_steps_trained: 283800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.701977123029457e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3520761728286743\n",
      "      kl: 0.0010718904668465257\n",
      "      policy_loss: -0.0004628241586033255\n",
      "      total_loss: 378.17596435546875\n",
      "      vf_explained_var: 0.4565049707889557\n",
      "      vf_loss: 378.17645263671875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.403954246058914e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.270790457725525\n",
      "      kl: 0.0024289865978062153\n",
      "      policy_loss: 3.19402024615556e-05\n",
      "      total_loss: 922.6600952148438\n",
      "      vf_explained_var: 0.18820758163928986\n",
      "      vf_loss: 922.6600952148438\n",
      "    sample_time_ms: 5601.69\n",
      "    update_time_ms: 9.007\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.72012239780966\n",
      "    rl_1: 94.90409930590276\n",
      "  time_since_restore: 922.4987092018127\n",
      "  time_this_iter_s: 5.968606472015381\n",
      "  time_total_s: 922.4987092018127\n",
      "  timestamp: 1552316159\n",
      "  timesteps_since_restore: 283800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 283800\n",
      "  training_iteration: 129\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 922 s, 129 iter, 283800 ts, 139 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-56-07\n",
      "  done: false\n",
      "  episode_len_mean: 97.65\n",
      "  episode_reward_max: 219.6439429773641\n",
      "  episode_reward_mean: 130.5641963416714\n",
      "  episode_reward_min: -158.09936106365836\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2829\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 811.514\n",
      "    load_time_ms: 2.512\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3509892621639607e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3315669298171997\n",
      "      kl: 0.0023690923117101192\n",
      "      policy_loss: -0.0010575783671811223\n",
      "      total_loss: 432.71612548828125\n",
      "      vf_explained_var: 0.3627416789531708\n",
      "      vf_loss: 432.71722412109375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.701977123029457e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3104503154754639\n",
      "      kl: 0.007935197092592716\n",
      "      policy_loss: -0.007618659641593695\n",
      "      total_loss: 785.5418090820312\n",
      "      vf_explained_var: 0.2225601226091385\n",
      "      vf_loss: 785.5493774414062\n",
      "    sample_time_ms: 5747.963\n",
      "    update_time_ms: 9.373\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.8148538527108\n",
      "    rl_1: 90.74934248896062\n",
      "  time_since_restore: 929.9959177970886\n",
      "  time_this_iter_s: 7.497208595275879\n",
      "  time_total_s: 929.9959177970886\n",
      "  timestamp: 1552316167\n",
      "  timesteps_since_restore: 286000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 130\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 929 s, 130 iter, 286000 ts, 131 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-56-15\n",
      "  done: false\n",
      "  episode_len_mean: 96.96\n",
      "  episode_reward_max: 219.6439429773641\n",
      "  episode_reward_mean: 123.63429087708353\n",
      "  episode_reward_min: -158.09936106365836\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2853\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 795.4\n",
      "    load_time_ms: 2.398\n",
      "    num_steps_sampled: 288200\n",
      "    num_steps_trained: 288200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1754946310819804e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.314681053161621\n",
      "      kl: 0.006347020156681538\n",
      "      policy_loss: -0.009849374182522297\n",
      "      total_loss: 436.2575988769531\n",
      "      vf_explained_var: 0.43811702728271484\n",
      "      vf_loss: 436.2674255371094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3509892621639607e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3131920099258423\n",
      "      kl: 0.005316594149917364\n",
      "      policy_loss: -0.0027158905286341906\n",
      "      total_loss: 738.9208374023438\n",
      "      vf_explained_var: 0.2718164324760437\n",
      "      vf_loss: 738.9235229492188\n",
      "    sample_time_ms: 5895.144\n",
      "    update_time_ms: 9.278\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.24195814728259\n",
      "    rl_1: 87.39233272980093\n",
      "  time_since_restore: 938.3731603622437\n",
      "  time_this_iter_s: 8.37724256515503\n",
      "  time_total_s: 938.3731603622437\n",
      "  timestamp: 1552316175\n",
      "  timesteps_since_restore: 288200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 288200\n",
      "  training_iteration: 131\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 938 s, 131 iter, 288200 ts, 124 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 95.88\n",
      "  episode_reward_max: 219.6439429773641\n",
      "  episode_reward_mean: 116.99002298284395\n",
      "  episode_reward_min: -158.09936106365836\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2875\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 794.324\n",
      "    load_time_ms: 2.373\n",
      "    num_steps_sampled: 290400\n",
      "    num_steps_trained: 290400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.87746614891758e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3329102993011475\n",
      "      kl: 0.005794895812869072\n",
      "      policy_loss: -0.004102876875549555\n",
      "      total_loss: 241.6222686767578\n",
      "      vf_explained_var: 0.6233397722244263\n",
      "      vf_loss: 241.62640380859375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1754946310819804e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2752795219421387\n",
      "      kl: 0.000957494368776679\n",
      "      policy_loss: 0.0012770496541634202\n",
      "      total_loss: 745.1538696289062\n",
      "      vf_explained_var: 0.39827433228492737\n",
      "      vf_loss: 745.1526489257812\n",
      "    sample_time_ms: 6096.97\n",
      "    update_time_ms: 9.535\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.14465256870096\n",
      "    rl_1: 84.845370414143\n",
      "  time_since_restore: 946.5844502449036\n",
      "  time_this_iter_s: 8.211289882659912\n",
      "  time_total_s: 946.5844502449036\n",
      "  timestamp: 1552316183\n",
      "  timesteps_since_restore: 290400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 290400\n",
      "  training_iteration: 132\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 946 s, 132 iter, 290400 ts, 117 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-56-30\n",
      "  done: false\n",
      "  episode_len_mean: 96.23\n",
      "  episode_reward_max: 219.6439429773641\n",
      "  episode_reward_mean: 121.31328470332633\n",
      "  episode_reward_min: -158.09936106365836\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2898\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 800.553\n",
      "    load_time_ms: 2.297\n",
      "    num_steps_sampled: 292600\n",
      "    num_steps_trained: 292600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.93873307445879e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3209283351898193\n",
      "      kl: 0.0015406779712066054\n",
      "      policy_loss: -0.0008394275791943073\n",
      "      total_loss: 255.85592651367188\n",
      "      vf_explained_var: 0.5034642219543457\n",
      "      vf_loss: 255.8567657470703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.87746614891758e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3313658237457275\n",
      "      kl: 0.004032451193779707\n",
      "      policy_loss: -0.005715882871299982\n",
      "      total_loss: 605.6608276367188\n",
      "      vf_explained_var: 0.270624577999115\n",
      "      vf_loss: 605.66650390625\n",
      "    sample_time_ms: 5850.222\n",
      "    update_time_ms: 8.375\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.7388114455875\n",
      "    rl_1: 86.57447325773883\n",
      "  time_since_restore: 953.4888405799866\n",
      "  time_this_iter_s: 6.904390335083008\n",
      "  time_total_s: 953.4888405799866\n",
      "  timestamp: 1552316190\n",
      "  timesteps_since_restore: 292600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 292600\n",
      "  training_iteration: 133\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 953 s, 133 iter, 292600 ts, 121 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-56-40\n",
      "  done: false\n",
      "  episode_len_mean: 96.01\n",
      "  episode_reward_max: 219.6439429773641\n",
      "  episode_reward_mean: 117.54097066803939\n",
      "  episode_reward_min: -158.09936106365836\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2921\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 826.746\n",
      "    load_time_ms: 2.35\n",
      "    num_steps_sampled: 294800\n",
      "    num_steps_trained: 294800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4693735437217167e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3177614212036133\n",
      "      kl: 0.003897137939929962\n",
      "      policy_loss: -0.0011489338940009475\n",
      "      total_loss: 404.92071533203125\n",
      "      vf_explained_var: 0.4267752468585968\n",
      "      vf_loss: 404.92181396484375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.93873307445879e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3120518922805786\n",
      "      kl: 0.015308523550629616\n",
      "      policy_loss: -0.008917383849620819\n",
      "      total_loss: 752.5150756835938\n",
      "      vf_explained_var: 0.2814832627773285\n",
      "      vf_loss: 752.5239868164062\n",
      "    sample_time_ms: 6201.096\n",
      "    update_time_ms: 8.488\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.67943230876992\n",
      "    rl_1: 84.8615383592695\n",
      "  time_since_restore: 963.46986079216\n",
      "  time_this_iter_s: 9.981020212173462\n",
      "  time_total_s: 963.46986079216\n",
      "  timestamp: 1552316200\n",
      "  timesteps_since_restore: 294800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 294800\n",
      "  training_iteration: 134\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 963 s, 134 iter, 294800 ts, 118 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-56-48\n",
      "  done: false\n",
      "  episode_len_mean: 95.14\n",
      "  episode_reward_max: 212.23138767719786\n",
      "  episode_reward_mean: 109.64198654170697\n",
      "  episode_reward_min: -166.57277856069885\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2944\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 894.168\n",
      "    load_time_ms: 2.609\n",
      "    num_steps_sampled: 297000\n",
      "    num_steps_trained: 297000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.346867718608583e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2651565074920654\n",
      "      kl: 0.006889798678457737\n",
      "      policy_loss: -0.00711700227111578\n",
      "      total_loss: 445.7174987792969\n",
      "      vf_explained_var: 0.5737577676773071\n",
      "      vf_loss: 445.7245788574219\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.93873307445879e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3396955728530884\n",
      "      kl: 0.002052290365099907\n",
      "      policy_loss: 0.0007632510387338698\n",
      "      total_loss: 855.6656494140625\n",
      "      vf_explained_var: 0.4233590066432953\n",
      "      vf_loss: 855.6648559570312\n",
      "    sample_time_ms: 6302.661\n",
      "    update_time_ms: 8.367\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.732754752138707\n",
      "    rl_1: 78.90923178956825\n",
      "  time_since_restore: 971.4148690700531\n",
      "  time_this_iter_s: 7.945008277893066\n",
      "  time_total_s: 971.4148690700531\n",
      "  timestamp: 1552316208\n",
      "  timesteps_since_restore: 297000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 297000\n",
      "  training_iteration: 135\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 971 s, 135 iter, 297000 ts, 110 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-56-59\n",
      "  done: false\n",
      "  episode_len_mean: 95.88\n",
      "  episode_reward_max: 212.23138767719786\n",
      "  episode_reward_mean: 115.26675822370376\n",
      "  episode_reward_min: -166.57277856069885\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2966\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 935.304\n",
      "    load_time_ms: 2.927\n",
      "    num_steps_sampled: 299200\n",
      "    num_steps_trained: 299200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6733637943810755e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2782245874404907\n",
      "      kl: 0.005076204892247915\n",
      "      policy_loss: -0.0016759013524278998\n",
      "      total_loss: 306.8747253417969\n",
      "      vf_explained_var: 0.480122447013855\n",
      "      vf_loss: 306.8763732910156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4693735437217167e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3616251945495605\n",
      "      kl: 0.0026353048160672188\n",
      "      policy_loss: -0.0013315016403794289\n",
      "      total_loss: 632.8553466796875\n",
      "      vf_explained_var: 0.32966214418411255\n",
      "      vf_loss: 632.8567504882812\n",
      "    sample_time_ms: 6756.768\n",
      "    update_time_ms: 8.699\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.889619152043444\n",
      "    rl_1: 81.3771390716603\n",
      "  time_since_restore: 982.2266511917114\n",
      "  time_this_iter_s: 10.811782121658325\n",
      "  time_total_s: 982.2266511917114\n",
      "  timestamp: 1552316219\n",
      "  timesteps_since_restore: 299200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 299200\n",
      "  training_iteration: 136\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 982 s, 136 iter, 299200 ts, 115 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-57-09\n",
      "  done: false\n",
      "  episode_len_mean: 95.49\n",
      "  episode_reward_max: 209.90091201250175\n",
      "  episode_reward_mean: 113.81547770515371\n",
      "  episode_reward_min: -166.57277856069885\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2990\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1015.337\n",
      "    load_time_ms: 2.879\n",
      "    num_steps_sampled: 301400\n",
      "    num_steps_trained: 301400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8366818971905377e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2910337448120117\n",
      "      kl: 0.007824159227311611\n",
      "      policy_loss: -0.005963694769889116\n",
      "      total_loss: 425.6622009277344\n",
      "      vf_explained_var: 0.32269471883773804\n",
      "      vf_loss: 425.6681213378906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.346867718608583e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3266403675079346\n",
      "      kl: 0.0029018823988735676\n",
      "      policy_loss: -0.00010651350021362305\n",
      "      total_loss: 816.718994140625\n",
      "      vf_explained_var: 0.24668923020362854\n",
      "      vf_loss: 816.7191162109375\n",
      "    sample_time_ms: 7088.031\n",
      "    update_time_ms: 8.781\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.646137375389856\n",
      "    rl_1: 81.16934032976387\n",
      "  time_since_restore: 992.1334505081177\n",
      "  time_this_iter_s: 9.90679931640625\n",
      "  time_total_s: 992.1334505081177\n",
      "  timestamp: 1552316229\n",
      "  timesteps_since_restore: 301400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 301400\n",
      "  training_iteration: 137\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 992 s, 137 iter, 301400 ts, 114 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 95.52\n",
      "  episode_reward_max: 209.90091201250175\n",
      "  episode_reward_mean: 112.11571028299663\n",
      "  episode_reward_min: -166.57277856069885\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3012\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1095.648\n",
      "    load_time_ms: 3.212\n",
      "    num_steps_sampled: 303600\n",
      "    num_steps_trained: 303600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.184110135184851e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2924712896347046\n",
      "      kl: 0.006253300700336695\n",
      "      policy_loss: -0.00899107102304697\n",
      "      total_loss: 278.2530517578125\n",
      "      vf_explained_var: 0.54572993516922\n",
      "      vf_loss: 278.2619934082031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6733637943810755e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3199920654296875\n",
      "      kl: 0.006238805595785379\n",
      "      policy_loss: -0.006305280607193708\n",
      "      total_loss: 534.2385864257812\n",
      "      vf_explained_var: 0.34696459770202637\n",
      "      vf_loss: 534.2448120117188\n",
      "    sample_time_ms: 7375.269\n",
      "    update_time_ms: 8.592\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.5454935746856\n",
      "    rl_1: 79.57021670831101\n",
      "  time_since_restore: 1001.479808807373\n",
      "  time_this_iter_s: 9.346358299255371\n",
      "  time_total_s: 1001.479808807373\n",
      "  timestamp: 1552316238\n",
      "  timesteps_since_restore: 303600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 303600\n",
      "  training_iteration: 138\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1001 s, 138 iter, 303600 ts, 112 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-57-29\n",
      "  done: false\n",
      "  episode_len_mean: 96.78\n",
      "  episode_reward_max: 209.90091201250175\n",
      "  episode_reward_mean: 118.25172639744287\n",
      "  episode_reward_min: -166.57277856069885\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3035\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1103.787\n",
      "    load_time_ms: 3.185\n",
      "    num_steps_sampled: 305800\n",
      "    num_steps_trained: 305800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.5920550675924255e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2830877304077148\n",
      "      kl: 0.0031621563248336315\n",
      "      policy_loss: -0.0012607189128175378\n",
      "      total_loss: 319.57037353515625\n",
      "      vf_explained_var: 0.586308479309082\n",
      "      vf_loss: 319.57159423828125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8366818971905377e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3091328144073486\n",
      "      kl: 0.003102889284491539\n",
      "      policy_loss: -0.0036200338508933783\n",
      "      total_loss: 661.948486328125\n",
      "      vf_explained_var: 0.46339353919029236\n",
      "      vf_loss: 661.9521484375\n",
      "    sample_time_ms: 7818.895\n",
      "    update_time_ms: 9.357\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.69949262463152\n",
      "    rl_1: 83.55223377281135\n",
      "  time_since_restore: 1011.9811840057373\n",
      "  time_this_iter_s: 10.501375198364258\n",
      "  time_total_s: 1011.9811840057373\n",
      "  timestamp: 1552316249\n",
      "  timesteps_since_restore: 305800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 305800\n",
      "  training_iteration: 139\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1011 s, 139 iter, 305800 ts, 118 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-57-38\n",
      "  done: false\n",
      "  episode_len_mean: 97.81\n",
      "  episode_reward_max: 207.7967520386459\n",
      "  episode_reward_mean: 120.69336981047927\n",
      "  episode_reward_min: -165.97566408025665\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3057\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1120.795\n",
      "    load_time_ms: 3.226\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2953268845640504e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2376960515975952\n",
      "      kl: 0.0048045930452644825\n",
      "      policy_loss: -0.004942729137837887\n",
      "      total_loss: 213.31219482421875\n",
      "      vf_explained_var: 0.6464049220085144\n",
      "      vf_loss: 213.31712341308594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.184110135184851e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3270180225372314\n",
      "      kl: 0.005389092490077019\n",
      "      policy_loss: -0.006827834993600845\n",
      "      total_loss: 459.6051940917969\n",
      "      vf_explained_var: 0.556522011756897\n",
      "      vf_loss: 459.6120300292969\n",
      "    sample_time_ms: 7975.213\n",
      "    update_time_ms: 9.124\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.772766580764056\n",
      "    rl_1: 83.9206032297152\n",
      "  time_since_restore: 1021.2100973129272\n",
      "  time_this_iter_s: 9.228913307189941\n",
      "  time_total_s: 1021.2100973129272\n",
      "  timestamp: 1552316258\n",
      "  timesteps_since_restore: 308000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 140\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1021 s, 140 iter, 308000 ts, 121 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-57-47\n",
      "  done: false\n",
      "  episode_len_mean: 97.18\n",
      "  episode_reward_max: 207.7967520386459\n",
      "  episode_reward_mean: 111.5241740129514\n",
      "  episode_reward_min: -165.97566408025665\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3079\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1113.953\n",
      "    load_time_ms: 3.345\n",
      "    num_steps_sampled: 310200\n",
      "    num_steps_trained: 310200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1476634422820252e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2970378398895264\n",
      "      kl: 0.006666678935289383\n",
      "      policy_loss: -0.005495950113981962\n",
      "      total_loss: 265.1323547363281\n",
      "      vf_explained_var: 0.6296806931495667\n",
      "      vf_loss: 265.13787841796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.5920550675924255e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2769159078598022\n",
      "      kl: 0.003513907315209508\n",
      "      policy_loss: -0.008067007176578045\n",
      "      total_loss: 666.5552368164062\n",
      "      vf_explained_var: 0.4388088285923004\n",
      "      vf_loss: 666.5632934570312\n",
      "    sample_time_ms: 8022.021\n",
      "    update_time_ms: 9.212\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.41410707885866\n",
      "    rl_1: 80.11006693409273\n",
      "  time_since_restore: 1029.9954993724823\n",
      "  time_this_iter_s: 8.785402059555054\n",
      "  time_total_s: 1029.9954993724823\n",
      "  timestamp: 1552316267\n",
      "  timesteps_since_restore: 310200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 310200\n",
      "  training_iteration: 141\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1029 s, 141 iter, 310200 ts, 112 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-57-57\n",
      "  done: false\n",
      "  episode_len_mean: 98.85\n",
      "  episode_reward_max: 201.93367357292755\n",
      "  episode_reward_mean: 119.67673570131444\n",
      "  episode_reward_min: -165.97566408025665\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3101\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1171.374\n",
      "    load_time_ms: 3.558\n",
      "    num_steps_sampled: 312400\n",
      "    num_steps_trained: 312400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.74532370373175e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2563804388046265\n",
      "      kl: 0.00334808137267828\n",
      "      policy_loss: -0.006171338725835085\n",
      "      total_loss: 120.48351287841797\n",
      "      vf_explained_var: 0.7490839958190918\n",
      "      vf_loss: 120.48967742919922\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2953268845640504e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2944756746292114\n",
      "      kl: 0.007473628502339125\n",
      "      policy_loss: -0.0068536982871592045\n",
      "      total_loss: 482.8423156738281\n",
      "      vf_explained_var: 0.5808365345001221\n",
      "      vf_loss: 482.8491516113281\n",
      "    sample_time_ms: 8068.454\n",
      "    update_time_ms: 9.331\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.78259825930724\n",
      "    rl_1: 84.89413744200719\n",
      "  time_since_restore: 1039.2502620220184\n",
      "  time_this_iter_s: 9.254762649536133\n",
      "  time_total_s: 1039.2502620220184\n",
      "  timestamp: 1552316277\n",
      "  timesteps_since_restore: 312400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 312400\n",
      "  training_iteration: 142\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1039 s, 142 iter, 312400 ts, 120 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-58-06\n",
      "  done: false\n",
      "  episode_len_mean: 99.41\n",
      "  episode_reward_max: 203.72239898653083\n",
      "  episode_reward_mean: 128.73100536404806\n",
      "  episode_reward_min: -157.38806628298883\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3124\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1251.927\n",
      "    load_time_ms: 3.643\n",
      "    num_steps_sampled: 314600\n",
      "    num_steps_trained: 314600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.872661851865875e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2278690338134766\n",
      "      kl: 0.0063106948509812355\n",
      "      policy_loss: -0.006706597283482552\n",
      "      total_loss: 230.6627960205078\n",
      "      vf_explained_var: 0.5666002631187439\n",
      "      vf_loss: 230.66949462890625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1476634422820252e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.334671974182129\n",
      "      kl: 0.013456572778522968\n",
      "      policy_loss: -0.006063405890017748\n",
      "      total_loss: 473.65081787109375\n",
      "      vf_explained_var: 0.4430422782897949\n",
      "      vf_loss: 473.6568908691406\n",
      "    sample_time_ms: 8196.78\n",
      "    update_time_ms: 9.793\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.37472579931319\n",
      "    rl_1: 89.35627956473489\n",
      "  time_since_restore: 1048.2570326328278\n",
      "  time_this_iter_s: 9.006770610809326\n",
      "  time_total_s: 1048.2570326328278\n",
      "  timestamp: 1552316286\n",
      "  timesteps_since_restore: 314600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 314600\n",
      "  training_iteration: 143\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1048 s, 143 iter, 314600 ts, 129 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-58-16\n",
      "  done: false\n",
      "  episode_len_mean: 99.2\n",
      "  episode_reward_max: 203.72239898653083\n",
      "  episode_reward_mean: 133.2357038103362\n",
      "  episode_reward_min: -164.02226937970917\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3146\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1335.675\n",
      "    load_time_ms: 3.61\n",
      "    num_steps_sampled: 316800\n",
      "    num_steps_trained: 316800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4293244336113134e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2817614078521729\n",
      "      kl: 0.009378627873957157\n",
      "      policy_loss: -0.009620554745197296\n",
      "      total_loss: 142.6375732421875\n",
      "      vf_explained_var: 0.684952437877655\n",
      "      vf_loss: 142.64718627929688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1476634422820252e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2561681270599365\n",
      "      kl: 0.008412864059209824\n",
      "      policy_loss: -0.006759927608072758\n",
      "      total_loss: 569.3233642578125\n",
      "      vf_explained_var: 0.18029943108558655\n",
      "      vf_loss: 569.330078125\n",
      "    sample_time_ms: 8185.701\n",
      "    update_time_ms: 10.366\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.839829196402505\n",
      "    rl_1: 93.3958746139337\n",
      "  time_since_restore: 1058.9832093715668\n",
      "  time_this_iter_s: 10.726176738739014\n",
      "  time_total_s: 1058.9832093715668\n",
      "  timestamp: 1552316296\n",
      "  timesteps_since_restore: 316800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 316800\n",
      "  training_iteration: 144\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1058 s, 144 iter, 316800 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-58-29\n",
      "  done: false\n",
      "  episode_len_mean: 98.55\n",
      "  episode_reward_max: 207.94430922421756\n",
      "  episode_reward_mean: 127.00192606750801\n",
      "  episode_reward_min: -164.02226937970917\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3169\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1311.656\n",
      "    load_time_ms: 3.478\n",
      "    num_steps_sampled: 319000\n",
      "    num_steps_trained: 319000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.146622168056567e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.269714593887329\n",
      "      kl: 0.0050214785151183605\n",
      "      policy_loss: -0.005280486773699522\n",
      "      total_loss: 325.24853515625\n",
      "      vf_explained_var: 0.6199873089790344\n",
      "      vf_loss: 325.2538146972656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.74532370373175e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2775602340698242\n",
      "      kl: 0.003028573701158166\n",
      "      policy_loss: -0.0008977889083325863\n",
      "      total_loss: 735.7822875976562\n",
      "      vf_explained_var: 0.4271300435066223\n",
      "      vf_loss: 735.783203125\n",
      "    sample_time_ms: 8617.542\n",
      "    update_time_ms: 12.27\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.20206770068857\n",
      "    rl_1: 90.79985836681945\n",
      "  time_since_restore: 1071.020579814911\n",
      "  time_this_iter_s: 12.037370443344116\n",
      "  time_total_s: 1071.020579814911\n",
      "  timestamp: 1552316309\n",
      "  timesteps_since_restore: 319000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 319000\n",
      "  training_iteration: 145\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1071 s, 145 iter, 319000 ts, 127 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-58-39\n",
      "  done: false\n",
      "  episode_len_mean: 99.26\n",
      "  episode_reward_max: 207.94430922421756\n",
      "  episode_reward_mean: 134.6776007671553\n",
      "  episode_reward_min: -164.02226937970917\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 3190\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1312.491\n",
      "    load_time_ms: 3.186\n",
      "    num_steps_sampled: 321200\n",
      "    num_steps_trained: 321200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6433760072445244e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2617695331573486\n",
      "      kl: 0.0036817179061472416\n",
      "      policy_loss: -0.0005846306448802352\n",
      "      total_loss: 197.2162628173828\n",
      "      vf_explained_var: 0.6401410102844238\n",
      "      vf_loss: 197.21685791015625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.872661851865875e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2790385484695435\n",
      "      kl: 0.0033218853641301394\n",
      "      policy_loss: -0.006115903612226248\n",
      "      total_loss: 639.94091796875\n",
      "      vf_explained_var: 0.4725320339202881\n",
      "      vf_loss: 639.947021484375\n",
      "    sample_time_ms: 8597.471\n",
      "    update_time_ms: 12.665\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.002353461531534\n",
      "    rl_1: 94.67524730562381\n",
      "  time_since_restore: 1081.6488499641418\n",
      "  time_this_iter_s: 10.628270149230957\n",
      "  time_total_s: 1081.6488499641418\n",
      "  timestamp: 1552316319\n",
      "  timesteps_since_restore: 321200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 321200\n",
      "  training_iteration: 146\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1081 s, 146 iter, 321200 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-58-48\n",
      "  done: false\n",
      "  episode_len_mean: 99.01\n",
      "  episode_reward_max: 207.94430922421756\n",
      "  episode_reward_mean: 134.39875315268887\n",
      "  episode_reward_min: -164.02226937970917\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3213\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1257.396\n",
      "    load_time_ms: 3.211\n",
      "    num_steps_sampled: 323400\n",
      "    num_steps_trained: 323400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8216880036222622e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.276392936706543\n",
      "      kl: 0.006321820430457592\n",
      "      policy_loss: -0.009364820085465908\n",
      "      total_loss: 231.43106079101562\n",
      "      vf_explained_var: 0.6274197697639465\n",
      "      vf_loss: 231.44044494628906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4293244336113134e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.264040231704712\n",
      "      kl: 0.008894145488739014\n",
      "      policy_loss: -0.004034125246107578\n",
      "      total_loss: 615.188720703125\n",
      "      vf_explained_var: 0.43249115347862244\n",
      "      vf_loss: 615.1928100585938\n",
      "    sample_time_ms: 8573.276\n",
      "    update_time_ms: 13.794\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.927093396054026\n",
      "    rl_1: 96.47165975663488\n",
      "  time_since_restore: 1090.7750885486603\n",
      "  time_this_iter_s: 9.126238584518433\n",
      "  time_total_s: 1090.7750885486603\n",
      "  timestamp: 1552316328\n",
      "  timesteps_since_restore: 323400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 323400\n",
      "  training_iteration: 147\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1090 s, 147 iter, 323400 ts, 134 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-59-00\n",
      "  done: false\n",
      "  episode_len_mean: 98.59\n",
      "  episode_reward_max: 207.94430922421756\n",
      "  episode_reward_mean: 132.1241461612106\n",
      "  episode_reward_min: -164.4469266107427\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3235\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1379.866\n",
      "    load_time_ms: 3.19\n",
      "    num_steps_sampled: 325600\n",
      "    num_steps_trained: 325600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.407790785948902e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2289241552352905\n",
      "      kl: 0.0031779438722878695\n",
      "      policy_loss: -0.000799694040324539\n",
      "      total_loss: 214.28477478027344\n",
      "      vf_explained_var: 0.6209202408790588\n",
      "      vf_loss: 214.28555297851562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.146622168056567e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3142213821411133\n",
      "      kl: 0.0021244592498987913\n",
      "      policy_loss: -0.002477731555700302\n",
      "      total_loss: 568.23095703125\n",
      "      vf_explained_var: 0.37813639640808105\n",
      "      vf_loss: 568.2335205078125\n",
      "    sample_time_ms: 8644.734\n",
      "    update_time_ms: 13.908\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.834440311029745\n",
      "    rl_1: 94.28970585018085\n",
      "  time_since_restore: 1102.067226409912\n",
      "  time_this_iter_s: 11.292137861251831\n",
      "  time_total_s: 1102.067226409912\n",
      "  timestamp: 1552316340\n",
      "  timesteps_since_restore: 325600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 325600\n",
      "  training_iteration: 148\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1102 s, 148 iter, 325600 ts, 132 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-59-13\n",
      "  done: false\n",
      "  episode_len_mean: 98.35\n",
      "  episode_reward_max: 203.12856739357636\n",
      "  episode_reward_mean: 128.8690416720891\n",
      "  episode_reward_min: -164.4469266107427\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3257\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1467.477\n",
      "    load_time_ms: 3.18\n",
      "    num_steps_sampled: 327800\n",
      "    num_steps_trained: 327800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.203895392974451e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2413628101348877\n",
      "      kl: 0.005238572135567665\n",
      "      policy_loss: -0.006124855484813452\n",
      "      total_loss: 183.46568298339844\n",
      "      vf_explained_var: 0.6973029971122742\n",
      "      vf_loss: 183.4718017578125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6433760072445244e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.282383680343628\n",
      "      kl: 0.008711539208889008\n",
      "      policy_loss: -0.003945945296436548\n",
      "      total_loss: 600.5743408203125\n",
      "      vf_explained_var: 0.47730883955955505\n",
      "      vf_loss: 600.5783081054688\n",
      "    sample_time_ms: 8834.988\n",
      "    update_time_ms: 14.294\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.99218140484601\n",
      "    rl_1: 91.87686026724312\n",
      "  time_since_restore: 1115.3494355678558\n",
      "  time_this_iter_s: 13.282209157943726\n",
      "  time_total_s: 1115.3494355678558\n",
      "  timestamp: 1552316353\n",
      "  timesteps_since_restore: 327800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 327800\n",
      "  training_iteration: 149\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1115 s, 149 iter, 327800 ts, 129 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-59-27\n",
      "  done: false\n",
      "  episode_len_mean: 99.65\n",
      "  episode_reward_max: 210.4519148490745\n",
      "  episode_reward_mean: 144.3603625682406\n",
      "  episode_reward_min: -164.4469266107427\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3279\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1561.439\n",
      "    load_time_ms: 3.374\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.802596928649634e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2936639785766602\n",
      "      kl: 0.0023489699233323336\n",
      "      policy_loss: -0.001282232697121799\n",
      "      total_loss: 131.6630096435547\n",
      "      vf_explained_var: 0.7170395255088806\n",
      "      vf_loss: 131.66429138183594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8216880036222622e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2777559757232666\n",
      "      kl: 0.003830964444205165\n",
      "      policy_loss: -0.003015905385836959\n",
      "      total_loss: 595.8679809570312\n",
      "      vf_explained_var: 0.2330939918756485\n",
      "      vf_loss: 595.8709716796875\n",
      "    sample_time_ms: 9158.739\n",
      "    update_time_ms: 15.053\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.37343087184964\n",
      "    rl_1: 100.98693169639097\n",
      "  time_since_restore: 1128.7769916057587\n",
      "  time_this_iter_s: 13.427556037902832\n",
      "  time_total_s: 1128.7769916057587\n",
      "  timestamp: 1552316367\n",
      "  timesteps_since_restore: 330000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 150\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1128 s, 150 iter, 330000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-59-41\n",
      "  done: false\n",
      "  episode_len_mean: 98.88\n",
      "  episode_reward_max: 210.4519148490745\n",
      "  episode_reward_mean: 141.10762118881462\n",
      "  episode_reward_min: -164.4469266107427\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3302\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1623.646\n",
      "    load_time_ms: 3.329\n",
      "    num_steps_sampled: 332200\n",
      "    num_steps_trained: 332200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.401298464324817e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2277040481567383\n",
      "      kl: 0.003992254380136728\n",
      "      policy_loss: -0.001672430895268917\n",
      "      total_loss: 220.37139892578125\n",
      "      vf_explained_var: 0.6775458455085754\n",
      "      vf_loss: 220.37307739257812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.407790785948902e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3344365358352661\n",
      "      kl: 0.004973466042429209\n",
      "      policy_loss: -0.0005178232677280903\n",
      "      total_loss: 642.4224853515625\n",
      "      vf_explained_var: 0.43015575408935547\n",
      "      vf_loss: 642.4231567382812\n",
      "    sample_time_ms: 9622.835\n",
      "    update_time_ms: 16.945\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.92402233537615\n",
      "    rl_1: 98.18359885343853\n",
      "  time_since_restore: 1142.8426134586334\n",
      "  time_this_iter_s: 14.065621852874756\n",
      "  time_total_s: 1142.8426134586334\n",
      "  timestamp: 1552316381\n",
      "  timesteps_since_restore: 332200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 332200\n",
      "  training_iteration: 151\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1142 s, 151 iter, 332200 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-59-49\n",
      "  done: false\n",
      "  episode_len_mean: 99.55\n",
      "  episode_reward_max: 210.4519148490745\n",
      "  episode_reward_mean: 145.20512885723815\n",
      "  episode_reward_min: -164.4469266107427\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3324\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1560.457\n",
      "    load_time_ms: 3.102\n",
      "    num_steps_sampled: 334400\n",
      "    num_steps_trained: 334400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.299747347831726\n",
      "      kl: 0.0017238797154277563\n",
      "      policy_loss: -0.0002537904365453869\n",
      "      total_loss: 148.26876831054688\n",
      "      vf_explained_var: 0.6799177527427673\n",
      "      vf_loss: 148.26901245117188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.203895392974451e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.304336667060852\n",
      "      kl: 0.005449311807751656\n",
      "      policy_loss: -0.0027798914816230536\n",
      "      total_loss: 513.2413940429688\n",
      "      vf_explained_var: 0.37959498167037964\n",
      "      vf_loss: 513.2442016601562\n",
      "    sample_time_ms: 9529.478\n",
      "    update_time_ms: 17.13\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.56219216595144\n",
      "    rl_1: 102.64293669128672\n",
      "  time_since_restore: 1150.5293517112732\n",
      "  time_this_iter_s: 7.6867382526397705\n",
      "  time_total_s: 1150.5293517112732\n",
      "  timestamp: 1552316389\n",
      "  timesteps_since_restore: 334400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 334400\n",
      "  training_iteration: 152\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1150 s, 152 iter, 334400 ts, 145 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_15-59-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.14\n",
      "  episode_reward_max: 210.4519148490745\n",
      "  episode_reward_mean: 148.67841829508478\n",
      "  episode_reward_min: -161.9941901121163\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3346\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1470.243\n",
      "    load_time_ms: 3.112\n",
      "    num_steps_sampled: 336600\n",
      "    num_steps_trained: 336600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2313026189804077\n",
      "      kl: 0.0031174817122519016\n",
      "      policy_loss: -0.006173815578222275\n",
      "      total_loss: 166.12496948242188\n",
      "      vf_explained_var: 0.7699798345565796\n",
      "      vf_loss: 166.13113403320312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.802596928649634e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3058538436889648\n",
      "      kl: 0.0144663006067276\n",
      "      policy_loss: -0.012032553553581238\n",
      "      total_loss: 490.79400634765625\n",
      "      vf_explained_var: 0.6381842494010925\n",
      "      vf_loss: 490.8059997558594\n",
      "    sample_time_ms: 9362.687\n",
      "    update_time_ms: 17.189\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.6956847535202\n",
      "    rl_1: 104.9827335415646\n",
      "  time_since_restore: 1156.9579927921295\n",
      "  time_this_iter_s: 6.428641080856323\n",
      "  time_total_s: 1156.9579927921295\n",
      "  timestamp: 1552316395\n",
      "  timesteps_since_restore: 336600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 336600\n",
      "  training_iteration: 153\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1156 s, 153 iter, 336600 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 99.31\n",
      "  episode_reward_max: 210.4519148490745\n",
      "  episode_reward_mean: 141.34840919526957\n",
      "  episode_reward_min: -161.9941901121163\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 3367\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1368.657\n",
      "    load_time_ms: 3.069\n",
      "    num_steps_sampled: 338800\n",
      "    num_steps_trained: 338800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2409470081329346\n",
      "      kl: 0.004205348901450634\n",
      "      policy_loss: -0.00200288207270205\n",
      "      total_loss: 202.0601806640625\n",
      "      vf_explained_var: 0.7531564235687256\n",
      "      vf_loss: 202.06219482421875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.802596928649634e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.288633942604065\n",
      "      kl: 0.002728794002905488\n",
      "      policy_loss: -0.003909172490239143\n",
      "      total_loss: 645.328369140625\n",
      "      vf_explained_var: 0.5788347721099854\n",
      "      vf_loss: 645.332275390625\n",
      "    sample_time_ms: 9060.26\n",
      "    update_time_ms: 16.525\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.76424133570042\n",
      "    rl_1: 99.58416785956916\n",
      "  time_since_restore: 1163.6236786842346\n",
      "  time_this_iter_s: 6.6656858921051025\n",
      "  time_total_s: 1163.6236786842346\n",
      "  timestamp: 1552316402\n",
      "  timesteps_since_restore: 338800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 338800\n",
      "  training_iteration: 154\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1163 s, 154 iter, 338800 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-00-12\n",
      "  done: false\n",
      "  episode_len_mean: 99.51\n",
      "  episode_reward_max: 209.4267740934472\n",
      "  episode_reward_mean: 135.9027411915414\n",
      "  episode_reward_min: -139.07372512495544\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3390\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1341.615\n",
      "    load_time_ms: 3.029\n",
      "    num_steps_sampled: 341000\n",
      "    num_steps_trained: 341000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.33184015750885\n",
      "      kl: 0.0021953117102384567\n",
      "      policy_loss: -0.002281771507114172\n",
      "      total_loss: 213.4462890625\n",
      "      vf_explained_var: 0.620029091835022\n",
      "      vf_loss: 213.4485626220703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.401298464324817e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2290877103805542\n",
      "      kl: 0.002577136503532529\n",
      "      policy_loss: -0.004427924752235413\n",
      "      total_loss: 672.615478515625\n",
      "      vf_explained_var: 0.506405770778656\n",
      "      vf_loss: 672.619873046875\n",
      "    sample_time_ms: 8886.469\n",
      "    update_time_ms: 14.685\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.60382605804972\n",
      "    rl_1: 99.2989151334917\n",
      "  time_since_restore: 1173.6313934326172\n",
      "  time_this_iter_s: 10.007714748382568\n",
      "  time_total_s: 1173.6313934326172\n",
      "  timestamp: 1552316412\n",
      "  timesteps_since_restore: 341000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 341000\n",
      "  training_iteration: 155\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1173 s, 155 iter, 341000 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-00-19\n",
      "  done: false\n",
      "  episode_len_mean: 98.37\n",
      "  episode_reward_max: 215.66226219121592\n",
      "  episode_reward_mean: 125.63247037571026\n",
      "  episode_reward_min: -146.9305038653859\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3413\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1302.685\n",
      "    load_time_ms: 3.142\n",
      "    num_steps_sampled: 343200\n",
      "    num_steps_trained: 343200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2726794481277466\n",
      "      kl: 0.005936264060437679\n",
      "      policy_loss: -0.00456969952210784\n",
      "      total_loss: 245.94964599609375\n",
      "      vf_explained_var: 0.6868324875831604\n",
      "      vf_loss: 245.9542236328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.278165578842163\n",
      "      kl: 0.006007347255945206\n",
      "      policy_loss: -0.0014437310164794326\n",
      "      total_loss: 636.27685546875\n",
      "      vf_explained_var: 0.5564122200012207\n",
      "      vf_loss: 636.2782592773438\n",
      "    sample_time_ms: 8538.202\n",
      "    update_time_ms: 14.149\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.6774315816364\n",
      "    rl_1: 93.95503879407389\n",
      "  time_since_restore: 1180.374050617218\n",
      "  time_this_iter_s: 6.74265718460083\n",
      "  time_total_s: 1180.374050617218\n",
      "  timestamp: 1552316419\n",
      "  timesteps_since_restore: 343200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 343200\n",
      "  training_iteration: 156\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1180 s, 156 iter, 343200 ts, 126 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-00-27\n",
      "  done: false\n",
      "  episode_len_mean: 97.2\n",
      "  episode_reward_max: 216.35103573052112\n",
      "  episode_reward_mean: 121.3015326425788\n",
      "  episode_reward_min: -154.56841582599412\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 3437\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1334.805\n",
      "    load_time_ms: 3.169\n",
      "    num_steps_sampled: 345400\n",
      "    num_steps_trained: 345400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2618417739868164\n",
      "      kl: 0.0038296107668429613\n",
      "      policy_loss: -0.0033665623050183058\n",
      "      total_loss: 267.9157409667969\n",
      "      vf_explained_var: 0.6926833987236023\n",
      "      vf_loss: 267.91912841796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2373807430267334\n",
      "      kl: 0.009087769314646721\n",
      "      policy_loss: -0.00763457827270031\n",
      "      total_loss: 799.0372924804688\n",
      "      vf_explained_var: 0.44379299879074097\n",
      "      vf_loss: 799.0447998046875\n",
      "    sample_time_ms: 8384.045\n",
      "    update_time_ms: 12.875\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.43349783143925\n",
      "    rl_1: 91.86803481113954\n",
      "  time_since_restore: 1188.2725121974945\n",
      "  time_this_iter_s: 7.898461580276489\n",
      "  time_total_s: 1188.2725121974945\n",
      "  timestamp: 1552316427\n",
      "  timesteps_since_restore: 345400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 345400\n",
      "  training_iteration: 157\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1188 s, 157 iter, 345400 ts, 121 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-00-34\n",
      "  done: false\n",
      "  episode_len_mean: 96.96\n",
      "  episode_reward_max: 216.35103573052112\n",
      "  episode_reward_mean: 120.09507596663616\n",
      "  episode_reward_min: -154.56841582599412\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 3458\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1139.78\n",
      "    load_time_ms: 2.926\n",
      "    num_steps_sampled: 347600\n",
      "    num_steps_trained: 347600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2789257764816284\n",
      "      kl: 0.004182443022727966\n",
      "      policy_loss: -0.0028629181906580925\n",
      "      total_loss: 135.73313903808594\n",
      "      vf_explained_var: 0.7130831480026245\n",
      "      vf_loss: 135.73599243164062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2773315906524658\n",
      "      kl: 0.003663666546344757\n",
      "      policy_loss: -0.006265329197049141\n",
      "      total_loss: 574.79931640625\n",
      "      vf_explained_var: 0.37990862131118774\n",
      "      vf_loss: 574.8055419921875\n",
      "    sample_time_ms: 8219.002\n",
      "    update_time_ms: 13.169\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 27.95420995776909\n",
      "    rl_1: 92.14086600886704\n",
      "  time_since_restore: 1195.9511804580688\n",
      "  time_this_iter_s: 7.678668260574341\n",
      "  time_total_s: 1195.9511804580688\n",
      "  timestamp: 1552316434\n",
      "  timesteps_since_restore: 347600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 347600\n",
      "  training_iteration: 158\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1195 s, 158 iter, 347600 ts, 120 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-00-41\n",
      "  done: false\n",
      "  episode_len_mean: 97.24\n",
      "  episode_reward_max: 221.72225488413036\n",
      "  episode_reward_mean: 128.81512571703465\n",
      "  episode_reward_min: -154.56841582599412\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3481\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1048.442\n",
      "    load_time_ms: 2.972\n",
      "    num_steps_sampled: 349800\n",
      "    num_steps_trained: 349800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.184404969215393\n",
      "      kl: 0.008853165432810783\n",
      "      policy_loss: -0.004656880162656307\n",
      "      total_loss: 232.1912078857422\n",
      "      vf_explained_var: 0.753307580947876\n",
      "      vf_loss: 232.1958770751953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3034621477127075\n",
      "      kl: 0.004323754925280809\n",
      "      policy_loss: -0.002920364961028099\n",
      "      total_loss: 695.253662109375\n",
      "      vf_explained_var: 0.5942708849906921\n",
      "      vf_loss: 695.2565307617188\n",
      "    sample_time_ms: 7615.716\n",
      "    update_time_ms: 11.889\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.90378654550032\n",
      "    rl_1: 96.91133917153432\n",
      "  time_since_restore: 1202.2717468738556\n",
      "  time_this_iter_s: 6.320566415786743\n",
      "  time_total_s: 1202.2717468738556\n",
      "  timestamp: 1552316441\n",
      "  timesteps_since_restore: 349800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 349800\n",
      "  training_iteration: 159\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1202 s, 159 iter, 349800 ts, 129 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-00-48\n",
      "  done: false\n",
      "  episode_len_mean: 97.24\n",
      "  episode_reward_max: 221.72225488413036\n",
      "  episode_reward_mean: 133.58994017714903\n",
      "  episode_reward_min: -154.56841582599412\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 3502\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 944.06\n",
      "    load_time_ms: 2.814\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2761958837509155\n",
      "      kl: 0.0012208414264023304\n",
      "      policy_loss: -0.004537135828286409\n",
      "      total_loss: 215.9759979248047\n",
      "      vf_explained_var: 0.5296852588653564\n",
      "      vf_loss: 215.98052978515625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.299989938735962\n",
      "      kl: 0.0026851051952689886\n",
      "      policy_loss: -0.00017011954332701862\n",
      "      total_loss: 666.0203857421875\n",
      "      vf_explained_var: 0.16464166343212128\n",
      "      vf_loss: 666.0205688476562\n",
      "    sample_time_ms: 7048.519\n",
      "    update_time_ms: 11.09\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.90453984432358\n",
      "    rl_1: 98.6854003328254\n",
      "  time_since_restore: 1208.9628794193268\n",
      "  time_this_iter_s: 6.691132545471191\n",
      "  time_total_s: 1208.9628794193268\n",
      "  timestamp: 1552316448\n",
      "  timesteps_since_restore: 352000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 160\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1208 s, 160 iter, 352000 ts, 134 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-00-54\n",
      "  done: false\n",
      "  episode_len_mean: 99.12\n",
      "  episode_reward_max: 221.72225488413036\n",
      "  episode_reward_mean: 145.5837812722906\n",
      "  episode_reward_min: -166.43711882833736\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3525\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 869.928\n",
      "    load_time_ms: 2.703\n",
      "    num_steps_sampled: 354200\n",
      "    num_steps_trained: 354200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4108120203018188\n",
      "      kl: 0.0047159381210803986\n",
      "      policy_loss: -0.0077704875729978085\n",
      "      total_loss: 116.38391876220703\n",
      "      vf_explained_var: 0.7407212257385254\n",
      "      vf_loss: 116.39169311523438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2308926582336426\n",
      "      kl: 0.008348016999661922\n",
      "      policy_loss: -0.0036642432678490877\n",
      "      total_loss: 686.7457275390625\n",
      "      vf_explained_var: 0.11368470638990402\n",
      "      vf_loss: 686.7493896484375\n",
      "    sample_time_ms: 6340.665\n",
      "    update_time_ms: 9.168\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.65425782990084\n",
      "    rl_1: 107.92952344238975\n",
      "  time_since_restore: 1215.183691740036\n",
      "  time_this_iter_s: 6.2208123207092285\n",
      "  time_total_s: 1215.183691740036\n",
      "  timestamp: 1552316454\n",
      "  timesteps_since_restore: 354200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 354200\n",
      "  training_iteration: 161\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1215 s, 161 iter, 354200 ts, 146 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-00-59\n",
      "  done: false\n",
      "  episode_len_mean: 99.03\n",
      "  episode_reward_max: 221.72225488413036\n",
      "  episode_reward_mean: 149.12783559209197\n",
      "  episode_reward_min: -166.43711882833736\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3547\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 854.404\n",
      "    load_time_ms: 2.739\n",
      "    num_steps_sampled: 356400\n",
      "    num_steps_trained: 356400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1763063669204712\n",
      "      kl: 0.0013915894087404013\n",
      "      policy_loss: -0.002512045670300722\n",
      "      total_loss: 257.90478515625\n",
      "      vf_explained_var: 0.6825900673866272\n",
      "      vf_loss: 257.90728759765625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.380199909210205\n",
      "      kl: 0.0072658793069422245\n",
      "      policy_loss: -0.00813200045377016\n",
      "      total_loss: 747.556396484375\n",
      "      vf_explained_var: 0.3651837706565857\n",
      "      vf_loss: 747.5645141601562\n",
      "    sample_time_ms: 6144.564\n",
      "    update_time_ms: 8.677\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.18804086651116\n",
      "    rl_1: 107.9397947255808\n",
      "  time_since_restore: 1220.7480144500732\n",
      "  time_this_iter_s: 5.5643227100372314\n",
      "  time_total_s: 1220.7480144500732\n",
      "  timestamp: 1552316459\n",
      "  timesteps_since_restore: 356400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 356400\n",
      "  training_iteration: 162\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1220 s, 162 iter, 356400 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-01-05\n",
      "  done: false\n",
      "  episode_len_mean: 98.47\n",
      "  episode_reward_max: 221.72225488413036\n",
      "  episode_reward_mean: 144.9706555468407\n",
      "  episode_reward_min: -166.43711882833736\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3569\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 860.952\n",
      "    load_time_ms: 2.633\n",
      "    num_steps_sampled: 358600\n",
      "    num_steps_trained: 358600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2303590774536133\n",
      "      kl: 0.004848048090934753\n",
      "      policy_loss: -0.006146494764834642\n",
      "      total_loss: 229.48672485351562\n",
      "      vf_explained_var: 0.7230864763259888\n",
      "      vf_loss: 229.4928741455078\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3466421365737915\n",
      "      kl: 0.010967746376991272\n",
      "      policy_loss: -0.009405474178493023\n",
      "      total_loss: 583.6851196289062\n",
      "      vf_explained_var: 0.5459766387939453\n",
      "      vf_loss: 583.694580078125\n",
      "    sample_time_ms: 6092.76\n",
      "    update_time_ms: 8.108\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.821161609195045\n",
      "    rl_1: 104.14949393764567\n",
      "  time_since_restore: 1226.7185835838318\n",
      "  time_this_iter_s: 5.970569133758545\n",
      "  time_total_s: 1226.7185835838318\n",
      "  timestamp: 1552316465\n",
      "  timesteps_since_restore: 358600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 358600\n",
      "  training_iteration: 163\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1226 s, 163 iter, 358600 ts, 145 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-01-12\n",
      "  done: false\n",
      "  episode_len_mean: 98.55\n",
      "  episode_reward_max: 220.99627139156175\n",
      "  episode_reward_mean: 145.61475030615784\n",
      "  episode_reward_min: -166.43711882833736\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3592\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 846.89\n",
      "    load_time_ms: 2.632\n",
      "    num_steps_sampled: 360800\n",
      "    num_steps_trained: 360800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2220518589019775\n",
      "      kl: 0.005420725792646408\n",
      "      policy_loss: -0.005680518224835396\n",
      "      total_loss: 207.38735961914062\n",
      "      vf_explained_var: 0.7037233710289001\n",
      "      vf_loss: 207.39305114746094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3312190771102905\n",
      "      kl: 0.0034641490783542395\n",
      "      policy_loss: -0.0005509984912350774\n",
      "      total_loss: 638.6066284179688\n",
      "      vf_explained_var: 0.41676345467567444\n",
      "      vf_loss: 638.6071166992188\n",
      "    sample_time_ms: 6039.291\n",
      "    update_time_ms: 8.471\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.13507482480982\n",
      "    rl_1: 104.47967548134798\n",
      "  time_since_restore: 1232.7136099338531\n",
      "  time_this_iter_s: 5.995026350021362\n",
      "  time_total_s: 1232.7136099338531\n",
      "  timestamp: 1552316472\n",
      "  timesteps_since_restore: 360800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 360800\n",
      "  training_iteration: 164\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1232 s, 164 iter, 360800 ts, 146 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-01-18\n",
      "  done: false\n",
      "  episode_len_mean: 96.96\n",
      "  episode_reward_max: 220.99627139156175\n",
      "  episode_reward_mean: 133.097143995007\n",
      "  episode_reward_min: -166.43711882833736\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 3616\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 846.124\n",
      "    load_time_ms: 2.56\n",
      "    num_steps_sampled: 363000\n",
      "    num_steps_trained: 363000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.348610758781433\n",
      "      kl: 0.005988576449453831\n",
      "      policy_loss: -0.0068203299306333065\n",
      "      total_loss: 253.6280975341797\n",
      "      vf_explained_var: 0.6697640419006348\n",
      "      vf_loss: 253.6349334716797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.292418360710144\n",
      "      kl: 0.0035707030910998583\n",
      "      policy_loss: -0.006941282190382481\n",
      "      total_loss: 707.3448486328125\n",
      "      vf_explained_var: 0.4670668840408325\n",
      "      vf_loss: 707.351806640625\n",
      "    sample_time_ms: 5639.689\n",
      "    update_time_ms: 8.298\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.67202575005671\n",
      "    rl_1: 97.42511824495028\n",
      "  time_since_restore: 1238.7181687355042\n",
      "  time_this_iter_s: 6.004558801651001\n",
      "  time_total_s: 1238.7181687355042\n",
      "  timestamp: 1552316478\n",
      "  timesteps_since_restore: 363000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 363000\n",
      "  training_iteration: 165\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1238 s, 165 iter, 363000 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-01-24\n",
      "  done: false\n",
      "  episode_len_mean: 98.63\n",
      "  episode_reward_max: 228.008991799895\n",
      "  episode_reward_mean: 143.7403538535262\n",
      "  episode_reward_min: -141.72968669233563\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 3637\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 858.312\n",
      "    load_time_ms: 2.423\n",
      "    num_steps_sampled: 365200\n",
      "    num_steps_trained: 365200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2361338138580322\n",
      "      kl: 0.006169385276734829\n",
      "      policy_loss: -0.003950641490519047\n",
      "      total_loss: 153.60964965820312\n",
      "      vf_explained_var: 0.7881572842597961\n",
      "      vf_loss: 153.6136016845703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.334693193435669\n",
      "      kl: 0.0018862157594412565\n",
      "      policy_loss: -0.00036296839243732393\n",
      "      total_loss: 599.6865844726562\n",
      "      vf_explained_var: 0.34632059931755066\n",
      "      vf_loss: 599.6869506835938\n",
      "    sample_time_ms: 5614.271\n",
      "    update_time_ms: 8.556\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.11425925235917\n",
      "    rl_1: 103.62609460116703\n",
      "  time_since_restore: 1245.328731060028\n",
      "  time_this_iter_s: 6.610562324523926\n",
      "  time_total_s: 1245.328731060028\n",
      "  timestamp: 1552316484\n",
      "  timesteps_since_restore: 365200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 365200\n",
      "  training_iteration: 166\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1245 s, 166 iter, 365200 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-01-31\n",
      "  done: false\n",
      "  episode_len_mean: 97.65\n",
      "  episode_reward_max: 228.008991799895\n",
      "  episode_reward_mean: 140.14335528558692\n",
      "  episode_reward_min: -141.72968669233563\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3660\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 804.216\n",
      "    load_time_ms: 2.276\n",
      "    num_steps_sampled: 367400\n",
      "    num_steps_trained: 367400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0699927806854248\n",
      "      kl: 0.010670386254787445\n",
      "      policy_loss: -0.0033087015617638826\n",
      "      total_loss: 287.6965026855469\n",
      "      vf_explained_var: 0.7058429718017578\n",
      "      vf_loss: 287.6997985839844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3776224851608276\n",
      "      kl: 0.0013192625483497977\n",
      "      policy_loss: -0.0010008091339841485\n",
      "      total_loss: 643.2631225585938\n",
      "      vf_explained_var: 0.563971996307373\n",
      "      vf_loss: 643.2640991210938\n",
      "    sample_time_ms: 5582.871\n",
      "    update_time_ms: 8.713\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.8961440075374\n",
      "    rl_1: 100.24721127804949\n",
      "  time_since_restore: 1252.3637602329254\n",
      "  time_this_iter_s: 7.035029172897339\n",
      "  time_total_s: 1252.3637602329254\n",
      "  timestamp: 1552316491\n",
      "  timesteps_since_restore: 367400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 367400\n",
      "  training_iteration: 167\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1252 s, 167 iter, 367400 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-01-37\n",
      "  done: false\n",
      "  episode_len_mean: 97.01\n",
      "  episode_reward_max: 228.008991799895\n",
      "  episode_reward_mean: 137.54520509950345\n",
      "  episode_reward_min: -145.29783135786528\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3682\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 797.33\n",
      "    load_time_ms: 2.299\n",
      "    num_steps_sampled: 369600\n",
      "    num_steps_trained: 369600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2281101942062378\n",
      "      kl: 0.006175556220114231\n",
      "      policy_loss: -0.004089396446943283\n",
      "      total_loss: 255.76409912109375\n",
      "      vf_explained_var: 0.6334203481674194\n",
      "      vf_loss: 255.76820373535156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.29976487159729\n",
      "      kl: 0.004982237704098225\n",
      "      policy_loss: -0.007222483865916729\n",
      "      total_loss: 666.090087890625\n",
      "      vf_explained_var: 0.35618215799331665\n",
      "      vf_loss: 666.0972900390625\n",
      "    sample_time_ms: 5405.547\n",
      "    update_time_ms: 8.461\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.33714060524721\n",
      "    rl_1: 100.20806449425623\n",
      "  time_since_restore: 1258.1972351074219\n",
      "  time_this_iter_s: 5.83347487449646\n",
      "  time_total_s: 1258.1972351074219\n",
      "  timestamp: 1552316497\n",
      "  timesteps_since_restore: 369600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 369600\n",
      "  training_iteration: 168\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1258 s, 168 iter, 369600 ts, 138 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-01-45\n",
      "  done: false\n",
      "  episode_len_mean: 96.96\n",
      "  episode_reward_max: 228.008991799895\n",
      "  episode_reward_mean: 134.6036647849005\n",
      "  episode_reward_min: -168.60545036844218\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3707\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 807.159\n",
      "    load_time_ms: 2.343\n",
      "    num_steps_sampled: 371800\n",
      "    num_steps_trained: 371800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2827259302139282\n",
      "      kl: 0.004858755972236395\n",
      "      policy_loss: -0.00218831654638052\n",
      "      total_loss: 407.3114013671875\n",
      "      vf_explained_var: 0.6700022220611572\n",
      "      vf_loss: 407.3135681152344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3217099905014038\n",
      "      kl: 0.003172872122377157\n",
      "      policy_loss: 0.001007330371066928\n",
      "      total_loss: 890.9241943359375\n",
      "      vf_explained_var: 0.5103959441184998\n",
      "      vf_loss: 890.9232177734375\n",
      "    sample_time_ms: 5541.133\n",
      "    update_time_ms: 8.47\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.735663218132075\n",
      "    rl_1: 98.86800156676847\n",
      "  time_since_restore: 1265.970665216446\n",
      "  time_this_iter_s: 7.773430109024048\n",
      "  time_total_s: 1265.970665216446\n",
      "  timestamp: 1552316505\n",
      "  timesteps_since_restore: 371800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 371800\n",
      "  training_iteration: 169\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1265 s, 169 iter, 371800 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-01-52\n",
      "  done: false\n",
      "  episode_len_mean: 96.44\n",
      "  episode_reward_max: 227.62228286646135\n",
      "  episode_reward_mean: 134.16096690411473\n",
      "  episode_reward_min: -168.60545036844218\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3729\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 825.896\n",
      "    load_time_ms: 2.269\n",
      "    num_steps_sampled: 374000\n",
      "    num_steps_trained: 374000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1072168350219727\n",
      "      kl: 0.007354432251304388\n",
      "      policy_loss: -0.003852916182950139\n",
      "      total_loss: 252.12464904785156\n",
      "      vf_explained_var: 0.6839923858642578\n",
      "      vf_loss: 252.12850952148438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3461995124816895\n",
      "      kl: 0.007158105727285147\n",
      "      policy_loss: -0.0065908790566027164\n",
      "      total_loss: 706.3134765625\n",
      "      vf_explained_var: 0.4499549865722656\n",
      "      vf_loss: 706.320068359375\n",
      "    sample_time_ms: 5501.764\n",
      "    update_time_ms: 8.453\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.011483178958464\n",
      "    rl_1: 96.14948372515627\n",
      "  time_since_restore: 1272.4587728977203\n",
      "  time_this_iter_s: 6.488107681274414\n",
      "  time_total_s: 1272.4587728977203\n",
      "  timestamp: 1552316512\n",
      "  timesteps_since_restore: 374000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 374000\n",
      "  training_iteration: 170\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1272 s, 170 iter, 374000 ts, 134 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-01-59\n",
      "  done: false\n",
      "  episode_len_mean: 95.7\n",
      "  episode_reward_max: 228.57513801501543\n",
      "  episode_reward_mean: 127.82892495909422\n",
      "  episode_reward_min: -168.60545036844218\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 3753\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 835.609\n",
      "    load_time_ms: 2.37\n",
      "    num_steps_sampled: 376200\n",
      "    num_steps_trained: 376200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.279089331626892\n",
      "      kl: 0.0050681158900260925\n",
      "      policy_loss: -0.003920475486665964\n",
      "      total_loss: 398.9228820800781\n",
      "      vf_explained_var: 0.5246310830116272\n",
      "      vf_loss: 398.9267883300781\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2701468467712402\n",
      "      kl: 0.001317942631430924\n",
      "      policy_loss: -0.004311308730393648\n",
      "      total_loss: 1045.351806640625\n",
      "      vf_explained_var: 0.14980128407478333\n",
      "      vf_loss: 1045.3560791015625\n",
      "    sample_time_ms: 5572.218\n",
      "    update_time_ms: 8.712\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.50399012320362\n",
      "    rl_1: 95.32493483589063\n",
      "  time_since_restore: 1279.4863777160645\n",
      "  time_this_iter_s: 7.027604818344116\n",
      "  time_total_s: 1279.4863777160645\n",
      "  timestamp: 1552316519\n",
      "  timesteps_since_restore: 376200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 376200\n",
      "  training_iteration: 171\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1279 s, 171 iter, 376200 ts, 128 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-02-08\n",
      "  done: false\n",
      "  episode_len_mean: 95.54\n",
      "  episode_reward_max: 228.57513801501543\n",
      "  episode_reward_mean: 128.73485532246556\n",
      "  episode_reward_min: -168.60545036844218\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3775\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 903.328\n",
      "    load_time_ms: 2.535\n",
      "    num_steps_sampled: 378400\n",
      "    num_steps_trained: 378400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1900291442871094\n",
      "      kl: 0.004828257951885462\n",
      "      policy_loss: -0.0036839242093265057\n",
      "      total_loss: 296.8200378417969\n",
      "      vf_explained_var: 0.6253128051757812\n",
      "      vf_loss: 296.82366943359375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3181222677230835\n",
      "      kl: 0.005203967448323965\n",
      "      policy_loss: -0.011064535938203335\n",
      "      total_loss: 828.1419067382812\n",
      "      vf_explained_var: 0.30396926403045654\n",
      "      vf_loss: 828.1529541015625\n",
      "    sample_time_ms: 5873.68\n",
      "    update_time_ms: 8.66\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.448089020978266\n",
      "    rl_1: 95.28676630148732\n",
      "  time_since_restore: 1288.748004436493\n",
      "  time_this_iter_s: 9.261626720428467\n",
      "  time_total_s: 1288.748004436493\n",
      "  timestamp: 1552316528\n",
      "  timesteps_since_restore: 378400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 378400\n",
      "  training_iteration: 172\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1288 s, 172 iter, 378400 ts, 129 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-02-16\n",
      "  done: false\n",
      "  episode_len_mean: 94.78\n",
      "  episode_reward_max: 232.36691839749744\n",
      "  episode_reward_mean: 125.44161625041828\n",
      "  episode_reward_min: -168.60545036844218\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3798\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 896.842\n",
      "    load_time_ms: 2.682\n",
      "    num_steps_sampled: 380600\n",
      "    num_steps_trained: 380600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2688310146331787\n",
      "      kl: 0.003800423815846443\n",
      "      policy_loss: -0.0011998270638287067\n",
      "      total_loss: 524.90283203125\n",
      "      vf_explained_var: 0.4940465986728668\n",
      "      vf_loss: 524.904052734375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.292929768562317\n",
      "      kl: 0.012191347777843475\n",
      "      policy_loss: -0.012547959573566914\n",
      "      total_loss: 1044.9459228515625\n",
      "      vf_explained_var: 0.3490135967731476\n",
      "      vf_loss: 1044.9583740234375\n",
      "    sample_time_ms: 6072.544\n",
      "    update_time_ms: 9.245\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.331415117809737\n",
      "    rl_1: 95.11020113260858\n",
      "  time_since_restore: 1296.6490557193756\n",
      "  time_this_iter_s: 7.90105128288269\n",
      "  time_total_s: 1296.6490557193756\n",
      "  timestamp: 1552316536\n",
      "  timesteps_since_restore: 380600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 380600\n",
      "  training_iteration: 173\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1296 s, 173 iter, 380600 ts, 125 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-02-23\n",
      "  done: false\n",
      "  episode_len_mean: 95.66\n",
      "  episode_reward_max: 232.36691839749744\n",
      "  episode_reward_mean: 131.11052498136348\n",
      "  episode_reward_min: -168.41556106700298\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3821\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 904.454\n",
      "    load_time_ms: 2.763\n",
      "    num_steps_sampled: 382800\n",
      "    num_steps_trained: 382800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2674579620361328\n",
      "      kl: 0.004906564950942993\n",
      "      policy_loss: -0.004601696040481329\n",
      "      total_loss: 212.58688354492188\n",
      "      vf_explained_var: 0.7388421893119812\n",
      "      vf_loss: 212.5914764404297\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2506588697433472\n",
      "      kl: 0.01043859962373972\n",
      "      policy_loss: -0.006073892116546631\n",
      "      total_loss: 683.51318359375\n",
      "      vf_explained_var: 0.5646865963935852\n",
      "      vf_loss: 683.519287109375\n",
      "    sample_time_ms: 6121.992\n",
      "    update_time_ms: 9.076\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.71638426323809\n",
      "    rl_1: 99.39414071812541\n",
      "  time_since_restore: 1303.2126953601837\n",
      "  time_this_iter_s: 6.5636396408081055\n",
      "  time_total_s: 1303.2126953601837\n",
      "  timestamp: 1552316543\n",
      "  timesteps_since_restore: 382800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 382800\n",
      "  training_iteration: 174\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1303 s, 174 iter, 382800 ts, 131 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 94.67\n",
      "  episode_reward_max: 241.58410756747188\n",
      "  episode_reward_mean: 127.49338044307751\n",
      "  episode_reward_min: -169.84269680195897\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3844\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 911.447\n",
      "    load_time_ms: 2.859\n",
      "    num_steps_sampled: 385000\n",
      "    num_steps_trained: 385000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1699331998825073\n",
      "      kl: 0.01102614775300026\n",
      "      policy_loss: -0.011073424480855465\n",
      "      total_loss: 406.83612060546875\n",
      "      vf_explained_var: 0.5720451474189758\n",
      "      vf_loss: 406.8471984863281\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2390979528427124\n",
      "      kl: 0.007592944428324699\n",
      "      policy_loss: -0.00982997938990593\n",
      "      total_loss: 1024.5810546875\n",
      "      vf_explained_var: 0.12868984043598175\n",
      "      vf_loss: 1024.5909423828125\n",
      "    sample_time_ms: 6498.498\n",
      "    update_time_ms: 9.164\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.766947392207186\n",
      "    rl_1: 96.72643305087033\n",
      "  time_since_restore: 1313.051643371582\n",
      "  time_this_iter_s: 9.838948011398315\n",
      "  time_total_s: 1313.051643371582\n",
      "  timestamp: 1552316552\n",
      "  timesteps_since_restore: 385000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 385000\n",
      "  training_iteration: 175\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1313 s, 175 iter, 385000 ts, 127 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-02-39\n",
      "  done: false\n",
      "  episode_len_mean: 93.27\n",
      "  episode_reward_max: 241.58410756747188\n",
      "  episode_reward_mean: 116.18520404189113\n",
      "  episode_reward_min: -169.84269680195897\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 3870\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 899.063\n",
      "    load_time_ms: 2.963\n",
      "    num_steps_sampled: 387200\n",
      "    num_steps_trained: 387200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1548117399215698\n",
      "      kl: 0.004889604169875383\n",
      "      policy_loss: -0.015801653265953064\n",
      "      total_loss: 845.9781494140625\n",
      "      vf_explained_var: 0.38960400223731995\n",
      "      vf_loss: 845.9940185546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2616063356399536\n",
      "      kl: 0.0029791195411235094\n",
      "      policy_loss: -0.0021505707409232855\n",
      "      total_loss: 1273.12451171875\n",
      "      vf_explained_var: 0.36453381180763245\n",
      "      vf_loss: 1273.126708984375\n",
      "    sample_time_ms: 6496.686\n",
      "    update_time_ms: 8.914\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.498812172439017\n",
      "    rl_1: 89.68639186945215\n",
      "  time_since_restore: 1319.51948428154\n",
      "  time_this_iter_s: 6.467840909957886\n",
      "  time_total_s: 1319.51948428154\n",
      "  timestamp: 1552316559\n",
      "  timesteps_since_restore: 387200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 387200\n",
      "  training_iteration: 176\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1319 s, 176 iter, 387200 ts, 116 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-02-46\n",
      "  done: false\n",
      "  episode_len_mean: 92.42\n",
      "  episode_reward_max: 243.98924253690728\n",
      "  episode_reward_mean: 111.95491529763943\n",
      "  episode_reward_min: -169.84269680195897\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 3894\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 901.848\n",
      "    load_time_ms: 3.039\n",
      "    num_steps_sampled: 389400\n",
      "    num_steps_trained: 389400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.194777011871338\n",
      "      kl: 0.0046201907098293304\n",
      "      policy_loss: -0.0037518346216529608\n",
      "      total_loss: 423.970703125\n",
      "      vf_explained_var: 0.5849417448043823\n",
      "      vf_loss: 423.9744873046875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2565513849258423\n",
      "      kl: 0.004008109215646982\n",
      "      policy_loss: -0.005434687715023756\n",
      "      total_loss: 981.5921630859375\n",
      "      vf_explained_var: 0.3967113196849823\n",
      "      vf_loss: 981.59765625\n",
      "    sample_time_ms: 6458.898\n",
      "    update_time_ms: 8.804\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.340922019010183\n",
      "    rl_1: 86.61399327862927\n",
      "  time_since_restore: 1326.2047514915466\n",
      "  time_this_iter_s: 6.685267210006714\n",
      "  time_total_s: 1326.2047514915466\n",
      "  timestamp: 1552316566\n",
      "  timesteps_since_restore: 389400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 389400\n",
      "  training_iteration: 177\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1326 s, 177 iter, 389400 ts, 112 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-02-53\n",
      "  done: false\n",
      "  episode_len_mean: 89.59\n",
      "  episode_reward_max: 243.98924253690728\n",
      "  episode_reward_mean: 94.46395582225854\n",
      "  episode_reward_min: -169.84269680195897\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3919\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 911.055\n",
      "    load_time_ms: 3.01\n",
      "    num_steps_sampled: 391600\n",
      "    num_steps_trained: 391600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2517030239105225\n",
      "      kl: 0.005961820017546415\n",
      "      policy_loss: -0.0012217293260619044\n",
      "      total_loss: 454.764892578125\n",
      "      vf_explained_var: 0.5411335825920105\n",
      "      vf_loss: 454.76611328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2440835237503052\n",
      "      kl: 0.005691285245120525\n",
      "      policy_loss: -0.006937001366168261\n",
      "      total_loss: 1108.818603515625\n",
      "      vf_explained_var: 0.41818708181381226\n",
      "      vf_loss: 1108.8255615234375\n",
      "    sample_time_ms: 6560.978\n",
      "    update_time_ms: 8.781\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 18.907923053491412\n",
      "    rl_1: 75.55603276876714\n",
      "  time_since_restore: 1333.150776386261\n",
      "  time_this_iter_s: 6.9460248947143555\n",
      "  time_total_s: 1333.150776386261\n",
      "  timestamp: 1552316573\n",
      "  timesteps_since_restore: 391600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 391600\n",
      "  training_iteration: 178\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1333 s, 178 iter, 391600 ts, 94.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-02-59\n",
      "  done: false\n",
      "  episode_len_mean: 90.96\n",
      "  episode_reward_max: 243.98924253690728\n",
      "  episode_reward_mean: 96.90797802067675\n",
      "  episode_reward_min: -162.77544957774248\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3942\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 900.246\n",
      "    load_time_ms: 2.963\n",
      "    num_steps_sampled: 393800\n",
      "    num_steps_trained: 393800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2900326251983643\n",
      "      kl: 0.0035742863547056913\n",
      "      policy_loss: -0.006149267312139273\n",
      "      total_loss: 383.1239929199219\n",
      "      vf_explained_var: 0.47063469886779785\n",
      "      vf_loss: 383.130126953125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2504308223724365\n",
      "      kl: 0.0025169989094138145\n",
      "      policy_loss: -0.002421238459646702\n",
      "      total_loss: 947.3465576171875\n",
      "      vf_explained_var: 0.3463241755962372\n",
      "      vf_loss: 947.3490600585938\n",
      "    sample_time_ms: 6463.676\n",
      "    update_time_ms: 8.759\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 18.05408618703286\n",
      "    rl_1: 78.85389183364384\n",
      "  time_since_restore: 1339.8438255786896\n",
      "  time_this_iter_s: 6.693049192428589\n",
      "  time_total_s: 1339.8438255786896\n",
      "  timestamp: 1552316579\n",
      "  timesteps_since_restore: 393800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 393800\n",
      "  training_iteration: 179\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1339 s, 179 iter, 393800 ts, 96.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 92.87\n",
      "  episode_reward_max: 243.98924253690728\n",
      "  episode_reward_mean: 109.89882069743864\n",
      "  episode_reward_min: -162.77544957774248\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3965\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 891.325\n",
      "    load_time_ms: 3.053\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1574740409851074\n",
      "      kl: 0.0109963808208704\n",
      "      policy_loss: -0.006750437431037426\n",
      "      total_loss: 376.05706787109375\n",
      "      vf_explained_var: 0.566964864730835\n",
      "      vf_loss: 376.0638122558594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2963995933532715\n",
      "      kl: 0.004354726057499647\n",
      "      policy_loss: -0.0021966409403830767\n",
      "      total_loss: 813.2705078125\n",
      "      vf_explained_var: 0.5251965522766113\n",
      "      vf_loss: 813.272705078125\n",
      "    sample_time_ms: 6455.693\n",
      "    update_time_ms: 8.56\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.81983697010245\n",
      "    rl_1: 86.07898372733618\n",
      "  time_since_restore: 1346.158504486084\n",
      "  time_this_iter_s: 6.314678907394409\n",
      "  time_total_s: 1346.158504486084\n",
      "  timestamp: 1552316586\n",
      "  timesteps_since_restore: 396000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 180\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1346 s, 180 iter, 396000 ts, 110 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-03-12\n",
      "  done: false\n",
      "  episode_len_mean: 92.96\n",
      "  episode_reward_max: 243.7490042855054\n",
      "  episode_reward_mean: 109.84550803378198\n",
      "  episode_reward_min: -167.91374438509206\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3988\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 876.872\n",
      "    load_time_ms: 3.057\n",
      "    num_steps_sampled: 398200\n",
      "    num_steps_trained: 398200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1560356616973877\n",
      "      kl: 0.0043375007808208466\n",
      "      policy_loss: -0.007326640188694\n",
      "      total_loss: 613.2088623046875\n",
      "      vf_explained_var: 0.31323325634002686\n",
      "      vf_loss: 613.2162475585938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2846096754074097\n",
      "      kl: 0.008844412863254547\n",
      "      policy_loss: -0.00798820611089468\n",
      "      total_loss: 1051.494384765625\n",
      "      vf_explained_var: 0.238435298204422\n",
      "      vf_loss: 1051.5023193359375\n",
      "    sample_time_ms: 6425.563\n",
      "    update_time_ms: 8.365\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.34593692569125\n",
      "    rl_1: 86.49957110809075\n",
      "  time_since_restore: 1352.738421201706\n",
      "  time_this_iter_s: 6.579916715621948\n",
      "  time_total_s: 1352.738421201706\n",
      "  timestamp: 1552316592\n",
      "  timesteps_since_restore: 398200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 398200\n",
      "  training_iteration: 181\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1352 s, 181 iter, 398200 ts, 110 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-03-19\n",
      "  done: false\n",
      "  episode_len_mean: 93.89\n",
      "  episode_reward_max: 243.7490042855054\n",
      "  episode_reward_mean: 121.00072024934735\n",
      "  episode_reward_min: -167.91374438509206\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4013\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 819.762\n",
      "    load_time_ms: 2.97\n",
      "    num_steps_sampled: 400400\n",
      "    num_steps_trained: 400400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1277961730957031\n",
      "      kl: 0.001977387582883239\n",
      "      policy_loss: -0.0037099127657711506\n",
      "      total_loss: 647.53564453125\n",
      "      vf_explained_var: 0.46484315395355225\n",
      "      vf_loss: 647.5393676757812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3023288249969482\n",
      "      kl: 0.0037983928341418505\n",
      "      policy_loss: -0.0029679755680263042\n",
      "      total_loss: 1247.070556640625\n",
      "      vf_explained_var: 0.2614869475364685\n",
      "      vf_loss: 1247.0736083984375\n",
      "    sample_time_ms: 6187.471\n",
      "    update_time_ms: 8.395\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.64010258270533\n",
      "    rl_1: 89.36061766664201\n",
      "  time_since_restore: 1359.0432209968567\n",
      "  time_this_iter_s: 6.304799795150757\n",
      "  time_total_s: 1359.0432209968567\n",
      "  timestamp: 1552316599\n",
      "  timesteps_since_restore: 400400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 400400\n",
      "  training_iteration: 182\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1359 s, 182 iter, 400400 ts, 121 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-03-27\n",
      "  done: false\n",
      "  episode_len_mean: 93.18\n",
      "  episode_reward_max: 241.2149523195538\n",
      "  episode_reward_mean: 121.2176763748973\n",
      "  episode_reward_min: -167.91374438509206\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 4037\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 828.941\n",
      "    load_time_ms: 2.936\n",
      "    num_steps_sampled: 402600\n",
      "    num_steps_trained: 402600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1195836067199707\n",
      "      kl: 0.003310385625809431\n",
      "      policy_loss: -0.002508479868993163\n",
      "      total_loss: 569.4267578125\n",
      "      vf_explained_var: 0.3721165359020233\n",
      "      vf_loss: 569.4292602539062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2735739946365356\n",
      "      kl: 0.0027731377631425858\n",
      "      policy_loss: -0.0004895029123872519\n",
      "      total_loss: 1038.1923828125\n",
      "      vf_explained_var: 0.271040141582489\n",
      "      vf_loss: 1038.1929931640625\n",
      "    sample_time_ms: 6194.851\n",
      "    update_time_ms: 7.89\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.26952582157835\n",
      "    rl_1: 87.94815055331897\n",
      "  time_since_restore: 1367.1059050559998\n",
      "  time_this_iter_s: 8.062684059143066\n",
      "  time_total_s: 1367.1059050559998\n",
      "  timestamp: 1552316607\n",
      "  timesteps_since_restore: 402600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 402600\n",
      "  training_iteration: 183\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1367 s, 183 iter, 402600 ts, 121 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-03-33\n",
      "  done: false\n",
      "  episode_len_mean: 92.01\n",
      "  episode_reward_max: 241.2149523195538\n",
      "  episode_reward_mean: 116.33360586153294\n",
      "  episode_reward_min: -167.91374438509206\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 4061\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 822.917\n",
      "    load_time_ms: 2.866\n",
      "    num_steps_sampled: 404800\n",
      "    num_steps_trained: 404800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0681934356689453\n",
      "      kl: 0.01354110985994339\n",
      "      policy_loss: -0.0050091673620045185\n",
      "      total_loss: 660.4171752929688\n",
      "      vf_explained_var: 0.3708898425102234\n",
      "      vf_loss: 660.422119140625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2881724834442139\n",
      "      kl: 0.011480196379125118\n",
      "      policy_loss: -0.006913795135915279\n",
      "      total_loss: 1091.995361328125\n",
      "      vf_explained_var: 0.28760826587677\n",
      "      vf_loss: 1092.0023193359375\n",
      "    sample_time_ms: 6161.951\n",
      "    update_time_ms: 7.87\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.28697156817322\n",
      "    rl_1: 83.04663429335973\n",
      "  time_since_restore: 1373.2783777713776\n",
      "  time_this_iter_s: 6.172472715377808\n",
      "  time_total_s: 1373.2783777713776\n",
      "  timestamp: 1552316613\n",
      "  timesteps_since_restore: 404800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 404800\n",
      "  training_iteration: 184\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1373 s, 184 iter, 404800 ts, 116 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-03-39\n",
      "  done: false\n",
      "  episode_len_mean: 91.73\n",
      "  episode_reward_max: 241.0317016475445\n",
      "  episode_reward_mean: 112.51137015865332\n",
      "  episode_reward_min: -161.99126618904197\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4084\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 806.73\n",
      "    load_time_ms: 2.853\n",
      "    num_steps_sampled: 407000\n",
      "    num_steps_trained: 407000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1315335035324097\n",
      "      kl: 0.004592530895024538\n",
      "      policy_loss: -0.0004633581847883761\n",
      "      total_loss: 344.41229248046875\n",
      "      vf_explained_var: 0.6833707690238953\n",
      "      vf_loss: 344.4127502441406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2719253301620483\n",
      "      kl: 0.012817609123885632\n",
      "      policy_loss: -0.011403007432818413\n",
      "      total_loss: 775.6268310546875\n",
      "      vf_explained_var: 0.5477524995803833\n",
      "      vf_loss: 775.6383056640625\n",
      "    sample_time_ms: 5795.309\n",
      "    update_time_ms: 7.72\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.172931379192132\n",
      "    rl_1: 81.3384387794612\n",
      "  time_since_restore: 1379.2863414287567\n",
      "  time_this_iter_s: 6.00796365737915\n",
      "  time_total_s: 1379.2863414287567\n",
      "  timestamp: 1552316619\n",
      "  timesteps_since_restore: 407000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 407000\n",
      "  training_iteration: 185\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1379 s, 185 iter, 407000 ts, 113 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-03-45\n",
      "  done: false\n",
      "  episode_len_mean: 93.07\n",
      "  episode_reward_max: 241.0317016475445\n",
      "  episode_reward_mean: 118.89225670197882\n",
      "  episode_reward_min: -160.09591236387112\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 4108\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 806.213\n",
      "    load_time_ms: 2.759\n",
      "    num_steps_sampled: 409200\n",
      "    num_steps_trained: 409200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2231296300888062\n",
      "      kl: 0.0014388166600838304\n",
      "      policy_loss: -0.00391742866486311\n",
      "      total_loss: 329.2853088378906\n",
      "      vf_explained_var: 0.5770072340965271\n",
      "      vf_loss: 329.2892150878906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.280019760131836\n",
      "      kl: 0.002129222732037306\n",
      "      policy_loss: 0.0013760797446593642\n",
      "      total_loss: 877.8704833984375\n",
      "      vf_explained_var: 0.25895956158638\n",
      "      vf_loss: 877.8690795898438\n",
      "    sample_time_ms: 5741.109\n",
      "    update_time_ms: 7.434\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.27491286519207\n",
      "    rl_1: 86.61734383678674\n",
      "  time_since_restore: 1385.203054189682\n",
      "  time_this_iter_s: 5.916712760925293\n",
      "  time_total_s: 1385.203054189682\n",
      "  timestamp: 1552316625\n",
      "  timesteps_since_restore: 409200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 409200\n",
      "  training_iteration: 186\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1385 s, 186 iter, 409200 ts, 119 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-03-51\n",
      "  done: false\n",
      "  episode_len_mean: 92.9\n",
      "  episode_reward_max: 236.34285812954934\n",
      "  episode_reward_mean: 113.77844765689433\n",
      "  episode_reward_min: -160.09591236387112\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4131\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 810.432\n",
      "    load_time_ms: 2.758\n",
      "    num_steps_sampled: 411400\n",
      "    num_steps_trained: 411400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1491731405258179\n",
      "      kl: 0.00879617128521204\n",
      "      policy_loss: -0.0068965209648013115\n",
      "      total_loss: 615.4990844726562\n",
      "      vf_explained_var: 0.3812424838542938\n",
      "      vf_loss: 615.5060424804688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2669556140899658\n",
      "      kl: 0.004649505019187927\n",
      "      policy_loss: -0.004514170344918966\n",
      "      total_loss: 1053.455322265625\n",
      "      vf_explained_var: 0.4032946228981018\n",
      "      vf_loss: 1053.4598388671875\n",
      "    sample_time_ms: 5647.104\n",
      "    update_time_ms: 7.385\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.595163722889737\n",
      "    rl_1: 83.18328393400459\n",
      "  time_since_restore: 1390.9890961647034\n",
      "  time_this_iter_s: 5.786041975021362\n",
      "  time_total_s: 1390.9890961647034\n",
      "  timestamp: 1552316631\n",
      "  timesteps_since_restore: 411400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 411400\n",
      "  training_iteration: 187\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1390 s, 187 iter, 411400 ts, 114 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-03-57\n",
      "  done: false\n",
      "  episode_len_mean: 94.17\n",
      "  episode_reward_max: 234.59524425361337\n",
      "  episode_reward_mean: 117.94605577559996\n",
      "  episode_reward_min: -157.7792714805761\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4154\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 808.299\n",
      "    load_time_ms: 2.679\n",
      "    num_steps_sampled: 413600\n",
      "    num_steps_trained: 413600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1129015684127808\n",
      "      kl: 0.003883731784299016\n",
      "      policy_loss: -0.0006213068845681846\n",
      "      total_loss: 292.4289855957031\n",
      "      vf_explained_var: 0.6454052925109863\n",
      "      vf_loss: 292.4295959472656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2904821634292603\n",
      "      kl: 0.009801877662539482\n",
      "      policy_loss: -0.0047883703373372555\n",
      "      total_loss: 707.2647705078125\n",
      "      vf_explained_var: 0.46471211314201355\n",
      "      vf_loss: 707.2695922851562\n",
      "    sample_time_ms: 5577.961\n",
      "    update_time_ms: 7.138\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.29328574991233\n",
      "    rl_1: 85.65277002568763\n",
      "  time_since_restore: 1397.2196674346924\n",
      "  time_this_iter_s: 6.230571269989014\n",
      "  time_total_s: 1397.2196674346924\n",
      "  timestamp: 1552316637\n",
      "  timesteps_since_restore: 413600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 413600\n",
      "  training_iteration: 188\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1397 s, 188 iter, 413600 ts, 118 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-04-04\n",
      "  done: false\n",
      "  episode_len_mean: 93.98\n",
      "  episode_reward_max: 228.63638868841863\n",
      "  episode_reward_mean: 113.79836778309905\n",
      "  episode_reward_min: -157.7792714805761\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4177\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 811.09\n",
      "    load_time_ms: 2.663\n",
      "    num_steps_sampled: 415800\n",
      "    num_steps_trained: 415800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1336852312088013\n",
      "      kl: 0.004674933385103941\n",
      "      policy_loss: -0.0021382176782935858\n",
      "      total_loss: 428.927490234375\n",
      "      vf_explained_var: 0.483795166015625\n",
      "      vf_loss: 428.9296569824219\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2882860898971558\n",
      "      kl: 0.0022614318877458572\n",
      "      policy_loss: -0.003446143353357911\n",
      "      total_loss: 755.2719116210938\n",
      "      vf_explained_var: 0.37263166904449463\n",
      "      vf_loss: 755.275390625\n",
      "    sample_time_ms: 5546.669\n",
      "    update_time_ms: 7.046\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.6222976151212\n",
      "    rl_1: 83.17607016797785\n",
      "  time_since_restore: 1403.6290998458862\n",
      "  time_this_iter_s: 6.409432411193848\n",
      "  time_total_s: 1403.6290998458862\n",
      "  timestamp: 1552316644\n",
      "  timesteps_since_restore: 415800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 415800\n",
      "  training_iteration: 189\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1403 s, 189 iter, 415800 ts, 114 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-04-10\n",
      "  done: false\n",
      "  episode_len_mean: 96.77\n",
      "  episode_reward_max: 228.63638868841863\n",
      "  episode_reward_mean: 132.9572132927694\n",
      "  episode_reward_min: -157.7792714805761\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4199\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 794.732\n",
      "    load_time_ms: 2.65\n",
      "    num_steps_sampled: 418000\n",
      "    num_steps_trained: 418000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0956873893737793\n",
      "      kl: 0.0033663157373666763\n",
      "      policy_loss: -0.003078538691624999\n",
      "      total_loss: 182.0037078857422\n",
      "      vf_explained_var: 0.7397764921188354\n",
      "      vf_loss: 182.00680541992188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3108441829681396\n",
      "      kl: 0.004129557404667139\n",
      "      policy_loss: 0.0005663414485752583\n",
      "      total_loss: 484.1382751464844\n",
      "      vf_explained_var: 0.621894896030426\n",
      "      vf_loss: 484.1376953125\n",
      "    sample_time_ms: 5590.234\n",
      "    update_time_ms: 7.095\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.81764418609529\n",
      "    rl_1: 93.13956910667412\n",
      "  time_since_restore: 1410.2145369052887\n",
      "  time_this_iter_s: 6.585437059402466\n",
      "  time_total_s: 1410.2145369052887\n",
      "  timestamp: 1552316650\n",
      "  timesteps_since_restore: 418000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 418000\n",
      "  training_iteration: 190\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1410 s, 190 iter, 418000 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-04-17\n",
      "  done: false\n",
      "  episode_len_mean: 96.31\n",
      "  episode_reward_max: 228.63638868841863\n",
      "  episode_reward_mean: 132.8096358827246\n",
      "  episode_reward_min: -165.85552267309717\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 4223\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 812.417\n",
      "    load_time_ms: 2.554\n",
      "    num_steps_sampled: 420200\n",
      "    num_steps_trained: 420200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.136033535003662\n",
      "      kl: 0.008978166617453098\n",
      "      policy_loss: -0.01100933738052845\n",
      "      total_loss: 625.1719970703125\n",
      "      vf_explained_var: 0.35939034819602966\n",
      "      vf_loss: 625.1830444335938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.288428544998169\n",
      "      kl: 0.0034030769020318985\n",
      "      policy_loss: -0.0008408777648583055\n",
      "      total_loss: 1064.9964599609375\n",
      "      vf_explained_var: 0.26426416635513306\n",
      "      vf_loss: 1064.9971923828125\n",
      "    sample_time_ms: 5539.646\n",
      "    update_time_ms: 6.933\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.076437176025294\n",
      "    rl_1: 92.73319870669928\n",
      "  time_since_restore: 1416.460904121399\n",
      "  time_this_iter_s: 6.2463672161102295\n",
      "  time_total_s: 1416.460904121399\n",
      "  timestamp: 1552316657\n",
      "  timesteps_since_restore: 420200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 420200\n",
      "  training_iteration: 191\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1416 s, 191 iter, 420200 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-04-24\n",
      "  done: false\n",
      "  episode_len_mean: 96.55\n",
      "  episode_reward_max: 227.44727310907214\n",
      "  episode_reward_mean: 136.19608056363046\n",
      "  episode_reward_min: -165.85552267309717\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4245\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 812.307\n",
      "    load_time_ms: 2.569\n",
      "    num_steps_sampled: 422400\n",
      "    num_steps_trained: 422400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.159580945968628\n",
      "      kl: 0.00994028802961111\n",
      "      policy_loss: -0.011914119124412537\n",
      "      total_loss: 303.822021484375\n",
      "      vf_explained_var: 0.5094795823097229\n",
      "      vf_loss: 303.8338928222656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2753742933273315\n",
      "      kl: 0.004132265690714121\n",
      "      policy_loss: -0.0010452975984662771\n",
      "      total_loss: 769.2142333984375\n",
      "      vf_explained_var: 0.32793188095092773\n",
      "      vf_loss: 769.2152709960938\n",
      "    sample_time_ms: 5690.325\n",
      "    update_time_ms: 6.962\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.748430879184234\n",
      "    rl_1: 94.44764968444622\n",
      "  time_since_restore: 1424.2717442512512\n",
      "  time_this_iter_s: 7.810840129852295\n",
      "  time_total_s: 1424.2717442512512\n",
      "  timestamp: 1552316664\n",
      "  timesteps_since_restore: 422400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 422400\n",
      "  training_iteration: 192\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1424 s, 192 iter, 422400 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-04-31\n",
      "  done: false\n",
      "  episode_len_mean: 94.86\n",
      "  episode_reward_max: 227.44727310907214\n",
      "  episode_reward_mean: 122.76648247737727\n",
      "  episode_reward_min: -165.85552267309717\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4270\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 802.596\n",
      "    load_time_ms: 2.452\n",
      "    num_steps_sampled: 424600\n",
      "    num_steps_trained: 424600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1271849870681763\n",
      "      kl: 0.00399056077003479\n",
      "      policy_loss: -0.004461200442165136\n",
      "      total_loss: 447.7245178222656\n",
      "      vf_explained_var: 0.6064391732215881\n",
      "      vf_loss: 447.72900390625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2979074716567993\n",
      "      kl: 0.0031305935699492693\n",
      "      policy_loss: -0.0038586724549531937\n",
      "      total_loss: 1043.6932373046875\n",
      "      vf_explained_var: 0.4077244997024536\n",
      "      vf_loss: 1043.6971435546875\n",
      "    sample_time_ms: 5562.304\n",
      "    update_time_ms: 6.839\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.32588157232864\n",
      "    rl_1: 86.44060090504863\n",
      "  time_since_restore: 1430.9526941776276\n",
      "  time_this_iter_s: 6.680949926376343\n",
      "  time_total_s: 1430.9526941776276\n",
      "  timestamp: 1552316671\n",
      "  timesteps_since_restore: 424600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 424600\n",
      "  training_iteration: 193\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1430 s, 193 iter, 424600 ts, 123 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-04-37\n",
      "  done: false\n",
      "  episode_len_mean: 94.91\n",
      "  episode_reward_max: 225.61770540418388\n",
      "  episode_reward_mean: 124.81814993380377\n",
      "  episode_reward_min: -165.85552267309717\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4292\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 807.924\n",
      "    load_time_ms: 2.479\n",
      "    num_steps_sampled: 426800\n",
      "    num_steps_trained: 426800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2073689699172974\n",
      "      kl: 0.007089842110872269\n",
      "      policy_loss: -0.0039037500973790884\n",
      "      total_loss: 326.1053771972656\n",
      "      vf_explained_var: 0.5318393111228943\n",
      "      vf_loss: 326.1092834472656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2759370803833008\n",
      "      kl: 0.0059497845359146595\n",
      "      policy_loss: -0.0033212960697710514\n",
      "      total_loss: 708.544677734375\n",
      "      vf_explained_var: 0.3634929955005646\n",
      "      vf_loss: 708.548095703125\n",
      "    sample_time_ms: 5537.884\n",
      "    update_time_ms: 6.596\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.47770181308373\n",
      "    rl_1: 89.34044812072003\n",
      "  time_since_restore: 1436.932602405548\n",
      "  time_this_iter_s: 5.979908227920532\n",
      "  time_total_s: 1436.932602405548\n",
      "  timestamp: 1552316677\n",
      "  timesteps_since_restore: 426800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 426800\n",
      "  training_iteration: 194\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1436 s, 194 iter, 426800 ts, 125 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-04-45\n",
      "  done: false\n",
      "  episode_len_mean: 93.98\n",
      "  episode_reward_max: 225.61770540418388\n",
      "  episode_reward_mean: 119.45427405512294\n",
      "  episode_reward_min: -158.45297829865203\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4317\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 816.942\n",
      "    load_time_ms: 2.476\n",
      "    num_steps_sampled: 429000\n",
      "    num_steps_trained: 429000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0769513845443726\n",
      "      kl: 0.010908316820859909\n",
      "      policy_loss: -0.007147990632802248\n",
      "      total_loss: 555.7936401367188\n",
      "      vf_explained_var: 0.5081684589385986\n",
      "      vf_loss: 555.8006591796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.251352310180664\n",
      "      kl: 0.008776746690273285\n",
      "      policy_loss: -0.007337369490414858\n",
      "      total_loss: 1015.4247436523438\n",
      "      vf_explained_var: 0.35662898421287537\n",
      "      vf_loss: 1015.4320068359375\n",
      "    sample_time_ms: 5743.092\n",
      "    update_time_ms: 6.873\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.01891096471972\n",
      "    rl_1: 86.43536309040324\n",
      "  time_since_restore: 1445.088054895401\n",
      "  time_this_iter_s: 8.155452489852905\n",
      "  time_total_s: 1445.088054895401\n",
      "  timestamp: 1552316685\n",
      "  timesteps_since_restore: 429000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 429000\n",
      "  training_iteration: 195\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1445 s, 195 iter, 429000 ts, 119 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-04-54\n",
      "  done: false\n",
      "  episode_len_mean: 92.91\n",
      "  episode_reward_max: 223.76572697391356\n",
      "  episode_reward_mean: 102.68295342405773\n",
      "  episode_reward_min: -158.45297829865203\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4340\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 826.282\n",
      "    load_time_ms: 2.565\n",
      "    num_steps_sampled: 431200\n",
      "    num_steps_trained: 431200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0386779308319092\n",
      "      kl: 0.009669595398008823\n",
      "      policy_loss: -0.010743554681539536\n",
      "      total_loss: 573.75048828125\n",
      "      vf_explained_var: 0.5683168172836304\n",
      "      vf_loss: 573.7611694335938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3162670135498047\n",
      "      kl: 0.0026767970994114876\n",
      "      policy_loss: -0.0006364277214743197\n",
      "      total_loss: 847.470703125\n",
      "      vf_explained_var: 0.6038144826889038\n",
      "      vf_loss: 847.4713134765625\n",
      "    sample_time_ms: 5957.91\n",
      "    update_time_ms: 6.969\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.095853591551858\n",
      "    rl_1: 76.5870998325059\n",
      "  time_since_restore: 1453.249321937561\n",
      "  time_this_iter_s: 8.161267042160034\n",
      "  time_total_s: 1453.249321937561\n",
      "  timestamp: 1552316694\n",
      "  timesteps_since_restore: 431200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 431200\n",
      "  training_iteration: 196\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1453 s, 196 iter, 431200 ts, 103 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-05-02\n",
      "  done: false\n",
      "  episode_len_mean: 93.97\n",
      "  episode_reward_max: 227.1655132821298\n",
      "  episode_reward_mean: 109.59587772509082\n",
      "  episode_reward_min: -158.45297829865203\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4363\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 841.519\n",
      "    load_time_ms: 2.684\n",
      "    num_steps_sampled: 433400\n",
      "    num_steps_trained: 433400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0932177305221558\n",
      "      kl: 0.004350610077381134\n",
      "      policy_loss: -0.005647667217999697\n",
      "      total_loss: 243.0072021484375\n",
      "      vf_explained_var: 0.6242706179618835\n",
      "      vf_loss: 243.0128631591797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2841975688934326\n",
      "      kl: 0.0022499302867799997\n",
      "      policy_loss: 0.0008887709700502455\n",
      "      total_loss: 593.7465209960938\n",
      "      vf_explained_var: 0.5121732950210571\n",
      "      vf_loss: 593.74560546875\n",
      "    sample_time_ms: 6178.66\n",
      "    update_time_ms: 7.234\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.74139759744812\n",
      "    rl_1: 79.85448012764272\n",
      "  time_since_restore: 1461.4025752544403\n",
      "  time_this_iter_s: 8.153253316879272\n",
      "  time_total_s: 1461.4025752544403\n",
      "  timestamp: 1552316702\n",
      "  timesteps_since_restore: 433400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 433400\n",
      "  training_iteration: 197\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1461 s, 197 iter, 433400 ts, 110 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-05-10\n",
      "  done: false\n",
      "  episode_len_mean: 94.83\n",
      "  episode_reward_max: 227.74200576432258\n",
      "  episode_reward_mean: 117.10760133468649\n",
      "  episode_reward_min: -158.45297829865203\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4385\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 847.528\n",
      "    load_time_ms: 2.82\n",
      "    num_steps_sampled: 435600\n",
      "    num_steps_trained: 435600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0756487846374512\n",
      "      kl: 0.017105480656027794\n",
      "      policy_loss: -0.00994023960083723\n",
      "      total_loss: 335.03863525390625\n",
      "      vf_explained_var: 0.6098867654800415\n",
      "      vf_loss: 335.048583984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.254714012145996\n",
      "      kl: 0.009383132681250572\n",
      "      policy_loss: -0.003441085573285818\n",
      "      total_loss: 770.4198608398438\n",
      "      vf_explained_var: 0.46362391114234924\n",
      "      vf_loss: 770.42333984375\n",
      "    sample_time_ms: 6349.615\n",
      "    update_time_ms: 7.369\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.82719237320108\n",
      "    rl_1: 84.28040896148542\n",
      "  time_since_restore: 1469.405472278595\n",
      "  time_this_iter_s: 8.002897024154663\n",
      "  time_total_s: 1469.405472278595\n",
      "  timestamp: 1552316710\n",
      "  timesteps_since_restore: 435600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 435600\n",
      "  training_iteration: 198\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1469 s, 198 iter, 435600 ts, 117 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-05-16\n",
      "  done: false\n",
      "  episode_len_mean: 96.28\n",
      "  episode_reward_max: 231.58208312605882\n",
      "  episode_reward_mean: 127.24511669809128\n",
      "  episode_reward_min: -158.45297829865203\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4408\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 846.521\n",
      "    load_time_ms: 2.846\n",
      "    num_steps_sampled: 437800\n",
      "    num_steps_trained: 437800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0423814058303833\n",
      "      kl: 0.006236599758267403\n",
      "      policy_loss: -0.00396350072696805\n",
      "      total_loss: 290.61907958984375\n",
      "      vf_explained_var: 0.6974408626556396\n",
      "      vf_loss: 290.6230163574219\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2569317817687988\n",
      "      kl: 0.012566141784191132\n",
      "      policy_loss: -0.007726338692009449\n",
      "      total_loss: 827.6593627929688\n",
      "      vf_explained_var: 0.5055389404296875\n",
      "      vf_loss: 827.6671752929688\n",
      "    sample_time_ms: 6345.55\n",
      "    update_time_ms: 7.456\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.3881468204751\n",
      "    rl_1: 89.85696987761615\n",
      "  time_since_restore: 1475.761759519577\n",
      "  time_this_iter_s: 6.356287240982056\n",
      "  time_total_s: 1475.761759519577\n",
      "  timestamp: 1552316716\n",
      "  timesteps_since_restore: 437800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 437800\n",
      "  training_iteration: 199\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1475 s, 199 iter, 437800 ts, 127 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-05-23\n",
      "  done: false\n",
      "  episode_len_mean: 95.1\n",
      "  episode_reward_max: 232.48050606005233\n",
      "  episode_reward_mean: 122.34757533057174\n",
      "  episode_reward_min: -157.1581162571676\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4433\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 850.915\n",
      "    load_time_ms: 2.794\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1095833778381348\n",
      "      kl: 0.005511419847607613\n",
      "      policy_loss: -0.007606375031173229\n",
      "      total_loss: 508.18267822265625\n",
      "      vf_explained_var: 0.6001791954040527\n",
      "      vf_loss: 508.19024658203125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.247939109802246\n",
      "      kl: 0.00801316648721695\n",
      "      policy_loss: -0.008191985078155994\n",
      "      total_loss: 1118.54443359375\n",
      "      vf_explained_var: 0.49147358536720276\n",
      "      vf_loss: 1118.5526123046875\n",
      "    sample_time_ms: 6323.246\n",
      "    update_time_ms: 7.628\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.15662419389385\n",
      "    rl_1: 87.19095113667792\n",
      "  time_since_restore: 1482.1701259613037\n",
      "  time_this_iter_s: 6.408366441726685\n",
      "  time_total_s: 1482.1701259613037\n",
      "  timestamp: 1552316723\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 200\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1482 s, 200 iter, 440000 ts, 122 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-05-31\n",
      "  done: false\n",
      "  episode_len_mean: 94.16\n",
      "  episode_reward_max: 233.13109997524873\n",
      "  episode_reward_mean: 118.90289787063074\n",
      "  episode_reward_min: -157.1581162571676\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4456\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 848.755\n",
      "    load_time_ms: 2.823\n",
      "    num_steps_sampled: 442200\n",
      "    num_steps_trained: 442200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0978553295135498\n",
      "      kl: 0.008150534704327583\n",
      "      policy_loss: -0.00038082414539530873\n",
      "      total_loss: 316.832763671875\n",
      "      vf_explained_var: 0.6702562570571899\n",
      "      vf_loss: 316.8331604003906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2472920417785645\n",
      "      kl: 0.004748877137899399\n",
      "      policy_loss: -0.003143545938655734\n",
      "      total_loss: 887.8670043945312\n",
      "      vf_explained_var: 0.5491305589675903\n",
      "      vf_loss: 887.8701782226562\n",
      "    sample_time_ms: 6506.709\n",
      "    update_time_ms: 7.707\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.95225111602242\n",
      "    rl_1: 87.95064675460833\n",
      "  time_since_restore: 1490.2317774295807\n",
      "  time_this_iter_s: 8.061651468276978\n",
      "  time_total_s: 1490.2317774295807\n",
      "  timestamp: 1552316731\n",
      "  timesteps_since_restore: 442200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 442200\n",
      "  training_iteration: 201\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1490 s, 201 iter, 442200 ts, 119 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-05-38\n",
      "  done: false\n",
      "  episode_len_mean: 94.49\n",
      "  episode_reward_max: 233.84795331689136\n",
      "  episode_reward_mean: 121.93058970145024\n",
      "  episode_reward_min: -157.1581162571676\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 4480\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 861.488\n",
      "    load_time_ms: 3.146\n",
      "    num_steps_sampled: 444400\n",
      "    num_steps_trained: 444400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0932023525238037\n",
      "      kl: 0.0058704763650894165\n",
      "      policy_loss: -0.004216610454022884\n",
      "      total_loss: 240.6929168701172\n",
      "      vf_explained_var: 0.6981255412101746\n",
      "      vf_loss: 240.69711303710938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2511240243911743\n",
      "      kl: 0.004098847974091768\n",
      "      policy_loss: -0.00044615642400458455\n",
      "      total_loss: 726.4894409179688\n",
      "      vf_explained_var: 0.5286461114883423\n",
      "      vf_loss: 726.4898071289062\n",
      "    sample_time_ms: 6428.816\n",
      "    update_time_ms: 7.718\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.392806910719628\n",
      "    rl_1: 90.53778279073066\n",
      "  time_since_restore: 1497.3953227996826\n",
      "  time_this_iter_s: 7.163545370101929\n",
      "  time_total_s: 1497.3953227996826\n",
      "  timestamp: 1552316738\n",
      "  timesteps_since_restore: 444400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 444400\n",
      "  training_iteration: 202\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1497 s, 202 iter, 444400 ts, 122 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-05-44\n",
      "  done: false\n",
      "  episode_len_mean: 94.07\n",
      "  episode_reward_max: 233.84795331689136\n",
      "  episode_reward_mean: 121.65757625549207\n",
      "  episode_reward_min: -167.00011666723907\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4502\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 862.626\n",
      "    load_time_ms: 3.252\n",
      "    num_steps_sampled: 446600\n",
      "    num_steps_trained: 446600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0391018390655518\n",
      "      kl: 0.007277326658368111\n",
      "      policy_loss: -0.0034162383526563644\n",
      "      total_loss: 238.43017578125\n",
      "      vf_explained_var: 0.7138729691505432\n",
      "      vf_loss: 238.43359375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2683351039886475\n",
      "      kl: 0.007044111844152212\n",
      "      policy_loss: -0.002028634073212743\n",
      "      total_loss: 761.4220581054688\n",
      "      vf_explained_var: 0.5398416519165039\n",
      "      vf_loss: 761.4241333007812\n",
      "    sample_time_ms: 6394.546\n",
      "    update_time_ms: 7.956\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.21761893967912\n",
      "    rl_1: 89.43995731581299\n",
      "  time_since_restore: 1503.7479965686798\n",
      "  time_this_iter_s: 6.352673768997192\n",
      "  time_total_s: 1503.7479965686798\n",
      "  timestamp: 1552316744\n",
      "  timesteps_since_restore: 446600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 446600\n",
      "  training_iteration: 203\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1503 s, 203 iter, 446600 ts, 122 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-05-51\n",
      "  done: false\n",
      "  episode_len_mean: 96.28\n",
      "  episode_reward_max: 238.25098715908064\n",
      "  episode_reward_mean: 133.44667654637487\n",
      "  episode_reward_min: -167.00011666723907\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4524\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 859.975\n",
      "    load_time_ms: 3.143\n",
      "    num_steps_sampled: 448800\n",
      "    num_steps_trained: 448800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1943588256835938\n",
      "      kl: 0.00473159970715642\n",
      "      policy_loss: -0.0024854005314409733\n",
      "      total_loss: 205.5626220703125\n",
      "      vf_explained_var: 0.6697589159011841\n",
      "      vf_loss: 205.56509399414062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2639294862747192\n",
      "      kl: 0.005629018880426884\n",
      "      policy_loss: -0.004655459895730019\n",
      "      total_loss: 716.7982788085938\n",
      "      vf_explained_var: 0.5066051483154297\n",
      "      vf_loss: 716.802978515625\n",
      "    sample_time_ms: 6419.322\n",
      "    update_time_ms: 7.998\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.5685811977839\n",
      "    rl_1: 96.87809534859097\n",
      "  time_since_restore: 1509.948667049408\n",
      "  time_this_iter_s: 6.200670480728149\n",
      "  time_total_s: 1509.948667049408\n",
      "  timestamp: 1552316751\n",
      "  timesteps_since_restore: 448800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 448800\n",
      "  training_iteration: 204\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1509 s, 204 iter, 448800 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-05-59\n",
      "  done: false\n",
      "  episode_len_mean: 97.15\n",
      "  episode_reward_max: 238.25098715908064\n",
      "  episode_reward_mean: 139.98206969014385\n",
      "  episode_reward_min: -167.00011666723907\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4547\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 861.663\n",
      "    load_time_ms: 3.178\n",
      "    num_steps_sampled: 451000\n",
      "    num_steps_trained: 451000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2467050552368164\n",
      "      kl: 0.004487333819270134\n",
      "      policy_loss: -0.002787298057228327\n",
      "      total_loss: 351.8761901855469\n",
      "      vf_explained_var: 0.5539755821228027\n",
      "      vf_loss: 351.8789978027344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2172837257385254\n",
      "      kl: 0.008955225348472595\n",
      "      policy_loss: -0.010611101053655148\n",
      "      total_loss: 904.8251953125\n",
      "      vf_explained_var: 0.44576120376586914\n",
      "      vf_loss: 904.8357543945312\n",
      "    sample_time_ms: 6411.63\n",
      "    update_time_ms: 7.94\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.263750981972045\n",
      "    rl_1: 101.7183187081718\n",
      "  time_since_restore: 1518.041249036789\n",
      "  time_this_iter_s: 8.092581987380981\n",
      "  time_total_s: 1518.041249036789\n",
      "  timestamp: 1552316759\n",
      "  timesteps_since_restore: 451000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 451000\n",
      "  training_iteration: 205\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1518 s, 205 iter, 451000 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-06-06\n",
      "  done: false\n",
      "  episode_len_mean: 96.89\n",
      "  episode_reward_max: 238.25098715908064\n",
      "  episode_reward_mean: 141.4141490203452\n",
      "  episode_reward_min: -167.00011666723907\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4570\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 863.405\n",
      "    load_time_ms: 3.189\n",
      "    num_steps_sampled: 453200\n",
      "    num_steps_trained: 453200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9903903007507324\n",
      "      kl: 0.004256893880665302\n",
      "      policy_loss: -0.0018263853853568435\n",
      "      total_loss: 291.724853515625\n",
      "      vf_explained_var: 0.7167665362358093\n",
      "      vf_loss: 291.7266845703125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2724223136901855\n",
      "      kl: 0.0050675999373197556\n",
      "      policy_loss: -0.002388196997344494\n",
      "      total_loss: 727.749267578125\n",
      "      vf_explained_var: 0.6010458469390869\n",
      "      vf_loss: 727.7516479492188\n",
      "    sample_time_ms: 6253.795\n",
      "    update_time_ms: 8.301\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.07344349150132\n",
      "    rl_1: 101.3407055288439\n",
      "  time_since_restore: 1524.6452267169952\n",
      "  time_this_iter_s: 6.603977680206299\n",
      "  time_total_s: 1524.6452267169952\n",
      "  timestamp: 1552316766\n",
      "  timesteps_since_restore: 453200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 453200\n",
      "  training_iteration: 206\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1524 s, 206 iter, 453200 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-06-12\n",
      "  done: false\n",
      "  episode_len_mean: 96.76\n",
      "  episode_reward_max: 238.25098715908064\n",
      "  episode_reward_mean: 139.7017853914798\n",
      "  episode_reward_min: -167.00011666723907\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4593\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 847.978\n",
      "    load_time_ms: 3.001\n",
      "    num_steps_sampled: 455400\n",
      "    num_steps_trained: 455400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0358656644821167\n",
      "      kl: 0.004142439924180508\n",
      "      policy_loss: -0.002234895247966051\n",
      "      total_loss: 339.8135070800781\n",
      "      vf_explained_var: 0.6467142105102539\n",
      "      vf_loss: 339.81573486328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2444486618041992\n",
      "      kl: 0.009944033809006214\n",
      "      policy_loss: -0.008366151712834835\n",
      "      total_loss: 721.037353515625\n",
      "      vf_explained_var: 0.5397375226020813\n",
      "      vf_loss: 721.0455932617188\n",
      "    sample_time_ms: 6079.292\n",
      "    update_time_ms: 8.483\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.16180944345399\n",
      "    rl_1: 100.53997594802584\n",
      "  time_since_restore: 1530.8980417251587\n",
      "  time_this_iter_s: 6.252815008163452\n",
      "  time_total_s: 1530.8980417251587\n",
      "  timestamp: 1552316772\n",
      "  timesteps_since_restore: 455400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 455400\n",
      "  training_iteration: 207\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1530 s, 207 iter, 455400 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-06-18\n",
      "  done: false\n",
      "  episode_len_mean: 97.1\n",
      "  episode_reward_max: 236.7359919117698\n",
      "  episode_reward_mean: 142.4938680478618\n",
      "  episode_reward_min: -152.42249193560096\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 4614\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 836.029\n",
      "    load_time_ms: 2.941\n",
      "    num_steps_sampled: 457600\n",
      "    num_steps_trained: 457600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0319414138793945\n",
      "      kl: 0.005824292078614235\n",
      "      policy_loss: 0.001539484946988523\n",
      "      total_loss: 197.408203125\n",
      "      vf_explained_var: 0.761092483997345\n",
      "      vf_loss: 197.40664672851562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2203433513641357\n",
      "      kl: 0.0036933808587491512\n",
      "      policy_loss: -0.0022939597256481647\n",
      "      total_loss: 696.7837524414062\n",
      "      vf_explained_var: 0.6182634830474854\n",
      "      vf_loss: 696.7860717773438\n",
      "    sample_time_ms: 5898.641\n",
      "    update_time_ms: 8.385\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.3380699236381\n",
      "    rl_1: 102.15579812422371\n",
      "  time_since_restore: 1536.9738552570343\n",
      "  time_this_iter_s: 6.07581353187561\n",
      "  time_total_s: 1536.9738552570343\n",
      "  timestamp: 1552316778\n",
      "  timesteps_since_restore: 457600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 457600\n",
      "  training_iteration: 208\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1536 s, 208 iter, 457600 ts, 142 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-06-26\n",
      "  done: false\n",
      "  episode_len_mean: 96.35\n",
      "  episode_reward_max: 236.7359919117698\n",
      "  episode_reward_mean: 142.95573727595382\n",
      "  episode_reward_min: -161.29704716027328\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 4638\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 836.5\n",
      "    load_time_ms: 2.96\n",
      "    num_steps_sampled: 459800\n",
      "    num_steps_trained: 459800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0161930322647095\n",
      "      kl: 0.007354268338531256\n",
      "      policy_loss: -0.006726584397256374\n",
      "      total_loss: 302.78497314453125\n",
      "      vf_explained_var: 0.7437720894813538\n",
      "      vf_loss: 302.7917175292969\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.17917001247406\n",
      "      kl: 0.0023842183873057365\n",
      "      policy_loss: -0.005166849587112665\n",
      "      total_loss: 816.5903930664062\n",
      "      vf_explained_var: 0.5729591846466064\n",
      "      vf_loss: 816.5955810546875\n",
      "    sample_time_ms: 6039.992\n",
      "    update_time_ms: 8.31\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.05914398530971\n",
      "    rl_1: 100.8965932906441\n",
      "  time_since_restore: 1544.7487411499023\n",
      "  time_this_iter_s: 7.774885892868042\n",
      "  time_total_s: 1544.7487411499023\n",
      "  timestamp: 1552316786\n",
      "  timesteps_since_restore: 459800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 459800\n",
      "  training_iteration: 209\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1544 s, 209 iter, 459800 ts, 143 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-06-32\n",
      "  done: false\n",
      "  episode_len_mean: 95.87\n",
      "  episode_reward_max: 237.30580011975164\n",
      "  episode_reward_mean: 142.58935413275867\n",
      "  episode_reward_min: -161.29704716027328\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4661\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 832.595\n",
      "    load_time_ms: 2.996\n",
      "    num_steps_sampled: 462000\n",
      "    num_steps_trained: 462000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0582051277160645\n",
      "      kl: 0.011929720640182495\n",
      "      policy_loss: -0.00443922309204936\n",
      "      total_loss: 339.8548278808594\n",
      "      vf_explained_var: 0.6351956129074097\n",
      "      vf_loss: 339.8592529296875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2165491580963135\n",
      "      kl: 0.0066217719577252865\n",
      "      policy_loss: -0.004634249955415726\n",
      "      total_loss: 888.7855224609375\n",
      "      vf_explained_var: 0.3733917474746704\n",
      "      vf_loss: 888.7901611328125\n",
      "    sample_time_ms: 6052.697\n",
      "    update_time_ms: 8.359\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.638498692759825\n",
      "    rl_1: 100.95085543999882\n",
      "  time_since_restore: 1551.2454171180725\n",
      "  time_this_iter_s: 6.496675968170166\n",
      "  time_total_s: 1551.2454171180725\n",
      "  timestamp: 1552316792\n",
      "  timesteps_since_restore: 462000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 462000\n",
      "  training_iteration: 210\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1551 s, 210 iter, 462000 ts, 143 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-06-39\n",
      "  done: false\n",
      "  episode_len_mean: 96.89\n",
      "  episode_reward_max: 237.30580011975164\n",
      "  episode_reward_mean: 146.99579186338048\n",
      "  episode_reward_min: -161.29704716027328\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4684\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 817.338\n",
      "    load_time_ms: 2.977\n",
      "    num_steps_sampled: 464200\n",
      "    num_steps_trained: 464200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.251792311668396\n",
      "      kl: 0.006226872559636831\n",
      "      policy_loss: -0.005236160475760698\n",
      "      total_loss: 171.89784240722656\n",
      "      vf_explained_var: 0.7687244415283203\n",
      "      vf_loss: 171.90309143066406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.235300898551941\n",
      "      kl: 0.0034592044539749622\n",
      "      policy_loss: -0.002675173105672002\n",
      "      total_loss: 707.5316162109375\n",
      "      vf_explained_var: 0.5528906583786011\n",
      "      vf_loss: 707.5342407226562\n",
      "    sample_time_ms: 5881.348\n",
      "    update_time_ms: 8.222\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.33955345446836\n",
      "    rl_1: 106.65623840891207\n",
      "  time_since_restore: 1557.4383974075317\n",
      "  time_this_iter_s: 6.1929802894592285\n",
      "  time_total_s: 1557.4383974075317\n",
      "  timestamp: 1552316799\n",
      "  timesteps_since_restore: 464200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 464200\n",
      "  training_iteration: 211\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1557 s, 211 iter, 464200 ts, 147 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-06-45\n",
      "  done: false\n",
      "  episode_len_mean: 96.31\n",
      "  episode_reward_max: 239.7190968478321\n",
      "  episode_reward_mean: 140.78937729119713\n",
      "  episode_reward_min: -161.29704716027328\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4706\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 801.483\n",
      "    load_time_ms: 2.667\n",
      "    num_steps_sampled: 466400\n",
      "    num_steps_trained: 466400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2390861511230469\n",
      "      kl: 0.007479079533368349\n",
      "      policy_loss: -0.001091302721761167\n",
      "      total_loss: 269.6266784667969\n",
      "      vf_explained_var: 0.5359895825386047\n",
      "      vf_loss: 269.627685546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.213286280632019\n",
      "      kl: 0.004070730414241552\n",
      "      policy_loss: -4.708416236098856e-05\n",
      "      total_loss: 761.3217163085938\n",
      "      vf_explained_var: 0.3918512463569641\n",
      "      vf_loss: 761.32177734375\n",
      "    sample_time_ms: 5804.392\n",
      "    update_time_ms: 8.027\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.34670033714567\n",
      "    rl_1: 104.44267695405146\n",
      "  time_since_restore: 1563.6681563854218\n",
      "  time_this_iter_s: 6.229758977890015\n",
      "  time_total_s: 1563.6681563854218\n",
      "  timestamp: 1552316805\n",
      "  timesteps_since_restore: 466400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 466400\n",
      "  training_iteration: 212\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1563 s, 212 iter, 466400 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-06-51\n",
      "  done: false\n",
      "  episode_len_mean: 97.26\n",
      "  episode_reward_max: 239.7190968478321\n",
      "  episode_reward_mean: 148.33534857598548\n",
      "  episode_reward_min: -160.94738477134956\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4728\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 798.176\n",
      "    load_time_ms: 2.597\n",
      "    num_steps_sampled: 468600\n",
      "    num_steps_trained: 468600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1017059087753296\n",
      "      kl: 0.02237720787525177\n",
      "      policy_loss: -0.0052697560749948025\n",
      "      total_loss: 399.1239318847656\n",
      "      vf_explained_var: 0.401168555021286\n",
      "      vf_loss: 399.1291809082031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2356152534484863\n",
      "      kl: 0.01685900241136551\n",
      "      policy_loss: -0.013924239203333855\n",
      "      total_loss: 808.3477172851562\n",
      "      vf_explained_var: 0.21751749515533447\n",
      "      vf_loss: 808.3616943359375\n",
      "    sample_time_ms: 5813.204\n",
      "    update_time_ms: 7.948\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.57935889853249\n",
      "    rl_1: 109.75598967745303\n",
      "  time_since_restore: 1570.075352191925\n",
      "  time_this_iter_s: 6.407195806503296\n",
      "  time_total_s: 1570.075352191925\n",
      "  timestamp: 1552316811\n",
      "  timesteps_since_restore: 468600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 468600\n",
      "  training_iteration: 213\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1570 s, 213 iter, 468600 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-06-58\n",
      "  done: false\n",
      "  episode_len_mean: 95.42\n",
      "  episode_reward_max: 239.7190968478321\n",
      "  episode_reward_mean: 136.137238185782\n",
      "  episode_reward_min: -160.94738477134956\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4753\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 809.026\n",
      "    load_time_ms: 2.651\n",
      "    num_steps_sampled: 470800\n",
      "    num_steps_trained: 470800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9250192046165466\n",
      "      kl: 0.006587542127817869\n",
      "      policy_loss: -0.006606681272387505\n",
      "      total_loss: 544.6139526367188\n",
      "      vf_explained_var: 0.6527929902076721\n",
      "      vf_loss: 544.62060546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.23532235622406\n",
      "      kl: 0.007945467717945576\n",
      "      policy_loss: -0.004924088716506958\n",
      "      total_loss: 1121.9794921875\n",
      "      vf_explained_var: 0.5081731081008911\n",
      "      vf_loss: 1121.9842529296875\n",
      "    sample_time_ms: 5835.834\n",
      "    update_time_ms: 8.068\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.577025137018694\n",
      "    rl_1: 100.56021304876334\n",
      "  time_since_restore: 1576.6153681278229\n",
      "  time_this_iter_s: 6.540015935897827\n",
      "  time_total_s: 1576.6153681278229\n",
      "  timestamp: 1552316818\n",
      "  timesteps_since_restore: 470800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 470800\n",
      "  training_iteration: 214\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1576 s, 214 iter, 470800 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-07-07\n",
      "  done: false\n",
      "  episode_len_mean: 94.53\n",
      "  episode_reward_max: 240.3556101764815\n",
      "  episode_reward_mean: 131.69272279374377\n",
      "  episode_reward_min: -160.7887186962298\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 4777\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 814.197\n",
      "    load_time_ms: 2.552\n",
      "    num_steps_sampled: 473000\n",
      "    num_steps_trained: 473000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1140434741973877\n",
      "      kl: 0.009725179523229599\n",
      "      policy_loss: -0.012288333848118782\n",
      "      total_loss: 288.6474609375\n",
      "      vf_explained_var: 0.7496697902679443\n",
      "      vf_loss: 288.6597900390625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1639912128448486\n",
      "      kl: 0.0035222431179136038\n",
      "      policy_loss: -0.004719868302345276\n",
      "      total_loss: 881.8143310546875\n",
      "      vf_explained_var: 0.45973390340805054\n",
      "      vf_loss: 881.8190307617188\n",
      "    sample_time_ms: 5908.352\n",
      "    update_time_ms: 8.884\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.019139294732206\n",
      "    rl_1: 98.67358349901161\n",
      "  time_since_restore: 1585.4920465946198\n",
      "  time_this_iter_s: 8.876678466796875\n",
      "  time_total_s: 1585.4920465946198\n",
      "  timestamp: 1552316827\n",
      "  timesteps_since_restore: 473000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 473000\n",
      "  training_iteration: 215\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1585 s, 215 iter, 473000 ts, 132 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-07-14\n",
      "  done: false\n",
      "  episode_len_mean: 94.76\n",
      "  episode_reward_max: 240.3556101764815\n",
      "  episode_reward_mean: 135.7310504499101\n",
      "  episode_reward_min: -163.7850035238993\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4799\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 833.079\n",
      "    load_time_ms: 2.486\n",
      "    num_steps_sampled: 475200\n",
      "    num_steps_trained: 475200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1651835441589355\n",
      "      kl: 0.005669160280376673\n",
      "      policy_loss: -0.0019737347029149532\n",
      "      total_loss: 114.91775512695312\n",
      "      vf_explained_var: 0.7743531465530396\n",
      "      vf_loss: 114.91973114013672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.207235336303711\n",
      "      kl: 0.006213081069290638\n",
      "      policy_loss: -0.0036305540706962347\n",
      "      total_loss: 647.384765625\n",
      "      vf_explained_var: 0.2830575108528137\n",
      "      vf_loss: 647.388427734375\n",
      "    sample_time_ms: 5964.479\n",
      "    update_time_ms: 8.728\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.69755739577023\n",
      "    rl_1: 100.03349305413984\n",
      "  time_since_restore: 1592.8512587547302\n",
      "  time_this_iter_s: 7.359212160110474\n",
      "  time_total_s: 1592.8512587547302\n",
      "  timestamp: 1552316834\n",
      "  timesteps_since_restore: 475200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 475200\n",
      "  training_iteration: 216\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1592 s, 216 iter, 475200 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-07-21\n",
      "  done: false\n",
      "  episode_len_mean: 94.87\n",
      "  episode_reward_max: 240.3556101764815\n",
      "  episode_reward_mean: 134.2502814447127\n",
      "  episode_reward_min: -163.7850035238993\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4822\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 856.056\n",
      "    load_time_ms: 3.039\n",
      "    num_steps_sampled: 477400\n",
      "    num_steps_trained: 477400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2394335269927979\n",
      "      kl: 0.004837259650230408\n",
      "      policy_loss: -0.0019482234492897987\n",
      "      total_loss: 213.56675720214844\n",
      "      vf_explained_var: 0.7481306791305542\n",
      "      vf_loss: 213.56871032714844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1867132186889648\n",
      "      kl: 0.006244391202926636\n",
      "      policy_loss: -0.008205720223486423\n",
      "      total_loss: 816.32177734375\n",
      "      vf_explained_var: 0.4998112916946411\n",
      "      vf_loss: 816.330078125\n",
      "    sample_time_ms: 6027.497\n",
      "    update_time_ms: 8.532\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.43446139879045\n",
      "    rl_1: 99.81582004592227\n",
      "  time_since_restore: 1599.9754462242126\n",
      "  time_this_iter_s: 7.124187469482422\n",
      "  time_total_s: 1599.9754462242126\n",
      "  timestamp: 1552316841\n",
      "  timesteps_since_restore: 477400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 477400\n",
      "  training_iteration: 217\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1599 s, 217 iter, 477400 ts, 134 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-07-29\n",
      "  done: false\n",
      "  episode_len_mean: 97.0\n",
      "  episode_reward_max: 240.3556101764815\n",
      "  episode_reward_mean: 145.6288099520949\n",
      "  episode_reward_min: -163.7850035238993\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4844\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 880.874\n",
      "    load_time_ms: 3.01\n",
      "    num_steps_sampled: 479600\n",
      "    num_steps_trained: 479600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2351680994033813\n",
      "      kl: 0.010542192496359348\n",
      "      policy_loss: -0.01382727362215519\n",
      "      total_loss: 171.7324676513672\n",
      "      vf_explained_var: 0.8023501038551331\n",
      "      vf_loss: 171.74627685546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1872308254241943\n",
      "      kl: 0.004567879252135754\n",
      "      policy_loss: -0.002049517584964633\n",
      "      total_loss: 682.7633666992188\n",
      "      vf_explained_var: 0.6959571838378906\n",
      "      vf_loss: 682.765380859375\n",
      "    sample_time_ms: 6098.762\n",
      "    update_time_ms: 8.645\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.02144225641506\n",
      "    rl_1: 108.60736769567988\n",
      "  time_since_restore: 1607.0131039619446\n",
      "  time_this_iter_s: 7.037657737731934\n",
      "  time_total_s: 1607.0131039619446\n",
      "  timestamp: 1552316849\n",
      "  timesteps_since_restore: 479600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 479600\n",
      "  training_iteration: 218\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1607 s, 218 iter, 479600 ts, 146 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-07-39\n",
      "  done: false\n",
      "  episode_len_mean: 94.94\n",
      "  episode_reward_max: 237.73596779797404\n",
      "  episode_reward_mean: 131.31027005901475\n",
      "  episode_reward_min: -163.7850035238993\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 4868\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 945.859\n",
      "    load_time_ms: 3.056\n",
      "    num_steps_sampled: 481800\n",
      "    num_steps_trained: 481800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1432299613952637\n",
      "      kl: 0.003947792109102011\n",
      "      policy_loss: -0.0021948511712253094\n",
      "      total_loss: 302.9111633300781\n",
      "      vf_explained_var: 0.7290215492248535\n",
      "      vf_loss: 302.9133605957031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.171594262123108\n",
      "      kl: 0.003937229979783297\n",
      "      policy_loss: -0.008534248918294907\n",
      "      total_loss: 1033.951416015625\n",
      "      vf_explained_var: 0.47506117820739746\n",
      "      vf_loss: 1033.9599609375\n",
      "    sample_time_ms: 6291.858\n",
      "    update_time_ms: 8.859\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.8328222703898\n",
      "    rl_1: 100.47744778862497\n",
      "  time_since_restore: 1617.3793318271637\n",
      "  time_this_iter_s: 10.366227865219116\n",
      "  time_total_s: 1617.3793318271637\n",
      "  timestamp: 1552316859\n",
      "  timesteps_since_restore: 481800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 481800\n",
      "  training_iteration: 219\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1617 s, 219 iter, 481800 ts, 131 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-07-48\n",
      "  done: false\n",
      "  episode_len_mean: 96.56\n",
      "  episode_reward_max: 238.8936840764414\n",
      "  episode_reward_mean: 140.20934354237082\n",
      "  episode_reward_min: -155.07037064152078\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4891\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 989.933\n",
      "    load_time_ms: 3.296\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0203344821929932\n",
      "      kl: 0.0032200529240071774\n",
      "      policy_loss: -0.00994054228067398\n",
      "      total_loss: 254.1663055419922\n",
      "      vf_explained_var: 0.7242914438247681\n",
      "      vf_loss: 254.17620849609375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1725225448608398\n",
      "      kl: 0.0076565793715417385\n",
      "      policy_loss: -0.0014350125566124916\n",
      "      total_loss: 732.1473388671875\n",
      "      vf_explained_var: 0.599195122718811\n",
      "      vf_loss: 732.14892578125\n",
      "    sample_time_ms: 6530.518\n",
      "    update_time_ms: 9.553\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.51776320949986\n",
      "    rl_1: 106.69158033287094\n",
      "  time_since_restore: 1626.713653087616\n",
      "  time_this_iter_s: 9.33432126045227\n",
      "  time_total_s: 1626.713653087616\n",
      "  timestamp: 1552316868\n",
      "  timesteps_since_restore: 484000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 220\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1626 s, 220 iter, 484000 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-07-58\n",
      "  done: false\n",
      "  episode_len_mean: 95.78\n",
      "  episode_reward_max: 238.8936840764414\n",
      "  episode_reward_mean: 135.2049704691918\n",
      "  episode_reward_min: -155.07037064152078\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4914\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1019.033\n",
      "    load_time_ms: 3.318\n",
      "    num_steps_sampled: 486200\n",
      "    num_steps_trained: 486200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0815320014953613\n",
      "      kl: 0.0064347414299845695\n",
      "      policy_loss: -0.003369186772033572\n",
      "      total_loss: 207.2830047607422\n",
      "      vf_explained_var: 0.7946779131889343\n",
      "      vf_loss: 207.286376953125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1784265041351318\n",
      "      kl: 0.004997309297323227\n",
      "      policy_loss: -0.006346497219055891\n",
      "      total_loss: 745.9349975585938\n",
      "      vf_explained_var: 0.6547872424125671\n",
      "      vf_loss: 745.9412841796875\n",
      "    sample_time_ms: 6801.632\n",
      "    update_time_ms: 9.808\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.40993741208125\n",
      "    rl_1: 103.79503305711056\n",
      "  time_since_restore: 1635.91588306427\n",
      "  time_this_iter_s: 9.202229976654053\n",
      "  time_total_s: 1635.91588306427\n",
      "  timestamp: 1552316878\n",
      "  timesteps_since_restore: 486200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 486200\n",
      "  training_iteration: 221\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1635 s, 221 iter, 486200 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-08-10\n",
      "  done: false\n",
      "  episode_len_mean: 96.08\n",
      "  episode_reward_max: 238.8936840764414\n",
      "  episode_reward_mean: 144.57180162800364\n",
      "  episode_reward_min: -155.07037064152078\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4936\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1055.557\n",
      "    load_time_ms: 3.211\n",
      "    num_steps_sampled: 488400\n",
      "    num_steps_trained: 488400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9980814456939697\n",
      "      kl: 0.015648189932107925\n",
      "      policy_loss: -0.006522478070110083\n",
      "      total_loss: 301.5893249511719\n",
      "      vf_explained_var: 0.5914797782897949\n",
      "      vf_loss: 301.5958557128906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1438260078430176\n",
      "      kl: 0.001725958427414298\n",
      "      policy_loss: -1.7196816770592704e-05\n",
      "      total_loss: 896.6725463867188\n",
      "      vf_explained_var: 0.34171873331069946\n",
      "      vf_loss: 896.6725463867188\n",
      "    sample_time_ms: 7334.018\n",
      "    update_time_ms: 10.448\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.94280529213857\n",
      "    rl_1: 107.62899633586511\n",
      "  time_since_restore: 1647.8426678180695\n",
      "  time_this_iter_s: 11.926784753799438\n",
      "  time_total_s: 1647.8426678180695\n",
      "  timestamp: 1552316890\n",
      "  timesteps_since_restore: 488400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 488400\n",
      "  training_iteration: 222\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1647 s, 222 iter, 488400 ts, 145 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-08-22\n",
      "  done: false\n",
      "  episode_len_mean: 96.85\n",
      "  episode_reward_max: 238.8936840764414\n",
      "  episode_reward_mean: 153.2342863079712\n",
      "  episode_reward_min: -155.07037064152078\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4958\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1226.2\n",
      "    load_time_ms: 3.729\n",
      "    num_steps_sampled: 490600\n",
      "    num_steps_trained: 490600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.180091381072998\n",
      "      kl: 0.03346243500709534\n",
      "      policy_loss: -0.010482708923518658\n",
      "      total_loss: 214.28494262695312\n",
      "      vf_explained_var: 0.6022722721099854\n",
      "      vf_loss: 214.2954559326172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1424586772918701\n",
      "      kl: 0.003949307836592197\n",
      "      policy_loss: -0.0039704106748104095\n",
      "      total_loss: 705.9877319335938\n",
      "      vf_explained_var: 0.27483099699020386\n",
      "      vf_loss: 705.99169921875\n",
      "    sample_time_ms: 7752.382\n",
      "    update_time_ms: 10.301\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.837207427548464\n",
      "    rl_1: 112.39707888042274\n",
      "  time_since_restore: 1660.1549007892609\n",
      "  time_this_iter_s: 12.312232971191406\n",
      "  time_total_s: 1660.1549007892609\n",
      "  timestamp: 1552316902\n",
      "  timesteps_since_restore: 490600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 490600\n",
      "  training_iteration: 223\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1660 s, 223 iter, 490600 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-08-45\n",
      "  done: false\n",
      "  episode_len_mean: 98.61\n",
      "  episode_reward_max: 238.77604471039933\n",
      "  episode_reward_mean: 162.70731306280734\n",
      "  episode_reward_min: -149.6073012379351\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4980\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1344.48\n",
      "    load_time_ms: 4.124\n",
      "    num_steps_sampled: 492800\n",
      "    num_steps_trained: 492800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1308505535125732\n",
      "      kl: 0.0038470139261335135\n",
      "      policy_loss: -0.005687481723725796\n",
      "      total_loss: 156.64561462402344\n",
      "      vf_explained_var: 0.7678790092468262\n",
      "      vf_loss: 156.6512908935547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.160517692565918\n",
      "      kl: 0.00583135848864913\n",
      "      policy_loss: -0.0019419088494032621\n",
      "      total_loss: 686.364990234375\n",
      "      vf_explained_var: 0.5219805240631104\n",
      "      vf_loss: 686.3670043945312\n",
      "    sample_time_ms: 9219.619\n",
      "    update_time_ms: 12.497\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.54890037052727\n",
      "    rl_1: 118.1584126922801\n",
      "  time_since_restore: 1682.5893940925598\n",
      "  time_this_iter_s: 22.43449330329895\n",
      "  time_total_s: 1682.5893940925598\n",
      "  timestamp: 1552316925\n",
      "  timesteps_since_restore: 492800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 492800\n",
      "  training_iteration: 224\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1682 s, 224 iter, 492800 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-09-00\n",
      "  done: false\n",
      "  episode_len_mean: 99.29\n",
      "  episode_reward_max: 240.096585187164\n",
      "  episode_reward_mean: 170.78956437857664\n",
      "  episode_reward_min: -149.6073012379351\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 5002\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1405.085\n",
      "    load_time_ms: 4.246\n",
      "    num_steps_sampled: 495000\n",
      "    num_steps_trained: 495000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.989587664604187\n",
      "      kl: 0.006926714442670345\n",
      "      policy_loss: -0.005555721931159496\n",
      "      total_loss: 162.0602264404297\n",
      "      vf_explained_var: 0.8062310218811035\n",
      "      vf_loss: 162.06578063964844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.156847596168518\n",
      "      kl: 0.0035875330213457346\n",
      "      policy_loss: -0.005324160680174828\n",
      "      total_loss: 712.8331298828125\n",
      "      vf_explained_var: 0.6387883424758911\n",
      "      vf_loss: 712.83837890625\n",
      "    sample_time_ms: 9843.546\n",
      "    update_time_ms: 13.45\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.196033449000446\n",
      "    rl_1: 122.59353092957623\n",
      "  time_since_restore: 1698.330195903778\n",
      "  time_this_iter_s: 15.740801811218262\n",
      "  time_total_s: 1698.330195903778\n",
      "  timestamp: 1552316940\n",
      "  timesteps_since_restore: 495000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 495000\n",
      "  training_iteration: 225\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1698 s, 225 iter, 495000 ts, 171 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-09-09\n",
      "  done: false\n",
      "  episode_len_mean: 99.15\n",
      "  episode_reward_max: 240.096585187164\n",
      "  episode_reward_mean: 166.08316950198036\n",
      "  episode_reward_min: -149.6073012379351\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 5024\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1378.388\n",
      "    load_time_ms: 4.292\n",
      "    num_steps_sampled: 497200\n",
      "    num_steps_trained: 497200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9677964448928833\n",
      "      kl: 0.001928251702338457\n",
      "      policy_loss: -0.00372029235586524\n",
      "      total_loss: 204.34898376464844\n",
      "      vf_explained_var: 0.8261566758155823\n",
      "      vf_loss: 204.35267639160156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1656244993209839\n",
      "      kl: 0.005264144856482744\n",
      "      policy_loss: -0.0018790399190038443\n",
      "      total_loss: 742.880615234375\n",
      "      vf_explained_var: 0.7233465313911438\n",
      "      vf_loss: 742.8824462890625\n",
      "    sample_time_ms: 9956.768\n",
      "    update_time_ms: 14.455\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 46.2634913280551\n",
      "    rl_1: 119.81967817392523\n",
      "  time_since_restore: 1706.5562629699707\n",
      "  time_this_iter_s: 8.226067066192627\n",
      "  time_total_s: 1706.5562629699707\n",
      "  timestamp: 1552316949\n",
      "  timesteps_since_restore: 497200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 497200\n",
      "  training_iteration: 226\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1706 s, 226 iter, 497200 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-09-18\n",
      "  done: false\n",
      "  episode_len_mean: 97.77\n",
      "  episode_reward_max: 240.096585187164\n",
      "  episode_reward_mean: 153.57155527115606\n",
      "  episode_reward_min: -158.6449424838581\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 5048\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1447.345\n",
      "    load_time_ms: 3.945\n",
      "    num_steps_sampled: 499400\n",
      "    num_steps_trained: 499400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0228034257888794\n",
      "      kl: 0.015439571812748909\n",
      "      policy_loss: -0.0045340899378061295\n",
      "      total_loss: 530.8917236328125\n",
      "      vf_explained_var: 0.514306366443634\n",
      "      vf_loss: 530.8963623046875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2062586545944214\n",
      "      kl: 0.004480060189962387\n",
      "      policy_loss: -0.009630325250327587\n",
      "      total_loss: 1035.2855224609375\n",
      "      vf_explained_var: 0.4993545413017273\n",
      "      vf_loss: 1035.2952880859375\n",
      "    sample_time_ms: 10085.272\n",
      "    update_time_ms: 14.533\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.40431778883166\n",
      "    rl_1: 112.16723748232444\n",
      "  time_since_restore: 1715.6612820625305\n",
      "  time_this_iter_s: 9.105019092559814\n",
      "  time_total_s: 1715.6612820625305\n",
      "  timestamp: 1552316958\n",
      "  timesteps_since_restore: 499400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 499400\n",
      "  training_iteration: 227\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1715 s, 227 iter, 499400 ts, 154 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-09-27\n",
      "  done: false\n",
      "  episode_len_mean: 96.78\n",
      "  episode_reward_max: 240.096585187164\n",
      "  episode_reward_mean: 147.26121674526073\n",
      "  episode_reward_min: -158.6449424838581\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5071\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1426.915\n",
      "    load_time_ms: 3.938\n",
      "    num_steps_sampled: 501600\n",
      "    num_steps_trained: 501600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.076931118965149\n",
      "      kl: 0.006258606445044279\n",
      "      policy_loss: -0.0045635695569217205\n",
      "      total_loss: 152.1838836669922\n",
      "      vf_explained_var: 0.8066260814666748\n",
      "      vf_loss: 152.1884307861328\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1549824476242065\n",
      "      kl: 0.006188909523189068\n",
      "      policy_loss: -0.007224719971418381\n",
      "      total_loss: 797.2262573242188\n",
      "      vf_explained_var: 0.5289179086685181\n",
      "      vf_loss: 797.2335205078125\n",
      "    sample_time_ms: 10331.9\n",
      "    update_time_ms: 15.638\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.807863887022656\n",
      "    rl_1: 108.45335285823805\n",
      "  time_since_restore: 1724.9709815979004\n",
      "  time_this_iter_s: 9.309699535369873\n",
      "  time_total_s: 1724.9709815979004\n",
      "  timestamp: 1552316967\n",
      "  timesteps_since_restore: 501600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 501600\n",
      "  training_iteration: 228\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1724 s, 228 iter, 501600 ts, 147 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-09-35\n",
      "  done: false\n",
      "  episode_len_mean: 97.14\n",
      "  episode_reward_max: 240.096585187164\n",
      "  episode_reward_mean: 151.62130930070498\n",
      "  episode_reward_min: -158.6449424838581\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 5092\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1358.023\n",
      "    load_time_ms: 3.886\n",
      "    num_steps_sampled: 503800\n",
      "    num_steps_trained: 503800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9462989568710327\n",
      "      kl: 0.010236729867756367\n",
      "      policy_loss: -0.0008795785252004862\n",
      "      total_loss: 70.91526794433594\n",
      "      vf_explained_var: 0.8747178912162781\n",
      "      vf_loss: 70.91614532470703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.145589828491211\n",
      "      kl: 0.008959557861089706\n",
      "      policy_loss: -0.0056304968893527985\n",
      "      total_loss: 628.3369750976562\n",
      "      vf_explained_var: 0.4079258143901825\n",
      "      vf_loss: 628.342529296875\n",
      "    sample_time_ms: 10117.745\n",
      "    update_time_ms: 15.653\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.109943637460944\n",
      "    rl_1: 111.51136566324405\n",
      "  time_since_restore: 1732.4995958805084\n",
      "  time_this_iter_s: 7.528614282608032\n",
      "  time_total_s: 1732.4995958805084\n",
      "  timestamp: 1552316975\n",
      "  timesteps_since_restore: 503800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 503800\n",
      "  training_iteration: 229\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1732 s, 229 iter, 503800 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-09-41\n",
      "  done: false\n",
      "  episode_len_mean: 96.36\n",
      "  episode_reward_max: 239.9165804027806\n",
      "  episode_reward_mean: 151.96962701103521\n",
      "  episode_reward_min: -158.6449424838581\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5115\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1320.865\n",
      "    load_time_ms: 3.603\n",
      "    num_steps_sampled: 506000\n",
      "    num_steps_trained: 506000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0126181840896606\n",
      "      kl: 0.004654018208384514\n",
      "      policy_loss: -0.004724584519863129\n",
      "      total_loss: 239.96560668945312\n",
      "      vf_explained_var: 0.7816862463951111\n",
      "      vf_loss: 239.9703369140625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.138802170753479\n",
      "      kl: 0.007977135479450226\n",
      "      policy_loss: -0.0032419876661151648\n",
      "      total_loss: 1008.4110717773438\n",
      "      vf_explained_var: 0.5129642486572266\n",
      "      vf_loss: 1008.414306640625\n",
      "    sample_time_ms: 9849.771\n",
      "    update_time_ms: 14.882\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.41957023911428\n",
      "    rl_1: 111.55005677192095\n",
      "  time_since_restore: 1738.7725248336792\n",
      "  time_this_iter_s: 6.272928953170776\n",
      "  time_total_s: 1738.7725248336792\n",
      "  timestamp: 1552316981\n",
      "  timesteps_since_restore: 506000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 506000\n",
      "  training_iteration: 230\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1738 s, 230 iter, 506000 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-09-51\n",
      "  done: false\n",
      "  episode_len_mean: 97.01\n",
      "  episode_reward_max: 240.36487212440696\n",
      "  episode_reward_mean: 155.87652985054387\n",
      "  episode_reward_min: -157.24009904239855\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 5139\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1334.212\n",
      "    load_time_ms: 3.698\n",
      "    num_steps_sampled: 508200\n",
      "    num_steps_trained: 508200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0816041231155396\n",
      "      kl: 0.012174470350146294\n",
      "      policy_loss: -0.011097059585154057\n",
      "      total_loss: 379.05511474609375\n",
      "      vf_explained_var: 0.5966197848320007\n",
      "      vf_loss: 379.0661926269531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.113337755203247\n",
      "      kl: 0.01063605584204197\n",
      "      policy_loss: -0.012262370437383652\n",
      "      total_loss: 996.2056274414062\n",
      "      vf_explained_var: 0.4029551148414612\n",
      "      vf_loss: 996.2179565429688\n",
      "    sample_time_ms: 9856.865\n",
      "    update_time_ms: 14.842\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.06737343051317\n",
      "    rl_1: 114.80915642003075\n",
      "  time_since_restore: 1748.1806058883667\n",
      "  time_this_iter_s: 9.4080810546875\n",
      "  time_total_s: 1748.1806058883667\n",
      "  timestamp: 1552316991\n",
      "  timesteps_since_restore: 508200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 508200\n",
      "  training_iteration: 231\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1748 s, 231 iter, 508200 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-09-58\n",
      "  done: false\n",
      "  episode_len_mean: 94.42\n",
      "  episode_reward_max: 240.36487212440696\n",
      "  episode_reward_mean: 139.48707964202242\n",
      "  episode_reward_min: -160.60369761109695\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5164\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1302.77\n",
      "    load_time_ms: 3.733\n",
      "    num_steps_sampled: 510400\n",
      "    num_steps_trained: 510400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9913923740386963\n",
      "      kl: 0.010031310841441154\n",
      "      policy_loss: -0.007102174684405327\n",
      "      total_loss: 314.5411071777344\n",
      "      vf_explained_var: 0.8122181296348572\n",
      "      vf_loss: 314.5482482910156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1064757108688354\n",
      "      kl: 0.0038316596765071154\n",
      "      policy_loss: 0.0008267911616712809\n",
      "      total_loss: 1041.4783935546875\n",
      "      vf_explained_var: 0.6470367312431335\n",
      "      vf_loss: 1041.4775390625\n",
      "    sample_time_ms: 9424.895\n",
      "    update_time_ms: 14.711\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.43943111685403\n",
      "    rl_1: 105.04764852516844\n",
      "  time_since_restore: 1755.4708051681519\n",
      "  time_this_iter_s: 7.290199279785156\n",
      "  time_total_s: 1755.4708051681519\n",
      "  timestamp: 1552316998\n",
      "  timesteps_since_restore: 510400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 510400\n",
      "  training_iteration: 232\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1755 s, 232 iter, 510400 ts, 139 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-10-05\n",
      "  done: false\n",
      "  episode_len_mean: 94.28\n",
      "  episode_reward_max: 241.96730236038806\n",
      "  episode_reward_mean: 137.38704516844416\n",
      "  episode_reward_min: -160.60369761109695\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 5186\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1132.646\n",
      "    load_time_ms: 3.3\n",
      "    num_steps_sampled: 512600\n",
      "    num_steps_trained: 512600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.910502016544342\n",
      "      kl: 0.007732510566711426\n",
      "      policy_loss: -0.008435533381998539\n",
      "      total_loss: 165.39224243164062\n",
      "      vf_explained_var: 0.8075418472290039\n",
      "      vf_loss: 165.4007110595703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1490767002105713\n",
      "      kl: 0.0037047360092401505\n",
      "      policy_loss: -0.00032651599030941725\n",
      "      total_loss: 559.8795776367188\n",
      "      vf_explained_var: 0.6645352840423584\n",
      "      vf_loss: 559.8799438476562\n",
      "    sample_time_ms: 9047.643\n",
      "    update_time_ms: 14.895\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.59066222270156\n",
      "    rl_1: 102.79638294574255\n",
      "  time_since_restore: 1762.2960269451141\n",
      "  time_this_iter_s: 6.82522177696228\n",
      "  time_total_s: 1762.2960269451141\n",
      "  timestamp: 1552317005\n",
      "  timesteps_since_restore: 512600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 512600\n",
      "  training_iteration: 233\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1762 s, 233 iter, 512600 ts, 137 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-10-11\n",
      "  done: false\n",
      "  episode_len_mean: 95.9\n",
      "  episode_reward_max: 241.96730236038806\n",
      "  episode_reward_mean: 144.80390123657725\n",
      "  episode_reward_min: -160.60369761109695\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 5208\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1008.345\n",
      "    load_time_ms: 3.028\n",
      "    num_steps_sampled: 514800\n",
      "    num_steps_trained: 514800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1387234926223755\n",
      "      kl: 0.008040222339332104\n",
      "      policy_loss: -0.007873017340898514\n",
      "      total_loss: 65.32678985595703\n",
      "      vf_explained_var: 0.883624255657196\n",
      "      vf_loss: 65.33465576171875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.106752634048462\n",
      "      kl: 0.0033923545852303505\n",
      "      policy_loss: -0.002667916938662529\n",
      "      total_loss: 578.8264770507812\n",
      "      vf_explained_var: 0.38992515206336975\n",
      "      vf_loss: 578.8291625976562\n",
      "    sample_time_ms: 7574.481\n",
      "    update_time_ms: 12.768\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.42060845924808\n",
      "    rl_1: 108.38329277732917\n",
      "  time_since_restore: 1768.7177302837372\n",
      "  time_this_iter_s: 6.421703338623047\n",
      "  time_total_s: 1768.7177302837372\n",
      "  timestamp: 1552317011\n",
      "  timesteps_since_restore: 514800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 514800\n",
      "  training_iteration: 234\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1768 s, 234 iter, 514800 ts, 145 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-10-18\n",
      "  done: false\n",
      "  episode_len_mean: 95.56\n",
      "  episode_reward_max: 241.96730236038806\n",
      "  episode_reward_mean: 145.46201411244084\n",
      "  episode_reward_min: -160.60369761109695\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 5230\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 937.19\n",
      "    load_time_ms: 2.996\n",
      "    num_steps_sampled: 517000\n",
      "    num_steps_trained: 517000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8708034157752991\n",
      "      kl: 0.0028611302841454744\n",
      "      policy_loss: -0.0014976206002756953\n",
      "      total_loss: 247.9285125732422\n",
      "      vf_explained_var: 0.7495333552360535\n",
      "      vf_loss: 247.9299774169922\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1134865283966064\n",
      "      kl: 0.009799584746360779\n",
      "      policy_loss: -0.006416951771825552\n",
      "      total_loss: 748.95654296875\n",
      "      vf_explained_var: 0.598949670791626\n",
      "      vf_loss: 748.9630126953125\n",
      "    sample_time_ms: 6719.144\n",
      "    update_time_ms: 10.922\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.39109225287965\n",
      "    rl_1: 107.07092185956117\n",
      "  time_since_restore: 1775.1673760414124\n",
      "  time_this_iter_s: 6.449645757675171\n",
      "  time_total_s: 1775.1673760414124\n",
      "  timestamp: 1552317018\n",
      "  timesteps_since_restore: 517000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 517000\n",
      "  training_iteration: 235\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1775 s, 235 iter, 517000 ts, 145 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-10-25\n",
      "  done: false\n",
      "  episode_len_mean: 97.58\n",
      "  episode_reward_max: 241.96730236038806\n",
      "  episode_reward_mean: 159.5931880116345\n",
      "  episode_reward_min: -156.15152704799516\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5253\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 932.262\n",
      "    load_time_ms: 3.044\n",
      "    num_steps_sampled: 519200\n",
      "    num_steps_trained: 519200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0454497337341309\n",
      "      kl: 0.008758141659200191\n",
      "      policy_loss: -0.005482828710228205\n",
      "      total_loss: 163.0181427001953\n",
      "      vf_explained_var: 0.7970035076141357\n",
      "      vf_loss: 163.0236358642578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0685158967971802\n",
      "      kl: 0.005061975680291653\n",
      "      policy_loss: -0.004813733510673046\n",
      "      total_loss: 813.86376953125\n",
      "      vf_explained_var: 0.43813252449035645\n",
      "      vf_loss: 813.86865234375\n",
      "    sample_time_ms: 6595.485\n",
      "    update_time_ms: 9.943\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.089474607349814\n",
      "    rl_1: 115.5037134042847\n",
      "  time_since_restore: 1782.0999717712402\n",
      "  time_this_iter_s: 6.932595729827881\n",
      "  time_total_s: 1782.0999717712402\n",
      "  timestamp: 1552317025\n",
      "  timesteps_since_restore: 519200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 519200\n",
      "  training_iteration: 236\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1782 s, 236 iter, 519200 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-10-31\n",
      "  done: false\n",
      "  episode_len_mean: 96.99\n",
      "  episode_reward_max: 241.96730236038806\n",
      "  episode_reward_mean: 159.4732978985605\n",
      "  episode_reward_min: -156.15152704799516\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5276\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 858.293\n",
      "    load_time_ms: 2.951\n",
      "    num_steps_sampled: 521400\n",
      "    num_steps_trained: 521400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8388574719429016\n",
      "      kl: 0.01048724539577961\n",
      "      policy_loss: -0.010134153999388218\n",
      "      total_loss: 253.7288818359375\n",
      "      vf_explained_var: 0.8165585994720459\n",
      "      vf_loss: 253.73902893066406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0665898323059082\n",
      "      kl: 0.001337447902187705\n",
      "      policy_loss: -0.0024325584527105093\n",
      "      total_loss: 880.9434204101562\n",
      "      vf_explained_var: 0.6463257074356079\n",
      "      vf_loss: 880.9458618164062\n",
      "    sample_time_ms: 6403.272\n",
      "    update_time_ms: 9.911\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.86140218370476\n",
      "    rl_1: 115.61189571485578\n",
      "  time_since_restore: 1788.5248484611511\n",
      "  time_this_iter_s: 6.424876689910889\n",
      "  time_total_s: 1788.5248484611511\n",
      "  timestamp: 1552317031\n",
      "  timesteps_since_restore: 521400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 521400\n",
      "  training_iteration: 237\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1788 s, 237 iter, 521400 ts, 159 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-10-40\n",
      "  done: false\n",
      "  episode_len_mean: 96.98\n",
      "  episode_reward_max: 240.39691149650116\n",
      "  episode_reward_mean: 156.60120099161787\n",
      "  episode_reward_min: -156.15152704799516\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5299\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 894.713\n",
      "    load_time_ms: 3.047\n",
      "    num_steps_sampled: 523600\n",
      "    num_steps_trained: 523600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9873881340026855\n",
      "      kl: 0.004748548846691847\n",
      "      policy_loss: -0.004882293287664652\n",
      "      total_loss: 142.2525177001953\n",
      "      vf_explained_var: 0.8335438370704651\n",
      "      vf_loss: 142.2574005126953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.125543236732483\n",
      "      kl: 0.008675286546349525\n",
      "      policy_loss: -0.004258099477738142\n",
      "      total_loss: 614.8928833007812\n",
      "      vf_explained_var: 0.6727501749992371\n",
      "      vf_loss: 614.8971557617188\n",
      "    sample_time_ms: 6296.23\n",
      "    update_time_ms: 8.837\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.07720690133822\n",
      "    rl_1: 113.52399409027964\n",
      "  time_since_restore: 1797.121542930603\n",
      "  time_this_iter_s: 8.596694469451904\n",
      "  time_total_s: 1797.121542930603\n",
      "  timestamp: 1552317040\n",
      "  timesteps_since_restore: 523600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 523600\n",
      "  training_iteration: 238\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1797 s, 238 iter, 523600 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-10-48\n",
      "  done: false\n",
      "  episode_len_mean: 96.49\n",
      "  episode_reward_max: 240.03604148406347\n",
      "  episode_reward_mean: 154.72661025571017\n",
      "  episode_reward_min: -156.15152704799516\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5322\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 896.423\n",
      "    load_time_ms: 3.046\n",
      "    num_steps_sampled: 525800\n",
      "    num_steps_trained: 525800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9662338495254517\n",
      "      kl: 0.009588204324245453\n",
      "      policy_loss: -0.0026335681322962046\n",
      "      total_loss: 166.03292846679688\n",
      "      vf_explained_var: 0.8345656394958496\n",
      "      vf_loss: 166.0355682373047\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.040816068649292\n",
      "      kl: 0.0064530991949141026\n",
      "      policy_loss: -0.006481585558503866\n",
      "      total_loss: 743.8016357421875\n",
      "      vf_explained_var: 0.540863573551178\n",
      "      vf_loss: 743.80810546875\n",
      "    sample_time_ms: 6348.925\n",
      "    update_time_ms: 9.266\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.20267767459558\n",
      "    rl_1: 112.52393258111456\n",
      "  time_since_restore: 1805.1967158317566\n",
      "  time_this_iter_s: 8.075172901153564\n",
      "  time_total_s: 1805.1967158317566\n",
      "  timestamp: 1552317048\n",
      "  timesteps_since_restore: 525800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 525800\n",
      "  training_iteration: 239\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1805 s, 239 iter, 525800 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-10-55\n",
      "  done: false\n",
      "  episode_len_mean: 94.87\n",
      "  episode_reward_max: 242.19443925693798\n",
      "  episode_reward_mean: 145.39811284366635\n",
      "  episode_reward_min: -166.43000662314245\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 5346\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 912.221\n",
      "    load_time_ms: 3.168\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8650066256523132\n",
      "      kl: 0.005077164154499769\n",
      "      policy_loss: -0.0023566510062664747\n",
      "      total_loss: 266.14263916015625\n",
      "      vf_explained_var: 0.8090717792510986\n",
      "      vf_loss: 266.1449890136719\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0445692539215088\n",
      "      kl: 0.0041432930156588554\n",
      "      policy_loss: -0.0026323655620217323\n",
      "      total_loss: 856.5848388671875\n",
      "      vf_explained_var: 0.6269469261169434\n",
      "      vf_loss: 856.58740234375\n",
      "    sample_time_ms: 6390.948\n",
      "    update_time_ms: 9.16\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.70246916766624\n",
      "    rl_1: 106.69564367600009\n",
      "  time_since_restore: 1812.0494253635406\n",
      "  time_this_iter_s: 6.852709531784058\n",
      "  time_total_s: 1812.0494253635406\n",
      "  timestamp: 1552317055\n",
      "  timesteps_since_restore: 528000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 240\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1812 s, 240 iter, 528000 ts, 145 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 95.73\n",
      "  episode_reward_max: 242.19443925693798\n",
      "  episode_reward_mean: 148.62595662338026\n",
      "  episode_reward_min: -166.43000662314245\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 5368\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 873.748\n",
      "    load_time_ms: 3.061\n",
      "    num_steps_sampled: 530200\n",
      "    num_steps_trained: 530200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2051188945770264\n",
      "      kl: 0.007276647258549929\n",
      "      policy_loss: -0.003047804581001401\n",
      "      total_loss: 170.8866729736328\n",
      "      vf_explained_var: 0.7664047479629517\n",
      "      vf_loss: 170.88973999023438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.053645133972168\n",
      "      kl: 0.006680130492895842\n",
      "      policy_loss: 0.0008854339830577374\n",
      "      total_loss: 935.2952270507812\n",
      "      vf_explained_var: 0.3136793076992035\n",
      "      vf_loss: 935.2943115234375\n",
      "    sample_time_ms: 6135.366\n",
      "    update_time_ms: 9.072\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.82533326245394\n",
      "    rl_1: 109.80062336092631\n",
      "  time_since_restore: 1818.510432958603\n",
      "  time_this_iter_s: 6.461007595062256\n",
      "  time_total_s: 1818.510432958603\n",
      "  timestamp: 1552317061\n",
      "  timesteps_since_restore: 530200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 530200\n",
      "  training_iteration: 241\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1818 s, 241 iter, 530200 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-11-08\n",
      "  done: false\n",
      "  episode_len_mean: 92.66\n",
      "  episode_reward_max: 242.91792255786706\n",
      "  episode_reward_mean: 131.49491817087338\n",
      "  episode_reward_min: -166.43000662314245\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 5394\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 864.429\n",
      "    load_time_ms: 3.057\n",
      "    num_steps_sampled: 532400\n",
      "    num_steps_trained: 532400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7559361457824707\n",
      "      kl: 0.008795005269348621\n",
      "      policy_loss: -0.002190664876252413\n",
      "      total_loss: 603.4043579101562\n",
      "      vf_explained_var: 0.6755980253219604\n",
      "      vf_loss: 603.4065551757812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0354650020599365\n",
      "      kl: 0.005909937433898449\n",
      "      policy_loss: -0.002891808282583952\n",
      "      total_loss: 1157.7410888671875\n",
      "      vf_explained_var: 0.590194821357727\n",
      "      vf_loss: 1157.743896484375\n",
      "    sample_time_ms: 6042.929\n",
      "    update_time_ms: 8.705\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.25515521900446\n",
      "    rl_1: 99.23976295186893\n",
      "  time_since_restore: 1824.7784790992737\n",
      "  time_this_iter_s: 6.268046140670776\n",
      "  time_total_s: 1824.7784790992737\n",
      "  timestamp: 1552317068\n",
      "  timesteps_since_restore: 532400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 532400\n",
      "  training_iteration: 242\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1824 s, 242 iter, 532400 ts, 131 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-11-14\n",
      "  done: false\n",
      "  episode_len_mean: 90.54\n",
      "  episode_reward_max: 242.91792255786706\n",
      "  episode_reward_mean: 118.45151736028284\n",
      "  episode_reward_min: -166.43000662314245\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5419\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 859.143\n",
      "    load_time_ms: 3.076\n",
      "    num_steps_sampled: 534600\n",
      "    num_steps_trained: 534600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8836577534675598\n",
      "      kl: 0.0038239529822021723\n",
      "      policy_loss: -0.0058134086430072784\n",
      "      total_loss: 296.7465515136719\n",
      "      vf_explained_var: 0.800362229347229\n",
      "      vf_loss: 296.75238037109375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0264683961868286\n",
      "      kl: 0.0036371955648064613\n",
      "      policy_loss: -2.3362610590993427e-05\n",
      "      total_loss: 939.0233764648438\n",
      "      vf_explained_var: 0.6320642232894897\n",
      "      vf_loss: 939.0233154296875\n",
      "    sample_time_ms: 5941.442\n",
      "    update_time_ms: 8.402\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 27.00846767706827\n",
      "    rl_1: 91.44304968321454\n",
      "  time_since_restore: 1830.5331571102142\n",
      "  time_this_iter_s: 5.754678010940552\n",
      "  time_total_s: 1830.5331571102142\n",
      "  timestamp: 1552317074\n",
      "  timesteps_since_restore: 534600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 534600\n",
      "  training_iteration: 243\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1830 s, 243 iter, 534600 ts, 118 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-11-20\n",
      "  done: false\n",
      "  episode_len_mean: 90.2\n",
      "  episode_reward_max: 242.91792255786706\n",
      "  episode_reward_mean: 114.10204924635502\n",
      "  episode_reward_min: -168.26902011365763\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 5443\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 861.241\n",
      "    load_time_ms: 2.901\n",
      "    num_steps_sampled: 536800\n",
      "    num_steps_trained: 536800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0468103885650635\n",
      "      kl: 0.008180860430002213\n",
      "      policy_loss: -0.008228866383433342\n",
      "      total_loss: 225.96218872070312\n",
      "      vf_explained_var: 0.8110846281051636\n",
      "      vf_loss: 225.9704132080078\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0474870204925537\n",
      "      kl: 0.005872875452041626\n",
      "      policy_loss: -0.001715159392915666\n",
      "      total_loss: 1071.0286865234375\n",
      "      vf_explained_var: 0.5526152849197388\n",
      "      vf_loss: 1071.0303955078125\n",
      "    sample_time_ms: 5921.002\n",
      "    update_time_ms: 8.24\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.921522153385695\n",
      "    rl_1: 89.1805270929693\n",
      "  time_since_restore: 1836.7679374217987\n",
      "  time_this_iter_s: 6.234780311584473\n",
      "  time_total_s: 1836.7679374217987\n",
      "  timestamp: 1552317080\n",
      "  timesteps_since_restore: 536800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 536800\n",
      "  training_iteration: 244\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1836 s, 244 iter, 536800 ts, 114 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-11-26\n",
      "  done: false\n",
      "  episode_len_mean: 88.26\n",
      "  episode_reward_max: 242.91792255786706\n",
      "  episode_reward_mean: 101.80121691836003\n",
      "  episode_reward_min: -168.26902011365763\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5468\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 871.433\n",
      "    load_time_ms: 2.992\n",
      "    num_steps_sampled: 539000\n",
      "    num_steps_trained: 539000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6978750228881836\n",
      "      kl: 0.009727905504405499\n",
      "      policy_loss: -0.007102080155164003\n",
      "      total_loss: 437.47540283203125\n",
      "      vf_explained_var: 0.7743822336196899\n",
      "      vf_loss: 437.4825134277344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0173112154006958\n",
      "      kl: 0.00381408235989511\n",
      "      policy_loss: -0.001101974630728364\n",
      "      total_loss: 1006.1795654296875\n",
      "      vf_explained_var: 0.6740678548812866\n",
      "      vf_loss: 1006.1806030273438\n",
      "    sample_time_ms: 5892.038\n",
      "    update_time_ms: 8.344\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 21.309894812694466\n",
      "    rl_1: 80.49132210566559\n",
      "  time_since_restore: 1843.0325963497162\n",
      "  time_this_iter_s: 6.2646589279174805\n",
      "  time_total_s: 1843.0325963497162\n",
      "  timestamp: 1552317086\n",
      "  timesteps_since_restore: 539000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 539000\n",
      "  training_iteration: 245\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1843 s, 245 iter, 539000 ts, 102 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-11-35\n",
      "  done: false\n",
      "  episode_len_mean: 90.79\n",
      "  episode_reward_max: 242.79314322704258\n",
      "  episode_reward_mean: 118.97258263556155\n",
      "  episode_reward_min: -168.26902011365763\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 5490\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 891.249\n",
      "    load_time_ms: 2.9\n",
      "    num_steps_sampled: 541200\n",
      "    num_steps_trained: 541200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9132245182991028\n",
      "      kl: 0.008102932013571262\n",
      "      policy_loss: -0.005036430433392525\n",
      "      total_loss: 149.19056701660156\n",
      "      vf_explained_var: 0.8611466288566589\n",
      "      vf_loss: 149.1956024169922\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0037200450897217\n",
      "      kl: 0.014797250740230083\n",
      "      policy_loss: -0.007645945530384779\n",
      "      total_loss: 763.59814453125\n",
      "      vf_explained_var: 0.640888512134552\n",
      "      vf_loss: 763.6057739257812\n",
      "    sample_time_ms: 6015.185\n",
      "    update_time_ms: 8.061\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.04831388531503\n",
      "    rl_1: 90.92426875024655\n",
      "  time_since_restore: 1851.3915302753448\n",
      "  time_this_iter_s: 8.358933925628662\n",
      "  time_total_s: 1851.3915302753448\n",
      "  timestamp: 1552317095\n",
      "  timesteps_since_restore: 541200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 541200\n",
      "  training_iteration: 246\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1851 s, 246 iter, 541200 ts, 119 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-11-41\n",
      "  done: false\n",
      "  episode_len_mean: 93.8\n",
      "  episode_reward_max: 242.79314322704258\n",
      "  episode_reward_mean: 136.13505905085725\n",
      "  episode_reward_min: -168.26902011365763\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 5512\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 869.148\n",
      "    load_time_ms: 2.79\n",
      "    num_steps_sampled: 543400\n",
      "    num_steps_trained: 543400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9210735559463501\n",
      "      kl: 0.010243278928101063\n",
      "      policy_loss: -0.005466306582093239\n",
      "      total_loss: 100.11840057373047\n",
      "      vf_explained_var: 0.9166169166564941\n",
      "      vf_loss: 100.12386322021484\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.993192195892334\n",
      "      kl: 0.017577996477484703\n",
      "      policy_loss: -0.009444303810596466\n",
      "      total_loss: 630.9075317382812\n",
      "      vf_explained_var: 0.7617250680923462\n",
      "      vf_loss: 630.9169311523438\n",
      "    sample_time_ms: 6054.683\n",
      "    update_time_ms: 7.982\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.07741630775932\n",
      "    rl_1: 102.05764274309793\n",
      "  time_since_restore: 1857.986344575882\n",
      "  time_this_iter_s: 6.594814300537109\n",
      "  time_total_s: 1857.986344575882\n",
      "  timestamp: 1552317101\n",
      "  timesteps_since_restore: 543400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 543400\n",
      "  training_iteration: 247\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1857 s, 247 iter, 543400 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-11-47\n",
      "  done: false\n",
      "  episode_len_mean: 94.41\n",
      "  episode_reward_max: 240.01590802048307\n",
      "  episode_reward_mean: 141.24262114299364\n",
      "  episode_reward_min: -162.71456853456868\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5535\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 829.95\n",
      "    load_time_ms: 2.668\n",
      "    num_steps_sampled: 545600\n",
      "    num_steps_trained: 545600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8432652950286865\n",
      "      kl: 0.010564301162958145\n",
      "      policy_loss: -0.0017932027112692595\n",
      "      total_loss: 183.6112518310547\n",
      "      vf_explained_var: 0.8246599435806274\n",
      "      vf_loss: 183.613037109375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.01421058177948\n",
      "      kl: 0.009923606179654598\n",
      "      policy_loss: -0.005891855806112289\n",
      "      total_loss: 674.1573486328125\n",
      "      vf_explained_var: 0.7032588124275208\n",
      "      vf_loss: 674.1632080078125\n",
      "    sample_time_ms: 5855.755\n",
      "    update_time_ms: 8.242\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.0736366755823\n",
      "    rl_1: 104.16898446741133\n",
      "  time_since_restore: 1864.198787689209\n",
      "  time_this_iter_s: 6.212443113327026\n",
      "  time_total_s: 1864.198787689209\n",
      "  timestamp: 1552317107\n",
      "  timesteps_since_restore: 545600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 545600\n",
      "  training_iteration: 248\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1864 s, 248 iter, 545600 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-11-55\n",
      "  done: false\n",
      "  episode_len_mean: 96.5\n",
      "  episode_reward_max: 240.01590802048307\n",
      "  episode_reward_mean: 155.37513272863316\n",
      "  episode_reward_min: -162.71456853456868\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5558\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 869.118\n",
      "    load_time_ms: 2.864\n",
      "    num_steps_sampled: 547800\n",
      "    num_steps_trained: 547800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7829431891441345\n",
      "      kl: 0.0040865023620426655\n",
      "      policy_loss: -0.008037391118705273\n",
      "      total_loss: 179.3043975830078\n",
      "      vf_explained_var: 0.8416022062301636\n",
      "      vf_loss: 179.3124237060547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9746836423873901\n",
      "      kl: 0.010107573121786118\n",
      "      policy_loss: -0.004378478042781353\n",
      "      total_loss: 758.394287109375\n",
      "      vf_explained_var: 0.6418680548667908\n",
      "      vf_loss: 758.3987426757812\n",
      "    sample_time_ms: 5744.777\n",
      "    update_time_ms: 7.802\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.87918282815345\n",
      "    rl_1: 112.4959499004797\n",
      "  time_since_restore: 1871.5630135536194\n",
      "  time_this_iter_s: 7.3642258644104\n",
      "  time_total_s: 1871.5630135536194\n",
      "  timestamp: 1552317115\n",
      "  timesteps_since_restore: 547800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 547800\n",
      "  training_iteration: 249\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1871 s, 249 iter, 547800 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-12-02\n",
      "  done: false\n",
      "  episode_len_mean: 97.09\n",
      "  episode_reward_max: 240.32440388864978\n",
      "  episode_reward_mean: 162.43495610557375\n",
      "  episode_reward_min: -158.2452360537232\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5581\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 849.749\n",
      "    load_time_ms: 2.806\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7346407175064087\n",
      "      kl: 0.005985027179121971\n",
      "      policy_loss: -0.0023802071809768677\n",
      "      total_loss: 442.02642822265625\n",
      "      vf_explained_var: 0.5652183890342712\n",
      "      vf_loss: 442.0287780761719\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0295301675796509\n",
      "      kl: 0.0072792284190654755\n",
      "      policy_loss: -0.005227978341281414\n",
      "      total_loss: 849.0420532226562\n",
      "      vf_explained_var: 0.5211650133132935\n",
      "      vf_loss: 849.0472412109375\n",
      "    sample_time_ms: 5781.299\n",
      "    update_time_ms: 8.183\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 46.77147644482844\n",
      "    rl_1: 115.66347966074528\n",
      "  time_since_restore: 1878.5916767120361\n",
      "  time_this_iter_s: 7.028663158416748\n",
      "  time_total_s: 1878.5916767120361\n",
      "  timestamp: 1552317122\n",
      "  timesteps_since_restore: 550000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 250\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1878 s, 250 iter, 550000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-12-09\n",
      "  done: false\n",
      "  episode_len_mean: 96.78\n",
      "  episode_reward_max: 240.32440388864978\n",
      "  episode_reward_mean: 162.6887847085429\n",
      "  episode_reward_min: -158.2452360537232\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 5603\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 853.726\n",
      "    load_time_ms: 2.868\n",
      "    num_steps_sampled: 552200\n",
      "    num_steps_trained: 552200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8204771876335144\n",
      "      kl: 0.008866255171597004\n",
      "      policy_loss: -0.00973407831043005\n",
      "      total_loss: 106.380126953125\n",
      "      vf_explained_var: 0.8870911598205566\n",
      "      vf_loss: 106.38986206054688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9855243563652039\n",
      "      kl: 0.009788239374756813\n",
      "      policy_loss: -0.005494064185768366\n",
      "      total_loss: 688.5804443359375\n",
      "      vf_explained_var: 0.6723580956459045\n",
      "      vf_loss: 688.5858764648438\n",
      "    sample_time_ms: 5817.074\n",
      "    update_time_ms: 8.187\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 47.28593199064298\n",
      "    rl_1: 115.40285271789989\n",
      "  time_since_restore: 1885.453497171402\n",
      "  time_this_iter_s: 6.861820459365845\n",
      "  time_total_s: 1885.453497171402\n",
      "  timestamp: 1552317129\n",
      "  timesteps_since_restore: 552200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 552200\n",
      "  training_iteration: 251\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1885 s, 251 iter, 552200 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-12-16\n",
      "  done: false\n",
      "  episode_len_mean: 97.28\n",
      "  episode_reward_max: 240.32440388864978\n",
      "  episode_reward_mean: 169.54942247878927\n",
      "  episode_reward_min: -158.2452360537232\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5626\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 859.368\n",
      "    load_time_ms: 2.897\n",
      "    num_steps_sampled: 554400\n",
      "    num_steps_trained: 554400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8256883025169373\n",
      "      kl: 0.01586298458278179\n",
      "      policy_loss: -0.003023860277608037\n",
      "      total_loss: 164.5848846435547\n",
      "      vf_explained_var: 0.8044865727424622\n",
      "      vf_loss: 164.58792114257812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0000689029693604\n",
      "      kl: 0.0041563003323972225\n",
      "      policy_loss: -0.0030332773458212614\n",
      "      total_loss: 747.6738891601562\n",
      "      vf_explained_var: 0.5667418837547302\n",
      "      vf_loss: 747.6768798828125\n",
      "    sample_time_ms: 5871.615\n",
      "    update_time_ms: 8.45\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.70255025117978\n",
      "    rl_1: 118.84687222760944\n",
      "  time_since_restore: 1892.3255395889282\n",
      "  time_this_iter_s: 6.872042417526245\n",
      "  time_total_s: 1892.3255395889282\n",
      "  timestamp: 1552317136\n",
      "  timesteps_since_restore: 554400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 554400\n",
      "  training_iteration: 252\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1892 s, 252 iter, 554400 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-12-23\n",
      "  done: false\n",
      "  episode_len_mean: 96.55\n",
      "  episode_reward_max: 240.32440388864978\n",
      "  episode_reward_mean: 162.56398607945889\n",
      "  episode_reward_min: -158.2452360537232\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 5650\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 895.149\n",
      "    load_time_ms: 2.844\n",
      "    num_steps_sampled: 556600\n",
      "    num_steps_trained: 556600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8673827648162842\n",
      "      kl: 0.003256578929722309\n",
      "      policy_loss: -0.0037537028547376394\n",
      "      total_loss: 181.98770141601562\n",
      "      vf_explained_var: 0.8471986651420593\n",
      "      vf_loss: 181.99147033691406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9932899475097656\n",
      "      kl: 0.010206756182014942\n",
      "      policy_loss: -0.006541671231389046\n",
      "      total_loss: 712.6032104492188\n",
      "      vf_explained_var: 0.7014578580856323\n",
      "      vf_loss: 712.6098022460938\n",
      "    sample_time_ms: 5966.447\n",
      "    update_time_ms: 8.555\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 47.70528349475237\n",
      "    rl_1: 114.85870258470648\n",
      "  time_since_restore: 1899.3900933265686\n",
      "  time_this_iter_s: 7.064553737640381\n",
      "  time_total_s: 1899.3900933265686\n",
      "  timestamp: 1552317143\n",
      "  timesteps_since_restore: 556600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 556600\n",
      "  training_iteration: 253\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1899 s, 253 iter, 556600 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-12-30\n",
      "  done: false\n",
      "  episode_len_mean: 97.11\n",
      "  episode_reward_max: 239.53408892473087\n",
      "  episode_reward_mean: 165.86304271171701\n",
      "  episode_reward_min: -162.59114545019173\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 5672\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 917.843\n",
      "    load_time_ms: 2.871\n",
      "    num_steps_sampled: 558800\n",
      "    num_steps_trained: 558800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9332550764083862\n",
      "      kl: 0.005096198059618473\n",
      "      policy_loss: -0.004103753715753555\n",
      "      total_loss: 167.71031188964844\n",
      "      vf_explained_var: 0.8603233098983765\n",
      "      vf_loss: 167.7144012451172\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.968712329864502\n",
      "      kl: 0.01436547003686428\n",
      "      policy_loss: -0.008824203163385391\n",
      "      total_loss: 822.8224487304688\n",
      "      vf_explained_var: 0.6804308891296387\n",
      "      vf_loss: 822.8312377929688\n",
      "    sample_time_ms: 6045.691\n",
      "    update_time_ms: 8.718\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.18232928239621\n",
      "    rl_1: 117.68071342932086\n",
      "  time_since_restore: 1906.6455211639404\n",
      "  time_this_iter_s: 7.255427837371826\n",
      "  time_total_s: 1906.6455211639404\n",
      "  timestamp: 1552317150\n",
      "  timesteps_since_restore: 558800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 558800\n",
      "  training_iteration: 254\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1906 s, 254 iter, 558800 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-12-40\n",
      "  done: false\n",
      "  episode_len_mean: 95.51\n",
      "  episode_reward_max: 242.27019239833066\n",
      "  episode_reward_mean: 153.77326255799255\n",
      "  episode_reward_min: -162.59114545019173\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 5696\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 925.809\n",
      "    load_time_ms: 2.769\n",
      "    num_steps_sampled: 561000\n",
      "    num_steps_trained: 561000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8736523985862732\n",
      "      kl: 0.009384742937982082\n",
      "      policy_loss: -0.009632669389247894\n",
      "      total_loss: 627.2400512695312\n",
      "      vf_explained_var: 0.44126948714256287\n",
      "      vf_loss: 627.2496948242188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.951410174369812\n",
      "      kl: 0.0050557502545416355\n",
      "      policy_loss: -0.004046231042593718\n",
      "      total_loss: 1224.8280029296875\n",
      "      vf_explained_var: 0.34881889820098877\n",
      "      vf_loss: 1224.8321533203125\n",
      "    sample_time_ms: 6420.259\n",
      "    update_time_ms: 8.612\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.56250490279454\n",
      "    rl_1: 111.210757655198\n",
      "  time_since_restore: 1916.7333445549011\n",
      "  time_this_iter_s: 10.087823390960693\n",
      "  time_total_s: 1916.7333445549011\n",
      "  timestamp: 1552317160\n",
      "  timesteps_since_restore: 561000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 561000\n",
      "  training_iteration: 255\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1916 s, 255 iter, 561000 ts, 154 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-12-49\n",
      "  done: false\n",
      "  episode_len_mean: 94.44\n",
      "  episode_reward_max: 242.27019239833066\n",
      "  episode_reward_mean: 146.47183105709263\n",
      "  episode_reward_min: -162.59114545019173\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 5720\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 912.774\n",
      "    load_time_ms: 2.786\n",
      "    num_steps_sampled: 563200\n",
      "    num_steps_trained: 563200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8417006134986877\n",
      "      kl: 0.004101254045963287\n",
      "      policy_loss: -0.01136584673076868\n",
      "      total_loss: 231.86383056640625\n",
      "      vf_explained_var: 0.8237687349319458\n",
      "      vf_loss: 231.8751678466797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9543672204017639\n",
      "      kl: 0.005762625951319933\n",
      "      policy_loss: -0.005302602890878916\n",
      "      total_loss: 798.34619140625\n",
      "      vf_explained_var: 0.7099073529243469\n",
      "      vf_loss: 798.3514404296875\n",
      "    sample_time_ms: 6464.156\n",
      "    update_time_ms: 8.794\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.237826730132355\n",
      "    rl_1: 107.23400432696029\n",
      "  time_since_restore: 1925.4018757343292\n",
      "  time_this_iter_s: 8.6685311794281\n",
      "  time_total_s: 1925.4018757343292\n",
      "  timestamp: 1552317169\n",
      "  timesteps_since_restore: 563200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 563200\n",
      "  training_iteration: 256\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1925 s, 256 iter, 563200 ts, 146 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-12-56\n",
      "  done: false\n",
      "  episode_len_mean: 94.68\n",
      "  episode_reward_max: 242.27019239833066\n",
      "  episode_reward_mean: 149.76183921165367\n",
      "  episode_reward_min: -162.59114545019173\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5743\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 915.801\n",
      "    load_time_ms: 2.912\n",
      "    num_steps_sampled: 565400\n",
      "    num_steps_trained: 565400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.786872386932373\n",
      "      kl: 0.002912148367613554\n",
      "      policy_loss: -0.002762328367680311\n",
      "      total_loss: 180.245849609375\n",
      "      vf_explained_var: 0.8401830196380615\n",
      "      vf_loss: 180.2486114501953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9209662675857544\n",
      "      kl: 0.021014416590332985\n",
      "      policy_loss: -0.01232699304819107\n",
      "      total_loss: 734.40966796875\n",
      "      vf_explained_var: 0.6767491102218628\n",
      "      vf_loss: 734.4219970703125\n",
      "    sample_time_ms: 6501.789\n",
      "    update_time_ms: 8.76\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.68633570160958\n",
      "    rl_1: 109.07550351004407\n",
      "  time_since_restore: 1932.404450416565\n",
      "  time_this_iter_s: 7.002574682235718\n",
      "  time_total_s: 1932.404450416565\n",
      "  timestamp: 1552317176\n",
      "  timesteps_since_restore: 565400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 565400\n",
      "  training_iteration: 257\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1932 s, 257 iter, 565400 ts, 150 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-13-03\n",
      "  done: false\n",
      "  episode_len_mean: 91.43\n",
      "  episode_reward_max: 242.9359332764787\n",
      "  episode_reward_mean: 129.6062235211635\n",
      "  episode_reward_min: -165.0921293940879\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5768\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 928.03\n",
      "    load_time_ms: 2.913\n",
      "    num_steps_sampled: 567600\n",
      "    num_steps_trained: 567600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6088964343070984\n",
      "      kl: 0.007488723378628492\n",
      "      policy_loss: -6.582061905646697e-05\n",
      "      total_loss: 338.0242614746094\n",
      "      vf_explained_var: 0.8122829794883728\n",
      "      vf_loss: 338.0243225097656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9117558598518372\n",
      "      kl: 0.004316161386668682\n",
      "      policy_loss: -0.003885117592290044\n",
      "      total_loss: 1010.21826171875\n",
      "      vf_explained_var: 0.6709914803504944\n",
      "      vf_loss: 1010.22216796875\n",
      "    sample_time_ms: 6562.528\n",
      "    update_time_ms: 8.46\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.68414866927662\n",
      "    rl_1: 96.92207485188692\n",
      "  time_since_restore: 1939.3473069667816\n",
      "  time_this_iter_s: 6.942856550216675\n",
      "  time_total_s: 1939.3473069667816\n",
      "  timestamp: 1552317183\n",
      "  timesteps_since_restore: 567600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 567600\n",
      "  training_iteration: 258\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1939 s, 258 iter, 567600 ts, 130 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-13-16\n",
      "  done: false\n",
      "  episode_len_mean: 92.88\n",
      "  episode_reward_max: 242.9359332764787\n",
      "  episode_reward_mean: 141.42140645913202\n",
      "  episode_reward_min: -165.0921293940879\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5791\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 968.202\n",
      "    load_time_ms: 2.757\n",
      "    num_steps_sampled: 569800\n",
      "    num_steps_trained: 569800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8737316131591797\n",
      "      kl: 0.005071468185633421\n",
      "      policy_loss: -0.006644629407674074\n",
      "      total_loss: 177.3139190673828\n",
      "      vf_explained_var: 0.783742368221283\n",
      "      vf_loss: 177.32058715820312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9721154570579529\n",
      "      kl: 0.014231127686798573\n",
      "      policy_loss: -0.007861214689910412\n",
      "      total_loss: 760.6917114257812\n",
      "      vf_explained_var: 0.6083469390869141\n",
      "      vf_loss: 760.6995239257812\n",
      "    sample_time_ms: 7060.267\n",
      "    update_time_ms: 8.378\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.411652747266096\n",
      "    rl_1: 103.00975371186591\n",
      "  time_since_restore: 1952.0856063365936\n",
      "  time_this_iter_s: 12.738299369812012\n",
      "  time_total_s: 1952.0856063365936\n",
      "  timestamp: 1552317196\n",
      "  timesteps_since_restore: 569800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 569800\n",
      "  training_iteration: 259\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1952 s, 259 iter, 569800 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-13-26\n",
      "  done: false\n",
      "  episode_len_mean: 93.13\n",
      "  episode_reward_max: 242.9359332764787\n",
      "  episode_reward_mean: 141.33594086020014\n",
      "  episode_reward_min: -165.0921293940879\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 5815\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 986.35\n",
      "    load_time_ms: 2.776\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8172053098678589\n",
      "      kl: 0.006821326911449432\n",
      "      policy_loss: -0.00455797603353858\n",
      "      total_loss: 209.2939453125\n",
      "      vf_explained_var: 0.8659964203834534\n",
      "      vf_loss: 209.29849243164062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9797589778900146\n",
      "      kl: 0.0048519703559577465\n",
      "      policy_loss: -0.005536360666155815\n",
      "      total_loss: 682.5253295898438\n",
      "      vf_explained_var: 0.7772866487503052\n",
      "      vf_loss: 682.53076171875\n",
      "    sample_time_ms: 7351.054\n",
      "    update_time_ms: 8.945\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.91463524181169\n",
      "    rl_1: 102.42130561838844\n",
      "  time_since_restore: 1962.2066643238068\n",
      "  time_this_iter_s: 10.121057987213135\n",
      "  time_total_s: 1962.2066643238068\n",
      "  timestamp: 1552317206\n",
      "  timesteps_since_restore: 572000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 260\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1962 s, 260 iter, 572000 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-13-34\n",
      "  done: false\n",
      "  episode_len_mean: 94.08\n",
      "  episode_reward_max: 242.9359332764787\n",
      "  episode_reward_mean: 148.21890686467202\n",
      "  episode_reward_min: -165.0921293940879\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 5836\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 993.967\n",
      "    load_time_ms: 2.771\n",
      "    num_steps_sampled: 574200\n",
      "    num_steps_trained: 574200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.013583779335022\n",
      "      kl: 0.00863556656986475\n",
      "      policy_loss: -0.009776510298252106\n",
      "      total_loss: 54.3153190612793\n",
      "      vf_explained_var: 0.8875293731689453\n",
      "      vf_loss: 54.325096130371094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9394927024841309\n",
      "      kl: 0.006309960503131151\n",
      "      policy_loss: -0.006619106978178024\n",
      "      total_loss: 616.1824951171875\n",
      "      vf_explained_var: 0.495587021112442\n",
      "      vf_loss: 616.1890869140625\n",
      "    sample_time_ms: 7428.38\n",
      "    update_time_ms: 8.955\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.036864644676534\n",
      "    rl_1: 107.18204221999545\n",
      "  time_since_restore: 1969.9153234958649\n",
      "  time_this_iter_s: 7.7086591720581055\n",
      "  time_total_s: 1969.9153234958649\n",
      "  timestamp: 1552317214\n",
      "  timesteps_since_restore: 574200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 574200\n",
      "  training_iteration: 261\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1969 s, 261 iter, 574200 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-13-42\n",
      "  done: false\n",
      "  episode_len_mean: 95.33\n",
      "  episode_reward_max: 241.99689248616124\n",
      "  episode_reward_mean: 148.76915408292155\n",
      "  episode_reward_min: -156.13598582070543\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5861\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1006.799\n",
      "    load_time_ms: 2.676\n",
      "    num_steps_sampled: 576400\n",
      "    num_steps_trained: 576400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0066609382629395\n",
      "      kl: 0.007560331840068102\n",
      "      policy_loss: -0.0030210718978196383\n",
      "      total_loss: 207.4850311279297\n",
      "      vf_explained_var: 0.8494410514831543\n",
      "      vf_loss: 207.48805236816406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0294697284698486\n",
      "      kl: 0.0025058838073164225\n",
      "      policy_loss: 0.0004735176917165518\n",
      "      total_loss: 706.5889892578125\n",
      "      vf_explained_var: 0.7681301236152649\n",
      "      vf_loss: 706.5884399414062\n",
      "    sample_time_ms: 7501.973\n",
      "    update_time_ms: 8.799\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.96109333713791\n",
      "    rl_1: 108.80806074578364\n",
      "  time_since_restore: 1977.6511662006378\n",
      "  time_this_iter_s: 7.735842704772949\n",
      "  time_total_s: 1977.6511662006378\n",
      "  timestamp: 1552317222\n",
      "  timesteps_since_restore: 576400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 576400\n",
      "  training_iteration: 262\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1977 s, 262 iter, 576400 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-13-53\n",
      "  done: false\n",
      "  episode_len_mean: 96.3\n",
      "  episode_reward_max: 241.99689248616124\n",
      "  episode_reward_mean: 155.39699254020698\n",
      "  episode_reward_min: -156.13598582070543\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 5883\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1086.084\n",
      "    load_time_ms: 2.647\n",
      "    num_steps_sampled: 578600\n",
      "    num_steps_trained: 578600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8741354942321777\n",
      "      kl: 0.004614903591573238\n",
      "      policy_loss: -0.0028006872162222862\n",
      "      total_loss: 122.82810974121094\n",
      "      vf_explained_var: 0.8681440353393555\n",
      "      vf_loss: 122.8309097290039\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9319726228713989\n",
      "      kl: 0.013618426397442818\n",
      "      policy_loss: -0.008367341943085194\n",
      "      total_loss: 646.9224853515625\n",
      "      vf_explained_var: 0.6783729791641235\n",
      "      vf_loss: 646.9308471679688\n",
      "    sample_time_ms: 7843.013\n",
      "    update_time_ms: 8.732\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.116711544004126\n",
      "    rl_1: 113.28028099620283\n",
      "  time_since_restore: 1988.9237899780273\n",
      "  time_this_iter_s: 11.272623777389526\n",
      "  time_total_s: 1988.9237899780273\n",
      "  timestamp: 1552317233\n",
      "  timesteps_since_restore: 578600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 578600\n",
      "  training_iteration: 263\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 1988 s, 263 iter, 578600 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-14-09\n",
      "  done: false\n",
      "  episode_len_mean: 94.18\n",
      "  episode_reward_max: 240.40790184668037\n",
      "  episode_reward_mean: 140.2358683780387\n",
      "  episode_reward_min: -161.30249704038164\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5908\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1191.756\n",
      "    load_time_ms: 3.143\n",
      "    num_steps_sampled: 580800\n",
      "    num_steps_trained: 580800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9242727756500244\n",
      "      kl: 0.0043135276064276695\n",
      "      policy_loss: -0.0036930933129042387\n",
      "      total_loss: 230.28469848632812\n",
      "      vf_explained_var: 0.8586177229881287\n",
      "      vf_loss: 230.2883758544922\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9595112800598145\n",
      "      kl: 0.008429217152297497\n",
      "      policy_loss: 8.008453733054921e-05\n",
      "      total_loss: 705.7380981445312\n",
      "      vf_explained_var: 0.7609970569610596\n",
      "      vf_loss: 705.738037109375\n",
      "    sample_time_ms: 8573.043\n",
      "    update_time_ms: 9.602\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.372400913094225\n",
      "    rl_1: 104.86346746494452\n",
      "  time_since_restore: 2004.5614039897919\n",
      "  time_this_iter_s: 15.637614011764526\n",
      "  time_total_s: 2004.5614039897919\n",
      "  timestamp: 1552317249\n",
      "  timesteps_since_restore: 580800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 580800\n",
      "  training_iteration: 264\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2004 s, 264 iter, 580800 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-14-20\n",
      "  done: false\n",
      "  episode_len_mean: 92.25\n",
      "  episode_reward_max: 240.40790184668037\n",
      "  episode_reward_mean: 126.77948686736903\n",
      "  episode_reward_min: -161.30249704038164\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 5932\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1220.139\n",
      "    load_time_ms: 3.184\n",
      "    num_steps_sampled: 583000\n",
      "    num_steps_trained: 583000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9271187782287598\n",
      "      kl: 0.008971292525529861\n",
      "      policy_loss: -0.009464721195399761\n",
      "      total_loss: 352.6189880371094\n",
      "      vf_explained_var: 0.7239864468574524\n",
      "      vf_loss: 352.6285095214844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9457044005393982\n",
      "      kl: 0.008472013287246227\n",
      "      policy_loss: 0.0001908089325297624\n",
      "      total_loss: 884.12646484375\n",
      "      vf_explained_var: 0.6689987778663635\n",
      "      vf_loss: 884.1263427734375\n",
      "    sample_time_ms: 8697.504\n",
      "    update_time_ms: 10.317\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.83374848423692\n",
      "    rl_1: 95.9457383831321\n",
      "  time_since_restore: 2016.1893367767334\n",
      "  time_this_iter_s: 11.627932786941528\n",
      "  time_total_s: 2016.1893367767334\n",
      "  timestamp: 1552317260\n",
      "  timesteps_since_restore: 583000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 583000\n",
      "  training_iteration: 265\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2016 s, 265 iter, 583000 ts, 127 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-14-30\n",
      "  done: false\n",
      "  episode_len_mean: 93.25\n",
      "  episode_reward_max: 240.73216508234822\n",
      "  episode_reward_mean: 132.60113705263586\n",
      "  episode_reward_min: -161.30249704038164\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 5955\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1271.611\n",
      "    load_time_ms: 3.227\n",
      "    num_steps_sampled: 585200\n",
      "    num_steps_trained: 585200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9891624450683594\n",
      "      kl: 0.006509515456855297\n",
      "      policy_loss: -0.0038169235922396183\n",
      "      total_loss: 258.26898193359375\n",
      "      vf_explained_var: 0.7647137641906738\n",
      "      vf_loss: 258.2728271484375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.996131420135498\n",
      "      kl: 0.0059781973250210285\n",
      "      policy_loss: -0.004521459341049194\n",
      "      total_loss: 859.5900268554688\n",
      "      vf_explained_var: 0.7000772953033447\n",
      "      vf_loss: 859.5945434570312\n",
      "    sample_time_ms: 8697.033\n",
      "    update_time_ms: 10.863\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.26362084299713\n",
      "    rl_1: 99.33751620963876\n",
      "  time_since_restore: 2025.3775916099548\n",
      "  time_this_iter_s: 9.188254833221436\n",
      "  time_total_s: 2025.3775916099548\n",
      "  timestamp: 1552317270\n",
      "  timesteps_since_restore: 585200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 585200\n",
      "  training_iteration: 266\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2025 s, 266 iter, 585200 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-14-42\n",
      "  done: false\n",
      "  episode_len_mean: 91.6\n",
      "  episode_reward_max: 240.73216508234822\n",
      "  episode_reward_mean: 120.38319658938555\n",
      "  episode_reward_min: -161.30249704038164\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 5979\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1351.045\n",
      "    load_time_ms: 3.181\n",
      "    num_steps_sampled: 587400\n",
      "    num_steps_trained: 587400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8545958399772644\n",
      "      kl: 0.00599181093275547\n",
      "      policy_loss: -0.006030383054167032\n",
      "      total_loss: 190.22044372558594\n",
      "      vf_explained_var: 0.8635870814323425\n",
      "      vf_loss: 190.22645568847656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9170857667922974\n",
      "      kl: 0.00294407713226974\n",
      "      policy_loss: -0.008066556416451931\n",
      "      total_loss: 731.5005493164062\n",
      "      vf_explained_var: 0.7426636815071106\n",
      "      vf_loss: 731.5086059570312\n",
      "    sample_time_ms: 9126.552\n",
      "    update_time_ms: 11.379\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.117717662693163\n",
      "    rl_1: 92.26547892669238\n",
      "  time_since_restore: 2037.4805133342743\n",
      "  time_this_iter_s: 12.102921724319458\n",
      "  time_total_s: 2037.4805133342743\n",
      "  timestamp: 1552317282\n",
      "  timesteps_since_restore: 587400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 587400\n",
      "  training_iteration: 267\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2037 s, 267 iter, 587400 ts, 120 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-14-52\n",
      "  done: false\n",
      "  episode_len_mean: 91.93\n",
      "  episode_reward_max: 240.73216508234822\n",
      "  episode_reward_mean: 122.06953854579154\n",
      "  episode_reward_min: -162.26071574350703\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 6003\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1391.922\n",
      "    load_time_ms: 3.454\n",
      "    num_steps_sampled: 589600\n",
      "    num_steps_trained: 589600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8215347528457642\n",
      "      kl: 0.009860267862677574\n",
      "      policy_loss: -0.0038811415433883667\n",
      "      total_loss: 207.5552978515625\n",
      "      vf_explained_var: 0.872222363948822\n",
      "      vf_loss: 207.55918884277344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9252843856811523\n",
      "      kl: 0.011114243417978287\n",
      "      policy_loss: -0.005400581751018763\n",
      "      total_loss: 697.46923828125\n",
      "      vf_explained_var: 0.793696403503418\n",
      "      vf_loss: 697.4746704101562\n",
      "    sample_time_ms: 9346.738\n",
      "    update_time_ms: 11.62\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.728609521851318\n",
      "    rl_1: 93.34092902394022\n",
      "  time_since_restore: 2047.0448517799377\n",
      "  time_this_iter_s: 9.564338445663452\n",
      "  time_total_s: 2047.0448517799377\n",
      "  timestamp: 1552317292\n",
      "  timesteps_since_restore: 589600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 589600\n",
      "  training_iteration: 268\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2047 s, 268 iter, 589600 ts, 122 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-14-59\n",
      "  done: false\n",
      "  episode_len_mean: 93.86\n",
      "  episode_reward_max: 240.73216508234822\n",
      "  episode_reward_mean: 132.4728801277215\n",
      "  episode_reward_min: -162.26071574350703\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6025\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1342.984\n",
      "    load_time_ms: 3.468\n",
      "    num_steps_sampled: 591800\n",
      "    num_steps_trained: 591800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9890687465667725\n",
      "      kl: 0.007241515908390284\n",
      "      policy_loss: -0.0050335051491856575\n",
      "      total_loss: 139.9095001220703\n",
      "      vf_explained_var: 0.8581572771072388\n",
      "      vf_loss: 139.91452026367188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.927049994468689\n",
      "      kl: 0.003323665587231517\n",
      "      policy_loss: 0.0019086984684690833\n",
      "      total_loss: 646.2612915039062\n",
      "      vf_explained_var: 0.7064641714096069\n",
      "      vf_loss: 646.2594604492188\n",
      "    sample_time_ms: 8862.643\n",
      "    update_time_ms: 11.914\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.408287999348964\n",
      "    rl_1: 100.06459212837257\n",
      "  time_since_restore: 2054.4534463882446\n",
      "  time_this_iter_s: 7.408594608306885\n",
      "  time_total_s: 2054.4534463882446\n",
      "  timestamp: 1552317299\n",
      "  timesteps_since_restore: 591800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 591800\n",
      "  training_iteration: 269\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2054 s, 269 iter, 591800 ts, 132 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-15-06\n",
      "  done: false\n",
      "  episode_len_mean: 93.7\n",
      "  episode_reward_max: 240.73216508234822\n",
      "  episode_reward_mean: 137.7026385656263\n",
      "  episode_reward_min: -163.50023133931617\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 6049\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1320.109\n",
      "    load_time_ms: 3.41\n",
      "    num_steps_sampled: 594000\n",
      "    num_steps_trained: 594000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.883965015411377\n",
      "      kl: 0.004609987139701843\n",
      "      policy_loss: -0.0007709663477726281\n",
      "      total_loss: 240.7344970703125\n",
      "      vf_explained_var: 0.8127480149269104\n",
      "      vf_loss: 240.7352752685547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8996935486793518\n",
      "      kl: 0.007017543539404869\n",
      "      policy_loss: -0.004779156297445297\n",
      "      total_loss: 825.2727661132812\n",
      "      vf_explained_var: 0.6766617894172668\n",
      "      vf_loss: 825.2775268554688\n",
      "    sample_time_ms: 8557.118\n",
      "    update_time_ms: 11.342\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.8659491983382\n",
      "    rl_1: 102.83668936728807\n",
      "  time_since_restore: 2061.281746864319\n",
      "  time_this_iter_s: 6.828300476074219\n",
      "  time_total_s: 2061.281746864319\n",
      "  timestamp: 1552317306\n",
      "  timesteps_since_restore: 594000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 594000\n",
      "  training_iteration: 270\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2061 s, 270 iter, 594000 ts, 138 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-15-12\n",
      "  done: false\n",
      "  episode_len_mean: 94.26\n",
      "  episode_reward_max: 240.3997739803615\n",
      "  episode_reward_mean: 142.83754761982172\n",
      "  episode_reward_min: -163.50023133931617\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6072\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1304.007\n",
      "    load_time_ms: 3.355\n",
      "    num_steps_sampled: 596200\n",
      "    num_steps_trained: 596200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7348324656486511\n",
      "      kl: 0.013249917887151241\n",
      "      policy_loss: -0.006945109460502863\n",
      "      total_loss: 350.18817138671875\n",
      "      vf_explained_var: 0.7159825563430786\n",
      "      vf_loss: 350.19512939453125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.905642032623291\n",
      "      kl: 0.009466789662837982\n",
      "      policy_loss: -0.005538180470466614\n",
      "      total_loss: 708.54443359375\n",
      "      vf_explained_var: 0.6721726059913635\n",
      "      vf_loss: 708.5499877929688\n",
      "    sample_time_ms: 8452.011\n",
      "    update_time_ms: 11.326\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.380226906323\n",
      "    rl_1: 105.45732071349869\n",
      "  time_since_restore: 2067.778154850006\n",
      "  time_this_iter_s: 6.496407985687256\n",
      "  time_total_s: 2067.778154850006\n",
      "  timestamp: 1552317312\n",
      "  timesteps_since_restore: 596200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 596200\n",
      "  training_iteration: 271\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2067 s, 271 iter, 596200 ts, 143 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-15-19\n",
      "  done: false\n",
      "  episode_len_mean: 94.35\n",
      "  episode_reward_max: 240.3997739803615\n",
      "  episode_reward_mean: 143.01566735895264\n",
      "  episode_reward_min: -165.3178224103353\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6095\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1292.824\n",
      "    load_time_ms: 3.472\n",
      "    num_steps_sampled: 598400\n",
      "    num_steps_trained: 598400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9377985000610352\n",
      "      kl: 0.0052360473200678825\n",
      "      policy_loss: -0.009392445906996727\n",
      "      total_loss: 142.68551635742188\n",
      "      vf_explained_var: 0.8841250538825989\n",
      "      vf_loss: 142.69493103027344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9031102061271667\n",
      "      kl: 0.0024804449640214443\n",
      "      policy_loss: -0.004254330415278673\n",
      "      total_loss: 656.0104370117188\n",
      "      vf_explained_var: 0.7623977065086365\n",
      "      vf_loss: 656.0147094726562\n",
      "    sample_time_ms: 8371.644\n",
      "    update_time_ms: 11.164\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.01102608146234\n",
      "    rl_1: 106.00464127749026\n",
      "  time_since_restore: 2074.595860481262\n",
      "  time_this_iter_s: 6.8177056312561035\n",
      "  time_total_s: 2074.595860481262\n",
      "  timestamp: 1552317319\n",
      "  timesteps_since_restore: 598400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 598400\n",
      "  training_iteration: 272\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2074 s, 272 iter, 598400 ts, 143 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-15-25\n",
      "  done: false\n",
      "  episode_len_mean: 95.66\n",
      "  episode_reward_max: 242.32340813255155\n",
      "  episode_reward_mean: 155.05412352078392\n",
      "  episode_reward_min: -165.3178224103353\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6117\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1187.484\n",
      "    load_time_ms: 3.509\n",
      "    num_steps_sampled: 600600\n",
      "    num_steps_trained: 600600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9554998874664307\n",
      "      kl: 0.010792599990963936\n",
      "      policy_loss: -0.004160682205110788\n",
      "      total_loss: 255.5343475341797\n",
      "      vf_explained_var: 0.5979419946670532\n",
      "      vf_loss: 255.53851318359375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8946912288665771\n",
      "      kl: 0.004037490580230951\n",
      "      policy_loss: -0.006812044885009527\n",
      "      total_loss: 688.9584350585938\n",
      "      vf_explained_var: 0.43882885575294495\n",
      "      vf_loss: 688.9652099609375\n",
      "    sample_time_ms: 7961.406\n",
      "    update_time_ms: 11.353\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.174597792269466\n",
      "    rl_1: 113.87952572851442\n",
      "  time_since_restore: 2080.707659959793\n",
      "  time_this_iter_s: 6.111799478530884\n",
      "  time_total_s: 2080.707659959793\n",
      "  timestamp: 1552317325\n",
      "  timesteps_since_restore: 600600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 600600\n",
      "  training_iteration: 273\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2080 s, 273 iter, 600600 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-15-34\n",
      "  done: false\n",
      "  episode_len_mean: 97.01\n",
      "  episode_reward_max: 242.32340813255155\n",
      "  episode_reward_mean: 165.46653866212745\n",
      "  episode_reward_min: -165.3178224103353\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6139\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1091.958\n",
      "    load_time_ms: 3.083\n",
      "    num_steps_sampled: 602800\n",
      "    num_steps_trained: 602800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7360191941261292\n",
      "      kl: 0.007914533838629723\n",
      "      policy_loss: -0.002921317471191287\n",
      "      total_loss: 251.32818603515625\n",
      "      vf_explained_var: 0.7258291244506836\n",
      "      vf_loss: 251.33111572265625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9262893199920654\n",
      "      kl: 0.002292051911354065\n",
      "      policy_loss: -0.008497168309986591\n",
      "      total_loss: 598.6376953125\n",
      "      vf_explained_var: 0.6852115988731384\n",
      "      vf_loss: 598.6461791992188\n",
      "    sample_time_ms: 7372.716\n",
      "    update_time_ms: 10.43\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 46.30226309528029\n",
      "    rl_1: 119.16427556684712\n",
      "  time_since_restore: 2089.4808099269867\n",
      "  time_this_iter_s: 8.773149967193604\n",
      "  time_total_s: 2089.4808099269867\n",
      "  timestamp: 1552317334\n",
      "  timesteps_since_restore: 602800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 602800\n",
      "  training_iteration: 274\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2089 s, 274 iter, 602800 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-15-42\n",
      "  done: false\n",
      "  episode_len_mean: 97.59\n",
      "  episode_reward_max: 242.32340813255155\n",
      "  episode_reward_mean: 168.6753523939946\n",
      "  episode_reward_min: -165.3178224103353\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6162\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1037.945\n",
      "    load_time_ms: 3.066\n",
      "    num_steps_sampled: 605000\n",
      "    num_steps_trained: 605000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0060453414916992\n",
      "      kl: 0.0043619112111628056\n",
      "      policy_loss: -0.000280582724371925\n",
      "      total_loss: 111.6141586303711\n",
      "      vf_explained_var: 0.8938339352607727\n",
      "      vf_loss: 111.61444091796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.901732861995697\n",
      "      kl: 0.010761907324194908\n",
      "      policy_loss: -0.0038584922440350056\n",
      "      total_loss: 569.7354125976562\n",
      "      vf_explained_var: 0.7607801556587219\n",
      "      vf_loss: 569.7393188476562\n",
      "    sample_time_ms: 7005.423\n",
      "    update_time_ms: 10.016\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 46.603007179174256\n",
      "    rl_1: 122.07234521482034\n",
      "  time_since_restore: 2096.8859148025513\n",
      "  time_this_iter_s: 7.405104875564575\n",
      "  time_total_s: 2096.8859148025513\n",
      "  timestamp: 1552317342\n",
      "  timesteps_since_restore: 605000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 605000\n",
      "  training_iteration: 275\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2096 s, 275 iter, 605000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-15-49\n",
      "  done: false\n",
      "  episode_len_mean: 97.05\n",
      "  episode_reward_max: 242.32340813255155\n",
      "  episode_reward_mean: 165.58727841109499\n",
      "  episode_reward_min: -166.9033773441019\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 6187\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 984.75\n",
      "    load_time_ms: 3.09\n",
      "    num_steps_sampled: 607200\n",
      "    num_steps_trained: 607200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6730738282203674\n",
      "      kl: 0.005656453315168619\n",
      "      policy_loss: -0.009784527122974396\n",
      "      total_loss: 280.111572265625\n",
      "      vf_explained_var: 0.8409305810928345\n",
      "      vf_loss: 280.121337890625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8717313408851624\n",
      "      kl: 0.013667814433574677\n",
      "      policy_loss: -0.007892866618931293\n",
      "      total_loss: 710.0609130859375\n",
      "      vf_explained_var: 0.7797528505325317\n",
      "      vf_loss: 710.06884765625\n",
      "    sample_time_ms: 6830.585\n",
      "    update_time_ms: 9.228\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.27949615212793\n",
      "    rl_1: 120.30778225896705\n",
      "  time_since_restore: 2103.7819278240204\n",
      "  time_this_iter_s: 6.896013021469116\n",
      "  time_total_s: 2103.7819278240204\n",
      "  timestamp: 1552317349\n",
      "  timesteps_since_restore: 607200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 607200\n",
      "  training_iteration: 276\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2103 s, 276 iter, 607200 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-15-56\n",
      "  done: false\n",
      "  episode_len_mean: 95.89\n",
      "  episode_reward_max: 242.32340813255155\n",
      "  episode_reward_mean: 159.4534868496478\n",
      "  episode_reward_min: -166.9033773441019\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6210\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 921.352\n",
      "    load_time_ms: 3.087\n",
      "    num_steps_sampled: 609400\n",
      "    num_steps_trained: 609400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6604401469230652\n",
      "      kl: 0.004805362317711115\n",
      "      policy_loss: -0.0035482689272612333\n",
      "      total_loss: 322.824951171875\n",
      "      vf_explained_var: 0.7136344313621521\n",
      "      vf_loss: 322.8284606933594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8710892796516418\n",
      "      kl: 0.0046296813525259495\n",
      "      policy_loss: -0.0024809720925986767\n",
      "      total_loss: 698.40966796875\n",
      "      vf_explained_var: 0.681363582611084\n",
      "      vf_loss: 698.4121704101562\n",
      "    sample_time_ms: 6374.309\n",
      "    update_time_ms: 8.673\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.61143564879601\n",
      "    rl_1: 114.8420512008518\n",
      "  time_since_restore: 2110.6777651309967\n",
      "  time_this_iter_s: 6.895837306976318\n",
      "  time_total_s: 2110.6777651309967\n",
      "  timestamp: 1552317356\n",
      "  timesteps_since_restore: 609400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 609400\n",
      "  training_iteration: 277\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2110 s, 277 iter, 609400 ts, 159 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-16-02\n",
      "  done: false\n",
      "  episode_len_mean: 95.18\n",
      "  episode_reward_max: 241.53740883613193\n",
      "  episode_reward_mean: 154.67689687955806\n",
      "  episode_reward_min: -166.9033773441019\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6232\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 884.93\n",
      "    load_time_ms: 2.873\n",
      "    num_steps_sampled: 611600\n",
      "    num_steps_trained: 611600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8282044529914856\n",
      "      kl: 0.00580888707190752\n",
      "      policy_loss: -0.00353626417927444\n",
      "      total_loss: 108.79411315917969\n",
      "      vf_explained_var: 0.8773245811462402\n",
      "      vf_loss: 108.79763793945312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9056702256202698\n",
      "      kl: 0.021390456706285477\n",
      "      policy_loss: -0.010837405920028687\n",
      "      total_loss: 529.43310546875\n",
      "      vf_explained_var: 0.7586455345153809\n",
      "      vf_loss: 529.4439697265625\n",
      "    sample_time_ms: 6108.949\n",
      "    update_time_ms: 8.413\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.07894696446175\n",
      "    rl_1: 112.59794991509634\n",
      "  time_since_restore: 2117.2123548984528\n",
      "  time_this_iter_s: 6.534589767456055\n",
      "  time_total_s: 2117.2123548984528\n",
      "  timestamp: 1552317362\n",
      "  timesteps_since_restore: 611600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 611600\n",
      "  training_iteration: 278\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2117 s, 278 iter, 611600 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-16-12\n",
      "  done: false\n",
      "  episode_len_mean: 95.03\n",
      "  episode_reward_max: 241.53740883613193\n",
      "  episode_reward_mean: 152.36830849636678\n",
      "  episode_reward_min: -166.9033773441019\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6254\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 878.212\n",
      "    load_time_ms: 2.874\n",
      "    num_steps_sampled: 613800\n",
      "    num_steps_trained: 613800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0071049928665161\n",
      "      kl: 0.005805391818284988\n",
      "      policy_loss: -0.010100618936121464\n",
      "      total_loss: 160.60462951660156\n",
      "      vf_explained_var: 0.8302409648895264\n",
      "      vf_loss: 160.6147003173828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9005864262580872\n",
      "      kl: 0.004041471518576145\n",
      "      policy_loss: -0.004286047071218491\n",
      "      total_loss: 687.3681030273438\n",
      "      vf_explained_var: 0.6664723753929138\n",
      "      vf_loss: 687.3724975585938\n",
      "    sample_time_ms: 6380.373\n",
      "    update_time_ms: 8.142\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.254685410739256\n",
      "    rl_1: 111.11362308562751\n",
      "  time_since_restore: 2127.2625348567963\n",
      "  time_this_iter_s: 10.050179958343506\n",
      "  time_total_s: 2127.2625348567963\n",
      "  timestamp: 1552317372\n",
      "  timesteps_since_restore: 613800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 613800\n",
      "  training_iteration: 279\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2127 s, 279 iter, 613800 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-16-21\n",
      "  done: false\n",
      "  episode_len_mean: 96.12\n",
      "  episode_reward_max: 241.04115589475168\n",
      "  episode_reward_mean: 160.5424514373566\n",
      "  episode_reward_min: -166.9033773441019\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 6278\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 892.834\n",
      "    load_time_ms: 2.959\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8623655438423157\n",
      "      kl: 0.005027846898883581\n",
      "      policy_loss: -0.009568349458277225\n",
      "      total_loss: 188.16404724121094\n",
      "      vf_explained_var: 0.8087525963783264\n",
      "      vf_loss: 188.17361450195312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8844557404518127\n",
      "      kl: 0.007859076373279095\n",
      "      policy_loss: -0.005547692999243736\n",
      "      total_loss: 689.0814819335938\n",
      "      vf_explained_var: 0.6360478401184082\n",
      "      vf_loss: 689.0869750976562\n",
      "    sample_time_ms: 6541.664\n",
      "    update_time_ms: 7.882\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.16173563208849\n",
      "    rl_1: 116.38071580526812\n",
      "  time_since_restore: 2135.8503596782684\n",
      "  time_this_iter_s: 8.587824821472168\n",
      "  time_total_s: 2135.8503596782684\n",
      "  timestamp: 1552317381\n",
      "  timesteps_since_restore: 616000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 280\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2135 s, 280 iter, 616000 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-16-29\n",
      "  done: false\n",
      "  episode_len_mean: 97.19\n",
      "  episode_reward_max: 242.42764117982315\n",
      "  episode_reward_mean: 169.04280692635828\n",
      "  episode_reward_min: -166.77815644951988\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6300\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 907.399\n",
      "    load_time_ms: 3.027\n",
      "    num_steps_sampled: 618200\n",
      "    num_steps_trained: 618200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6870117783546448\n",
      "      kl: 0.007014391012489796\n",
      "      policy_loss: -0.006955924443900585\n",
      "      total_loss: 243.9099578857422\n",
      "      vf_explained_var: 0.7936432361602783\n",
      "      vf_loss: 243.91690063476562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8482711911201477\n",
      "      kl: 0.006839703768491745\n",
      "      policy_loss: -0.006240801885724068\n",
      "      total_loss: 579.7129516601562\n",
      "      vf_explained_var: 0.7685771584510803\n",
      "      vf_loss: 579.71923828125\n",
      "    sample_time_ms: 6719.835\n",
      "    update_time_ms: 7.963\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 47.55829514557769\n",
      "    rl_1: 121.48451178078057\n",
      "  time_since_restore: 2144.274633169174\n",
      "  time_this_iter_s: 8.424273490905762\n",
      "  time_total_s: 2144.274633169174\n",
      "  timestamp: 1552317389\n",
      "  timesteps_since_restore: 618200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 618200\n",
      "  training_iteration: 281\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2144 s, 281 iter, 618200 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 95.73\n",
      "  episode_reward_max: 242.42764117982315\n",
      "  episode_reward_mean: 153.72574655741332\n",
      "  episode_reward_min: -166.77815644951988\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 6324\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 914.058\n",
      "    load_time_ms: 3.026\n",
      "    num_steps_sampled: 620400\n",
      "    num_steps_trained: 620400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8058182597160339\n",
      "      kl: 0.011270017363131046\n",
      "      policy_loss: -0.0029749374371021986\n",
      "      total_loss: 141.51231384277344\n",
      "      vf_explained_var: 0.9188399910926819\n",
      "      vf_loss: 141.5152587890625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9090536236763\n",
      "      kl: 0.005448380950838327\n",
      "      policy_loss: -0.006080222316086292\n",
      "      total_loss: 511.2590637207031\n",
      "      vf_explained_var: 0.8604078888893127\n",
      "      vf_loss: 511.2651672363281\n",
      "    sample_time_ms: 6707.079\n",
      "    update_time_ms: 8.214\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.000422169360924\n",
      "    rl_1: 112.72532438805237\n",
      "  time_since_restore: 2151.035019636154\n",
      "  time_this_iter_s: 6.7603864669799805\n",
      "  time_total_s: 2151.035019636154\n",
      "  timestamp: 1552317396\n",
      "  timesteps_since_restore: 620400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 620400\n",
      "  training_iteration: 282\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2151 s, 282 iter, 620400 ts, 154 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-16-45\n",
      "  done: false\n",
      "  episode_len_mean: 96.38\n",
      "  episode_reward_max: 242.42764117982315\n",
      "  episode_reward_mean: 160.6199899121175\n",
      "  episode_reward_min: -166.77815644951988\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6346\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 957.246\n",
      "    load_time_ms: 3.227\n",
      "    num_steps_sampled: 622600\n",
      "    num_steps_trained: 622600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7953570485115051\n",
      "      kl: 0.004566174931824207\n",
      "      policy_loss: -0.002437399700284004\n",
      "      total_loss: 145.69607543945312\n",
      "      vf_explained_var: 0.8267080783843994\n",
      "      vf_loss: 145.69850158691406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8574687242507935\n",
      "      kl: 0.006050143390893936\n",
      "      policy_loss: -0.0022077837493270636\n",
      "      total_loss: 678.00927734375\n",
      "      vf_explained_var: 0.6969459652900696\n",
      "      vf_loss: 678.011474609375\n",
      "    sample_time_ms: 6969.715\n",
      "    update_time_ms: 8.116\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.864136164109475\n",
      "    rl_1: 115.75585374800801\n",
      "  time_since_restore: 2160.210160970688\n",
      "  time_this_iter_s: 9.175141334533691\n",
      "  time_total_s: 2160.210160970688\n",
      "  timestamp: 1552317405\n",
      "  timesteps_since_restore: 622600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 622600\n",
      "  training_iteration: 283\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2160 s, 283 iter, 622600 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-16-53\n",
      "  done: false\n",
      "  episode_len_mean: 96.52\n",
      "  episode_reward_max: 242.42764117982315\n",
      "  episode_reward_mean: 161.7518840983302\n",
      "  episode_reward_min: -166.77815644951988\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6369\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 916.759\n",
      "    load_time_ms: 3.338\n",
      "    num_steps_sampled: 624800\n",
      "    num_steps_trained: 624800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7990424633026123\n",
      "      kl: 0.008746695704758167\n",
      "      policy_loss: -0.006371496245265007\n",
      "      total_loss: 184.8863067626953\n",
      "      vf_explained_var: 0.844044029712677\n",
      "      vf_loss: 184.89266967773438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8484703302383423\n",
      "      kl: 0.01102464459836483\n",
      "      policy_loss: -0.00760204903781414\n",
      "      total_loss: 638.2920532226562\n",
      "      vf_explained_var: 0.7170419096946716\n",
      "      vf_loss: 638.2996826171875\n",
      "    sample_time_ms: 6867.857\n",
      "    update_time_ms: 8.444\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.95448970839321\n",
      "    rl_1: 115.79739438993705\n",
      "  time_since_restore: 2167.5607800483704\n",
      "  time_this_iter_s: 7.350619077682495\n",
      "  time_total_s: 2167.5607800483704\n",
      "  timestamp: 1552317413\n",
      "  timesteps_since_restore: 624800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 624800\n",
      "  training_iteration: 284\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2167 s, 284 iter, 624800 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-16-59\n",
      "  done: false\n",
      "  episode_len_mean: 96.02\n",
      "  episode_reward_max: 240.7784945780714\n",
      "  episode_reward_mean: 155.84864193327297\n",
      "  episode_reward_min: -144.00750629555708\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6392\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 929.176\n",
      "    load_time_ms: 3.249\n",
      "    num_steps_sampled: 627000\n",
      "    num_steps_trained: 627000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.911747395992279\n",
      "      kl: 0.010778870433568954\n",
      "      policy_loss: -0.005213033873587847\n",
      "      total_loss: 168.92306518554688\n",
      "      vf_explained_var: 0.8336772322654724\n",
      "      vf_loss: 168.92828369140625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8481595516204834\n",
      "      kl: 0.004375956486910582\n",
      "      policy_loss: -0.0013578437501564622\n",
      "      total_loss: 667.8350830078125\n",
      "      vf_explained_var: 0.6941457986831665\n",
      "      vf_loss: 667.83642578125\n",
      "    sample_time_ms: 6756.522\n",
      "    update_time_ms: 8.07\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.8308559892659\n",
      "    rl_1: 113.01778594400709\n",
      "  time_since_restore: 2173.9745512008667\n",
      "  time_this_iter_s: 6.413771152496338\n",
      "  time_total_s: 2173.9745512008667\n",
      "  timestamp: 1552317419\n",
      "  timesteps_since_restore: 627000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 627000\n",
      "  training_iteration: 285\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2173 s, 285 iter, 627000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-17-06\n",
      "  done: false\n",
      "  episode_len_mean: 95.51\n",
      "  episode_reward_max: 240.7784945780714\n",
      "  episode_reward_mean: 153.00543883373703\n",
      "  episode_reward_min: -143.17823973349053\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 6416\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 936.368\n",
      "    load_time_ms: 3.308\n",
      "    num_steps_sampled: 629200\n",
      "    num_steps_trained: 629200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8980569839477539\n",
      "      kl: 0.006389003712683916\n",
      "      policy_loss: -0.00246802787296474\n",
      "      total_loss: 825.3804321289062\n",
      "      vf_explained_var: 0.30312106013298035\n",
      "      vf_loss: 825.383056640625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8213974237442017\n",
      "      kl: 0.004671253263950348\n",
      "      policy_loss: -0.00251463963650167\n",
      "      total_loss: 1216.73681640625\n",
      "      vf_explained_var: 0.41448378562927246\n",
      "      vf_loss: 1216.7392578125\n",
      "    sample_time_ms: 6705.11\n",
      "    update_time_ms: 8.303\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.241200933759835\n",
      "    rl_1: 111.76423789997712\n",
      "  time_since_restore: 2180.4348607063293\n",
      "  time_this_iter_s: 6.4603095054626465\n",
      "  time_total_s: 2180.4348607063293\n",
      "  timestamp: 1552317426\n",
      "  timesteps_since_restore: 629200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 629200\n",
      "  training_iteration: 286\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2180 s, 286 iter, 629200 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-17-15\n",
      "  done: false\n",
      "  episode_len_mean: 93.78\n",
      "  episode_reward_max: 241.50050286320663\n",
      "  episode_reward_mean: 140.44822262287798\n",
      "  episode_reward_min: -156.53310042807374\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 6440\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 950.868\n",
      "    load_time_ms: 3.326\n",
      "    num_steps_sampled: 631400\n",
      "    num_steps_trained: 631400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8749194145202637\n",
      "      kl: 0.0069230711087584496\n",
      "      policy_loss: -0.002036552643403411\n",
      "      total_loss: 256.0854797363281\n",
      "      vf_explained_var: 0.8058825135231018\n",
      "      vf_loss: 256.0875549316406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8596687316894531\n",
      "      kl: 0.0047545889392495155\n",
      "      policy_loss: -0.00838780589401722\n",
      "      total_loss: 776.7901611328125\n",
      "      vf_explained_var: 0.6895861625671387\n",
      "      vf_loss: 776.7985229492188\n",
      "    sample_time_ms: 6949.435\n",
      "    update_time_ms: 8.551\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.10122908106709\n",
      "    rl_1: 104.34699354181086\n",
      "  time_since_restore: 2189.9248597621918\n",
      "  time_this_iter_s: 9.489999055862427\n",
      "  time_total_s: 2189.9248597621918\n",
      "  timestamp: 1552317435\n",
      "  timesteps_since_restore: 631400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 631400\n",
      "  training_iteration: 287\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2189 s, 287 iter, 631400 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-17-22\n",
      "  done: false\n",
      "  episode_len_mean: 92.49\n",
      "  episode_reward_max: 241.56952446750591\n",
      "  episode_reward_mean: 133.38539445937076\n",
      "  episode_reward_min: -160.1255205498794\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 6464\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 930.139\n",
      "    load_time_ms: 3.374\n",
      "    num_steps_sampled: 633600\n",
      "    num_steps_trained: 633600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.810914933681488\n",
      "      kl: 0.00539761409163475\n",
      "      policy_loss: 0.001068432116881013\n",
      "      total_loss: 200.96127319335938\n",
      "      vf_explained_var: 0.8350042104721069\n",
      "      vf_loss: 200.960205078125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8535147905349731\n",
      "      kl: 0.005853889510035515\n",
      "      policy_loss: -0.007432613987475634\n",
      "      total_loss: 655.7805786132812\n",
      "      vf_explained_var: 0.7140019536018372\n",
      "      vf_loss: 655.7879638671875\n",
      "    sample_time_ms: 6966.436\n",
      "    update_time_ms: 8.78\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.370667305183225\n",
      "    rl_1: 100.01472715418751\n",
      "  time_since_restore: 2196.4244351387024\n",
      "  time_this_iter_s: 6.49957537651062\n",
      "  time_total_s: 2196.4244351387024\n",
      "  timestamp: 1552317442\n",
      "  timesteps_since_restore: 633600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 633600\n",
      "  training_iteration: 288\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2196 s, 288 iter, 633600 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-17-29\n",
      "  done: false\n",
      "  episode_len_mean: 93.4\n",
      "  episode_reward_max: 241.56952446750591\n",
      "  episode_reward_mean: 140.1926944800371\n",
      "  episode_reward_min: -160.1255205498794\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 6485\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 920.454\n",
      "    load_time_ms: 3.21\n",
      "    num_steps_sampled: 635800\n",
      "    num_steps_trained: 635800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7445458173751831\n",
      "      kl: 0.005340264644473791\n",
      "      policy_loss: -0.0030651662964373827\n",
      "      total_loss: 142.74705505371094\n",
      "      vf_explained_var: 0.8105388879776001\n",
      "      vf_loss: 142.7501220703125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8614140748977661\n",
      "      kl: 0.005115805193781853\n",
      "      policy_loss: -0.005626994650810957\n",
      "      total_loss: 645.7761840820312\n",
      "      vf_explained_var: 0.6279318332672119\n",
      "      vf_loss: 645.78173828125\n",
      "    sample_time_ms: 6690.275\n",
      "    update_time_ms: 8.765\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.431172908122115\n",
      "    rl_1: 103.76152157191497\n",
      "  time_since_restore: 2203.612149477005\n",
      "  time_this_iter_s: 7.187714338302612\n",
      "  time_total_s: 2203.612149477005\n",
      "  timestamp: 1552317449\n",
      "  timesteps_since_restore: 635800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 635800\n",
      "  training_iteration: 289\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2203 s, 289 iter, 635800 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-17-36\n",
      "  done: false\n",
      "  episode_len_mean: 94.38\n",
      "  episode_reward_max: 241.56952446750591\n",
      "  episode_reward_mean: 147.43675680350105\n",
      "  episode_reward_min: -160.1255205498794\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 6509\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 913.012\n",
      "    load_time_ms: 3.12\n",
      "    num_steps_sampled: 638000\n",
      "    num_steps_trained: 638000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0154719352722168\n",
      "      kl: 0.0041975597850978374\n",
      "      policy_loss: -0.0069076488725841045\n",
      "      total_loss: 134.98129272460938\n",
      "      vf_explained_var: 0.8732975125312805\n",
      "      vf_loss: 134.9882049560547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8128251433372498\n",
      "      kl: 0.004042482003569603\n",
      "      policy_loss: -0.0065294126980006695\n",
      "      total_loss: 592.3106689453125\n",
      "      vf_explained_var: 0.7427396774291992\n",
      "      vf_loss: 592.3171997070312\n",
      "    sample_time_ms: 6478.416\n",
      "    update_time_ms: 8.677\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.93160998513072\n",
      "    rl_1: 108.50514681837032\n",
      "  time_since_restore: 2210.0045042037964\n",
      "  time_this_iter_s: 6.392354726791382\n",
      "  time_total_s: 2210.0045042037964\n",
      "  timestamp: 1552317456\n",
      "  timesteps_since_restore: 638000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 638000\n",
      "  training_iteration: 290\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2210 s, 290 iter, 638000 ts, 147 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-17-46\n",
      "  done: false\n",
      "  episode_len_mean: 95.17\n",
      "  episode_reward_max: 241.56952446750591\n",
      "  episode_reward_mean: 154.54584786343835\n",
      "  episode_reward_min: -162.84056122736183\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6532\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 906.557\n",
      "    load_time_ms: 3.048\n",
      "    num_steps_sampled: 640200\n",
      "    num_steps_trained: 640200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.054412841796875\n",
      "      kl: 0.012331696227192879\n",
      "      policy_loss: -0.004923365544527769\n",
      "      total_loss: 320.165283203125\n",
      "      vf_explained_var: 0.6448880434036255\n",
      "      vf_loss: 320.1701965332031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8383931517601013\n",
      "      kl: 0.009329849854111671\n",
      "      policy_loss: -0.003883783472701907\n",
      "      total_loss: 787.00732421875\n",
      "      vf_explained_var: 0.560793936252594\n",
      "      vf_loss: 787.01123046875\n",
      "    sample_time_ms: 6652.755\n",
      "    update_time_ms: 8.683\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.290284909880626\n",
      "    rl_1: 113.25556295355771\n",
      "  time_since_restore: 2220.108945131302\n",
      "  time_this_iter_s: 10.104440927505493\n",
      "  time_total_s: 2220.108945131302\n",
      "  timestamp: 1552317466\n",
      "  timesteps_since_restore: 640200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 640200\n",
      "  training_iteration: 291\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2220 s, 291 iter, 640200 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-17-56\n",
      "  done: false\n",
      "  episode_len_mean: 96.85\n",
      "  episode_reward_max: 241.56952446750591\n",
      "  episode_reward_mean: 165.79882772902678\n",
      "  episode_reward_min: -162.84056122736183\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6554\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 910.17\n",
      "    load_time_ms: 3.054\n",
      "    num_steps_sampled: 642400\n",
      "    num_steps_trained: 642400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.926862359046936\n",
      "      kl: 0.013233249075710773\n",
      "      policy_loss: -0.006353347562253475\n",
      "      total_loss: 93.11853790283203\n",
      "      vf_explained_var: 0.8997406363487244\n",
      "      vf_loss: 93.12489318847656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7706196308135986\n",
      "      kl: 0.006538171786814928\n",
      "      policy_loss: -0.004641968756914139\n",
      "      total_loss: 475.44305419921875\n",
      "      vf_explained_var: 0.7780209183692932\n",
      "      vf_loss: 475.44769287109375\n",
      "    sample_time_ms: 6998.496\n",
      "    update_time_ms: 9.241\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.70479243175394\n",
      "    rl_1: 120.09403529727284\n",
      "  time_since_restore: 2230.367961168289\n",
      "  time_this_iter_s: 10.259016036987305\n",
      "  time_total_s: 2230.367961168289\n",
      "  timestamp: 1552317476\n",
      "  timesteps_since_restore: 642400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 642400\n",
      "  training_iteration: 292\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2230 s, 292 iter, 642400 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-18-03\n",
      "  done: false\n",
      "  episode_len_mean: 97.0\n",
      "  episode_reward_max: 241.2377389356343\n",
      "  episode_reward_mean: 163.28354724540182\n",
      "  episode_reward_min: -162.84056122736183\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6577\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 873.708\n",
      "    load_time_ms: 2.789\n",
      "    num_steps_sampled: 644600\n",
      "    num_steps_trained: 644600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.969736635684967\n",
      "      kl: 0.004614442586898804\n",
      "      policy_loss: -0.004917612764984369\n",
      "      total_loss: 289.0762023925781\n",
      "      vf_explained_var: 0.7064882516860962\n",
      "      vf_loss: 289.08111572265625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7705616354942322\n",
      "      kl: 0.007382045034319162\n",
      "      policy_loss: -0.006349589675664902\n",
      "      total_loss: 654.5875244140625\n",
      "      vf_explained_var: 0.6619153618812561\n",
      "      vf_loss: 654.5939331054688\n",
      "    sample_time_ms: 6845.229\n",
      "    update_time_ms: 9.209\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.309799097577425\n",
      "    rl_1: 119.97374814782441\n",
      "  time_since_restore: 2237.638571023941\n",
      "  time_this_iter_s: 7.2706098556518555\n",
      "  time_total_s: 2237.638571023941\n",
      "  timestamp: 1552317483\n",
      "  timesteps_since_restore: 644600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 644600\n",
      "  training_iteration: 293\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2237 s, 293 iter, 644600 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-18-15\n",
      "  done: false\n",
      "  episode_len_mean: 97.38\n",
      "  episode_reward_max: 239.90582230119387\n",
      "  episode_reward_mean: 167.479772718803\n",
      "  episode_reward_min: -162.84056122736183\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6599\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 912.932\n",
      "    load_time_ms: 2.66\n",
      "    num_steps_sampled: 646800\n",
      "    num_steps_trained: 646800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8446723818778992\n",
      "      kl: 0.011072027496993542\n",
      "      policy_loss: 0.0006303112022578716\n",
      "      total_loss: 126.18753051757812\n",
      "      vf_explained_var: 0.8006407022476196\n",
      "      vf_loss: 126.18689727783203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8092665672302246\n",
      "      kl: 0.014333164319396019\n",
      "      policy_loss: -0.010323654860258102\n",
      "      total_loss: 514.4766235351562\n",
      "      vf_explained_var: 0.6635007262229919\n",
      "      vf_loss: 514.4868774414062\n",
      "    sample_time_ms: 7239.409\n",
      "    update_time_ms: 8.779\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.067973175894416\n",
      "    rl_1: 122.41179954290861\n",
      "  time_since_restore: 2249.3216695785522\n",
      "  time_this_iter_s: 11.683098554611206\n",
      "  time_total_s: 2249.3216695785522\n",
      "  timestamp: 1552317495\n",
      "  timesteps_since_restore: 646800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 646800\n",
      "  training_iteration: 294\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2249 s, 294 iter, 646800 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-18-24\n",
      "  done: false\n",
      "  episode_len_mean: 97.42\n",
      "  episode_reward_max: 240.2476557391337\n",
      "  episode_reward_mean: 168.65528857621484\n",
      "  episode_reward_min: -162.84056122736183\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6621\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 935.523\n",
      "    load_time_ms: 2.689\n",
      "    num_steps_sampled: 649000\n",
      "    num_steps_trained: 649000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8801180124282837\n",
      "      kl: 0.003779039718210697\n",
      "      policy_loss: 0.001234059571288526\n",
      "      total_loss: 108.4666976928711\n",
      "      vf_explained_var: 0.8743315935134888\n",
      "      vf_loss: 108.46546173095703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7647367119789124\n",
      "      kl: 0.007050512824207544\n",
      "      policy_loss: -0.007090871222317219\n",
      "      total_loss: 532.916259765625\n",
      "      vf_explained_var: 0.7349576950073242\n",
      "      vf_loss: 532.92333984375\n",
      "    sample_time_ms: 7488.817\n",
      "    update_time_ms: 9.504\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.56400325288655\n",
      "    rl_1: 123.09128532332826\n",
      "  time_since_restore: 2258.4639427661896\n",
      "  time_this_iter_s: 9.142273187637329\n",
      "  time_total_s: 2258.4639427661896\n",
      "  timestamp: 1552317504\n",
      "  timesteps_since_restore: 649000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 649000\n",
      "  training_iteration: 295\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2258 s, 295 iter, 649000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-18-34\n",
      "  done: false\n",
      "  episode_len_mean: 98.43\n",
      "  episode_reward_max: 241.10785749638393\n",
      "  episode_reward_mean: 176.50630507853822\n",
      "  episode_reward_min: -156.92694781671938\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6644\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1001.041\n",
      "    load_time_ms: 2.705\n",
      "    num_steps_sampled: 651200\n",
      "    num_steps_trained: 651200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9433578252792358\n",
      "      kl: 0.006538587622344494\n",
      "      policy_loss: -0.0063359858468174934\n",
      "      total_loss: 252.9691162109375\n",
      "      vf_explained_var: 0.6652718782424927\n",
      "      vf_loss: 252.9754638671875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7642335891723633\n",
      "      kl: 0.01257732417434454\n",
      "      policy_loss: -0.006586811970919371\n",
      "      total_loss: 625.4492797851562\n",
      "      vf_explained_var: 0.5611553192138672\n",
      "      vf_loss: 625.4558715820312\n",
      "    sample_time_ms: 7708.358\n",
      "    update_time_ms: 9.824\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.04364061462998\n",
      "    rl_1: 127.46266446390817\n",
      "  time_since_restore: 2267.7858533859253\n",
      "  time_this_iter_s: 9.321910619735718\n",
      "  time_total_s: 2267.7858533859253\n",
      "  timestamp: 1552317514\n",
      "  timesteps_since_restore: 651200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 651200\n",
      "  training_iteration: 296\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2267 s, 296 iter, 651200 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-18-46\n",
      "  done: false\n",
      "  episode_len_mean: 98.57\n",
      "  episode_reward_max: 241.10785749638393\n",
      "  episode_reward_mean: 179.27405168724178\n",
      "  episode_reward_min: -156.92694781671938\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6666\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1016.37\n",
      "    load_time_ms: 2.702\n",
      "    num_steps_sampled: 653400\n",
      "    num_steps_trained: 653400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5411501526832581\n",
      "      kl: 0.0060673863627016544\n",
      "      policy_loss: -0.0005009586457163095\n",
      "      total_loss: 121.60120391845703\n",
      "      vf_explained_var: 0.8608548045158386\n",
      "      vf_loss: 121.6017074584961\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.722545862197876\n",
      "      kl: 0.006016827654093504\n",
      "      policy_loss: -0.008765878155827522\n",
      "      total_loss: 510.54388427734375\n",
      "      vf_explained_var: 0.7687674164772034\n",
      "      vf_loss: 510.55267333984375\n",
      "    sample_time_ms: 7920.82\n",
      "    update_time_ms: 10.132\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.558855962101745\n",
      "    rl_1: 127.71519572514\n",
      "  time_since_restore: 2279.556225299835\n",
      "  time_this_iter_s: 11.770371913909912\n",
      "  time_total_s: 2279.556225299835\n",
      "  timestamp: 1552317526\n",
      "  timesteps_since_restore: 653400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 653400\n",
      "  training_iteration: 297\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2279 s, 297 iter, 653400 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-18-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.05\n",
      "  episode_reward_max: 241.10785749638393\n",
      "  episode_reward_mean: 191.7882020563684\n",
      "  episode_reward_min: -156.92694781671938\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 6687\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1175.08\n",
      "    load_time_ms: 2.753\n",
      "    num_steps_sampled: 655600\n",
      "    num_steps_trained: 655600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5730023980140686\n",
      "      kl: 0.017840629443526268\n",
      "      policy_loss: -0.013324831612408161\n",
      "      total_loss: 44.642730712890625\n",
      "      vf_explained_var: 0.9214396476745605\n",
      "      vf_loss: 44.65605545043945\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7383031249046326\n",
      "      kl: 0.006914640311151743\n",
      "      policy_loss: -0.0018690131837502122\n",
      "      total_loss: 389.7637023925781\n",
      "      vf_explained_var: 0.7772544026374817\n",
      "      vf_loss: 389.76556396484375\n",
      "    sample_time_ms: 8410.474\n",
      "    update_time_ms: 10.347\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.58706558545564\n",
      "    rl_1: 134.20113647091276\n",
      "  time_since_restore: 2292.5593645572662\n",
      "  time_this_iter_s: 13.00313925743103\n",
      "  time_total_s: 2292.5593645572662\n",
      "  timestamp: 1552317539\n",
      "  timesteps_since_restore: 655600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 655600\n",
      "  training_iteration: 298\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2292 s, 298 iter, 655600 ts, 192 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-19-10\n",
      "  done: false\n",
      "  episode_len_mean: 99.12\n",
      "  episode_reward_max: 241.10785749638393\n",
      "  episode_reward_mean: 184.06563451455895\n",
      "  episode_reward_min: -156.92694781671938\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 6711\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1231.173\n",
      "    load_time_ms: 3.07\n",
      "    num_steps_sampled: 657800\n",
      "    num_steps_trained: 657800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8983110785484314\n",
      "      kl: 0.005523305851966143\n",
      "      policy_loss: -0.004182064440101385\n",
      "      total_loss: 297.30120849609375\n",
      "      vf_explained_var: 0.7163757681846619\n",
      "      vf_loss: 297.305419921875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7329078316688538\n",
      "      kl: 0.009909114800393581\n",
      "      policy_loss: -0.007937092334032059\n",
      "      total_loss: 640.068359375\n",
      "      vf_explained_var: 0.6924760341644287\n",
      "      vf_loss: 640.0762939453125\n",
      "    sample_time_ms: 8764.338\n",
      "    update_time_ms: 13.874\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.65728033400529\n",
      "    rl_1: 129.40835418055363\n",
      "  time_since_restore: 2303.893541574478\n",
      "  time_this_iter_s: 11.334177017211914\n",
      "  time_total_s: 2303.893541574478\n",
      "  timestamp: 1552317550\n",
      "  timesteps_since_restore: 657800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 657800\n",
      "  training_iteration: 299\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2303 s, 299 iter, 657800 ts, 184 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-19-26\n",
      "  done: false\n",
      "  episode_len_mean: 99.45\n",
      "  episode_reward_max: 240.46119583839584\n",
      "  episode_reward_mean: 187.22468723397685\n",
      "  episode_reward_min: -146.23380305212953\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 6732\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1324.776\n",
      "    load_time_ms: 3.246\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5772624015808105\n",
      "      kl: 0.013520817272365093\n",
      "      policy_loss: -0.00824824534356594\n",
      "      total_loss: 107.59130859375\n",
      "      vf_explained_var: 0.8887701630592346\n",
      "      vf_loss: 107.59955596923828\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6728540658950806\n",
      "      kl: 0.010587979108095169\n",
      "      policy_loss: -0.0100084925070405\n",
      "      total_loss: 512.909912109375\n",
      "      vf_explained_var: 0.7991218566894531\n",
      "      vf_loss: 512.919921875\n",
      "    sample_time_ms: 9572.979\n",
      "    update_time_ms: 14.644\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.87112982646458\n",
      "    rl_1: 130.35355740751223\n",
      "  time_since_restore: 2319.3297970294952\n",
      "  time_this_iter_s: 15.43625545501709\n",
      "  time_total_s: 2319.3297970294952\n",
      "  timestamp: 1552317566\n",
      "  timesteps_since_restore: 660000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 300\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2319 s, 300 iter, 660000 ts, 187 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-19-36\n",
      "  done: false\n",
      "  episode_len_mean: 96.16\n",
      "  episode_reward_max: 240.46119583839584\n",
      "  episode_reward_mean: 165.39361313993223\n",
      "  episode_reward_min: -161.75009473434432\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 6758\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1350.866\n",
      "    load_time_ms: 3.326\n",
      "    num_steps_sampled: 662200\n",
      "    num_steps_trained: 662200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6951264142990112\n",
      "      kl: 0.008009171113371849\n",
      "      policy_loss: -0.005029126536101103\n",
      "      total_loss: 684.6390380859375\n",
      "      vf_explained_var: 0.5484218001365662\n",
      "      vf_loss: 684.64404296875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.613900899887085\n",
      "      kl: 0.006284814793616533\n",
      "      policy_loss: -1.3652531379193533e-05\n",
      "      total_loss: 1226.072998046875\n",
      "      vf_explained_var: 0.5678099393844604\n",
      "      vf_loss: 1226.0731201171875\n",
      "    sample_time_ms: 9553.411\n",
      "    update_time_ms: 15.754\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 47.5017565006729\n",
      "    rl_1: 117.89185663925934\n",
      "  time_since_restore: 2329.5107221603394\n",
      "  time_this_iter_s: 10.180925130844116\n",
      "  time_total_s: 2329.5107221603394\n",
      "  timestamp: 1552317576\n",
      "  timesteps_since_restore: 662200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 662200\n",
      "  training_iteration: 301\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2329 s, 301 iter, 662200 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-19-45\n",
      "  done: false\n",
      "  episode_len_mean: 94.57\n",
      "  episode_reward_max: 241.0688495800505\n",
      "  episode_reward_mean: 151.88881484883785\n",
      "  episode_reward_min: -161.75009473434432\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6781\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1444.577\n",
      "    load_time_ms: 3.399\n",
      "    num_steps_sampled: 664400\n",
      "    num_steps_trained: 664400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8068381547927856\n",
      "      kl: 0.0075896442867815495\n",
      "      policy_loss: -0.004167054779827595\n",
      "      total_loss: 306.0448913574219\n",
      "      vf_explained_var: 0.6676530838012695\n",
      "      vf_loss: 306.049072265625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6682241559028625\n",
      "      kl: 0.0050397696904838085\n",
      "      policy_loss: -0.00485947635024786\n",
      "      total_loss: 755.7574462890625\n",
      "      vf_explained_var: 0.6071183681488037\n",
      "      vf_loss: 755.7623291015625\n",
      "    sample_time_ms: 9334.077\n",
      "    update_time_ms: 14.978\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.51962062042489\n",
      "    rl_1: 111.36919422841297\n",
      "  time_since_restore: 2338.5101416110992\n",
      "  time_this_iter_s: 8.999419450759888\n",
      "  time_total_s: 2338.5101416110992\n",
      "  timestamp: 1552317585\n",
      "  timesteps_since_restore: 664400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 664400\n",
      "  training_iteration: 302\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2338 s, 302 iter, 664400 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-19-56\n",
      "  done: false\n",
      "  episode_len_mean: 94.73\n",
      "  episode_reward_max: 241.0688495800505\n",
      "  episode_reward_mean: 151.30025285712335\n",
      "  episode_reward_min: -161.75009473434432\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6803\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1481.463\n",
      "    load_time_ms: 3.39\n",
      "    num_steps_sampled: 666600\n",
      "    num_steps_trained: 666600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8905641436576843\n",
      "      kl: 0.011170747689902782\n",
      "      policy_loss: -0.00045318988850340247\n",
      "      total_loss: 233.51611328125\n",
      "      vf_explained_var: 0.6858230829238892\n",
      "      vf_loss: 233.51657104492188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7039361000061035\n",
      "      kl: 0.006829988211393356\n",
      "      policy_loss: -0.002705216407775879\n",
      "      total_loss: 507.076904296875\n",
      "      vf_explained_var: 0.6709980368614197\n",
      "      vf_loss: 507.0796203613281\n",
      "    sample_time_ms: 9686.852\n",
      "    update_time_ms: 15.781\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.967615564711416\n",
      "    rl_1: 111.33263729241189\n",
      "  time_since_restore: 2349.689310312271\n",
      "  time_this_iter_s: 11.179168701171875\n",
      "  time_total_s: 2349.689310312271\n",
      "  timestamp: 1552317596\n",
      "  timesteps_since_restore: 666600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 666600\n",
      "  training_iteration: 303\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2349 s, 303 iter, 666600 ts, 151 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-20-15\n",
      "  done: false\n",
      "  episode_len_mean: 93.96\n",
      "  episode_reward_max: 241.0688495800505\n",
      "  episode_reward_mean: 148.60204694930292\n",
      "  episode_reward_min: -161.75009473434432\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 6827\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1658.35\n",
      "    load_time_ms: 4.247\n",
      "    num_steps_sampled: 668800\n",
      "    num_steps_trained: 668800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.415161669254303\n",
      "      kl: 0.01699666865170002\n",
      "      policy_loss: -0.013179786503314972\n",
      "      total_loss: 459.4052734375\n",
      "      vf_explained_var: 0.6569268703460693\n",
      "      vf_loss: 459.4184265136719\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6454821228981018\n",
      "      kl: 0.0046987575478851795\n",
      "      policy_loss: -0.003340347670018673\n",
      "      total_loss: 900.9403686523438\n",
      "      vf_explained_var: 0.644288182258606\n",
      "      vf_loss: 900.9437255859375\n",
      "    sample_time_ms: 10164.957\n",
      "    update_time_ms: 16.968\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.35961414455441\n",
      "    rl_1: 109.24243280474847\n",
      "  time_since_restore: 2367.9612741470337\n",
      "  time_this_iter_s: 18.271963834762573\n",
      "  time_total_s: 2367.9612741470337\n",
      "  timestamp: 1552317615\n",
      "  timesteps_since_restore: 668800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 668800\n",
      "  training_iteration: 304\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2367 s, 304 iter, 668800 ts, 149 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-20-41\n",
      "  done: false\n",
      "  episode_len_mean: 93.86\n",
      "  episode_reward_max: 241.0688495800505\n",
      "  episode_reward_mean: 143.59680336950817\n",
      "  episode_reward_min: -159.38073035624393\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 6851\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2013.164\n",
      "    load_time_ms: 4.838\n",
      "    num_steps_sampled: 671000\n",
      "    num_steps_trained: 671000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7654991149902344\n",
      "      kl: 0.005762824323028326\n",
      "      policy_loss: -0.00902276299893856\n",
      "      total_loss: 273.0785827636719\n",
      "      vf_explained_var: 0.7906007766723633\n",
      "      vf_loss: 273.0876159667969\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7067963480949402\n",
      "      kl: 0.007290740497410297\n",
      "      policy_loss: -0.0012198361800983548\n",
      "      total_loss: 727.0974731445312\n",
      "      vf_explained_var: 0.7284637093544006\n",
      "      vf_loss: 727.0987548828125\n",
      "    sample_time_ms: 11504.196\n",
      "    update_time_ms: 18.973\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.49832645312105\n",
      "    rl_1: 106.0984769163871\n",
      "  time_since_restore: 2394.109297513962\n",
      "  time_this_iter_s: 26.1480233669281\n",
      "  time_total_s: 2394.109297513962\n",
      "  timestamp: 1552317641\n",
      "  timesteps_since_restore: 671000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 671000\n",
      "  training_iteration: 305\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2394 s, 305 iter, 671000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-21-04\n",
      "  done: false\n",
      "  episode_len_mean: 95.27\n",
      "  episode_reward_max: 241.0688495800505\n",
      "  episode_reward_mean: 152.89230931931488\n",
      "  episode_reward_min: -158.89688304338932\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6874\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2112.596\n",
      "    load_time_ms: 4.886\n",
      "    num_steps_sampled: 673200\n",
      "    num_steps_trained: 673200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7380856275558472\n",
      "      kl: 0.00711791031062603\n",
      "      policy_loss: 0.0078797722235322\n",
      "      total_loss: 94.13006591796875\n",
      "      vf_explained_var: 0.9232995510101318\n",
      "      vf_loss: 94.12218475341797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6854118704795837\n",
      "      kl: 0.0030781084205955267\n",
      "      policy_loss: -0.00243086414411664\n",
      "      total_loss: 374.58367919921875\n",
      "      vf_explained_var: 0.8712908029556274\n",
      "      vf_loss: 374.58612060546875\n",
      "    sample_time_ms: 12784.542\n",
      "    update_time_ms: 21.056\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.82528062791844\n",
      "    rl_1: 111.06702869139644\n",
      "  time_since_restore: 2417.255359888077\n",
      "  time_this_iter_s: 23.14606237411499\n",
      "  time_total_s: 2417.255359888077\n",
      "  timestamp: 1552317664\n",
      "  timesteps_since_restore: 673200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 673200\n",
      "  training_iteration: 306\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2417 s, 306 iter, 673200 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-21-21\n",
      "  done: false\n",
      "  episode_len_mean: 93.86\n",
      "  episode_reward_max: 239.48802133219127\n",
      "  episode_reward_mean: 145.7785034664306\n",
      "  episode_reward_min: -159.53172584659401\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6897\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2171.815\n",
      "    load_time_ms: 5.038\n",
      "    num_steps_sampled: 675400\n",
      "    num_steps_trained: 675400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6753208637237549\n",
      "      kl: 0.006203643511980772\n",
      "      policy_loss: -0.0012440619757398963\n",
      "      total_loss: 153.8887176513672\n",
      "      vf_explained_var: 0.8547849059104919\n",
      "      vf_loss: 153.8899688720703\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6189103126525879\n",
      "      kl: 0.012427834793925285\n",
      "      policy_loss: -0.006174728274345398\n",
      "      total_loss: 603.69775390625\n",
      "      vf_explained_var: 0.7280539274215698\n",
      "      vf_loss: 603.703857421875\n",
      "    sample_time_ms: 13177.638\n",
      "    update_time_ms: 21.417\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.852365470368646\n",
      "    rl_1: 106.92613799606198\n",
      "  time_since_restore: 2433.560572862625\n",
      "  time_this_iter_s: 16.30521297454834\n",
      "  time_total_s: 2433.560572862625\n",
      "  timestamp: 1552317681\n",
      "  timesteps_since_restore: 675400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 675400\n",
      "  training_iteration: 307\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2433 s, 307 iter, 675400 ts, 146 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-21-36\n",
      "  done: false\n",
      "  episode_len_mean: 93.28\n",
      "  episode_reward_max: 239.48802133219127\n",
      "  episode_reward_mean: 141.20987158975305\n",
      "  episode_reward_min: -163.83137908324233\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 6921\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2141.861\n",
      "    load_time_ms: 5.099\n",
      "    num_steps_sampled: 677600\n",
      "    num_steps_trained: 677600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7181817293167114\n",
      "      kl: 0.009937888942658901\n",
      "      policy_loss: -0.004441781900823116\n",
      "      total_loss: 322.3001403808594\n",
      "      vf_explained_var: 0.7859659194946289\n",
      "      vf_loss: 322.3045959472656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.585806667804718\n",
      "      kl: 0.005272418260574341\n",
      "      policy_loss: -0.0016479474725201726\n",
      "      total_loss: 737.96533203125\n",
      "      vf_explained_var: 0.7680653929710388\n",
      "      vf_loss: 737.9671020507812\n",
      "    sample_time_ms: 13442.129\n",
      "    update_time_ms: 21.829\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.958774071284346\n",
      "    rl_1: 105.25109751846871\n",
      "  time_since_restore: 2448.926196336746\n",
      "  time_this_iter_s: 15.365623474121094\n",
      "  time_total_s: 2448.926196336746\n",
      "  timestamp: 1552317696\n",
      "  timesteps_since_restore: 677600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 677600\n",
      "  training_iteration: 308\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2448 s, 308 iter, 677600 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-21-46\n",
      "  done: false\n",
      "  episode_len_mean: 95.01\n",
      "  episode_reward_max: 239.4030453005124\n",
      "  episode_reward_mean: 155.12342584380474\n",
      "  episode_reward_min: -163.83137908324233\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6944\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 2072.064\n",
      "    load_time_ms: 4.886\n",
      "    num_steps_sampled: 679800\n",
      "    num_steps_trained: 679800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7040818333625793\n",
      "      kl: 0.01604265719652176\n",
      "      policy_loss: -0.009166000410914421\n",
      "      total_loss: 210.27032470703125\n",
      "      vf_explained_var: 0.701825737953186\n",
      "      vf_loss: 210.2794647216797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6399173140525818\n",
      "      kl: 0.00746166193857789\n",
      "      policy_loss: -0.009458883665502071\n",
      "      total_loss: 574.9368896484375\n",
      "      vf_explained_var: 0.6438641548156738\n",
      "      vf_loss: 574.9463500976562\n",
      "    sample_time_ms: 13391.165\n",
      "    update_time_ms: 19.671\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.211878300640784\n",
      "    rl_1: 113.91154754316392\n",
      "  time_since_restore: 2459.022947072983\n",
      "  time_this_iter_s: 10.096750736236572\n",
      "  time_total_s: 2459.022947072983\n",
      "  timestamp: 1552317706\n",
      "  timesteps_since_restore: 679800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 679800\n",
      "  training_iteration: 309\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2459 s, 309 iter, 679800 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-21-53\n",
      "  done: false\n",
      "  episode_len_mean: 95.77\n",
      "  episode_reward_max: 240.71794314336165\n",
      "  episode_reward_mean: 162.58800414084598\n",
      "  episode_reward_min: -163.83137908324233\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 6966\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1972.027\n",
      "    load_time_ms: 4.665\n",
      "    num_steps_sampled: 682000\n",
      "    num_steps_trained: 682000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.580166220664978\n",
      "      kl: 0.0038735498674213886\n",
      "      policy_loss: 0.00043294273200444877\n",
      "      total_loss: 193.24705505371094\n",
      "      vf_explained_var: 0.8049944043159485\n",
      "      vf_loss: 193.24664306640625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6015236377716064\n",
      "      kl: 0.006643643602728844\n",
      "      policy_loss: -0.006740720942616463\n",
      "      total_loss: 686.2849731445312\n",
      "      vf_explained_var: 0.6850208640098572\n",
      "      vf_loss: 686.2918090820312\n",
      "    sample_time_ms: 12609.676\n",
      "    update_time_ms: 19.279\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.314163014035\n",
      "    rl_1: 118.27384112681095\n",
      "  time_since_restore: 2465.624900341034\n",
      "  time_this_iter_s: 6.6019532680511475\n",
      "  time_total_s: 2465.624900341034\n",
      "  timestamp: 1552317713\n",
      "  timesteps_since_restore: 682000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 682000\n",
      "  training_iteration: 310\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2465 s, 310 iter, 682000 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-22-00\n",
      "  done: false\n",
      "  episode_len_mean: 95.21\n",
      "  episode_reward_max: 241.70994830773782\n",
      "  episode_reward_mean: 157.65732516128824\n",
      "  episode_reward_min: -163.83137908324233\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 6989\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1938.261\n",
      "    load_time_ms: 4.629\n",
      "    num_steps_sampled: 684200\n",
      "    num_steps_trained: 684200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4376737177371979\n",
      "      kl: 0.009363370016217232\n",
      "      policy_loss: -0.01414670329540968\n",
      "      total_loss: 427.8132629394531\n",
      "      vf_explained_var: 0.5941596627235413\n",
      "      vf_loss: 427.8274230957031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5962896943092346\n",
      "      kl: 0.007719516754150391\n",
      "      policy_loss: -0.0016812636749818921\n",
      "      total_loss: 746.0086059570312\n",
      "      vf_explained_var: 0.6432504057884216\n",
      "      vf_loss: 746.01025390625\n",
      "    sample_time_ms: 12303.592\n",
      "    update_time_ms: 17.929\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.92877142631605\n",
      "    rl_1: 114.72855373497221\n",
      "  time_since_restore: 2472.3923692703247\n",
      "  time_this_iter_s: 6.7674689292907715\n",
      "  time_total_s: 2472.3923692703247\n",
      "  timestamp: 1552317720\n",
      "  timesteps_since_restore: 684200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 684200\n",
      "  training_iteration: 311\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2472 s, 311 iter, 684200 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-22-06\n",
      "  done: false\n",
      "  episode_len_mean: 97.09\n",
      "  episode_reward_max: 241.70994830773782\n",
      "  episode_reward_mean: 170.33667327943175\n",
      "  episode_reward_min: -163.83137908324233\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7012\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1853.009\n",
      "    load_time_ms: 4.512\n",
      "    num_steps_sampled: 686400\n",
      "    num_steps_trained: 686400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8665356636047363\n",
      "      kl: 0.015255819074809551\n",
      "      policy_loss: -0.009357571601867676\n",
      "      total_loss: 349.247802734375\n",
      "      vf_explained_var: 0.5058919787406921\n",
      "      vf_loss: 349.2571716308594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5926772356033325\n",
      "      kl: 0.017228558659553528\n",
      "      policy_loss: -0.0066457996144890785\n",
      "      total_loss: 715.19140625\n",
      "      vf_explained_var: 0.5249230861663818\n",
      "      vf_loss: 715.1980590820312\n",
      "    sample_time_ms: 12139.153\n",
      "    update_time_ms: 18.173\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 47.64506486101527\n",
      "    rl_1: 122.69160841841651\n",
      "  time_since_restore: 2478.895626306534\n",
      "  time_this_iter_s: 6.5032570362091064\n",
      "  time_total_s: 2478.895626306534\n",
      "  timestamp: 1552317726\n",
      "  timesteps_since_restore: 686400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 686400\n",
      "  training_iteration: 312\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2478 s, 312 iter, 686400 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-22-15\n",
      "  done: false\n",
      "  episode_len_mean: 95.63\n",
      "  episode_reward_max: 241.70994830773782\n",
      "  episode_reward_mean: 158.64127443522239\n",
      "  episode_reward_min: -152.6246685307146\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7035\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1847.606\n",
      "    load_time_ms: 4.544\n",
      "    num_steps_sampled: 688600\n",
      "    num_steps_trained: 688600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8158640265464783\n",
      "      kl: 0.008937562815845013\n",
      "      policy_loss: -0.009296277537941933\n",
      "      total_loss: 384.38348388671875\n",
      "      vf_explained_var: 0.6376450061798096\n",
      "      vf_loss: 384.39276123046875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5802560448646545\n",
      "      kl: 0.005061245057731867\n",
      "      policy_loss: -0.0013687268365174532\n",
      "      total_loss: 903.0006103515625\n",
      "      vf_explained_var: 0.5902354121208191\n",
      "      vf_loss: 903.001953125\n",
      "    sample_time_ms: 11894.997\n",
      "    update_time_ms: 17.698\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.34100513642944\n",
      "    rl_1: 116.30026929879295\n",
      "  time_since_restore: 2487.568877696991\n",
      "  time_this_iter_s: 8.673251390457153\n",
      "  time_total_s: 2487.568877696991\n",
      "  timestamp: 1552317735\n",
      "  timesteps_since_restore: 688600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 688600\n",
      "  training_iteration: 313\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2487 s, 313 iter, 688600 ts, 159 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-22-27\n",
      "  done: false\n",
      "  episode_len_mean: 95.83\n",
      "  episode_reward_max: 241.70994830773782\n",
      "  episode_reward_mean: 159.2646799154185\n",
      "  episode_reward_min: -152.6246685307146\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7058\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1705.653\n",
      "    load_time_ms: 3.753\n",
      "    num_steps_sampled: 690800\n",
      "    num_steps_trained: 690800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6694743037223816\n",
      "      kl: 0.008068449795246124\n",
      "      policy_loss: -0.003950133919715881\n",
      "      total_loss: 113.35052490234375\n",
      "      vf_explained_var: 0.8746747374534607\n",
      "      vf_loss: 113.35447692871094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5721983909606934\n",
      "      kl: 0.0064186896197497845\n",
      "      policy_loss: -0.006010713055729866\n",
      "      total_loss: 446.7836608886719\n",
      "      vf_explained_var: 0.7961845993995667\n",
      "      vf_loss: 446.7897033691406\n",
      "    sample_time_ms: 11363.124\n",
      "    update_time_ms: 16.567\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.44387593649127\n",
      "    rl_1: 116.82080397892723\n",
      "  time_since_restore: 2499.0672147274017\n",
      "  time_this_iter_s: 11.498337030410767\n",
      "  time_total_s: 2499.0672147274017\n",
      "  timestamp: 1552317747\n",
      "  timesteps_since_restore: 690800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 690800\n",
      "  training_iteration: 314\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2499 s, 314 iter, 690800 ts, 159 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-22-36\n",
      "  done: false\n",
      "  episode_len_mean: 96.0\n",
      "  episode_reward_max: 239.73674611981642\n",
      "  episode_reward_mean: 160.03995306693224\n",
      "  episode_reward_min: -152.6246685307146\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7081\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1331.974\n",
      "    load_time_ms: 3.213\n",
      "    num_steps_sampled: 693000\n",
      "    num_steps_trained: 693000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9019175171852112\n",
      "      kl: 0.01235540397465229\n",
      "      policy_loss: -0.005907953716814518\n",
      "      total_loss: 273.88232421875\n",
      "      vf_explained_var: 0.6552514433860779\n",
      "      vf_loss: 273.8882751464844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5995872020721436\n",
      "      kl: 0.004891847260296345\n",
      "      policy_loss: 0.0022382691968232393\n",
      "      total_loss: 639.0349731445312\n",
      "      vf_explained_var: 0.6954846382141113\n",
      "      vf_loss: 639.03271484375\n",
      "    sample_time_ms: 10070.053\n",
      "    update_time_ms: 14.466\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.648727331773934\n",
      "    rl_1: 118.39122573515826\n",
      "  time_since_restore: 2508.478573322296\n",
      "  time_this_iter_s: 9.41135859489441\n",
      "  time_total_s: 2508.478573322296\n",
      "  timestamp: 1552317756\n",
      "  timesteps_since_restore: 693000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 693000\n",
      "  training_iteration: 315\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2508 s, 315 iter, 693000 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-22-45\n",
      "  done: false\n",
      "  episode_len_mean: 97.24\n",
      "  episode_reward_max: 239.73674611981642\n",
      "  episode_reward_mean: 165.81142484344957\n",
      "  episode_reward_min: -152.6246685307146\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 7103\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1184.53\n",
      "    load_time_ms: 3.021\n",
      "    num_steps_sampled: 695200\n",
      "    num_steps_trained: 695200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6788999438285828\n",
      "      kl: 0.00850570760667324\n",
      "      policy_loss: -0.007307872176170349\n",
      "      total_loss: 122.0340347290039\n",
      "      vf_explained_var: 0.8814762830734253\n",
      "      vf_loss: 122.04134368896484\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5634282231330872\n",
      "      kl: 0.00353577290661633\n",
      "      policy_loss: -0.005449792370200157\n",
      "      total_loss: 447.6565246582031\n",
      "      vf_explained_var: 0.8266188502311707\n",
      "      vf_loss: 447.6619873046875\n",
      "    sample_time_ms: 8770.742\n",
      "    update_time_ms: 12.664\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.53397425633957\n",
      "    rl_1: 122.27745058711001\n",
      "  time_since_restore: 2517.1226489543915\n",
      "  time_this_iter_s: 8.644075632095337\n",
      "  time_total_s: 2517.1226489543915\n",
      "  timestamp: 1552317765\n",
      "  timesteps_since_restore: 695200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 695200\n",
      "  training_iteration: 316\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2517 s, 316 iter, 695200 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-22-53\n",
      "  done: false\n",
      "  episode_len_mean: 95.64\n",
      "  episode_reward_max: 240.93783402411879\n",
      "  episode_reward_mean: 154.21088084872648\n",
      "  episode_reward_min: -150.59211407432545\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 7128\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1102.688\n",
      "    load_time_ms: 2.846\n",
      "    num_steps_sampled: 697400\n",
      "    num_steps_trained: 697400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6504489779472351\n",
      "      kl: 0.009922698140144348\n",
      "      policy_loss: -0.006408232264220715\n",
      "      total_loss: 311.2466125488281\n",
      "      vf_explained_var: 0.8106347918510437\n",
      "      vf_loss: 311.2530212402344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.47034314274787903\n",
      "      kl: 0.009611498564481735\n",
      "      policy_loss: -0.0033434326760470867\n",
      "      total_loss: 713.7178955078125\n",
      "      vf_explained_var: 0.7871432304382324\n",
      "      vf_loss: 713.7212524414062\n",
      "    sample_time_ms: 8004.18\n",
      "    update_time_ms: 12.093\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.22642976708841\n",
      "    rl_1: 114.98445108163806\n",
      "  time_since_restore: 2524.9301862716675\n",
      "  time_this_iter_s: 7.807537317276001\n",
      "  time_total_s: 2524.9301862716675\n",
      "  timestamp: 1552317773\n",
      "  timesteps_since_restore: 697400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 697400\n",
      "  training_iteration: 317\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2524 s, 317 iter, 697400 ts, 154 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-23-01\n",
      "  done: false\n",
      "  episode_len_mean: 95.48\n",
      "  episode_reward_max: 241.17722532774127\n",
      "  episode_reward_mean: 156.79165611997766\n",
      "  episode_reward_min: -150.59211407432545\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 7150\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 978.388\n",
      "    load_time_ms: 2.721\n",
      "    num_steps_sampled: 699600\n",
      "    num_steps_trained: 699600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3374883532524109\n",
      "      kl: 0.015073537826538086\n",
      "      policy_loss: 0.007499407511204481\n",
      "      total_loss: 300.9078369140625\n",
      "      vf_explained_var: 0.6009344458580017\n",
      "      vf_loss: 300.90032958984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5176794528961182\n",
      "      kl: 0.00853756070137024\n",
      "      policy_loss: -0.008050136268138885\n",
      "      total_loss: 677.5116577148438\n",
      "      vf_explained_var: 0.6145656108856201\n",
      "      vf_loss: 677.519775390625\n",
      "    sample_time_ms: 7396.623\n",
      "    update_time_ms: 11.511\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.74284683553525\n",
      "    rl_1: 115.04880928444237\n",
      "  time_since_restore: 2532.940369606018\n",
      "  time_this_iter_s: 8.010183334350586\n",
      "  time_total_s: 2532.940369606018\n",
      "  timestamp: 1552317781\n",
      "  timesteps_since_restore: 699600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 699600\n",
      "  training_iteration: 318\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2532 s, 318 iter, 699600 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-23-08\n",
      "  done: false\n",
      "  episode_len_mean: 94.97\n",
      "  episode_reward_max: 241.17722532774127\n",
      "  episode_reward_mean: 153.15344582878177\n",
      "  episode_reward_min: -152.72536485191114\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 7174\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 975.266\n",
      "    load_time_ms: 2.75\n",
      "    num_steps_sampled: 701800\n",
      "    num_steps_trained: 701800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7711314558982849\n",
      "      kl: 0.007516752928495407\n",
      "      policy_loss: -0.0045132446102797985\n",
      "      total_loss: 152.17247009277344\n",
      "      vf_explained_var: 0.8659163117408752\n",
      "      vf_loss: 152.17698669433594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5462998151779175\n",
      "      kl: 0.006496491376310587\n",
      "      policy_loss: -0.003686917247250676\n",
      "      total_loss: 497.5814514160156\n",
      "      vf_explained_var: 0.8035815358161926\n",
      "      vf_loss: 497.58514404296875\n",
      "    sample_time_ms: 7063.212\n",
      "    update_time_ms: 10.011\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.50019387817931\n",
      "    rl_1: 112.65325195060248\n",
      "  time_since_restore: 2539.6542041301727\n",
      "  time_this_iter_s: 6.713834524154663\n",
      "  time_total_s: 2539.6542041301727\n",
      "  timestamp: 1552317788\n",
      "  timesteps_since_restore: 701800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 701800\n",
      "  training_iteration: 319\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2539 s, 319 iter, 701800 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-23-17\n",
      "  done: false\n",
      "  episode_len_mean: 93.51\n",
      "  episode_reward_max: 241.17722532774127\n",
      "  episode_reward_mean: 143.7188468792846\n",
      "  episode_reward_min: -154.8064936518001\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7197\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1022.259\n",
      "    load_time_ms: 2.931\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5012513399124146\n",
      "      kl: 0.013862086459994316\n",
      "      policy_loss: -0.00021300881053321064\n",
      "      total_loss: 223.129150390625\n",
      "      vf_explained_var: 0.812639057636261\n",
      "      vf_loss: 223.12936401367188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5157971382141113\n",
      "      kl: 0.003541042795404792\n",
      "      policy_loss: 0.00016648015298414975\n",
      "      total_loss: 628.8007202148438\n",
      "      vf_explained_var: 0.7310836911201477\n",
      "      vf_loss: 628.800537109375\n",
      "    sample_time_ms: 7328.229\n",
      "    update_time_ms: 9.673\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.64823628135037\n",
      "    rl_1: 106.0706105979342\n",
      "  time_since_restore: 2549.379497051239\n",
      "  time_this_iter_s: 9.725292921066284\n",
      "  time_total_s: 2549.379497051239\n",
      "  timestamp: 1552317797\n",
      "  timesteps_since_restore: 704000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 320\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2549 s, 320 iter, 704000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-23-27\n",
      "  done: false\n",
      "  episode_len_mean: 92.26\n",
      "  episode_reward_max: 241.17722532774127\n",
      "  episode_reward_mean: 136.55613642119152\n",
      "  episode_reward_min: -160.89053422814095\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 7222\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1046.935\n",
      "    load_time_ms: 3.077\n",
      "    num_steps_sampled: 706200\n",
      "    num_steps_trained: 706200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4255650043487549\n",
      "      kl: 0.012558736838400364\n",
      "      policy_loss: -0.0072517599910497665\n",
      "      total_loss: 423.9775695800781\n",
      "      vf_explained_var: 0.7630800008773804\n",
      "      vf_loss: 423.9847717285156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.49851685762405396\n",
      "      kl: 0.0038261180743575096\n",
      "      policy_loss: -0.006028107833117247\n",
      "      total_loss: 853.0654296875\n",
      "      vf_explained_var: 0.7465240359306335\n",
      "      vf_loss: 853.0714721679688\n",
      "    sample_time_ms: 7537.464\n",
      "    update_time_ms: 9.908\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.41875890634958\n",
      "    rl_1: 101.13737751484194\n",
      "  time_since_restore: 2558.4920868873596\n",
      "  time_this_iter_s: 9.112589836120605\n",
      "  time_total_s: 2558.4920868873596\n",
      "  timestamp: 1552317807\n",
      "  timesteps_since_restore: 706200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 706200\n",
      "  training_iteration: 321\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2558 s, 321 iter, 706200 ts, 137 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-23-35\n",
      "  done: false\n",
      "  episode_len_mean: 93.33\n",
      "  episode_reward_max: 240.9876952433548\n",
      "  episode_reward_mean: 140.50546476110458\n",
      "  episode_reward_min: -160.89053422814095\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7245\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1025.772\n",
      "    load_time_ms: 3.035\n",
      "    num_steps_sampled: 708400\n",
      "    num_steps_trained: 708400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8466312885284424\n",
      "      kl: 0.006380660925060511\n",
      "      policy_loss: -0.00579858897253871\n",
      "      total_loss: 265.7314453125\n",
      "      vf_explained_var: 0.7269369959831238\n",
      "      vf_loss: 265.73724365234375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5034182071685791\n",
      "      kl: 0.008872165344655514\n",
      "      policy_loss: -0.011827483773231506\n",
      "      total_loss: 572.11767578125\n",
      "      vf_explained_var: 0.7628298401832581\n",
      "      vf_loss: 572.1295166015625\n",
      "    sample_time_ms: 7723.666\n",
      "    update_time_ms: 9.723\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.387657792930284\n",
      "    rl_1: 105.11780696817432\n",
      "  time_since_restore: 2566.6442153453827\n",
      "  time_this_iter_s: 8.152128458023071\n",
      "  time_total_s: 2566.6442153453827\n",
      "  timestamp: 1552317815\n",
      "  timesteps_since_restore: 708400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 708400\n",
      "  training_iteration: 322\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2566 s, 322 iter, 708400 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-23-41\n",
      "  done: false\n",
      "  episode_len_mean: 93.06\n",
      "  episode_reward_max: 240.9876952433548\n",
      "  episode_reward_mean: 140.20919880186727\n",
      "  episode_reward_min: -164.84876534932383\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 7269\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 986.205\n",
      "    load_time_ms: 3.232\n",
      "    num_steps_sampled: 710600\n",
      "    num_steps_trained: 710600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5408538579940796\n",
      "      kl: 0.011275828815996647\n",
      "      policy_loss: -0.005132733844220638\n",
      "      total_loss: 363.7838439941406\n",
      "      vf_explained_var: 0.6864469647407532\n",
      "      vf_loss: 363.7889099121094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5265460014343262\n",
      "      kl: 0.0046870033256709576\n",
      "      policy_loss: -0.0044567156583070755\n",
      "      total_loss: 794.0086059570312\n",
      "      vf_explained_var: 0.6672073006629944\n",
      "      vf_loss: 794.0130615234375\n",
      "    sample_time_ms: 7529.637\n",
      "    update_time_ms: 9.55\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.16480490134953\n",
      "    rl_1: 105.04439390051775\n",
      "  time_since_restore: 2572.983622074127\n",
      "  time_this_iter_s: 6.339406728744507\n",
      "  time_total_s: 2572.983622074127\n",
      "  timestamp: 1552317821\n",
      "  timesteps_since_restore: 710600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 710600\n",
      "  training_iteration: 323\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2572 s, 323 iter, 710600 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-23-51\n",
      "  done: false\n",
      "  episode_len_mean: 94.07\n",
      "  episode_reward_max: 240.63440696508576\n",
      "  episode_reward_mean: 146.70728400921308\n",
      "  episode_reward_min: -164.84876534932383\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 7291\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 942.227\n",
      "    load_time_ms: 3.249\n",
      "    num_steps_sampled: 712800\n",
      "    num_steps_trained: 712800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6829723119735718\n",
      "      kl: 0.01464100368320942\n",
      "      policy_loss: -0.006347316317260265\n",
      "      total_loss: 272.0140686035156\n",
      "      vf_explained_var: 0.7041327953338623\n",
      "      vf_loss: 272.0203857421875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4842159152030945\n",
      "      kl: 0.00971275195479393\n",
      "      policy_loss: -0.007620807737112045\n",
      "      total_loss: 642.8922729492188\n",
      "      vf_explained_var: 0.7080109119415283\n",
      "      vf_loss: 642.89990234375\n",
      "    sample_time_ms: 7370.85\n",
      "    update_time_ms: 9.519\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.34225840386345\n",
      "    rl_1: 109.36502560534969\n",
      "  time_since_restore: 2582.4506182670593\n",
      "  time_this_iter_s: 9.466996192932129\n",
      "  time_total_s: 2582.4506182670593\n",
      "  timestamp: 1552317831\n",
      "  timesteps_since_restore: 712800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 712800\n",
      "  training_iteration: 324\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2582 s, 324 iter, 712800 ts, 147 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-23-58\n",
      "  done: false\n",
      "  episode_len_mean: 93.67\n",
      "  episode_reward_max: 240.9923372411699\n",
      "  episode_reward_mean: 146.81290616058698\n",
      "  episode_reward_min: -164.84876534932383\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 7316\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 938.197\n",
      "    load_time_ms: 3.179\n",
      "    num_steps_sampled: 715000\n",
      "    num_steps_trained: 715000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.568491518497467\n",
      "      kl: 0.004542650189250708\n",
      "      policy_loss: 0.0007492058211937547\n",
      "      total_loss: 192.33106994628906\n",
      "      vf_explained_var: 0.8660157322883606\n",
      "      vf_loss: 192.330322265625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.47134241461753845\n",
      "      kl: 0.006083818152546883\n",
      "      policy_loss: -0.003890648018568754\n",
      "      total_loss: 588.47314453125\n",
      "      vf_explained_var: 0.7989690899848938\n",
      "      vf_loss: 588.47705078125\n",
      "    sample_time_ms: 7172.201\n",
      "    update_time_ms: 9.286\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.66678059097159\n",
      "    rl_1: 109.14612556961545\n",
      "  time_since_restore: 2589.8418016433716\n",
      "  time_this_iter_s: 7.391183376312256\n",
      "  time_total_s: 2589.8418016433716\n",
      "  timestamp: 1552317838\n",
      "  timesteps_since_restore: 715000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 715000\n",
      "  training_iteration: 325\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2589 s, 325 iter, 715000 ts, 147 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-24-05\n",
      "  done: false\n",
      "  episode_len_mean: 94.17\n",
      "  episode_reward_max: 240.9923372411699\n",
      "  episode_reward_mean: 149.78355946096576\n",
      "  episode_reward_min: -164.84876534932383\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7339\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 912.559\n",
      "    load_time_ms: 3.121\n",
      "    num_steps_sampled: 717200\n",
      "    num_steps_trained: 717200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6614772081375122\n",
      "      kl: 0.007427029777318239\n",
      "      policy_loss: -0.007157650776207447\n",
      "      total_loss: 121.79862976074219\n",
      "      vf_explained_var: 0.9152069687843323\n",
      "      vf_loss: 121.80577850341797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5234122276306152\n",
      "      kl: 0.009276218712329865\n",
      "      policy_loss: -0.0020053652115166187\n",
      "      total_loss: 369.2762451171875\n",
      "      vf_explained_var: 0.8884440660476685\n",
      "      vf_loss: 369.27825927734375\n",
      "    sample_time_ms: 7039.191\n",
      "    update_time_ms: 8.841\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.78382289917265\n",
      "    rl_1: 109.99973656179314\n",
      "  time_since_restore: 2596.891918182373\n",
      "  time_this_iter_s: 7.050116539001465\n",
      "  time_total_s: 2596.891918182373\n",
      "  timestamp: 1552317845\n",
      "  timesteps_since_restore: 717200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 717200\n",
      "  training_iteration: 326\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2596 s, 326 iter, 717200 ts, 150 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-24-12\n",
      "  done: false\n",
      "  episode_len_mean: 94.75\n",
      "  episode_reward_max: 240.9923372411699\n",
      "  episode_reward_mean: 150.4827156293135\n",
      "  episode_reward_min: -162.69828375984068\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7362\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 897.085\n",
      "    load_time_ms: 3.272\n",
      "    num_steps_sampled: 719400\n",
      "    num_steps_trained: 719400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6014719009399414\n",
      "      kl: 0.008781659416854382\n",
      "      policy_loss: -0.006292047444730997\n",
      "      total_loss: 448.9551696777344\n",
      "      vf_explained_var: 0.5894924402236938\n",
      "      vf_loss: 448.9614562988281\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4920233488082886\n",
      "      kl: 0.0068132271990180016\n",
      "      policy_loss: -0.0019514607265591621\n",
      "      total_loss: 856.7841186523438\n",
      "      vf_explained_var: 0.659633219242096\n",
      "      vf_loss: 856.7860717773438\n",
      "    sample_time_ms: 6930.976\n",
      "    update_time_ms: 8.448\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.928567773527504\n",
      "    rl_1: 110.55414785578603\n",
      "  time_since_restore: 2603.456472635269\n",
      "  time_this_iter_s: 6.564554452896118\n",
      "  time_total_s: 2603.456472635269\n",
      "  timestamp: 1552317852\n",
      "  timesteps_since_restore: 719400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 719400\n",
      "  training_iteration: 327\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2603 s, 327 iter, 719400 ts, 150 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-24-21\n",
      "  done: false\n",
      "  episode_len_mean: 94.27\n",
      "  episode_reward_max: 240.9923372411699\n",
      "  episode_reward_mean: 147.24070557944677\n",
      "  episode_reward_min: -162.69828375984068\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 7384\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 896.234\n",
      "    load_time_ms: 3.24\n",
      "    num_steps_sampled: 721600\n",
      "    num_steps_trained: 721600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7284272909164429\n",
      "      kl: 0.009809548035264015\n",
      "      policy_loss: -0.0024542261380702257\n",
      "      total_loss: 332.8194580078125\n",
      "      vf_explained_var: 0.6555043458938599\n",
      "      vf_loss: 332.8218688964844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4704477787017822\n",
      "      kl: 0.014850121922791004\n",
      "      policy_loss: -0.008890041150152683\n",
      "      total_loss: 723.2132568359375\n",
      "      vf_explained_var: 0.6744611859321594\n",
      "      vf_loss: 723.22216796875\n",
      "    sample_time_ms: 7089.727\n",
      "    update_time_ms: 8.381\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.39780751774594\n",
      "    rl_1: 108.8428980617008\n",
      "  time_since_restore: 2613.0437030792236\n",
      "  time_this_iter_s: 9.587230443954468\n",
      "  time_total_s: 2613.0437030792236\n",
      "  timestamp: 1552317861\n",
      "  timesteps_since_restore: 721600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 721600\n",
      "  training_iteration: 328\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2613 s, 328 iter, 721600 ts, 147 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-24-28\n",
      "  done: false\n",
      "  episode_len_mean: 94.88\n",
      "  episode_reward_max: 240.9713639610447\n",
      "  episode_reward_mean: 148.24582753096234\n",
      "  episode_reward_min: -162.57744885710292\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7407\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 887.906\n",
      "    load_time_ms: 3.184\n",
      "    num_steps_sampled: 723800\n",
      "    num_steps_trained: 723800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5032290816307068\n",
      "      kl: 0.008908158168196678\n",
      "      policy_loss: 0.005313511937856674\n",
      "      total_loss: 178.14324951171875\n",
      "      vf_explained_var: 0.8500970005989075\n",
      "      vf_loss: 178.137939453125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4892064332962036\n",
      "      kl: 0.003919648006558418\n",
      "      policy_loss: -0.0019920687191188335\n",
      "      total_loss: 541.6978759765625\n",
      "      vf_explained_var: 0.8238360285758972\n",
      "      vf_loss: 541.6998291015625\n",
      "    sample_time_ms: 7058.889\n",
      "    update_time_ms: 8.463\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.80351702504196\n",
      "    rl_1: 109.44231050592039\n",
      "  time_since_restore: 2619.366138935089\n",
      "  time_this_iter_s: 6.3224358558654785\n",
      "  time_total_s: 2619.366138935089\n",
      "  timestamp: 1552317868\n",
      "  timesteps_since_restore: 723800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 723800\n",
      "  training_iteration: 329\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2619 s, 329 iter, 723800 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-24-35\n",
      "  done: false\n",
      "  episode_len_mean: 95.8\n",
      "  episode_reward_max: 240.9713639610447\n",
      "  episode_reward_mean: 157.19545723790034\n",
      "  episode_reward_min: -156.4631541951889\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7430\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 853.389\n",
      "    load_time_ms: 2.998\n",
      "    num_steps_sampled: 726000\n",
      "    num_steps_trained: 726000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.46073412895202637\n",
      "      kl: 0.009043190628290176\n",
      "      policy_loss: -0.004172187764197588\n",
      "      total_loss: 154.8532257080078\n",
      "      vf_explained_var: 0.7819710969924927\n",
      "      vf_loss: 154.85740661621094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4215312898159027\n",
      "      kl: 0.009645234793424606\n",
      "      policy_loss: -0.0053191278129816055\n",
      "      total_loss: 499.9108581542969\n",
      "      vf_explained_var: 0.7826962471008301\n",
      "      vf_loss: 499.916259765625\n",
      "    sample_time_ms: 6815.738\n",
      "    update_time_ms: 8.475\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.38173923326391\n",
      "    rl_1: 113.81371800463643\n",
      "  time_since_restore: 2626.309173822403\n",
      "  time_this_iter_s: 6.943034887313843\n",
      "  time_total_s: 2626.309173822403\n",
      "  timestamp: 1552317875\n",
      "  timesteps_since_restore: 726000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 726000\n",
      "  training_iteration: 330\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2626 s, 330 iter, 726000 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-24-42\n",
      "  done: false\n",
      "  episode_len_mean: 96.29\n",
      "  episode_reward_max: 241.1160347752291\n",
      "  episode_reward_mean: 161.96011262690644\n",
      "  episode_reward_min: -164.35995566609583\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 7452\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 833.209\n",
      "    load_time_ms: 2.801\n",
      "    num_steps_sampled: 728200\n",
      "    num_steps_trained: 728200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5774019956588745\n",
      "      kl: 0.008937112055718899\n",
      "      policy_loss: -0.0019350777147337794\n",
      "      total_loss: 149.82968139648438\n",
      "      vf_explained_var: 0.8628511428833008\n",
      "      vf_loss: 149.83160400390625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4896768629550934\n",
      "      kl: 0.005476329009979963\n",
      "      policy_loss: -0.007307063788175583\n",
      "      total_loss: 500.7951965332031\n",
      "      vf_explained_var: 0.8119783997535706\n",
      "      vf_loss: 500.8024597167969\n",
      "    sample_time_ms: 6640.582\n",
      "    update_time_ms: 8.448\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.87976015292296\n",
      "    rl_1: 117.08035247398345\n",
      "  time_since_restore: 2633.462386369705\n",
      "  time_this_iter_s: 7.153212547302246\n",
      "  time_total_s: 2633.462386369705\n",
      "  timestamp: 1552317882\n",
      "  timesteps_since_restore: 728200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 728200\n",
      "  training_iteration: 331\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2633 s, 331 iter, 728200 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-24-49\n",
      "  done: false\n",
      "  episode_len_mean: 98.21\n",
      "  episode_reward_max: 241.1160347752291\n",
      "  episode_reward_mean: 178.0668129951977\n",
      "  episode_reward_min: -164.35995566609583\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7475\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 863.129\n",
      "    load_time_ms: 2.797\n",
      "    num_steps_sampled: 730400\n",
      "    num_steps_trained: 730400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.424543559551239\n",
      "      kl: 0.012216783128678799\n",
      "      policy_loss: -0.0015140044270083308\n",
      "      total_loss: 72.681396484375\n",
      "      vf_explained_var: 0.8956633806228638\n",
      "      vf_loss: 72.68292236328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4543735086917877\n",
      "      kl: 0.004822735674679279\n",
      "      policy_loss: -0.0027373686898499727\n",
      "      total_loss: 346.6783752441406\n",
      "      vf_explained_var: 0.8361449837684631\n",
      "      vf_loss: 346.68109130859375\n",
      "    sample_time_ms: 6517.538\n",
      "    update_time_ms: 8.654\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.20691025869362\n",
      "    rl_1: 125.8599027365041\n",
      "  time_since_restore: 2640.6808094978333\n",
      "  time_this_iter_s: 7.218423128128052\n",
      "  time_total_s: 2640.6808094978333\n",
      "  timestamp: 1552317889\n",
      "  timesteps_since_restore: 730400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 730400\n",
      "  training_iteration: 332\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2640 s, 332 iter, 730400 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-25-00\n",
      "  done: false\n",
      "  episode_len_mean: 97.99\n",
      "  episode_reward_max: 241.1160347752291\n",
      "  episode_reward_mean: 174.01610590342716\n",
      "  episode_reward_min: -164.35995566609583\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 7497\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1013.427\n",
      "    load_time_ms: 2.823\n",
      "    num_steps_sampled: 732600\n",
      "    num_steps_trained: 732600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6653297543525696\n",
      "      kl: 0.007907399907708168\n",
      "      policy_loss: -0.0018299603834748268\n",
      "      total_loss: 270.0389404296875\n",
      "      vf_explained_var: 0.7625001668930054\n",
      "      vf_loss: 270.0408020019531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.48466676473617554\n",
      "      kl: 0.006037468556314707\n",
      "      policy_loss: -0.0035575986839830875\n",
      "      total_loss: 560.8896484375\n",
      "      vf_explained_var: 0.8002296090126038\n",
      "      vf_loss: 560.8931884765625\n",
      "    sample_time_ms: 6830.714\n",
      "    update_time_ms: 9.389\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.92075415617806\n",
      "    rl_1: 124.09535174724908\n",
      "  time_since_restore: 2651.671163082123\n",
      "  time_this_iter_s: 10.99035358428955\n",
      "  time_total_s: 2651.671163082123\n",
      "  timestamp: 1552317900\n",
      "  timesteps_since_restore: 732600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 732600\n",
      "  training_iteration: 333\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2651 s, 333 iter, 732600 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-25-13\n",
      "  done: false\n",
      "  episode_len_mean: 98.8\n",
      "  episode_reward_max: 241.1160347752291\n",
      "  episode_reward_mean: 180.46419234622925\n",
      "  episode_reward_min: -164.35995566609583\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7520\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1054.523\n",
      "    load_time_ms: 3.014\n",
      "    num_steps_sampled: 734800\n",
      "    num_steps_trained: 734800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6636616587638855\n",
      "      kl: 0.012922677211463451\n",
      "      policy_loss: 0.012355920858681202\n",
      "      total_loss: 100.36618041992188\n",
      "      vf_explained_var: 0.8922179937362671\n",
      "      vf_loss: 100.35383605957031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.44706806540489197\n",
      "      kl: 0.006737468298524618\n",
      "      policy_loss: -0.0034917043522000313\n",
      "      total_loss: 390.30029296875\n",
      "      vf_explained_var: 0.8438944816589355\n",
      "      vf_loss: 390.3037414550781\n",
      "    sample_time_ms: 7095.046\n",
      "    update_time_ms: 10.298\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.11237158630059\n",
      "    rl_1: 128.35182075992864\n",
      "  time_since_restore: 2664.201623439789\n",
      "  time_this_iter_s: 12.530460357666016\n",
      "  time_total_s: 2664.201623439789\n",
      "  timestamp: 1552317913\n",
      "  timesteps_since_restore: 734800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 734800\n",
      "  training_iteration: 334\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2664 s, 334 iter, 734800 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-25-28\n",
      "  done: false\n",
      "  episode_len_mean: 97.74\n",
      "  episode_reward_max: 241.1160347752291\n",
      "  episode_reward_mean: 171.98641084188247\n",
      "  episode_reward_min: -164.35995566609583\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 7542\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1151.106\n",
      "    load_time_ms: 3.395\n",
      "    num_steps_sampled: 737000\n",
      "    num_steps_trained: 737000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8123093843460083\n",
      "      kl: 0.005905658472329378\n",
      "      policy_loss: -0.001435263198800385\n",
      "      total_loss: 437.01617431640625\n",
      "      vf_explained_var: 0.6051916480064392\n",
      "      vf_loss: 437.0176086425781\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4525485038757324\n",
      "      kl: 0.006521499250084162\n",
      "      policy_loss: -0.0021054649259895086\n",
      "      total_loss: 795.4922485351562\n",
      "      vf_explained_var: 0.6762319207191467\n",
      "      vf_loss: 795.4943237304688\n",
      "    sample_time_ms: 7722.71\n",
      "    update_time_ms: 11.222\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 47.907970854960496\n",
      "    rl_1: 124.07843998692194\n",
      "  time_since_restore: 2678.85178399086\n",
      "  time_this_iter_s: 14.650160551071167\n",
      "  time_total_s: 2678.85178399086\n",
      "  timestamp: 1552317928\n",
      "  timesteps_since_restore: 737000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 737000\n",
      "  training_iteration: 335\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2678 s, 335 iter, 737000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-25-41\n",
      "  done: false\n",
      "  episode_len_mean: 97.17\n",
      "  episode_reward_max: 241.15655617153016\n",
      "  episode_reward_mean: 164.429780379489\n",
      "  episode_reward_min: -145.27479521970702\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 7564\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1303.202\n",
      "    load_time_ms: 3.612\n",
      "    num_steps_sampled: 739200\n",
      "    num_steps_trained: 739200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6899072527885437\n",
      "      kl: 0.025873754173517227\n",
      "      policy_loss: -0.008957484737038612\n",
      "      total_loss: 213.22024536132812\n",
      "      vf_explained_var: 0.793496310710907\n",
      "      vf_loss: 213.22918701171875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4830681383609772\n",
      "      kl: 0.005594329908490181\n",
      "      policy_loss: -0.004106346517801285\n",
      "      total_loss: 538.8628540039062\n",
      "      vf_explained_var: 0.7914842367172241\n",
      "      vf_loss: 538.866943359375\n",
      "    sample_time_ms: 8214.301\n",
      "    update_time_ms: 12.822\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.099283385441765\n",
      "    rl_1: 119.3304969940472\n",
      "  time_since_restore: 2692.363249540329\n",
      "  time_this_iter_s: 13.511465549468994\n",
      "  time_total_s: 2692.363249540329\n",
      "  timestamp: 1552317941\n",
      "  timesteps_since_restore: 739200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 739200\n",
      "  training_iteration: 336\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2692 s, 336 iter, 739200 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-25-53\n",
      "  done: false\n",
      "  episode_len_mean: 96.3\n",
      "  episode_reward_max: 241.15655617153016\n",
      "  episode_reward_mean: 157.1873001688932\n",
      "  episode_reward_min: -145.27479521970702\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7587\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1325.943\n",
      "    load_time_ms: 3.507\n",
      "    num_steps_sampled: 741400\n",
      "    num_steps_trained: 741400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6143215894699097\n",
      "      kl: 0.010275826789438725\n",
      "      policy_loss: 0.00269814464263618\n",
      "      total_loss: 169.2799530029297\n",
      "      vf_explained_var: 0.839070737361908\n",
      "      vf_loss: 169.27725219726562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.40564242005348206\n",
      "      kl: 0.004417223855853081\n",
      "      policy_loss: -0.0031651263125240803\n",
      "      total_loss: 559.09228515625\n",
      "      vf_explained_var: 0.8036481738090515\n",
      "      vf_loss: 559.095458984375\n",
      "    sample_time_ms: 8728.889\n",
      "    update_time_ms: 13.292\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.95738510553467\n",
      "    rl_1: 115.22991506335853\n",
      "  time_since_restore: 2704.3109414577484\n",
      "  time_this_iter_s: 11.947691917419434\n",
      "  time_total_s: 2704.3109414577484\n",
      "  timestamp: 1552317953\n",
      "  timesteps_since_restore: 741400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 741400\n",
      "  training_iteration: 337\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2704 s, 337 iter, 741400 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-26-03\n",
      "  done: false\n",
      "  episode_len_mean: 96.63\n",
      "  episode_reward_max: 241.7068698337267\n",
      "  episode_reward_mean: 160.89894325140384\n",
      "  episode_reward_min: -145.86947196005076\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 7611\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1357.39\n",
      "    load_time_ms: 3.578\n",
      "    num_steps_sampled: 743600\n",
      "    num_steps_trained: 743600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2797485888004303\n",
      "      kl: 0.0059966216795146465\n",
      "      policy_loss: -0.00040005322080105543\n",
      "      total_loss: 428.939697265625\n",
      "      vf_explained_var: 0.6921706199645996\n",
      "      vf_loss: 428.9400939941406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3225991725921631\n",
      "      kl: 0.012692028656601906\n",
      "      policy_loss: -0.004066316410899162\n",
      "      total_loss: 735.140869140625\n",
      "      vf_explained_var: 0.7423552870750427\n",
      "      vf_loss: 735.1448364257812\n",
      "    sample_time_ms: 8689.984\n",
      "    update_time_ms: 13.824\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.47001008004214\n",
      "    rl_1: 117.42893317136175\n",
      "  time_since_restore: 2713.846729516983\n",
      "  time_this_iter_s: 9.53578805923462\n",
      "  time_total_s: 2713.846729516983\n",
      "  timestamp: 1552317963\n",
      "  timesteps_since_restore: 743600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 743600\n",
      "  training_iteration: 338\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2713 s, 338 iter, 743600 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-26-15\n",
      "  done: false\n",
      "  episode_len_mean: 93.77\n",
      "  episode_reward_max: 241.7068698337267\n",
      "  episode_reward_mean: 144.05925909875796\n",
      "  episode_reward_min: -164.68494560966036\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 7636\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1427.478\n",
      "    load_time_ms: 3.614\n",
      "    num_steps_sampled: 745800\n",
      "    num_steps_trained: 745800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.35705798864364624\n",
      "      kl: 0.010548444464802742\n",
      "      policy_loss: -0.004534740466624498\n",
      "      total_loss: 563.1336669921875\n",
      "      vf_explained_var: 0.6585516929626465\n",
      "      vf_loss: 563.13818359375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3517242968082428\n",
      "      kl: 0.004236325155943632\n",
      "      policy_loss: -0.002728123916313052\n",
      "      total_loss: 1060.5712890625\n",
      "      vf_explained_var: 0.6911605596542358\n",
      "      vf_loss: 1060.5740966796875\n",
      "    sample_time_ms: 9187.232\n",
      "    update_time_ms: 15.75\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.50068763375195\n",
      "    rl_1: 106.55857146500604\n",
      "  time_since_restore: 2725.8654491901398\n",
      "  time_this_iter_s: 12.018719673156738\n",
      "  time_total_s: 2725.8654491901398\n",
      "  timestamp: 1552317975\n",
      "  timesteps_since_restore: 745800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 745800\n",
      "  training_iteration: 339\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2725 s, 339 iter, 745800 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-26-22\n",
      "  done: false\n",
      "  episode_len_mean: 93.03\n",
      "  episode_reward_max: 241.7068698337267\n",
      "  episode_reward_mean: 141.67812262525865\n",
      "  episode_reward_min: -164.68494560966036\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7659\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1414.44\n",
      "    load_time_ms: 3.689\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.36198890209198\n",
      "      kl: 0.00799498800188303\n",
      "      policy_loss: 0.00034627580316737294\n",
      "      total_loss: 103.30442810058594\n",
      "      vf_explained_var: 0.9272284507751465\n",
      "      vf_loss: 103.30408477783203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.35422754287719727\n",
      "      kl: 0.016674568876624107\n",
      "      policy_loss: -0.01777082122862339\n",
      "      total_loss: 384.71099853515625\n",
      "      vf_explained_var: 0.8815885186195374\n",
      "      vf_loss: 384.7287902832031\n",
      "    sample_time_ms: 9234.274\n",
      "    update_time_ms: 16.187\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.51796334869577\n",
      "    rl_1: 105.16015927656292\n",
      "  time_since_restore: 2733.1518507003784\n",
      "  time_this_iter_s: 7.2864015102386475\n",
      "  time_total_s: 2733.1518507003784\n",
      "  timestamp: 1552317982\n",
      "  timesteps_since_restore: 748000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 340\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2733 s, 340 iter, 748000 ts, 142 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-26-30\n",
      "  done: false\n",
      "  episode_len_mean: 92.85\n",
      "  episode_reward_max: 241.7068698337267\n",
      "  episode_reward_mean: 141.06623586888244\n",
      "  episode_reward_min: -164.68494560966036\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 7683\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1465.254\n",
      "    load_time_ms: 3.878\n",
      "    num_steps_sampled: 750200\n",
      "    num_steps_trained: 750200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6844030022621155\n",
      "      kl: 0.014181979931890965\n",
      "      policy_loss: -0.002272110665217042\n",
      "      total_loss: 373.18505859375\n",
      "      vf_explained_var: 0.6183695197105408\n",
      "      vf_loss: 373.1873779296875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.44612425565719604\n",
      "      kl: 0.004268439952284098\n",
      "      policy_loss: -0.00767553923651576\n",
      "      total_loss: 733.3174438476562\n",
      "      vf_explained_var: 0.6923213005065918\n",
      "      vf_loss: 733.3250732421875\n",
      "    sample_time_ms: 9259.46\n",
      "    update_time_ms: 16.029\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.354485450802116\n",
      "    rl_1: 104.7117504180803\n",
      "  time_since_restore: 2741.072325706482\n",
      "  time_this_iter_s: 7.920475006103516\n",
      "  time_total_s: 2741.072325706482\n",
      "  timestamp: 1552317990\n",
      "  timesteps_since_restore: 750200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 750200\n",
      "  training_iteration: 341\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2741 s, 341 iter, 750200 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-26-39\n",
      "  done: false\n",
      "  episode_len_mean: 92.03\n",
      "  episode_reward_max: 240.48573379751895\n",
      "  episode_reward_mean: 136.50494698104114\n",
      "  episode_reward_min: -164.68494560966036\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 7707\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1433.43\n",
      "    load_time_ms: 3.943\n",
      "    num_steps_sampled: 752400\n",
      "    num_steps_trained: 752400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.28585368394851685\n",
      "      kl: 0.01334109902381897\n",
      "      policy_loss: -0.006575530860573053\n",
      "      total_loss: 614.6588745117188\n",
      "      vf_explained_var: 0.5234929919242859\n",
      "      vf_loss: 614.66552734375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3426778316497803\n",
      "      kl: 0.005545527208596468\n",
      "      policy_loss: -0.007892493158578873\n",
      "      total_loss: 1077.5498046875\n",
      "      vf_explained_var: 0.5895699262619019\n",
      "      vf_loss: 1077.5576171875\n",
      "    sample_time_ms: 9453.803\n",
      "    update_time_ms: 16.21\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.71459770631169\n",
      "    rl_1: 101.7903492747295\n",
      "  time_since_restore: 2749.918936252594\n",
      "  time_this_iter_s: 8.84661054611206\n",
      "  time_total_s: 2749.918936252594\n",
      "  timestamp: 1552317999\n",
      "  timesteps_since_restore: 752400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 752400\n",
      "  training_iteration: 342\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2749 s, 342 iter, 752400 ts, 137 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-26-48\n",
      "  done: false\n",
      "  episode_len_mean: 93.47\n",
      "  episode_reward_max: 241.98101122425308\n",
      "  episode_reward_mean: 145.69587653594965\n",
      "  episode_reward_min: -164.68494560966036\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 7729\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1309.802\n",
      "    load_time_ms: 3.817\n",
      "    num_steps_sampled: 754600\n",
      "    num_steps_trained: 754600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.692471981048584\n",
      "      kl: 0.027006149291992188\n",
      "      policy_loss: 0.015402604825794697\n",
      "      total_loss: 97.04631805419922\n",
      "      vf_explained_var: 0.8841055631637573\n",
      "      vf_loss: 97.03091430664062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4072975218296051\n",
      "      kl: 0.007207936141639948\n",
      "      policy_loss: -0.011670663952827454\n",
      "      total_loss: 402.0411071777344\n",
      "      vf_explained_var: 0.8325778245925903\n",
      "      vf_loss: 402.052734375\n",
      "    sample_time_ms: 9384.141\n",
      "    update_time_ms: 15.334\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.70536149056631\n",
      "    rl_1: 107.99051504538333\n",
      "  time_since_restore: 2758.9604692459106\n",
      "  time_this_iter_s: 9.04153299331665\n",
      "  time_total_s: 2758.9604692459106\n",
      "  timestamp: 1552318008\n",
      "  timesteps_since_restore: 754600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 754600\n",
      "  training_iteration: 343\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2758 s, 343 iter, 754600 ts, 146 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-26-57\n",
      "  done: false\n",
      "  episode_len_mean: 94.2\n",
      "  episode_reward_max: 241.98101122425308\n",
      "  episode_reward_mean: 152.55704381888737\n",
      "  episode_reward_min: -166.33196117948322\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7752\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1232.872\n",
      "    load_time_ms: 3.518\n",
      "    num_steps_sampled: 756800\n",
      "    num_steps_trained: 756800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5493064522743225\n",
      "      kl: 0.01236723456531763\n",
      "      policy_loss: 0.0029314905405044556\n",
      "      total_loss: 168.065673828125\n",
      "      vf_explained_var: 0.840318500995636\n",
      "      vf_loss: 168.06275939941406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4079519808292389\n",
      "      kl: 0.008819716982543468\n",
      "      policy_loss: -0.010374622419476509\n",
      "      total_loss: 531.2796630859375\n",
      "      vf_explained_var: 0.7923866510391235\n",
      "      vf_loss: 531.2899780273438\n",
      "    sample_time_ms: 9067.985\n",
      "    update_time_ms: 15.376\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.44137470415052\n",
      "    rl_1: 112.11566911473685\n",
      "  time_since_restore: 2767.5554587841034\n",
      "  time_this_iter_s: 8.594989538192749\n",
      "  time_total_s: 2767.5554587841034\n",
      "  timestamp: 1552318017\n",
      "  timesteps_since_restore: 756800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 756800\n",
      "  training_iteration: 344\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2767 s, 344 iter, 756800 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-27-04\n",
      "  done: false\n",
      "  episode_len_mean: 94.44\n",
      "  episode_reward_max: 241.98101122425308\n",
      "  episode_reward_mean: 152.87951443393638\n",
      "  episode_reward_min: -166.33196117948322\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 7776\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1128.294\n",
      "    load_time_ms: 3.099\n",
      "    num_steps_sampled: 759000\n",
      "    num_steps_trained: 759000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5643333196640015\n",
      "      kl: 0.006624795030802488\n",
      "      policy_loss: -0.004190121311694384\n",
      "      total_loss: 344.2194519042969\n",
      "      vf_explained_var: 0.6794613599777222\n",
      "      vf_loss: 344.2236328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.40869706869125366\n",
      "      kl: 0.010257594287395477\n",
      "      policy_loss: -0.0007211110205389559\n",
      "      total_loss: 712.0245361328125\n",
      "      vf_explained_var: 0.7098864316940308\n",
      "      vf_loss: 712.0252685546875\n",
      "    sample_time_ms: 8345.119\n",
      "    update_time_ms: 14.972\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.74792609873222\n",
      "    rl_1: 112.1315883352042\n",
      "  time_since_restore: 2773.9107666015625\n",
      "  time_this_iter_s: 6.3553078174591064\n",
      "  time_total_s: 2773.9107666015625\n",
      "  timestamp: 1552318024\n",
      "  timesteps_since_restore: 759000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 759000\n",
      "  training_iteration: 345\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2773 s, 345 iter, 759000 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-27-13\n",
      "  done: false\n",
      "  episode_len_mean: 94.99\n",
      "  episode_reward_max: 241.98101122425308\n",
      "  episode_reward_mean: 155.03289907610466\n",
      "  episode_reward_min: -166.33196117948322\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 7798\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1000.094\n",
      "    load_time_ms: 3.018\n",
      "    num_steps_sampled: 761200\n",
      "    num_steps_trained: 761200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6722044944763184\n",
      "      kl: 0.012143980711698532\n",
      "      policy_loss: -0.007364060264080763\n",
      "      total_loss: 304.1196594238281\n",
      "      vf_explained_var: 0.7204262614250183\n",
      "      vf_loss: 304.12701416015625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.41607391834259033\n",
      "      kl: 0.0065518757328391075\n",
      "      policy_loss: -0.006669916678220034\n",
      "      total_loss: 608.4757690429688\n",
      "      vf_explained_var: 0.7611643671989441\n",
      "      vf_loss: 608.482421875\n",
      "    sample_time_ms: 8028.547\n",
      "    update_time_ms: 13.135\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.27163711840229\n",
      "    rl_1: 114.76126195770236\n",
      "  time_since_restore: 2782.9502787590027\n",
      "  time_this_iter_s: 9.039512157440186\n",
      "  time_total_s: 2782.9502787590027\n",
      "  timestamp: 1552318033\n",
      "  timesteps_since_restore: 761200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 761200\n",
      "  training_iteration: 346\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2782 s, 346 iter, 761200 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-27-20\n",
      "  done: false\n",
      "  episode_len_mean: 97.42\n",
      "  episode_reward_max: 241.83780821041452\n",
      "  episode_reward_mean: 173.8083668501851\n",
      "  episode_reward_min: -166.33196117948322\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 7820\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 998.976\n",
      "    load_time_ms: 2.977\n",
      "    num_steps_sampled: 763400\n",
      "    num_steps_trained: 763400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6629911065101624\n",
      "      kl: 0.010935468599200249\n",
      "      policy_loss: -0.004627911373972893\n",
      "      total_loss: 45.623390197753906\n",
      "      vf_explained_var: 0.9255222082138062\n",
      "      vf_loss: 45.628021240234375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4171655774116516\n",
      "      kl: 0.02037925273180008\n",
      "      policy_loss: -0.008447856642305851\n",
      "      total_loss: 310.839111328125\n",
      "      vf_explained_var: 0.8587899208068848\n",
      "      vf_loss: 310.8475341796875\n",
      "    sample_time_ms: 7604.399\n",
      "    update_time_ms: 12.772\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.089107350883005\n",
      "    rl_1: 125.7192594993021\n",
      "  time_since_restore: 2790.637576818466\n",
      "  time_this_iter_s: 7.687298059463501\n",
      "  time_total_s: 2790.637576818466\n",
      "  timestamp: 1552318040\n",
      "  timesteps_since_restore: 763400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 763400\n",
      "  training_iteration: 347\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2790 s, 347 iter, 763400 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-27-28\n",
      "  done: false\n",
      "  episode_len_mean: 96.5\n",
      "  episode_reward_max: 241.83780821041452\n",
      "  episode_reward_mean: 166.10429844299088\n",
      "  episode_reward_min: -156.06815374745526\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 7844\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 961.912\n",
      "    load_time_ms: 2.843\n",
      "    num_steps_sampled: 765600\n",
      "    num_steps_trained: 765600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7895413637161255\n",
      "      kl: 0.010380638763308525\n",
      "      policy_loss: -0.00527071813121438\n",
      "      total_loss: 294.37994384765625\n",
      "      vf_explained_var: 0.6889269351959229\n",
      "      vf_loss: 294.3852233886719\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.48355376720428467\n",
      "      kl: 0.008293273858726025\n",
      "      policy_loss: -0.0036823770496994257\n",
      "      total_loss: 593.1171875\n",
      "      vf_explained_var: 0.7465033531188965\n",
      "      vf_loss: 593.1209106445312\n",
      "    sample_time_ms: 7459.568\n",
      "    update_time_ms: 11.98\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.52996966822805\n",
      "    rl_1: 121.57432877476282\n",
      "  time_since_restore: 2798.327940940857\n",
      "  time_this_iter_s: 7.690364122390747\n",
      "  time_total_s: 2798.327940940857\n",
      "  timestamp: 1552318048\n",
      "  timesteps_since_restore: 765600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 765600\n",
      "  training_iteration: 348\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2798 s, 348 iter, 765600 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-27-36\n",
      "  done: false\n",
      "  episode_len_mean: 95.83\n",
      "  episode_reward_max: 241.83780821041452\n",
      "  episode_reward_mean: 156.36496709120826\n",
      "  episode_reward_min: -151.48555810982862\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7867\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 941.781\n",
      "    load_time_ms: 2.882\n",
      "    num_steps_sampled: 767800\n",
      "    num_steps_trained: 767800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.48366788029670715\n",
      "      kl: 0.01593593694269657\n",
      "      policy_loss: 0.0045905690640211105\n",
      "      total_loss: 345.4079284667969\n",
      "      vf_explained_var: 0.8107051849365234\n",
      "      vf_loss: 345.4033203125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4732810854911804\n",
      "      kl: 0.0031284515280276537\n",
      "      policy_loss: 0.0002575475082267076\n",
      "      total_loss: 721.698974609375\n",
      "      vf_explained_var: 0.8208819031715393\n",
      "      vf_loss: 721.69873046875\n",
      "    sample_time_ms: 7081.004\n",
      "    update_time_ms: 10.097\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.32701191417467\n",
      "    rl_1: 116.03795517703352\n",
      "  time_since_restore: 2806.345672607422\n",
      "  time_this_iter_s: 8.017731666564941\n",
      "  time_total_s: 2806.345672607422\n",
      "  timestamp: 1552318056\n",
      "  timesteps_since_restore: 767800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 767800\n",
      "  training_iteration: 349\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2806 s, 349 iter, 767800 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-27-44\n",
      "  done: false\n",
      "  episode_len_mean: 96.27\n",
      "  episode_reward_max: 241.83780821041452\n",
      "  episode_reward_mean: 161.0041403641717\n",
      "  episode_reward_min: -151.48555810982862\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 7890\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 945.806\n",
      "    load_time_ms: 2.899\n",
      "    num_steps_sampled: 770000\n",
      "    num_steps_trained: 770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6000915169715881\n",
      "      kl: 0.008015675470232964\n",
      "      policy_loss: -0.001696350984275341\n",
      "      total_loss: 251.88027954101562\n",
      "      vf_explained_var: 0.5962019562721252\n",
      "      vf_loss: 251.8819580078125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4000200629234314\n",
      "      kl: 0.009855491109192371\n",
      "      policy_loss: -0.005796421319246292\n",
      "      total_loss: 579.3943481445312\n",
      "      vf_explained_var: 0.6656656265258789\n",
      "      vf_loss: 579.400146484375\n",
      "    sample_time_ms: 7142.182\n",
      "    update_time_ms: 10.237\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.798807268021335\n",
      "    rl_1: 118.20533309615031\n",
      "  time_since_restore: 2814.2880759239197\n",
      "  time_this_iter_s: 7.942403316497803\n",
      "  time_total_s: 2814.2880759239197\n",
      "  timestamp: 1552318064\n",
      "  timesteps_since_restore: 770000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 770000\n",
      "  training_iteration: 350\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2814 s, 350 iter, 770000 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-27-50\n",
      "  done: false\n",
      "  episode_len_mean: 94.74\n",
      "  episode_reward_max: 241.40632566553612\n",
      "  episode_reward_mean: 150.4139786100771\n",
      "  episode_reward_min: -151.6269008314547\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 7914\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 892.33\n",
      "    load_time_ms: 2.808\n",
      "    num_steps_sampled: 772200\n",
      "    num_steps_trained: 772200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4013100266456604\n",
      "      kl: 0.00938327144831419\n",
      "      policy_loss: -0.009727198630571365\n",
      "      total_loss: 465.44183349609375\n",
      "      vf_explained_var: 0.6075869202613831\n",
      "      vf_loss: 465.4515380859375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.42651084065437317\n",
      "      kl: 0.010565446689724922\n",
      "      policy_loss: -0.006436478812247515\n",
      "      total_loss: 873.9718627929688\n",
      "      vf_explained_var: 0.6711207628250122\n",
      "      vf_loss: 873.9783325195312\n",
      "    sample_time_ms: 7021.735\n",
      "    update_time_ms: 10.403\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.21788957886274\n",
      "    rl_1: 111.19608903121433\n",
      "  time_since_restore: 2820.464442014694\n",
      "  time_this_iter_s: 6.176366090774536\n",
      "  time_total_s: 2820.464442014694\n",
      "  timestamp: 1552318070\n",
      "  timesteps_since_restore: 772200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 772200\n",
      "  training_iteration: 351\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2820 s, 351 iter, 772200 ts, 150 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-28-00\n",
      "  done: false\n",
      "  episode_len_mean: 93.28\n",
      "  episode_reward_max: 240.59164980779096\n",
      "  episode_reward_mean: 141.1089123428229\n",
      "  episode_reward_min: -160.93255915554957\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 7938\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 927.465\n",
      "    load_time_ms: 2.8\n",
      "    num_steps_sampled: 774400\n",
      "    num_steps_trained: 774400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3557412028312683\n",
      "      kl: 0.015156998299062252\n",
      "      policy_loss: -0.004692403133958578\n",
      "      total_loss: 316.75933837890625\n",
      "      vf_explained_var: 0.7662653923034668\n",
      "      vf_loss: 316.7640380859375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.40491023659706116\n",
      "      kl: 0.007503330707550049\n",
      "      policy_loss: -0.007513584569096565\n",
      "      total_loss: 652.3748779296875\n",
      "      vf_explained_var: 0.7802067399024963\n",
      "      vf_loss: 652.3823852539062\n",
      "    sample_time_ms: 7099.803\n",
      "    update_time_ms: 10.197\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.40888450733469\n",
      "    rl_1: 104.70002783548814\n",
      "  time_since_restore: 2830.44415974617\n",
      "  time_this_iter_s: 9.97971773147583\n",
      "  time_total_s: 2830.44415974617\n",
      "  timestamp: 1552318080\n",
      "  timesteps_since_restore: 774400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 774400\n",
      "  training_iteration: 352\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2830 s, 352 iter, 774400 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-28-07\n",
      "  done: false\n",
      "  episode_len_mean: 96.12\n",
      "  episode_reward_max: 241.7167340920024\n",
      "  episode_reward_mean: 163.4862549932714\n",
      "  episode_reward_min: -160.93255915554957\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 7959\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 893.88\n",
      "    load_time_ms: 2.644\n",
      "    num_steps_sampled: 776600\n",
      "    num_steps_trained: 776600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6442683339118958\n",
      "      kl: 0.01862185075879097\n",
      "      policy_loss: -0.004060271196067333\n",
      "      total_loss: 31.342866897583008\n",
      "      vf_explained_var: 0.9328370094299316\n",
      "      vf_loss: 31.346927642822266\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3757663071155548\n",
      "      kl: 0.013384760357439518\n",
      "      policy_loss: -0.010089906863868237\n",
      "      total_loss: 255.43057250976562\n",
      "      vf_explained_var: 0.8742033243179321\n",
      "      vf_loss: 255.44065856933594\n",
      "    sample_time_ms: 6892.727\n",
      "    update_time_ms: 10.253\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.902120861641066\n",
      "    rl_1: 117.58413413163032\n",
      "  time_since_restore: 2837.07253241539\n",
      "  time_this_iter_s: 6.628372669219971\n",
      "  time_total_s: 2837.07253241539\n",
      "  timestamp: 1552318087\n",
      "  timesteps_since_restore: 776600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 776600\n",
      "  training_iteration: 353\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2837 s, 353 iter, 776600 ts, 163 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-28-14\n",
      "  done: false\n",
      "  episode_len_mean: 96.93\n",
      "  episode_reward_max: 241.7167340920024\n",
      "  episode_reward_mean: 172.92367129901908\n",
      "  episode_reward_min: -160.93255915554957\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 7980\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 893.139\n",
      "    load_time_ms: 2.641\n",
      "    num_steps_sampled: 778800\n",
      "    num_steps_trained: 778800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.37837743759155273\n",
      "      kl: 0.027040274813771248\n",
      "      policy_loss: 0.01114859152585268\n",
      "      total_loss: 29.867130279541016\n",
      "      vf_explained_var: 0.9505656957626343\n",
      "      vf_loss: 29.855981826782227\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.34380146861076355\n",
      "      kl: 0.021163269877433777\n",
      "      policy_loss: -0.01515531912446022\n",
      "      total_loss: 220.30072021484375\n",
      "      vf_explained_var: 0.8956772089004517\n",
      "      vf_loss: 220.31585693359375\n",
      "    sample_time_ms: 6688.142\n",
      "    update_time_ms: 9.455\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.9036743590016\n",
      "    rl_1: 123.01999694001745\n",
      "  time_since_restore: 2843.6059107780457\n",
      "  time_this_iter_s: 6.53337836265564\n",
      "  time_total_s: 2843.6059107780457\n",
      "  timestamp: 1552318094\n",
      "  timesteps_since_restore: 778800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 778800\n",
      "  training_iteration: 354\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2843 s, 354 iter, 778800 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-28-21\n",
      "  done: false\n",
      "  episode_len_mean: 98.73\n",
      "  episode_reward_max: 241.7167340920024\n",
      "  episode_reward_mean: 183.91781713020686\n",
      "  episode_reward_min: -160.93255915554957\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 8002\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 892.126\n",
      "    load_time_ms: 2.739\n",
      "    num_steps_sampled: 781000\n",
      "    num_steps_trained: 781000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6337037086486816\n",
      "      kl: 0.02203279361128807\n",
      "      policy_loss: 0.01112437155097723\n",
      "      total_loss: 118.82781982421875\n",
      "      vf_explained_var: 0.8857768177986145\n",
      "      vf_loss: 118.81671142578125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3927326798439026\n",
      "      kl: 0.009662475436925888\n",
      "      policy_loss: -0.007344892714172602\n",
      "      total_loss: 363.8082580566406\n",
      "      vf_explained_var: 0.871039628982544\n",
      "      vf_loss: 363.81561279296875\n",
      "    sample_time_ms: 6744.455\n",
      "    update_time_ms: 8.475\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.18696021702229\n",
      "    rl_1: 129.7308569131846\n",
      "  time_since_restore: 2850.504969358444\n",
      "  time_this_iter_s: 6.89905858039856\n",
      "  time_total_s: 2850.504969358444\n",
      "  timestamp: 1552318101\n",
      "  timesteps_since_restore: 781000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 781000\n",
      "  training_iteration: 355\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2850 s, 355 iter, 781000 ts, 184 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-28-27\n",
      "  done: false\n",
      "  episode_len_mean: 99.72\n",
      "  episode_reward_max: 241.7167340920024\n",
      "  episode_reward_mean: 188.24386353280312\n",
      "  episode_reward_min: -159.69946587409265\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8025\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 868.289\n",
      "    load_time_ms: 2.665\n",
      "    num_steps_sampled: 783200\n",
      "    num_steps_trained: 783200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6018396615982056\n",
      "      kl: 5.150440216064453\n",
      "      policy_loss: 0.13632357120513916\n",
      "      total_loss: 99.25482177734375\n",
      "      vf_explained_var: 0.9156123399734497\n",
      "      vf_loss: 99.11848449707031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.36703452467918396\n",
      "      kl: 0.004109695553779602\n",
      "      policy_loss: 0.00039947647019289434\n",
      "      total_loss: 349.7982177734375\n",
      "      vf_explained_var: 0.9005177021026611\n",
      "      vf_loss: 349.7978210449219\n",
      "    sample_time_ms: 6486.896\n",
      "    update_time_ms: 8.409\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.14798585118589\n",
      "    rl_1: 133.09587768161728\n",
      "  time_since_restore: 2856.729109287262\n",
      "  time_this_iter_s: 6.224139928817749\n",
      "  time_total_s: 2856.729109287262\n",
      "  timestamp: 1552318107\n",
      "  timesteps_since_restore: 783200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 783200\n",
      "  training_iteration: 356\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2856 s, 356 iter, 783200 ts, 188 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-28-36\n",
      "  done: false\n",
      "  episode_len_mean: 98.72\n",
      "  episode_reward_max: 241.94087295249108\n",
      "  episode_reward_mean: 178.81449084777316\n",
      "  episode_reward_min: -159.69946587409265\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 8049\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 884.948\n",
      "    load_time_ms: 2.688\n",
      "    num_steps_sampled: 785400\n",
      "    num_steps_trained: 785400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.35066917538642883\n",
      "      kl: 0.006104107480496168\n",
      "      policy_loss: 0.00757416570559144\n",
      "      total_loss: 143.56573486328125\n",
      "      vf_explained_var: 0.9080760478973389\n",
      "      vf_loss: 143.55816650390625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.362554132938385\n",
      "      kl: 0.004431644454598427\n",
      "      policy_loss: 0.00016402651090174913\n",
      "      total_loss: 414.903076171875\n",
      "      vf_explained_var: 0.8826334476470947\n",
      "      vf_loss: 414.9029541015625\n",
      "    sample_time_ms: 6606.667\n",
      "    update_time_ms: 8.407\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.84859205485745\n",
      "    rl_1: 126.96589879291574\n",
      "  time_since_restore: 2865.782632112503\n",
      "  time_this_iter_s: 9.053522825241089\n",
      "  time_total_s: 2865.782632112503\n",
      "  timestamp: 1552318116\n",
      "  timesteps_since_restore: 785400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 785400\n",
      "  training_iteration: 357\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2865 s, 357 iter, 785400 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-28-43\n",
      "  done: false\n",
      "  episode_len_mean: 98.2\n",
      "  episode_reward_max: 241.94087295249108\n",
      "  episode_reward_mean: 174.18763643038724\n",
      "  episode_reward_min: -159.69946587409265\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 8070\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 883.996\n",
      "    load_time_ms: 2.722\n",
      "    num_steps_sampled: 787600\n",
      "    num_steps_trained: 787600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6143969893455505\n",
      "      kl: 0.009411818347871304\n",
      "      policy_loss: -0.005702926777303219\n",
      "      total_loss: 257.5848083496094\n",
      "      vf_explained_var: 0.5860368609428406\n",
      "      vf_loss: 257.590576171875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.35682913661003113\n",
      "      kl: 0.01230543851852417\n",
      "      policy_loss: -0.006233919411897659\n",
      "      total_loss: 578.8848266601562\n",
      "      vf_explained_var: 0.6776100397109985\n",
      "      vf_loss: 578.8910522460938\n",
      "    sample_time_ms: 6490.556\n",
      "    update_time_ms: 8.838\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.9551209564916\n",
      "    rl_1: 124.23251547389566\n",
      "  time_since_restore: 2872.3059566020966\n",
      "  time_this_iter_s: 6.523324489593506\n",
      "  time_total_s: 2872.3059566020966\n",
      "  timestamp: 1552318123\n",
      "  timesteps_since_restore: 787600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 787600\n",
      "  training_iteration: 358\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2872 s, 358 iter, 787600 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-28-49\n",
      "  done: false\n",
      "  episode_len_mean: 95.88\n",
      "  episode_reward_max: 241.94087295249108\n",
      "  episode_reward_mean: 160.62660800732317\n",
      "  episode_reward_min: -153.11127082812646\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 8095\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 835.298\n",
      "    load_time_ms: 2.584\n",
      "    num_steps_sampled: 789800\n",
      "    num_steps_trained: 789800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.30348488688468933\n",
      "      kl: 0.011936438269913197\n",
      "      policy_loss: -0.0007321273442357779\n",
      "      total_loss: 469.24176025390625\n",
      "      vf_explained_var: 0.6852202415466309\n",
      "      vf_loss: 469.2424621582031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3212336599826813\n",
      "      kl: 0.003449295414611697\n",
      "      policy_loss: -0.0022610973101109266\n",
      "      total_loss: 870.77001953125\n",
      "      vf_explained_var: 0.7172780632972717\n",
      "      vf_loss: 870.7722778320312\n",
      "    sample_time_ms: 6384.627\n",
      "    update_time_ms: 8.759\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.02190241941416\n",
      "    rl_1: 115.60470558790901\n",
      "  time_since_restore: 2878.7684173583984\n",
      "  time_this_iter_s: 6.46246075630188\n",
      "  time_total_s: 2878.7684173583984\n",
      "  timestamp: 1552318129\n",
      "  timesteps_since_restore: 789800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 789800\n",
      "  training_iteration: 359\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2878 s, 359 iter, 789800 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-28-56\n",
      "  done: false\n",
      "  episode_len_mean: 94.57\n",
      "  episode_reward_max: 241.94087295249108\n",
      "  episode_reward_mean: 152.45088691445181\n",
      "  episode_reward_min: -157.59641870142383\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8118\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 869.957\n",
      "    load_time_ms: 2.746\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.653792679309845\n",
      "      kl: 0.00552340829744935\n",
      "      policy_loss: -0.0032445918768644333\n",
      "      total_loss: 461.81329345703125\n",
      "      vf_explained_var: 0.5517887473106384\n",
      "      vf_loss: 461.8165588378906\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3256179988384247\n",
      "      kl: 0.007436815649271011\n",
      "      policy_loss: -0.005179510917514563\n",
      "      total_loss: 874.93505859375\n",
      "      vf_explained_var: 0.6565807461738586\n",
      "      vf_loss: 874.940185546875\n",
      "    sample_time_ms: 6212.367\n",
      "    update_time_ms: 8.114\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.384710954432386\n",
      "    rl_1: 111.0661759600194\n",
      "  time_since_restore: 2885.3305928707123\n",
      "  time_this_iter_s: 6.562175512313843\n",
      "  time_total_s: 2885.3305928707123\n",
      "  timestamp: 1552318136\n",
      "  timesteps_since_restore: 792000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 360\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2885 s, 360 iter, 792000 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-29-02\n",
      "  done: false\n",
      "  episode_len_mean: 95.37\n",
      "  episode_reward_max: 241.94087295249108\n",
      "  episode_reward_mean: 161.75313427126994\n",
      "  episode_reward_min: -157.59641870142383\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 8142\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 869.14\n",
      "    load_time_ms: 2.789\n",
      "    num_steps_sampled: 794200\n",
      "    num_steps_trained: 794200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4289771318435669\n",
      "      kl: 0.01897527277469635\n",
      "      policy_loss: -0.0030062978621572256\n",
      "      total_loss: 110.07209777832031\n",
      "      vf_explained_var: 0.899724543094635\n",
      "      vf_loss: 110.07511901855469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3469570279121399\n",
      "      kl: 0.006266334559768438\n",
      "      policy_loss: -0.004424952436238527\n",
      "      total_loss: 366.3218078613281\n",
      "      vf_explained_var: 0.8628140091896057\n",
      "      vf_loss: 366.32623291015625\n",
      "    sample_time_ms: 6248.672\n",
      "    update_time_ms: 7.975\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.29296093249087\n",
      "    rl_1: 116.46017333877907\n",
      "  time_since_restore: 2891.8602206707\n",
      "  time_this_iter_s: 6.529627799987793\n",
      "  time_total_s: 2891.8602206707\n",
      "  timestamp: 1552318142\n",
      "  timesteps_since_restore: 794200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 794200\n",
      "  training_iteration: 361\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2891 s, 361 iter, 794200 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-29-11\n",
      "  done: false\n",
      "  episode_len_mean: 93.55\n",
      "  episode_reward_max: 241.1316658983555\n",
      "  episode_reward_mean: 147.57860978643356\n",
      "  episode_reward_min: -157.59641870142383\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8165\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 868.389\n",
      "    load_time_ms: 2.86\n",
      "    num_steps_sampled: 796400\n",
      "    num_steps_trained: 796400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.37170031666755676\n",
      "      kl: 0.011131877079606056\n",
      "      policy_loss: 0.0002388209104537964\n",
      "      total_loss: 170.61376953125\n",
      "      vf_explained_var: 0.8888863325119019\n",
      "      vf_loss: 170.613525390625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.33153343200683594\n",
      "      kl: 0.005436036270111799\n",
      "      policy_loss: -0.002842407440766692\n",
      "      total_loss: 467.9718017578125\n",
      "      vf_explained_var: 0.8682747483253479\n",
      "      vf_loss: 467.9746398925781\n",
      "    sample_time_ms: 6144.55\n",
      "    update_time_ms: 7.69\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.20189010840635\n",
      "    rl_1: 108.3767196780272\n",
      "  time_since_restore: 2900.7894310951233\n",
      "  time_this_iter_s: 8.929210424423218\n",
      "  time_total_s: 2900.7894310951233\n",
      "  timestamp: 1552318151\n",
      "  timesteps_since_restore: 796400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 796400\n",
      "  training_iteration: 362\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2900 s, 362 iter, 796400 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-29-19\n",
      "  done: false\n",
      "  episode_len_mean: 93.29\n",
      "  episode_reward_max: 241.1316658983555\n",
      "  episode_reward_mean: 146.33851503350635\n",
      "  episode_reward_min: -165.642359685687\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 8190\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 905.766\n",
      "    load_time_ms: 3.008\n",
      "    num_steps_sampled: 798600\n",
      "    num_steps_trained: 798600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6869050860404968\n",
      "      kl: 0.01033721398562193\n",
      "      policy_loss: -0.011123890988528728\n",
      "      total_loss: 730.1014404296875\n",
      "      vf_explained_var: 0.35756126046180725\n",
      "      vf_loss: 730.1126708984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3512021601200104\n",
      "      kl: 0.006445907987654209\n",
      "      policy_loss: -0.016154928132891655\n",
      "      total_loss: 1169.36865234375\n",
      "      vf_explained_var: 0.5286784172058105\n",
      "      vf_loss: 1169.3848876953125\n",
      "    sample_time_ms: 6245.578\n",
      "    update_time_ms: 8.019\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.12170073715982\n",
      "    rl_1: 108.21681429634653\n",
      "  time_since_restore: 2908.8128402233124\n",
      "  time_this_iter_s: 8.023409128189087\n",
      "  time_total_s: 2908.8128402233124\n",
      "  timestamp: 1552318159\n",
      "  timesteps_since_restore: 798600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 798600\n",
      "  training_iteration: 363\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2908 s, 363 iter, 798600 ts, 146 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-29-27\n",
      "  done: false\n",
      "  episode_len_mean: 94.06\n",
      "  episode_reward_max: 241.1316658983555\n",
      "  episode_reward_mean: 153.64830650015637\n",
      "  episode_reward_min: -165.642359685687\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8213\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 946.531\n",
      "    load_time_ms: 3.044\n",
      "    num_steps_sampled: 800800\n",
      "    num_steps_trained: 800800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5771416425704956\n",
      "      kl: 0.013026467524468899\n",
      "      policy_loss: -0.011539946310222149\n",
      "      total_loss: 302.7681884765625\n",
      "      vf_explained_var: 0.6233596801757812\n",
      "      vf_loss: 302.77972412109375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.35850173234939575\n",
      "      kl: 0.005914035253226757\n",
      "      policy_loss: -0.006186818238347769\n",
      "      total_loss: 662.2788696289062\n",
      "      vf_explained_var: 0.6540763974189758\n",
      "      vf_loss: 662.2850952148438\n",
      "    sample_time_ms: 6345.663\n",
      "    update_time_ms: 8.33\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.31069464227717\n",
      "    rl_1: 112.33761185787917\n",
      "  time_since_restore: 2916.7641241550446\n",
      "  time_this_iter_s: 7.951283931732178\n",
      "  time_total_s: 2916.7641241550446\n",
      "  timestamp: 1552318167\n",
      "  timesteps_since_restore: 800800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 800800\n",
      "  training_iteration: 364\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2916 s, 364 iter, 800800 ts, 154 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-29-35\n",
      "  done: false\n",
      "  episode_len_mean: 92.46\n",
      "  episode_reward_max: 240.49967907361255\n",
      "  episode_reward_mean: 142.20211749075258\n",
      "  episode_reward_min: -166.00544206606466\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 8235\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 939.778\n",
      "    load_time_ms: 2.991\n",
      "    num_steps_sampled: 803000\n",
      "    num_steps_trained: 803000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7715054154396057\n",
      "      kl: 0.00704301567748189\n",
      "      policy_loss: -0.005325383972376585\n",
      "      total_loss: 457.7712097167969\n",
      "      vf_explained_var: 0.49007439613342285\n",
      "      vf_loss: 457.77655029296875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3014756739139557\n",
      "      kl: 0.005897303577512503\n",
      "      policy_loss: -0.0023290810640901327\n",
      "      total_loss: 869.521728515625\n",
      "      vf_explained_var: 0.6016035676002502\n",
      "      vf_loss: 869.5240478515625\n",
      "    sample_time_ms: 6389.568\n",
      "    update_time_ms: 8.81\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.15280931769514\n",
      "    rl_1: 106.04930817305745\n",
      "  time_since_restore: 2924.0379695892334\n",
      "  time_this_iter_s: 7.273845434188843\n",
      "  time_total_s: 2924.0379695892334\n",
      "  timestamp: 1552318175\n",
      "  timesteps_since_restore: 803000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 803000\n",
      "  training_iteration: 365\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2924 s, 365 iter, 803000 ts, 142 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-29-41\n",
      "  done: false\n",
      "  episode_len_mean: 94.49\n",
      "  episode_reward_max: 241.09674517500574\n",
      "  episode_reward_mean: 154.88552510537932\n",
      "  episode_reward_min: -166.00544206606466\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8258\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 932.704\n",
      "    load_time_ms: 3.051\n",
      "    num_steps_sampled: 805200\n",
      "    num_steps_trained: 805200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6099123954772949\n",
      "      kl: 0.027240172028541565\n",
      "      policy_loss: 0.005445634014904499\n",
      "      total_loss: 103.70774841308594\n",
      "      vf_explained_var: 0.8861395120620728\n",
      "      vf_loss: 103.70230865478516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3035774230957031\n",
      "      kl: 0.008091670460999012\n",
      "      policy_loss: -0.006286932155489922\n",
      "      total_loss: 367.2066955566406\n",
      "      vf_explained_var: 0.8674349784851074\n",
      "      vf_loss: 367.2129821777344\n",
      "    sample_time_ms: 6417.822\n",
      "    update_time_ms: 9.011\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.59360586838689\n",
      "    rl_1: 114.29191923699244\n",
      "  time_since_restore: 2930.4765207767487\n",
      "  time_this_iter_s: 6.438551187515259\n",
      "  time_total_s: 2930.4765207767487\n",
      "  timestamp: 1552318181\n",
      "  timesteps_since_restore: 805200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 805200\n",
      "  training_iteration: 366\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2930 s, 366 iter, 805200 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-29-47\n",
      "  done: false\n",
      "  episode_len_mean: 96.53\n",
      "  episode_reward_max: 241.09674517500574\n",
      "  episode_reward_mean: 167.25833690413378\n",
      "  episode_reward_min: -166.00544206606466\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 8280\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 885.204\n",
      "    load_time_ms: 3.051\n",
      "    num_steps_sampled: 807400\n",
      "    num_steps_trained: 807400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5511035323143005\n",
      "      kl: 0.029151368886232376\n",
      "      policy_loss: 0.007614864967763424\n",
      "      total_loss: 75.01876068115234\n",
      "      vf_explained_var: 0.9222037196159363\n",
      "      vf_loss: 75.01114654541016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4160911738872528\n",
      "      kl: 0.02056177146732807\n",
      "      policy_loss: -0.011565945111215115\n",
      "      total_loss: 258.423095703125\n",
      "      vf_explained_var: 0.9188435077667236\n",
      "      vf_loss: 258.4346618652344\n",
      "    sample_time_ms: 6171.936\n",
      "    update_time_ms: 8.81\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.94365638580831\n",
      "    rl_1: 121.31468051832552\n",
      "  time_since_restore: 2936.5902688503265\n",
      "  time_this_iter_s: 6.113748073577881\n",
      "  time_total_s: 2936.5902688503265\n",
      "  timestamp: 1552318187\n",
      "  timesteps_since_restore: 807400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 807400\n",
      "  training_iteration: 367\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2936 s, 367 iter, 807400 ts, 167 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-29-53\n",
      "  done: false\n",
      "  episode_len_mean: 96.26\n",
      "  episode_reward_max: 241.09674517500574\n",
      "  episode_reward_mean: 165.56795527748963\n",
      "  episode_reward_min: -166.00544206606466\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8303\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 883.093\n",
      "    load_time_ms: 3.101\n",
      "    num_steps_sampled: 809600\n",
      "    num_steps_trained: 809600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.44984838366508484\n",
      "      kl: 0.018490083515644073\n",
      "      policy_loss: 0.0022472243290394545\n",
      "      total_loss: 300.90155029296875\n",
      "      vf_explained_var: 0.7341558933258057\n",
      "      vf_loss: 300.89923095703125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.26287421584129333\n",
      "      kl: 0.007594497874379158\n",
      "      policy_loss: -0.007348089944571257\n",
      "      total_loss: 643.8861083984375\n",
      "      vf_explained_var: 0.7594883441925049\n",
      "      vf_loss: 643.8933715820312\n",
      "    sample_time_ms: 6114.357\n",
      "    update_time_ms: 8.555\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.75943853113989\n",
      "    rl_1: 119.80851674634974\n",
      "  time_since_restore: 2942.5155839920044\n",
      "  time_this_iter_s: 5.9253151416778564\n",
      "  time_total_s: 2942.5155839920044\n",
      "  timestamp: 1552318193\n",
      "  timesteps_since_restore: 809600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 809600\n",
      "  training_iteration: 368\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2942 s, 368 iter, 809600 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-30-00\n",
      "  done: false\n",
      "  episode_len_mean: 95.95\n",
      "  episode_reward_max: 241.09674517500574\n",
      "  episode_reward_mean: 161.06264275106057\n",
      "  episode_reward_min: -166.00544206606466\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 8327\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 885.072\n",
      "    load_time_ms: 3.084\n",
      "    num_steps_sampled: 811800\n",
      "    num_steps_trained: 811800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.747560441493988\n",
      "      kl: 0.011769069358706474\n",
      "      policy_loss: -0.00809110514819622\n",
      "      total_loss: 409.0386657714844\n",
      "      vf_explained_var: 0.6140372157096863\n",
      "      vf_loss: 409.0467834472656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24984805285930634\n",
      "      kl: 0.008633175864815712\n",
      "      policy_loss: -0.005204931832849979\n",
      "      total_loss: 762.029541015625\n",
      "      vf_explained_var: 0.7324578762054443\n",
      "      vf_loss: 762.0347290039062\n",
      "    sample_time_ms: 6086.087\n",
      "    update_time_ms: 8.6\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.57253630392074\n",
      "    rl_1: 117.49010644713982\n",
      "  time_since_restore: 2948.716453552246\n",
      "  time_this_iter_s: 6.200869560241699\n",
      "  time_total_s: 2948.716453552246\n",
      "  timestamp: 1552318200\n",
      "  timesteps_since_restore: 811800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 811800\n",
      "  training_iteration: 369\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2948 s, 369 iter, 811800 ts, 161 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-30-08\n",
      "  done: false\n",
      "  episode_len_mean: 96.34\n",
      "  episode_reward_max: 241.03463946435463\n",
      "  episode_reward_mean: 161.8811057193254\n",
      "  episode_reward_min: -161.95031944653113\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8350\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 851.044\n",
      "    load_time_ms: 2.861\n",
      "    num_steps_sampled: 814000\n",
      "    num_steps_trained: 814000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5621099472045898\n",
      "      kl: 0.005078576970845461\n",
      "      policy_loss: -0.001006695325486362\n",
      "      total_loss: 265.7160949707031\n",
      "      vf_explained_var: 0.780419111251831\n",
      "      vf_loss: 265.71710205078125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3105837106704712\n",
      "      kl: 0.005726999137550592\n",
      "      policy_loss: -0.004512634128332138\n",
      "      total_loss: 538.4003295898438\n",
      "      vf_explained_var: 0.8301897644996643\n",
      "      vf_loss: 538.4048461914062\n",
      "    sample_time_ms: 6298.405\n",
      "    update_time_ms: 8.599\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.95906820392867\n",
      "    rl_1: 117.92203751539674\n",
      "  time_since_restore: 2957.058078765869\n",
      "  time_this_iter_s: 8.341625213623047\n",
      "  time_total_s: 2957.058078765869\n",
      "  timestamp: 1552318208\n",
      "  timesteps_since_restore: 814000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 814000\n",
      "  training_iteration: 370\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2957 s, 370 iter, 814000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-30-16\n",
      "  done: false\n",
      "  episode_len_mean: 96.34\n",
      "  episode_reward_max: 241.03463946435463\n",
      "  episode_reward_mean: 159.5506925889989\n",
      "  episode_reward_min: -161.95031944653113\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 8371\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 877.244\n",
      "    load_time_ms: 2.771\n",
      "    num_steps_sampled: 816200\n",
      "    num_steps_trained: 816200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.090528964996338\n",
      "      kl: 0.004053551238030195\n",
      "      policy_loss: -7.657794049009681e-05\n",
      "      total_loss: 113.66438293457031\n",
      "      vf_explained_var: 0.7787659764289856\n",
      "      vf_loss: 113.66445922851562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3955504596233368\n",
      "      kl: 0.004773650784045458\n",
      "      policy_loss: -0.001808689790777862\n",
      "      total_loss: 339.1720886230469\n",
      "      vf_explained_var: 0.8629016280174255\n",
      "      vf_loss: 339.17388916015625\n",
      "    sample_time_ms: 6372.594\n",
      "    update_time_ms: 8.692\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.762328393520995\n",
      "    rl_1: 116.78836419547791\n",
      "  time_since_restore: 2964.600733280182\n",
      "  time_this_iter_s: 7.542654514312744\n",
      "  time_total_s: 2964.600733280182\n",
      "  timestamp: 1552318216\n",
      "  timesteps_since_restore: 816200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 816200\n",
      "  training_iteration: 371\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2964 s, 371 iter, 816200 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-30-22\n",
      "  done: false\n",
      "  episode_len_mean: 94.4\n",
      "  episode_reward_max: 241.2454066366317\n",
      "  episode_reward_mean: 147.8417065277235\n",
      "  episode_reward_min: -154.91464728419368\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 8397\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 834.334\n",
      "    load_time_ms: 2.745\n",
      "    num_steps_sampled: 818400\n",
      "    num_steps_trained: 818400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.17812590301036835\n",
      "      kl: 0.005980100482702255\n",
      "      policy_loss: -0.006707449909299612\n",
      "      total_loss: 475.315185546875\n",
      "      vf_explained_var: 0.7667663097381592\n",
      "      vf_loss: 475.32196044921875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.15244053304195404\n",
      "      kl: 0.004175894428044558\n",
      "      policy_loss: -0.0011408249847590923\n",
      "      total_loss: 956.9388427734375\n",
      "      vf_explained_var: 0.7742770910263062\n",
      "      vf_loss: 956.93994140625\n",
      "    sample_time_ms: 6127.972\n",
      "    update_time_ms: 9.462\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.60373178769202\n",
      "    rl_1: 110.23797474003146\n",
      "  time_since_restore: 2970.657930135727\n",
      "  time_this_iter_s: 6.057196855545044\n",
      "  time_total_s: 2970.657930135727\n",
      "  timestamp: 1552318222\n",
      "  timesteps_since_restore: 818400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 818400\n",
      "  training_iteration: 372\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2970 s, 372 iter, 818400 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-30-27\n",
      "  done: false\n",
      "  episode_len_mean: 94.87\n",
      "  episode_reward_max: 241.2454066366317\n",
      "  episode_reward_mean: 152.25810089568893\n",
      "  episode_reward_min: -154.91464728419368\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8420\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 794.937\n",
      "    load_time_ms: 2.591\n",
      "    num_steps_sampled: 820600\n",
      "    num_steps_trained: 820600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.500564694404602\n",
      "      kl: 0.02215999737381935\n",
      "      policy_loss: -0.0004201628908049315\n",
      "      total_loss: 232.61114501953125\n",
      "      vf_explained_var: 0.6387631297111511\n",
      "      vf_loss: 232.6115264892578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.33560413122177124\n",
      "      kl: 0.006128950510174036\n",
      "      policy_loss: -0.003009913954883814\n",
      "      total_loss: 474.4508972167969\n",
      "      vf_explained_var: 0.7428641319274902\n",
      "      vf_loss: 474.4538879394531\n",
      "    sample_time_ms: 5936.199\n",
      "    update_time_ms: 8.911\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.253685436832946\n",
      "    rl_1: 112.00441545885596\n",
      "  time_since_restore: 2976.356220960617\n",
      "  time_this_iter_s: 5.698290824890137\n",
      "  time_total_s: 2976.356220960617\n",
      "  timestamp: 1552318227\n",
      "  timesteps_since_restore: 820600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 820600\n",
      "  training_iteration: 373\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2976 s, 373 iter, 820600 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-30-35\n",
      "  done: false\n",
      "  episode_len_mean: 94.6\n",
      "  episode_reward_max: 241.2454066366317\n",
      "  episode_reward_mean: 152.9764407539797\n",
      "  episode_reward_min: -154.91464728419368\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8443\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 760.123\n",
      "    load_time_ms: 2.578\n",
      "    num_steps_sampled: 822800\n",
      "    num_steps_trained: 822800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5502626299858093\n",
      "      kl: 0.007836730219423771\n",
      "      policy_loss: -0.005642862990498543\n",
      "      total_loss: 269.66021728515625\n",
      "      vf_explained_var: 0.7332158088684082\n",
      "      vf_loss: 269.66583251953125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.27860361337661743\n",
      "      kl: 0.006696335505694151\n",
      "      policy_loss: -0.01276933029294014\n",
      "      total_loss: 536.2282104492188\n",
      "      vf_explained_var: 0.7810031771659851\n",
      "      vf_loss: 536.240966796875\n",
      "    sample_time_ms: 5904.603\n",
      "    update_time_ms: 8.631\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.55457249129078\n",
      "    rl_1: 112.42186826268889\n",
      "  time_since_restore: 2983.6356117725372\n",
      "  time_this_iter_s: 7.279390811920166\n",
      "  time_total_s: 2983.6356117725372\n",
      "  timestamp: 1552318235\n",
      "  timesteps_since_restore: 822800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 822800\n",
      "  training_iteration: 374\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2983 s, 374 iter, 822800 ts, 153 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-30-41\n",
      "  done: false\n",
      "  episode_len_mean: 94.3\n",
      "  episode_reward_max: 241.2454066366317\n",
      "  episode_reward_mean: 155.03603398349512\n",
      "  episode_reward_min: -154.91464728419368\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 8465\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 763.388\n",
      "    load_time_ms: 2.558\n",
      "    num_steps_sampled: 825000\n",
      "    num_steps_trained: 825000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.45730090141296387\n",
      "      kl: 0.005331714637577534\n",
      "      policy_loss: 0.0006628803675994277\n",
      "      total_loss: 97.3924331665039\n",
      "      vf_explained_var: 0.8834371566772461\n",
      "      vf_loss: 97.39178466796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.29919883608818054\n",
      "      kl: 0.011052398942410946\n",
      "      policy_loss: -0.006852400954812765\n",
      "      total_loss: 332.0510559082031\n",
      "      vf_explained_var: 0.8777117729187012\n",
      "      vf_loss: 332.05792236328125\n",
      "    sample_time_ms: 5782.341\n",
      "    update_time_ms: 8.226\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.79392703311173\n",
      "    rl_1: 113.24210695038335\n",
      "  time_since_restore: 2989.7173528671265\n",
      "  time_this_iter_s: 6.081741094589233\n",
      "  time_total_s: 2989.7173528671265\n",
      "  timestamp: 1552318241\n",
      "  timesteps_since_restore: 825000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 825000\n",
      "  training_iteration: 375\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2989 s, 375 iter, 825000 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-30-48\n",
      "  done: false\n",
      "  episode_len_mean: 93.31\n",
      "  episode_reward_max: 239.99357272540345\n",
      "  episode_reward_mean: 143.77181671176848\n",
      "  episode_reward_min: -157.91149113235264\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 8492\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 788.599\n",
      "    load_time_ms: 2.502\n",
      "    num_steps_sampled: 827200\n",
      "    num_steps_trained: 827200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4549886882305145\n",
      "      kl: 0.010912321507930756\n",
      "      policy_loss: 0.0030087768100202084\n",
      "      total_loss: 394.89495849609375\n",
      "      vf_explained_var: 0.8073869347572327\n",
      "      vf_loss: 394.8919372558594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.31770259141921997\n",
      "      kl: 0.009417959488928318\n",
      "      policy_loss: -0.002535345498472452\n",
      "      total_loss: 733.9323120117188\n",
      "      vf_explained_var: 0.8521896004676819\n",
      "      vf_loss: 733.9349365234375\n",
      "    sample_time_ms: 5790.704\n",
      "    update_time_ms: 8.032\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.564294373052114\n",
      "    rl_1: 106.20752233871633\n",
      "  time_since_restore: 2996.4880590438843\n",
      "  time_this_iter_s: 6.7707061767578125\n",
      "  time_total_s: 2996.4880590438843\n",
      "  timestamp: 1552318248\n",
      "  timesteps_since_restore: 827200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 827200\n",
      "  training_iteration: 376\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 2996 s, 376 iter, 827200 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-30-56\n",
      "  done: false\n",
      "  episode_len_mean: 93.62\n",
      "  episode_reward_max: 239.99357272540345\n",
      "  episode_reward_mean: 146.89538816761987\n",
      "  episode_reward_min: -157.91149113235264\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 8514\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 831.187\n",
      "    load_time_ms: 2.586\n",
      "    num_steps_sampled: 829400\n",
      "    num_steps_trained: 829400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.46337777376174927\n",
      "      kl: 0.0065466598607599735\n",
      "      policy_loss: -0.007169889286160469\n",
      "      total_loss: 138.6024627685547\n",
      "      vf_explained_var: 0.8706132769584656\n",
      "      vf_loss: 138.60964965820312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3335871994495392\n",
      "      kl: 0.011884325183928013\n",
      "      policy_loss: -0.004938231781125069\n",
      "      total_loss: 420.5673828125\n",
      "      vf_explained_var: 0.8559557795524597\n",
      "      vf_loss: 420.57232666015625\n",
      "    sample_time_ms: 5956.727\n",
      "    update_time_ms: 8.174\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.40672814567272\n",
      "    rl_1: 107.48866002194713\n",
      "  time_since_restore: 3004.6964671611786\n",
      "  time_this_iter_s: 8.208408117294312\n",
      "  time_total_s: 3004.6964671611786\n",
      "  timestamp: 1552318256\n",
      "  timesteps_since_restore: 829400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 829400\n",
      "  training_iteration: 377\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3004 s, 377 iter, 829400 ts, 147 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-31-04\n",
      "  done: false\n",
      "  episode_len_mean: 91.63\n",
      "  episode_reward_max: 238.74335452042706\n",
      "  episode_reward_mean: 131.27793174622025\n",
      "  episode_reward_min: -157.91149113235264\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 8539\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 841.277\n",
      "    load_time_ms: 2.522\n",
      "    num_steps_sampled: 831600\n",
      "    num_steps_trained: 831600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.40920940041542053\n",
      "      kl: 0.003534052288159728\n",
      "      policy_loss: -0.001452636905014515\n",
      "      total_loss: 615.623779296875\n",
      "      vf_explained_var: 0.589066207408905\n",
      "      vf_loss: 615.625244140625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.25509560108184814\n",
      "      kl: 0.010086296126246452\n",
      "      policy_loss: -0.006191834341734648\n",
      "      total_loss: 1028.825927734375\n",
      "      vf_explained_var: 0.6853270530700684\n",
      "      vf_loss: 1028.832275390625\n",
      "    sample_time_ms: 6134.15\n",
      "    update_time_ms: 8.487\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.554646214216234\n",
      "    rl_1: 98.72328553200397\n",
      "  time_since_restore: 3012.4997589588165\n",
      "  time_this_iter_s: 7.8032917976379395\n",
      "  time_total_s: 3012.4997589588165\n",
      "  timestamp: 1552318264\n",
      "  timesteps_since_restore: 831600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 831600\n",
      "  training_iteration: 378\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3012 s, 378 iter, 831600 ts, 131 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-31-10\n",
      "  done: false\n",
      "  episode_len_mean: 91.6\n",
      "  episode_reward_max: 239.90086354523925\n",
      "  episode_reward_mean: 130.79816702929938\n",
      "  episode_reward_min: -164.2198158648497\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 8561\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 836.496\n",
      "    load_time_ms: 2.609\n",
      "    num_steps_sampled: 833800\n",
      "    num_steps_trained: 833800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8167334794998169\n",
      "      kl: 0.008949186652898788\n",
      "      policy_loss: -0.0025657275691628456\n",
      "      total_loss: 84.35900115966797\n",
      "      vf_explained_var: 0.903674304485321\n",
      "      vf_loss: 84.36155700683594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2810390293598175\n",
      "      kl: 0.011044767685234547\n",
      "      policy_loss: -0.00022181561507750303\n",
      "      total_loss: 283.142822265625\n",
      "      vf_explained_var: 0.8772987127304077\n",
      "      vf_loss: 283.14300537109375\n",
      "    sample_time_ms: 6114.727\n",
      "    update_time_ms: 8.506\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.26659314432641\n",
      "    rl_1: 98.53157388497294\n",
      "  time_since_restore: 3018.4582200050354\n",
      "  time_this_iter_s: 5.958461046218872\n",
      "  time_total_s: 3018.4582200050354\n",
      "  timestamp: 1552318270\n",
      "  timesteps_since_restore: 833800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 833800\n",
      "  training_iteration: 379\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3018 s, 379 iter, 833800 ts, 131 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-31-16\n",
      "  done: false\n",
      "  episode_len_mean: 93.77\n",
      "  episode_reward_max: 239.90086354523925\n",
      "  episode_reward_mean: 145.01348910002895\n",
      "  episode_reward_min: -164.2198158648497\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8584\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 829.649\n",
      "    load_time_ms: 2.754\n",
      "    num_steps_sampled: 836000\n",
      "    num_steps_trained: 836000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6828339099884033\n",
      "      kl: 0.012333767488598824\n",
      "      policy_loss: -0.004979840479791164\n",
      "      total_loss: 150.4633331298828\n",
      "      vf_explained_var: 0.8422608375549316\n",
      "      vf_loss: 150.46832275390625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.251302033662796\n",
      "      kl: 0.01253705658018589\n",
      "      policy_loss: -0.006939813494682312\n",
      "      total_loss: 399.4510192871094\n",
      "      vf_explained_var: 0.8696873784065247\n",
      "      vf_loss: 399.4579772949219\n",
      "    sample_time_ms: 5850.415\n",
      "    update_time_ms: 8.605\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.455491550382\n",
      "    rl_1: 107.55799754964697\n",
      "  time_since_restore: 3024.0897102355957\n",
      "  time_this_iter_s: 5.631490230560303\n",
      "  time_total_s: 3024.0897102355957\n",
      "  timestamp: 1552318276\n",
      "  timesteps_since_restore: 836000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 836000\n",
      "  training_iteration: 380\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3024 s, 380 iter, 836000 ts, 145 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-31-23\n",
      "  done: false\n",
      "  episode_len_mean: 93.81\n",
      "  episode_reward_max: 239.90086354523925\n",
      "  episode_reward_mean: 148.02233540779045\n",
      "  episode_reward_min: -164.2198158648497\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 8609\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 804.191\n",
      "    load_time_ms: 2.829\n",
      "    num_steps_sampled: 838200\n",
      "    num_steps_trained: 838200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4219434857368469\n",
      "      kl: 0.01459053810685873\n",
      "      policy_loss: -4.100383375771344e-05\n",
      "      total_loss: 308.61468505859375\n",
      "      vf_explained_var: 0.7832199931144714\n",
      "      vf_loss: 308.6147155761719\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.25365227460861206\n",
      "      kl: 0.010124518536031246\n",
      "      policy_loss: -0.006099067162722349\n",
      "      total_loss: 624.199462890625\n",
      "      vf_explained_var: 0.7970789074897766\n",
      "      vf_loss: 624.20556640625\n",
      "    sample_time_ms: 5878.807\n",
      "    update_time_ms: 8.513\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.49182537817956\n",
      "    rl_1: 109.53051002961092\n",
      "  time_since_restore: 3031.6526503562927\n",
      "  time_this_iter_s: 7.5629401206970215\n",
      "  time_total_s: 3031.6526503562927\n",
      "  timestamp: 1552318283\n",
      "  timesteps_since_restore: 838200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 838200\n",
      "  training_iteration: 381\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3031 s, 381 iter, 838200 ts, 148 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-31-30\n",
      "  done: false\n",
      "  episode_len_mean: 93.7\n",
      "  episode_reward_max: 239.90086354523925\n",
      "  episode_reward_mean: 146.66453177231259\n",
      "  episode_reward_min: -164.2198158648497\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 8631\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 863.324\n",
      "    load_time_ms: 2.895\n",
      "    num_steps_sampled: 840400\n",
      "    num_steps_trained: 840400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6308482885360718\n",
      "      kl: 0.013906317763030529\n",
      "      policy_loss: 0.0008583933231420815\n",
      "      total_loss: 156.68829345703125\n",
      "      vf_explained_var: 0.8814573884010315\n",
      "      vf_loss: 156.6874237060547\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3154001533985138\n",
      "      kl: 0.006112595088779926\n",
      "      policy_loss: -0.006422989536076784\n",
      "      total_loss: 406.20849609375\n",
      "      vf_explained_var: 0.8768481016159058\n",
      "      vf_loss: 406.21490478515625\n",
      "    sample_time_ms: 5930.032\n",
      "    update_time_ms: 7.766\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.89216627168327\n",
      "    rl_1: 108.77236550062926\n",
      "  time_since_restore: 3038.8124709129333\n",
      "  time_this_iter_s: 7.159820556640625\n",
      "  time_total_s: 3038.8124709129333\n",
      "  timestamp: 1552318290\n",
      "  timesteps_since_restore: 840400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 840400\n",
      "  training_iteration: 382\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3038 s, 382 iter, 840400 ts, 147 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-31-37\n",
      "  done: false\n",
      "  episode_len_mean: 94.52\n",
      "  episode_reward_max: 240.4695649669602\n",
      "  episode_reward_mean: 152.0661800687096\n",
      "  episode_reward_min: -164.06719252464598\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 8655\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 864.996\n",
      "    load_time_ms: 2.996\n",
      "    num_steps_sampled: 842600\n",
      "    num_steps_trained: 842600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5150543451309204\n",
      "      kl: 0.00841414276510477\n",
      "      policy_loss: 0.0005971597856841981\n",
      "      total_loss: 239.55734252929688\n",
      "      vf_explained_var: 0.8043999671936035\n",
      "      vf_loss: 239.5567626953125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.21415449678897858\n",
      "      kl: 0.009109432809054852\n",
      "      policy_loss: -0.0019616580102592707\n",
      "      total_loss: 487.44061279296875\n",
      "      vf_explained_var: 0.8456223607063293\n",
      "      vf_loss: 487.44256591796875\n",
      "    sample_time_ms: 5979.586\n",
      "    update_time_ms: 8.176\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.19880273496115\n",
      "    rl_1: 111.86737733374846\n",
      "  time_since_restore: 3045.0309104919434\n",
      "  time_this_iter_s: 6.21843957901001\n",
      "  time_total_s: 3045.0309104919434\n",
      "  timestamp: 1552318297\n",
      "  timesteps_since_restore: 842600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 842600\n",
      "  training_iteration: 383\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3045 s, 383 iter, 842600 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-31-42\n",
      "  done: false\n",
      "  episode_len_mean: 94.77\n",
      "  episode_reward_max: 240.4695649669602\n",
      "  episode_reward_mean: 151.1057493101373\n",
      "  episode_reward_min: -164.06719252464598\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8678\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 859.661\n",
      "    load_time_ms: 3.024\n",
      "    num_steps_sampled: 844800\n",
      "    num_steps_trained: 844800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8970383405685425\n",
      "      kl: 0.018202723935246468\n",
      "      policy_loss: -0.006039976142346859\n",
      "      total_loss: 231.51263427734375\n",
      "      vf_explained_var: 0.7546592950820923\n",
      "      vf_loss: 231.51869201660156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.27184051275253296\n",
      "      kl: 0.0059949494898319244\n",
      "      policy_loss: -0.009848194196820259\n",
      "      total_loss: 502.8431091308594\n",
      "      vf_explained_var: 0.8351803421974182\n",
      "      vf_loss: 502.8529357910156\n",
      "    sample_time_ms: 5834.87\n",
      "    update_time_ms: 7.986\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.94456548448561\n",
      "    rl_1: 112.16118382565169\n",
      "  time_since_restore: 3050.807071208954\n",
      "  time_this_iter_s: 5.776160717010498\n",
      "  time_total_s: 3050.807071208954\n",
      "  timestamp: 1552318302\n",
      "  timesteps_since_restore: 844800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 844800\n",
      "  training_iteration: 384\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3050 s, 384 iter, 844800 ts, 151 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-31-51\n",
      "  done: false\n",
      "  episode_len_mean: 94.8\n",
      "  episode_reward_max: 240.97782908225253\n",
      "  episode_reward_mean: 152.25516404319836\n",
      "  episode_reward_min: -164.06719252464598\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 8700\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 863.671\n",
      "    load_time_ms: 3.066\n",
      "    num_steps_sampled: 847000\n",
      "    num_steps_trained: 847000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39777541160583496\n",
      "      kl: 0.020826326683163643\n",
      "      policy_loss: -0.004733850713819265\n",
      "      total_loss: 110.1316146850586\n",
      "      vf_explained_var: 0.8723228573799133\n",
      "      vf_loss: 110.13634490966797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2683090269565582\n",
      "      kl: 0.01164926216006279\n",
      "      policy_loss: -0.00617223372682929\n",
      "      total_loss: 315.28106689453125\n",
      "      vf_explained_var: 0.8661859631538391\n",
      "      vf_loss: 315.2872619628906\n",
      "    sample_time_ms: 6059.293\n",
      "    update_time_ms: 8.134\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.930978429886736\n",
      "    rl_1: 112.32418561331167\n",
      "  time_since_restore: 3059.1737728118896\n",
      "  time_this_iter_s: 8.366701602935791\n",
      "  time_total_s: 3059.1737728118896\n",
      "  timestamp: 1552318311\n",
      "  timesteps_since_restore: 847000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 847000\n",
      "  training_iteration: 385\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3059 s, 385 iter, 847000 ts, 152 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-31-58\n",
      "  done: false\n",
      "  episode_len_mean: 95.63\n",
      "  episode_reward_max: 240.97782908225253\n",
      "  episode_reward_mean: 155.79404668637267\n",
      "  episode_reward_min: -158.77986669623607\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 8722\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 844.618\n",
      "    load_time_ms: 3.069\n",
      "    num_steps_sampled: 849200\n",
      "    num_steps_trained: 849200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6812788844108582\n",
      "      kl: 0.012694553472101688\n",
      "      policy_loss: -0.0046642255038022995\n",
      "      total_loss: 241.03372192382812\n",
      "      vf_explained_var: 0.7530649304389954\n",
      "      vf_loss: 241.0383758544922\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.291547566652298\n",
      "      kl: 0.00943779107183218\n",
      "      policy_loss: -0.0038187564350664616\n",
      "      total_loss: 483.570556640625\n",
      "      vf_explained_var: 0.82941073179245\n",
      "      vf_loss: 483.5743408203125\n",
      "    sample_time_ms: 6146.532\n",
      "    update_time_ms: 8.167\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.348785098290925\n",
      "    rl_1: 114.44526158808176\n",
      "  time_since_restore: 3066.626836538315\n",
      "  time_this_iter_s: 7.453063726425171\n",
      "  time_total_s: 3066.626836538315\n",
      "  timestamp: 1552318318\n",
      "  timesteps_since_restore: 849200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 849200\n",
      "  training_iteration: 386\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3066 s, 386 iter, 849200 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-32-05\n",
      "  done: false\n",
      "  episode_len_mean: 98.41\n",
      "  episode_reward_max: 240.97782908225253\n",
      "  episode_reward_mean: 176.9010316555293\n",
      "  episode_reward_min: -156.67892486142387\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8745\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 795.499\n",
      "    load_time_ms: 2.918\n",
      "    num_steps_sampled: 851400\n",
      "    num_steps_trained: 851400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4734955430030823\n",
      "      kl: 0.021388640627264977\n",
      "      policy_loss: -0.0014438979560509324\n",
      "      total_loss: 257.20013427734375\n",
      "      vf_explained_var: 0.6562204360961914\n",
      "      vf_loss: 257.2015686035156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2889624238014221\n",
      "      kl: 0.008049888536334038\n",
      "      policy_loss: -0.004681816790252924\n",
      "      total_loss: 547.6631469726562\n",
      "      vf_explained_var: 0.7345558404922485\n",
      "      vf_loss: 547.6677856445312\n",
      "    sample_time_ms: 6000.176\n",
      "    update_time_ms: 8.13\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.44187947579726\n",
      "    rl_1: 126.45915217973203\n",
      "  time_since_restore: 3072.8718242645264\n",
      "  time_this_iter_s: 6.244987726211548\n",
      "  time_total_s: 3072.8718242645264\n",
      "  timestamp: 1552318325\n",
      "  timesteps_since_restore: 851400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 851400\n",
      "  training_iteration: 387\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3072 s, 387 iter, 851400 ts, 177 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-32-11\n",
      "  done: false\n",
      "  episode_len_mean: 95.58\n",
      "  episode_reward_max: 240.97782908225253\n",
      "  episode_reward_mean: 160.3766144870696\n",
      "  episode_reward_min: -156.67892486142387\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 8770\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 818.102\n",
      "    load_time_ms: 3.003\n",
      "    num_steps_sampled: 853600\n",
      "    num_steps_trained: 853600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.31307318806648254\n",
      "      kl: 0.01726861298084259\n",
      "      policy_loss: 0.0005220269085839391\n",
      "      total_loss: 382.6930236816406\n",
      "      vf_explained_var: 0.7383708357810974\n",
      "      vf_loss: 382.69244384765625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2966204881668091\n",
      "      kl: 0.011063854210078716\n",
      "      policy_loss: -0.006049419287592173\n",
      "      total_loss: 800.6614379882812\n",
      "      vf_explained_var: 0.7696347236633301\n",
      "      vf_loss: 800.66748046875\n",
      "    sample_time_ms: 5791.779\n",
      "    update_time_ms: 7.564\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.53947228007651\n",
      "    rl_1: 114.83714220699308\n",
      "  time_since_restore: 3078.815332174301\n",
      "  time_this_iter_s: 5.94350790977478\n",
      "  time_total_s: 3078.815332174301\n",
      "  timestamp: 1552318331\n",
      "  timesteps_since_restore: 853600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 853600\n",
      "  training_iteration: 388\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3078 s, 388 iter, 853600 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-32-19\n",
      "  done: false\n",
      "  episode_len_mean: 94.63\n",
      "  episode_reward_max: 238.94589505878812\n",
      "  episode_reward_mean: 154.83488424455547\n",
      "  episode_reward_min: -156.67892486142387\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8793\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 852.568\n",
      "    load_time_ms: 2.968\n",
      "    num_steps_sampled: 855800\n",
      "    num_steps_trained: 855800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2676581144332886\n",
      "      kl: 0.01568756066262722\n",
      "      policy_loss: -0.009709777310490608\n",
      "      total_loss: 420.2380065917969\n",
      "      vf_explained_var: 0.6905999779701233\n",
      "      vf_loss: 420.2476806640625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.25723886489868164\n",
      "      kl: 0.01237483974546194\n",
      "      policy_loss: -0.007408774923533201\n",
      "      total_loss: 720.109619140625\n",
      "      vf_explained_var: 0.7783570289611816\n",
      "      vf_loss: 720.1170043945312\n",
      "    sample_time_ms: 5997.528\n",
      "    update_time_ms: 7.531\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.269835549915534\n",
      "    rl_1: 110.56504869463991\n",
      "  time_since_restore: 3087.1763248443604\n",
      "  time_this_iter_s: 8.360992670059204\n",
      "  time_total_s: 3087.1763248443604\n",
      "  timestamp: 1552318339\n",
      "  timesteps_since_restore: 855800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 855800\n",
      "  training_iteration: 389\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3087 s, 389 iter, 855800 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-32-26\n",
      "  done: false\n",
      "  episode_len_mean: 93.41\n",
      "  episode_reward_max: 238.94589505878812\n",
      "  episode_reward_mean: 144.3893497447586\n",
      "  episode_reward_min: -156.67892486142387\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 8817\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 851.265\n",
      "    load_time_ms: 2.795\n",
      "    num_steps_sampled: 858000\n",
      "    num_steps_trained: 858000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.631178081035614\n",
      "      kl: 0.015225443989038467\n",
      "      policy_loss: -0.0007051141583360732\n",
      "      total_loss: 323.0828857421875\n",
      "      vf_explained_var: 0.7604120373725891\n",
      "      vf_loss: 323.0835876464844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3089032471179962\n",
      "      kl: 0.01088391337543726\n",
      "      policy_loss: -0.004466119222342968\n",
      "      total_loss: 650.1156616210938\n",
      "      vf_explained_var: 0.7962403893470764\n",
      "      vf_loss: 650.1200561523438\n",
      "    sample_time_ms: 6078.9\n",
      "    update_time_ms: 7.458\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.859318760978184\n",
      "    rl_1: 104.53003098378042\n",
      "  time_since_restore: 3093.6100902557373\n",
      "  time_this_iter_s: 6.433765411376953\n",
      "  time_total_s: 3093.6100902557373\n",
      "  timestamp: 1552318346\n",
      "  timesteps_since_restore: 858000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 858000\n",
      "  training_iteration: 390\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3093 s, 390 iter, 858000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-32-32\n",
      "  done: false\n",
      "  episode_len_mean: 91.99\n",
      "  episode_reward_max: 238.60849067623468\n",
      "  episode_reward_mean: 134.24340171682096\n",
      "  episode_reward_min: -156.34162222220885\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8840\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 848.297\n",
      "    load_time_ms: 2.717\n",
      "    num_steps_sampled: 860200\n",
      "    num_steps_trained: 860200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4197345972061157\n",
      "      kl: 0.014739914797246456\n",
      "      policy_loss: -0.0034536155872046947\n",
      "      total_loss: 311.310791015625\n",
      "      vf_explained_var: 0.7648051977157593\n",
      "      vf_loss: 311.314208984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.18519628047943115\n",
      "      kl: 0.012174184434115887\n",
      "      policy_loss: 0.002760886214673519\n",
      "      total_loss: 672.041015625\n",
      "      vf_explained_var: 0.775039553642273\n",
      "      vf_loss: 672.0382690429688\n",
      "    sample_time_ms: 5951.42\n",
      "    update_time_ms: 7.508\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.62563126724661\n",
      "    rl_1: 98.61777044957437\n",
      "  time_since_restore: 3099.8687937259674\n",
      "  time_this_iter_s: 6.2587034702301025\n",
      "  time_total_s: 3099.8687937259674\n",
      "  timestamp: 1552318352\n",
      "  timesteps_since_restore: 860200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 860200\n",
      "  training_iteration: 391\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3099 s, 391 iter, 860200 ts, 134 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-32-40\n",
      "  done: false\n",
      "  episode_len_mean: 92.53\n",
      "  episode_reward_max: 239.11610759275666\n",
      "  episode_reward_mean: 135.2584169211706\n",
      "  episode_reward_min: -167.3688269562991\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 8865\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 816.908\n",
      "    load_time_ms: 2.81\n",
      "    num_steps_sampled: 862400\n",
      "    num_steps_trained: 862400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5029587745666504\n",
      "      kl: 0.008987806737422943\n",
      "      policy_loss: -0.006528215948492289\n",
      "      total_loss: 131.7542724609375\n",
      "      vf_explained_var: 0.9272779822349548\n",
      "      vf_loss: 131.76080322265625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.28452304005622864\n",
      "      kl: 0.009030750021338463\n",
      "      policy_loss: -0.001823526923544705\n",
      "      total_loss: 289.37872314453125\n",
      "      vf_explained_var: 0.9356814622879028\n",
      "      vf_loss: 289.380615234375\n",
      "    sample_time_ms: 6066.227\n",
      "    update_time_ms: 7.5\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.31888925831335\n",
      "    rl_1: 99.93952766285726\n",
      "  time_since_restore: 3107.8580791950226\n",
      "  time_this_iter_s: 7.989285469055176\n",
      "  time_total_s: 3107.8580791950226\n",
      "  timestamp: 1552318360\n",
      "  timesteps_since_restore: 862400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 862400\n",
      "  training_iteration: 392\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3107 s, 392 iter, 862400 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-32-49\n",
      "  done: false\n",
      "  episode_len_mean: 91.4\n",
      "  episode_reward_max: 239.5487040465608\n",
      "  episode_reward_mean: 126.72640641470629\n",
      "  episode_reward_min: -167.3688269562991\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 8889\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 823.087\n",
      "    load_time_ms: 2.734\n",
      "    num_steps_sampled: 864600\n",
      "    num_steps_trained: 864600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.44718995690345764\n",
      "      kl: 0.00932999700307846\n",
      "      policy_loss: 0.0010654391953721642\n",
      "      total_loss: 308.509521484375\n",
      "      vf_explained_var: 0.725480318069458\n",
      "      vf_loss: 308.5084533691406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3128603398799896\n",
      "      kl: 0.007073500659316778\n",
      "      policy_loss: -0.007254116702824831\n",
      "      total_loss: 641.55224609375\n",
      "      vf_explained_var: 0.7615360021591187\n",
      "      vf_loss: 641.5595703125\n",
      "    sample_time_ms: 6299.187\n",
      "    update_time_ms: 7.246\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.782175602659237\n",
      "    rl_1: 94.94423081204704\n",
      "  time_since_restore: 3116.4628217220306\n",
      "  time_this_iter_s: 8.604742527008057\n",
      "  time_total_s: 3116.4628217220306\n",
      "  timestamp: 1552318369\n",
      "  timesteps_since_restore: 864600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 864600\n",
      "  training_iteration: 393\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3116 s, 393 iter, 864600 ts, 127 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-32-56\n",
      "  done: false\n",
      "  episode_len_mean: 90.95\n",
      "  episode_reward_max: 239.5487040465608\n",
      "  episode_reward_mean: 124.91364870148664\n",
      "  episode_reward_min: -167.3688269562991\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 8913\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 865.355\n",
      "    load_time_ms: 2.648\n",
      "    num_steps_sampled: 866800\n",
      "    num_steps_trained: 866800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4599093794822693\n",
      "      kl: 0.012306480668485165\n",
      "      policy_loss: -0.0026402901858091354\n",
      "      total_loss: 586.2219848632812\n",
      "      vf_explained_var: 0.5924186706542969\n",
      "      vf_loss: 586.2245483398438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.27112430334091187\n",
      "      kl: 0.005539863836020231\n",
      "      policy_loss: -0.0012986017391085625\n",
      "      total_loss: 1020.2431030273438\n",
      "      vf_explained_var: 0.6710140109062195\n",
      "      vf_loss: 1020.2445068359375\n",
      "    sample_time_ms: 6368.229\n",
      "    update_time_ms: 7.252\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.056919729176446\n",
      "    rl_1: 93.85672897231015\n",
      "  time_since_restore: 3123.353570461273\n",
      "  time_this_iter_s: 6.890748739242554\n",
      "  time_total_s: 3123.353570461273\n",
      "  timestamp: 1552318376\n",
      "  timesteps_since_restore: 866800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 866800\n",
      "  training_iteration: 394\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3123 s, 394 iter, 866800 ts, 125 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-33-05\n",
      "  done: false\n",
      "  episode_len_mean: 91.74\n",
      "  episode_reward_max: 240.19705239231513\n",
      "  episode_reward_mean: 132.06457794625007\n",
      "  episode_reward_min: -167.3688269562991\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 8936\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 891.973\n",
      "    load_time_ms: 2.773\n",
      "    num_steps_sampled: 869000\n",
      "    num_steps_trained: 869000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4236995577812195\n",
      "      kl: 0.0174117274582386\n",
      "      policy_loss: 0.0061396388337016106\n",
      "      total_loss: 95.37989044189453\n",
      "      vf_explained_var: 0.9301266670227051\n",
      "      vf_loss: 95.3737564086914\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2975524365901947\n",
      "      kl: 0.014698896557092667\n",
      "      policy_loss: -0.010024970397353172\n",
      "      total_loss: 252.57679748535156\n",
      "      vf_explained_var: 0.9337908625602722\n",
      "      vf_loss: 252.58682250976562\n",
      "    sample_time_ms: 6448.408\n",
      "    update_time_ms: 7.988\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.2041625102649\n",
      "    rl_1: 97.86041543598516\n",
      "  time_since_restore: 3132.802456140518\n",
      "  time_this_iter_s: 9.448885679244995\n",
      "  time_total_s: 3132.802456140518\n",
      "  timestamp: 1552318385\n",
      "  timesteps_since_restore: 869000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 869000\n",
      "  training_iteration: 395\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3132 s, 395 iter, 869000 ts, 132 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-33-12\n",
      "  done: false\n",
      "  episode_len_mean: 92.11\n",
      "  episode_reward_max: 240.19705239231513\n",
      "  episode_reward_mean: 134.55804409895728\n",
      "  episode_reward_min: -167.3688269562991\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 8960\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 907.529\n",
      "    load_time_ms: 2.683\n",
      "    num_steps_sampled: 871200\n",
      "    num_steps_trained: 871200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7312141060829163\n",
      "      kl: 0.010629270225763321\n",
      "      policy_loss: 0.0021821155678480864\n",
      "      total_loss: 272.2590026855469\n",
      "      vf_explained_var: 0.7621173858642578\n",
      "      vf_loss: 272.2568054199219\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.347822904586792\n",
      "      kl: 0.008433551527559757\n",
      "      policy_loss: -0.003468878101557493\n",
      "      total_loss: 509.3880310058594\n",
      "      vf_explained_var: 0.8241216540336609\n",
      "      vf_loss: 509.39141845703125\n",
      "    sample_time_ms: 6376.736\n",
      "    update_time_ms: 8.78\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.11522287592299\n",
      "    rl_1: 99.4428212230343\n",
      "  time_since_restore: 3139.7104229927063\n",
      "  time_this_iter_s: 6.90796685218811\n",
      "  time_total_s: 3139.7104229927063\n",
      "  timestamp: 1552318392\n",
      "  timesteps_since_restore: 871200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 871200\n",
      "  training_iteration: 396\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3139 s, 396 iter, 871200 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-33-24\n",
      "  done: false\n",
      "  episode_len_mean: 91.98\n",
      "  episode_reward_max: 240.19705239231513\n",
      "  episode_reward_mean: 130.70635531438492\n",
      "  episode_reward_min: -157.5768822357791\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 8984\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 964.924\n",
      "    load_time_ms: 2.786\n",
      "    num_steps_sampled: 873400\n",
      "    num_steps_trained: 873400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6776606440544128\n",
      "      kl: 0.010431814007461071\n",
      "      policy_loss: -0.0006577258463948965\n",
      "      total_loss: 586.1875\n",
      "      vf_explained_var: 0.6041213870048523\n",
      "      vf_loss: 586.1881103515625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.21765485405921936\n",
      "      kl: 0.005960317328572273\n",
      "      policy_loss: -0.003003867110237479\n",
      "      total_loss: 1036.420654296875\n",
      "      vf_explained_var: 0.7282756567001343\n",
      "      vf_loss: 1036.423583984375\n",
      "    sample_time_ms: 6832.324\n",
      "    update_time_ms: 10.517\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.63651521636845\n",
      "    rl_1: 98.06984009801643\n",
      "  time_since_restore: 3151.106214761734\n",
      "  time_this_iter_s: 11.39579176902771\n",
      "  time_total_s: 3151.106214761734\n",
      "  timestamp: 1552318404\n",
      "  timesteps_since_restore: 873400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 873400\n",
      "  training_iteration: 397\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3151 s, 397 iter, 873400 ts, 131 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-33-32\n",
      "  done: false\n",
      "  episode_len_mean: 91.41\n",
      "  episode_reward_max: 240.19705239231513\n",
      "  episode_reward_mean: 126.83297741176308\n",
      "  episode_reward_min: -159.6566396409522\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 9010\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1030.089\n",
      "    load_time_ms: 2.952\n",
      "    num_steps_sampled: 875600\n",
      "    num_steps_trained: 875600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4008069932460785\n",
      "      kl: 0.009011211805045605\n",
      "      policy_loss: -0.006826519034802914\n",
      "      total_loss: 316.6879577636719\n",
      "      vf_explained_var: 0.8372774720191956\n",
      "      vf_loss: 316.6947937011719\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.23957274854183197\n",
      "      kl: 0.008252977393567562\n",
      "      policy_loss: -0.008836852386593819\n",
      "      total_loss: 632.7648315429688\n",
      "      vf_explained_var: 0.875278651714325\n",
      "      vf_loss: 632.7736206054688\n",
      "    sample_time_ms: 7033.357\n",
      "    update_time_ms: 11.444\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.69809469668779\n",
      "    rl_1: 96.13488271507525\n",
      "  time_since_restore: 3159.724166870117\n",
      "  time_this_iter_s: 8.617952108383179\n",
      "  time_total_s: 3159.724166870117\n",
      "  timestamp: 1552318412\n",
      "  timesteps_since_restore: 875600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 875600\n",
      "  training_iteration: 398\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3159 s, 398 iter, 875600 ts, 127 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-33-40\n",
      "  done: false\n",
      "  episode_len_mean: 92.2\n",
      "  episode_reward_max: 239.35840754227698\n",
      "  episode_reward_mean: 129.83050407946263\n",
      "  episode_reward_min: -159.6566396409522\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 9032\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1096.207\n",
      "    load_time_ms: 3.363\n",
      "    num_steps_sampled: 877800\n",
      "    num_steps_trained: 877800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8936702609062195\n",
      "      kl: 0.007170264143496752\n",
      "      policy_loss: -0.007947026751935482\n",
      "      total_loss: 383.9906005859375\n",
      "      vf_explained_var: 0.6277365684509277\n",
      "      vf_loss: 383.9985656738281\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32237139344215393\n",
      "      kl: 0.0075199175626039505\n",
      "      policy_loss: -0.006554228253662586\n",
      "      total_loss: 715.9039916992188\n",
      "      vf_explained_var: 0.762968897819519\n",
      "      vf_loss: 715.9105834960938\n",
      "    sample_time_ms: 6951.749\n",
      "    update_time_ms: 11.768\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.3610052612784\n",
      "    rl_1: 98.4694988181842\n",
      "  time_since_restore: 3167.938752412796\n",
      "  time_this_iter_s: 8.214585542678833\n",
      "  time_total_s: 3167.938752412796\n",
      "  timestamp: 1552318420\n",
      "  timesteps_since_restore: 877800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 877800\n",
      "  training_iteration: 399\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3167 s, 399 iter, 877800 ts, 130 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-33-48\n",
      "  done: false\n",
      "  episode_len_mean: 89.39\n",
      "  episode_reward_max: 241.48477796531748\n",
      "  episode_reward_mean: 111.8696304760306\n",
      "  episode_reward_min: -165.56951864422348\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 9058\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1133.965\n",
      "    load_time_ms: 3.455\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.46357831358909607\n",
      "      kl: 0.024036461487412453\n",
      "      policy_loss: -0.0002579549909569323\n",
      "      total_loss: 579.7317504882812\n",
      "      vf_explained_var: 0.6768154501914978\n",
      "      vf_loss: 579.7320556640625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.13223819434642792\n",
      "      kl: 0.010430360212922096\n",
      "      policy_loss: -0.008216893300414085\n",
      "      total_loss: 1070.0770263671875\n",
      "      vf_explained_var: 0.7491970062255859\n",
      "      vf_loss: 1070.0850830078125\n",
      "    sample_time_ms: 7023.787\n",
      "    update_time_ms: 12.117\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.776446281453442\n",
      "    rl_1: 88.09318419457718\n",
      "  time_since_restore: 3175.476053714752\n",
      "  time_this_iter_s: 7.537301301956177\n",
      "  time_total_s: 3175.476053714752\n",
      "  timestamp: 1552318428\n",
      "  timesteps_since_restore: 880000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 400\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3175 s, 400 iter, 880000 ts, 112 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-33-56\n",
      "  done: false\n",
      "  episode_len_mean: 88.93\n",
      "  episode_reward_max: 241.48477796531748\n",
      "  episode_reward_mean: 113.34904378470809\n",
      "  episode_reward_min: -165.56951864422348\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 9083\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1169.223\n",
      "    load_time_ms: 3.55\n",
      "    num_steps_sampled: 882200\n",
      "    num_steps_trained: 882200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.06184706091880798\n",
      "      kl: 0.018586041405797005\n",
      "      policy_loss: -0.0026147831231355667\n",
      "      total_loss: 582.27685546875\n",
      "      vf_explained_var: 0.5786042213439941\n",
      "      vf_loss: 582.2794799804688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.19124281406402588\n",
      "      kl: 0.012894406914710999\n",
      "      policy_loss: -0.0066817342303693295\n",
      "      total_loss: 1042.8487548828125\n",
      "      vf_explained_var: 0.6708220839500427\n",
      "      vf_loss: 1042.85546875\n",
      "    sample_time_ms: 7115.017\n",
      "    update_time_ms: 12.381\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.48711775040387\n",
      "    rl_1: 87.86192603430423\n",
      "  time_since_restore: 3183.0070140361786\n",
      "  time_this_iter_s: 7.530960321426392\n",
      "  time_total_s: 3183.0070140361786\n",
      "  timestamp: 1552318436\n",
      "  timesteps_since_restore: 882200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 882200\n",
      "  training_iteration: 401\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3183 s, 401 iter, 882200 ts, 113 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-34-02\n",
      "  done: false\n",
      "  episode_len_mean: 90.29\n",
      "  episode_reward_max: 241.48477796531748\n",
      "  episode_reward_mean: 123.79778744081744\n",
      "  episode_reward_min: -165.56951864422348\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 9107\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1143.872\n",
      "    load_time_ms: 3.252\n",
      "    num_steps_sampled: 884400\n",
      "    num_steps_trained: 884400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6136854887008667\n",
      "      kl: 0.010552559047937393\n",
      "      policy_loss: -0.0003002744051627815\n",
      "      total_loss: 252.3084716796875\n",
      "      vf_explained_var: 0.7808125615119934\n",
      "      vf_loss: 252.3087921142578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.26849454641342163\n",
      "      kl: 0.008603714406490326\n",
      "      policy_loss: -0.0036988239735364914\n",
      "      total_loss: 572.2645874023438\n",
      "      vf_explained_var: 0.8133323192596436\n",
      "      vf_loss: 572.268310546875\n",
      "    sample_time_ms: 6999.771\n",
      "    update_time_ms: 12.806\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.477982490195174\n",
      "    rl_1: 94.31980495062227\n",
      "  time_since_restore: 3189.590300798416\n",
      "  time_this_iter_s: 6.583286762237549\n",
      "  time_total_s: 3189.590300798416\n",
      "  timestamp: 1552318442\n",
      "  timesteps_since_restore: 884400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 884400\n",
      "  training_iteration: 402\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3189 s, 402 iter, 884400 ts, 124 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-34-11\n",
      "  done: false\n",
      "  episode_len_mean: 88.9\n",
      "  episode_reward_max: 241.48477796531748\n",
      "  episode_reward_mean: 117.25039468365927\n",
      "  episode_reward_min: -165.56951864422348\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 9132\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1164.102\n",
      "    load_time_ms: 3.484\n",
      "    num_steps_sampled: 886600\n",
      "    num_steps_trained: 886600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3810843527317047\n",
      "      kl: 0.014261984266340733\n",
      "      policy_loss: 0.0017352007562294602\n",
      "      total_loss: 326.073974609375\n",
      "      vf_explained_var: 0.7600462436676025\n",
      "      vf_loss: 326.0722351074219\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2568289041519165\n",
      "      kl: 0.01162079069763422\n",
      "      policy_loss: -0.007256507873535156\n",
      "      total_loss: 631.9053344726562\n",
      "      vf_explained_var: 0.8058158755302429\n",
      "      vf_loss: 631.91259765625\n",
      "    sample_time_ms: 6960.131\n",
      "    update_time_ms: 12.719\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 27.39667829406504\n",
      "    rl_1: 89.85371638959423\n",
      "  time_since_restore: 3198.0029571056366\n",
      "  time_this_iter_s: 8.412656307220459\n",
      "  time_total_s: 3198.0029571056366\n",
      "  timestamp: 1552318451\n",
      "  timesteps_since_restore: 886600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 886600\n",
      "  training_iteration: 403\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3198 s, 403 iter, 886600 ts, 117 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-34-21\n",
      "  done: false\n",
      "  episode_len_mean: 90.59\n",
      "  episode_reward_max: 238.23214633279116\n",
      "  episode_reward_mean: 126.37986261072473\n",
      "  episode_reward_min: -160.07673134706494\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 9155\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1243.788\n",
      "    load_time_ms: 3.901\n",
      "    num_steps_sampled: 888800\n",
      "    num_steps_trained: 888800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.18936504423618317\n",
      "      kl: 0.008519239723682404\n",
      "      policy_loss: -0.0029769812244921923\n",
      "      total_loss: 430.493896484375\n",
      "      vf_explained_var: 0.7236946821212769\n",
      "      vf_loss: 430.49688720703125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.28035590052604675\n",
      "      kl: 0.01731143146753311\n",
      "      policy_loss: -0.0121977673843503\n",
      "      total_loss: 821.4862670898438\n",
      "      vf_explained_var: 0.7857432961463928\n",
      "      vf_loss: 821.49853515625\n",
      "    sample_time_ms: 7244.338\n",
      "    update_time_ms: 13.112\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.05017716662984\n",
      "    rl_1: 94.3296854440949\n",
      "  time_since_restore: 3208.5489892959595\n",
      "  time_this_iter_s: 10.546032190322876\n",
      "  time_total_s: 3208.5489892959595\n",
      "  timestamp: 1552318461\n",
      "  timesteps_since_restore: 888800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 888800\n",
      "  training_iteration: 404\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3208 s, 404 iter, 888800 ts, 126 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-34-37\n",
      "  done: false\n",
      "  episode_len_mean: 92.33\n",
      "  episode_reward_max: 236.18984714643446\n",
      "  episode_reward_mean: 136.66983521867908\n",
      "  episode_reward_min: -160.07673134706494\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 9178\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1290.668\n",
      "    load_time_ms: 4.109\n",
      "    num_steps_sampled: 891000\n",
      "    num_steps_trained: 891000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5055211186408997\n",
      "      kl: 0.006657027639448643\n",
      "      policy_loss: -6.234821285033831e-06\n",
      "      total_loss: 326.4952087402344\n",
      "      vf_explained_var: 0.6268391013145447\n",
      "      vf_loss: 326.4952087402344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2681693434715271\n",
      "      kl: 0.017496682703495026\n",
      "      policy_loss: 0.0019823575858026743\n",
      "      total_loss: 652.0643920898438\n",
      "      vf_explained_var: 0.7495895624160767\n",
      "      vf_loss: 652.0624389648438\n",
      "    sample_time_ms: 7767.042\n",
      "    update_time_ms: 13.26\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.18166707122445\n",
      "    rl_1: 100.48816814745463\n",
      "  time_since_restore: 3223.6958758831024\n",
      "  time_this_iter_s: 15.146886587142944\n",
      "  time_total_s: 3223.6958758831024\n",
      "  timestamp: 1552318477\n",
      "  timesteps_since_restore: 891000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 891000\n",
      "  training_iteration: 405\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3223 s, 405 iter, 891000 ts, 137 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-34-45\n",
      "  done: false\n",
      "  episode_len_mean: 93.36\n",
      "  episode_reward_max: 236.18984714643446\n",
      "  episode_reward_mean: 140.08220346234404\n",
      "  episode_reward_min: -152.129205347079\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 9201\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1344.122\n",
      "    load_time_ms: 4.267\n",
      "    num_steps_sampled: 893200\n",
      "    num_steps_trained: 893200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5515701770782471\n",
      "      kl: 0.02448911964893341\n",
      "      policy_loss: 0.006436523515731096\n",
      "      total_loss: 131.65036010742188\n",
      "      vf_explained_var: 0.8932244181632996\n",
      "      vf_loss: 131.6439208984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3696773648262024\n",
      "      kl: 0.008479460142552853\n",
      "      policy_loss: -0.0033396908547729254\n",
      "      total_loss: 316.1366271972656\n",
      "      vf_explained_var: 0.9052413702011108\n",
      "      vf_loss: 316.13995361328125\n",
      "    sample_time_ms: 7863.643\n",
      "    update_time_ms: 12.821\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.279725538007526\n",
      "    rl_1: 102.80247792433653\n",
      "  time_since_restore: 3232.105471611023\n",
      "  time_this_iter_s: 8.409595727920532\n",
      "  time_total_s: 3232.105471611023\n",
      "  timestamp: 1552318485\n",
      "  timesteps_since_restore: 893200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 893200\n",
      "  training_iteration: 406\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3232 s, 406 iter, 893200 ts, 140 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-34-57\n",
      "  done: false\n",
      "  episode_len_mean: 93.09\n",
      "  episode_reward_max: 238.55826611925642\n",
      "  episode_reward_mean: 137.99391536328199\n",
      "  episode_reward_min: -152.56514767277991\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 9225\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1335.713\n",
      "    load_time_ms: 4.217\n",
      "    num_steps_sampled: 895400\n",
      "    num_steps_trained: 895400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2800830602645874\n",
      "      kl: 0.013361304067075253\n",
      "      policy_loss: -0.0004708810884039849\n",
      "      total_loss: 440.5283203125\n",
      "      vf_explained_var: 0.7489808201789856\n",
      "      vf_loss: 440.52880859375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2048669457435608\n",
      "      kl: 0.012088079936802387\n",
      "      policy_loss: 0.0005291665438562632\n",
      "      total_loss: 845.1177368164062\n",
      "      vf_explained_var: 0.7996719479560852\n",
      "      vf_loss: 845.1173095703125\n",
      "    sample_time_ms: 7910.929\n",
      "    update_time_ms: 11.997\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.790840316713044\n",
      "    rl_1: 101.203075046569\n",
      "  time_since_restore: 3243.8845200538635\n",
      "  time_this_iter_s: 11.779048442840576\n",
      "  time_total_s: 3243.8845200538635\n",
      "  timestamp: 1552318497\n",
      "  timesteps_since_restore: 895400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 895400\n",
      "  training_iteration: 407\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3243 s, 407 iter, 895400 ts, 138 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-35-06\n",
      "  done: false\n",
      "  episode_len_mean: 92.85\n",
      "  episode_reward_max: 238.55826611925642\n",
      "  episode_reward_mean: 133.80783531433258\n",
      "  episode_reward_min: -152.56514767277991\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 9250\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1256.54\n",
      "    load_time_ms: 4.037\n",
      "    num_steps_sampled: 897600\n",
      "    num_steps_trained: 897600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.34923285245895386\n",
      "      kl: 0.028029005974531174\n",
      "      policy_loss: -0.005059525370597839\n",
      "      total_loss: 267.1448974609375\n",
      "      vf_explained_var: 0.8556845188140869\n",
      "      vf_loss: 267.1499938964844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2243817299604416\n",
      "      kl: 0.011773239821195602\n",
      "      policy_loss: -0.002471145475283265\n",
      "      total_loss: 520.412109375\n",
      "      vf_explained_var: 0.889340877532959\n",
      "      vf_loss: 520.4146118164062\n",
      "    sample_time_ms: 8046.624\n",
      "    update_time_ms: 12.824\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.1814330501353\n",
      "    rl_1: 99.6264022641973\n",
      "  time_since_restore: 3253.06947183609\n",
      "  time_this_iter_s: 9.184951782226562\n",
      "  time_total_s: 3253.06947183609\n",
      "  timestamp: 1552318506\n",
      "  timesteps_since_restore: 897600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 897600\n",
      "  training_iteration: 408\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3253 s, 408 iter, 897600 ts, 134 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-35-14\n",
      "  done: false\n",
      "  episode_len_mean: 91.49\n",
      "  episode_reward_max: 239.00696989224892\n",
      "  episode_reward_mean: 124.839705307524\n",
      "  episode_reward_min: -155.9125395968146\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 9274\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1172.544\n",
      "    load_time_ms: 3.594\n",
      "    num_steps_sampled: 899800\n",
      "    num_steps_trained: 899800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6934263706207275\n",
      "      kl: 0.009992801584303379\n",
      "      policy_loss: -0.008030572906136513\n",
      "      total_loss: 286.0015869140625\n",
      "      vf_explained_var: 0.7608526349067688\n",
      "      vf_loss: 286.0096130371094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2339549958705902\n",
      "      kl: 0.013564526103436947\n",
      "      policy_loss: -0.006691154092550278\n",
      "      total_loss: 579.3563842773438\n",
      "      vf_explained_var: 0.805072546005249\n",
      "      vf_loss: 579.3629760742188\n",
      "    sample_time_ms: 8090.101\n",
      "    update_time_ms: 12.428\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.0324257452755\n",
      "    rl_1: 94.80727956224851\n",
      "  time_since_restore: 3260.8701899051666\n",
      "  time_this_iter_s: 7.800718069076538\n",
      "  time_total_s: 3260.8701899051666\n",
      "  timestamp: 1552318514\n",
      "  timesteps_since_restore: 899800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 899800\n",
      "  training_iteration: 409\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3260 s, 409 iter, 899800 ts, 125 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-35-22\n",
      "  done: false\n",
      "  episode_len_mean: 91.49\n",
      "  episode_reward_max: 239.00696989224892\n",
      "  episode_reward_mean: 125.3952536291121\n",
      "  episode_reward_min: -155.9125395968146\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 9297\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1185.497\n",
      "    load_time_ms: 3.568\n",
      "    num_steps_sampled: 902000\n",
      "    num_steps_trained: 902000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.81154865026474\n",
      "      kl: 0.009831276722252369\n",
      "      policy_loss: 0.0026027506683021784\n",
      "      total_loss: 108.47907257080078\n",
      "      vf_explained_var: 0.9257088899612427\n",
      "      vf_loss: 108.47647857666016\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.18430545926094055\n",
      "      kl: 0.012190927751362324\n",
      "      policy_loss: -0.001139377593062818\n",
      "      total_loss: 267.734619140625\n",
      "      vf_explained_var: 0.9351412653923035\n",
      "      vf_loss: 267.7357177734375\n",
      "    sample_time_ms: 8144.577\n",
      "    update_time_ms: 12.446\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.026336214034202\n",
      "    rl_1: 95.36891741507787\n",
      "  time_since_restore: 3269.082230567932\n",
      "  time_this_iter_s: 8.212040662765503\n",
      "  time_total_s: 3269.082230567932\n",
      "  timestamp: 1552318522\n",
      "  timesteps_since_restore: 902000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 902000\n",
      "  training_iteration: 410\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3269 s, 410 iter, 902000 ts, 125 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-35-36\n",
      "  done: false\n",
      "  episode_len_mean: 91.71\n",
      "  episode_reward_max: 239.00696989224892\n",
      "  episode_reward_mean: 129.06153260030084\n",
      "  episode_reward_min: -159.55643254691293\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 9321\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1303.602\n",
      "    load_time_ms: 3.748\n",
      "    num_steps_sampled: 904200\n",
      "    num_steps_trained: 904200\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3407529592514038\n",
      "      kl: 0.023850249126553535\n",
      "      policy_loss: 0.009643109515309334\n",
      "      total_loss: 93.3811264038086\n",
      "      vf_explained_var: 0.9333745241165161\n",
      "      vf_loss: 93.3714828491211\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.09529365599155426\n",
      "      kl: 0.009202777408063412\n",
      "      policy_loss: -0.007993526756763458\n",
      "      total_loss: 243.20005798339844\n",
      "      vf_explained_var: 0.939873218536377\n",
      "      vf_loss: 243.20806884765625\n",
      "    sample_time_ms: 8592.067\n",
      "    update_time_ms: 12.573\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.528082229821123\n",
      "    rl_1: 97.5334503704797\n",
      "  time_since_restore: 3282.275768995285\n",
      "  time_this_iter_s: 13.193538427352905\n",
      "  time_total_s: 3282.275768995285\n",
      "  timestamp: 1552318536\n",
      "  timesteps_since_restore: 904200\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 904200\n",
      "  training_iteration: 411\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3282 s, 411 iter, 904200 ts, 129 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-35-50\n",
      "  done: false\n",
      "  episode_len_mean: 92.82\n",
      "  episode_reward_max: 239.00696989224892\n",
      "  episode_reward_mean: 136.02503366337322\n",
      "  episode_reward_min: -159.55643254691293\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 9345\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1395.035\n",
      "    load_time_ms: 3.986\n",
      "    num_steps_sampled: 906400\n",
      "    num_steps_trained: 906400\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.34656596183776855\n",
      "      kl: 0.012715126387774944\n",
      "      policy_loss: -0.004745046608150005\n",
      "      total_loss: 261.9710998535156\n",
      "      vf_explained_var: 0.8212156295776367\n",
      "      vf_loss: 261.9758605957031\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.156956285238266\n",
      "      kl: 0.013700646348297596\n",
      "      policy_loss: -0.007527085021138191\n",
      "      total_loss: 516.939697265625\n",
      "      vf_explained_var: 0.8672665953636169\n",
      "      vf_loss: 516.9472045898438\n",
      "    sample_time_ms: 9231.241\n",
      "    update_time_ms: 13.207\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.224201094992125\n",
      "    rl_1: 101.80083256838114\n",
      "  time_since_restore: 3296.1872177124023\n",
      "  time_this_iter_s: 13.91144871711731\n",
      "  time_total_s: 3296.1872177124023\n",
      "  timestamp: 1552318550\n",
      "  timesteps_since_restore: 906400\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 906400\n",
      "  training_iteration: 412\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3296 s, 412 iter, 906400 ts, 136 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-36-05\n",
      "  done: false\n",
      "  episode_len_mean: 93.49\n",
      "  episode_reward_max: 239.00696989224892\n",
      "  episode_reward_mean: 140.83148570149027\n",
      "  episode_reward_min: -159.55643254691293\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 9368\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1595.466\n",
      "    load_time_ms: 4.299\n",
      "    num_steps_sampled: 908600\n",
      "    num_steps_trained: 908600\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.09909188747406\n",
      "      kl: 0.011553659103810787\n",
      "      policy_loss: -0.005432808306068182\n",
      "      total_loss: 452.9880676269531\n",
      "      vf_explained_var: 0.31208863854408264\n",
      "      vf_loss: 452.99346923828125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2310718148946762\n",
      "      kl: 0.00817380752414465\n",
      "      policy_loss: -0.011002530343830585\n",
      "      total_loss: 859.151611328125\n",
      "      vf_explained_var: 0.5744580626487732\n",
      "      vf_loss: 859.16259765625\n",
      "    sample_time_ms: 9724.282\n",
      "    update_time_ms: 13.929\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.671828777873806\n",
      "    rl_1: 105.15965692361644\n",
      "  time_since_restore: 3311.5622918605804\n",
      "  time_this_iter_s: 15.3750741481781\n",
      "  time_total_s: 3311.5622918605804\n",
      "  timestamp: 1552318565\n",
      "  timesteps_since_restore: 908600\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 908600\n",
      "  training_iteration: 413\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3311 s, 413 iter, 908600 ts, 141 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-36-23\n",
      "  done: false\n",
      "  episode_len_mean: 93.68\n",
      "  episode_reward_max: 241.04244136447002\n",
      "  episode_reward_mean: 144.11442302481794\n",
      "  episode_reward_min: -159.55643254691293\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 9392\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1631.384\n",
      "    load_time_ms: 4.088\n",
      "    num_steps_sampled: 910800\n",
      "    num_steps_trained: 910800\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4792332947254181\n",
      "      kl: 0.008243079297244549\n",
      "      policy_loss: -0.00038076017517596483\n",
      "      total_loss: 122.97150421142578\n",
      "      vf_explained_var: 0.9151919484138489\n",
      "      vf_loss: 122.97187805175781\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.15427416563034058\n",
      "      kl: 0.00952750351279974\n",
      "      policy_loss: -0.002474346663802862\n",
      "      total_loss: 315.12646484375\n",
      "      vf_explained_var: 0.9250064492225647\n",
      "      vf_loss: 315.12896728515625\n",
      "    sample_time_ms: 10396.468\n",
      "    update_time_ms: 16.679\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.27084483733023\n",
      "    rl_1: 106.8435781874877\n",
      "  time_since_restore: 3329.234534740448\n",
      "  time_this_iter_s: 17.672242879867554\n",
      "  time_total_s: 3329.234534740448\n",
      "  timestamp: 1552318583\n",
      "  timesteps_since_restore: 910800\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 910800\n",
      "  training_iteration: 414\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=18379], 3329 s, 414 iter, 910800 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-11_16-36-33\n",
      "  done: false\n",
      "  episode_len_mean: 93.86\n",
      "  episode_reward_max: 241.04244136447002\n",
      "  episode_reward_mean: 143.94228401746375\n",
      "  episode_reward_min: -158.56069366239578\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 9415\n",
      "  experiment_id: 4ce6132bd9ab404c9dd2064472fe6431\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 1592.982\n",
      "    load_time_ms: 3.77\n",
      "    num_steps_sampled: 913000\n",
      "    num_steps_trained: 913000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.673224151134491\n",
      "      kl: 0.00439930334687233\n",
      "      policy_loss: -0.003641101298853755\n",
      "      total_loss: 539.2178344726562\n",
      "      vf_explained_var: 0.5216079950332642\n",
      "      vf_loss: 539.221435546875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.10862361639738083\n",
      "      kl: 0.012119068764150143\n",
      "      policy_loss: -0.007503630593419075\n",
      "      total_loss: 962.179443359375\n",
      "      vf_explained_var: 0.6783714294433594\n",
      "      vf_loss: 962.18701171875\n",
      "    sample_time_ms: 9939.175\n",
      "    update_time_ms: 17.265\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 18379\n",
      "  policy_reward_mean:\n",
      "    rl_0: 36.79292668620264\n",
      "    rl_1: 107.14935733126109\n",
      "  time_since_restore: 3339.4229588508606\n",
      "  time_this_iter_s: 10.188424110412598\n",
      "  time_total_s: 3339.4229588508606\n",
      "  timestamp: 1552318593\n",
      "  timesteps_since_restore: 913000\n",
      "  timesteps_this_iter: 2200\n",
      "  timesteps_total: 913000\n",
      "  training_iteration: 415\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,  # RL algorithm to run\n",
    "        \"env\": gym_name,  # environment name generated earlier\n",
    "        \"config\": {  # configuration params (must match \"run\" value)\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1000,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow_2)",
   "language": "python",
   "name": "flow_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
