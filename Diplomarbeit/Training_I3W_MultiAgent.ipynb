{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING I3W\n",
    "\n",
    "\n",
    "# A) Create Envorinment, Vehicles etc\n",
    "\n",
    "### General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scenarios:\n",
      "['Scenario', 'BayBridgeScenario', 'BayBridgeTollScenario', 'BottleneckScenario', 'Figure8Scenario', 'SimpleGridScenario', 'HighwayScenario', 'LoopScenario', 'MergeScenario', 'TwoLoopsOneMergingScenario', 'MultiLoopScenario', 'IntersectionScenarioTW']\n",
      "\n",
      "Available environments:\n",
      "['MultiEnv', 'MultiAgentAccelEnv', 'MultiWaveAttenuationPOEnv', 'MultiAgentIntersectionEnv', 'MultiAgentTeamSpiritIntersectionEnv', 'MultiAgentIntersectionEnv_baseline_1', 'MultiAgentIntersectionEnv_baseline_2', 'MultiAgentIntersectionEnv_baseline_3']\n"
     ]
    }
   ],
   "source": [
    "# Define horizon as a variable to ensure consistent use across notebook (length of one rollout)\n",
    "HORIZON=500                                 #103 max Horizon, wenn es vor verlassen abbrechen soll!, default war 500\n",
    "\n",
    "# name of the experiment\n",
    "experiment_name = \"IntersectionExample\"\n",
    "\n",
    "# scenario class\n",
    "import flow.scenarios as scenarios\n",
    "print(\"Available scenarios:\")\n",
    "print(scenarios.__all__)\n",
    "scenario_name = \"IntersectionTWScenario\"\n",
    "\n",
    "# environment class\n",
    "import flow.multiagent_envs as flowenvs\n",
    "print(\"\\nAvailable environments:\")\n",
    "print(flowenvs.__all__)\n",
    "env_name = \"MultiAgentIntersectionEnv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "from flow.scenarios.intersection import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "additionalNetParams = {\n",
    "            \"edge_length\": 40,\n",
    "            \"lanes\": 1,\n",
    "            \"speed_limit\": 30\n",
    "        }\n",
    "\n",
    "net_params = NetParams( no_internal_links=False,                  #default: True   !! damit Kreuzungen nicht Ã¼berspr. werden\n",
    "                        inflows=None,                             #default: None\n",
    "                        osm_path=None,                            #default: None\n",
    "                        netfile=None,                             #default: None\n",
    "                        additional_params=additionalNetParams     #default: None   !!\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InitialConfig Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "\n",
    "initial_config = InitialConfig( shuffle=True,                            #default: False         !!\n",
    "                                spacing=\"custom\",                        #default: \"uniform\"     !!\n",
    "                                min_gap=10,                              #default: 0\n",
    "                                perturbation=29.99,                      #default: 0.0            !!        \n",
    "                                x0=0,                                    #default: 0\n",
    "                                bunching=0,                              #default: 0\n",
    "                                lanes_distribution=float(\"inf\"),         #default: float(\"inf\")\n",
    "                                edges_distribution=\"all\",                #default: \"all\"\n",
    "                                additional_params=None )                 #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams( port = None,                  #default: None\n",
    "                          sim_step=0.1,                 #default: 0.1\n",
    "                          emission_path=None,           #default: None\n",
    "                          lateral_resolution=None,      #default: None\n",
    "                          no_step_log=True,             #default: True\n",
    "                          render=False,                 #default: False\n",
    "                          save_render=False,            #default: False\n",
    "                          sight_radius=25,              #default: 25\n",
    "                          show_radius=False,            #default: False\n",
    "                          pxpm=2,                       #default: 2\n",
    "                          overtake_right=False,         #default: False    \n",
    "                          seed=None,                    #default: None\n",
    "                          restart_instance=False,       #default: False\n",
    "                          print_warnings=True,          #default: True\n",
    "                          teleport_time=-1,             #default: -1\n",
    "                          num_clients=1,                #default: 1\n",
    "                          sumo_binary=None )            #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "additionalEnvParams = {\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 3,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 3,\n",
    "        \"target_velocity\": 30\n",
    "    }\n",
    "\n",
    "env_params = EnvParams( additional_params=additionalEnvParams, #default: None    !!\n",
    "                        horizon=HORIZON,                       #default: 500     !!\n",
    "                        warmup_steps=0,                        #default: 0       \n",
    "                        sims_per_step=1,                       #default: 1\n",
    "                        evaluate=False )                       #default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# import vehicles dynamics models\n",
    "#from flow.controllers import SumoCarFollowingController\n",
    "from flow.controllers import ContinuousRouter\n",
    "#from flow.controllers.lane_change_controllers import SumoLaneChangeController\n",
    "from flow.controllers.lane_change_controllers import StaticLaneChanger\n",
    "from flow.controllers import RLController\n",
    "from flow.core.params import SumoLaneChangeParams\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "from random import *\n",
    "\n",
    "vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RL-Agent controlled vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car following parameters, default: None\n",
    "cf_parameter = SumoCarFollowingParams(\n",
    "                speed_mode=\"aggressive\")\n",
    "# lane change parameters, default: None\n",
    "lc_parameter =  None\n",
    "\n",
    "vehicles.add( # name of the vehicle\n",
    "                veh_id = \"rl\",\n",
    "              # acceleration controller, default: (SumoCarFollowingController, {})\n",
    "                acceleration_controller=(RLController, {}),\n",
    "              # lane_change_controller, default: (SumoLaneChangeController, {})\n",
    "                lane_change_controller=(StaticLaneChanger,{}),\n",
    "              # routing controller, default: None\n",
    "                routing_controller=(ContinuousRouter, {}),\n",
    "              # initial speed, default: 0\n",
    "                initial_speed=0,\n",
    "              # number of vehicles, default: 1 \n",
    "                num_vehicles=2,\n",
    "                \n",
    "                car_following_params=cf_parameter\n",
    "              # speed mode, default: \"right_of_way\"\n",
    "                #speed_mode=\"aggressive\",\n",
    "              # lane change mode, default: \"no_lat_collide\"\n",
    "                #lane_change_mode=\"aggressive\", \n",
    "              # car following parameter, default: None\n",
    "                #sumo_car_following_params=cf_parameter,\n",
    "              # lane change parameter, default: None\n",
    "                #sumo_lc_params=lc_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict( # name of the experiment\n",
    "                      exp_tag=experiment_name,\n",
    "                    # name of the flow environment the experiment is running on\n",
    "                      env_name=env_name,\n",
    "                    # name of the scenario class the experiment uses\n",
    "                      scenario=scenario_name,\n",
    "                    # simulator that is used by the experiment\n",
    "                      simulator='traci',\n",
    "                    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "                      sim=sumo_params,\n",
    "                    # environment related parameters (see flow.core.params.EnvParams)\n",
    "                      env=env_params,\n",
    "                    # network-related parameters (see flow.core.params.NetParams and\n",
    "                    # the scenario's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "                      net=net_params,\n",
    "                    # vehicles to be placed in the network at the start of a rollout \n",
    "                    # (see flow.core.vehicles.Vehicles)\n",
    "                      veh=vehicles,\n",
    "                   # (optional) parameters affecting the positioning of vehicles upon \n",
    "                   # initialization/reset (see flow.core.params.InitialConfig)\n",
    "                      initial=initial_config\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-03-20_18-43-52_5179/logs.\n",
      "Waiting for redis server at 127.0.0.1:58595 to respond...\n",
      "Waiting for redis server at 127.0.0.1:27052 to respond...\n",
      "Starting the Plasma object store with 6.554658406 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=2ddcf65e490eb487441c0b0eb3c5c1d690941bb451a99eb2\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.2.102',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-03-20_18-43-52_5179/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-03-20_18-43-52_5179/sockets/raylet'],\n",
       " 'redis_address': '192.168.2.102:58595',\n",
       " 'webui_url': 'http://localhost:8889/notebooks/ray_ui.ipynb?token=2ddcf65e490eb487441c0b0eb3c5c1d690941bb451a99eb2'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "\n",
    "ray.init(redirect_output=True, num_cpus=N_CPUS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate default 0.999\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [100, 50,25]})  # size of hidden layers in network default 64 32\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "#config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "#config[\"sample_batch_size\"] = config[\"train_batch_size\"]/config[\"num_workers\"] # 200 default, trotzdem zu hoch?\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 55263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.147038256975808\n",
      "6.886284052593682\n"
     ]
    }
   ],
   "source": [
    "# multi agent policy mapping\n",
    "test_env = create_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "\n",
    "def gen_policy():\n",
    "    return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "# Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "policy_graphs = {'rl_0': gen_policy(), 'rl_1': gen_policy()}\n",
    "    \n",
    "def policy_mapping_fn(agent_id):\n",
    "    return agent_id\n",
    "\n",
    "config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn)\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 2.7/16.4 GB\n",
      "\n",
      "Created LogSyncer for /home/thorsten/ray_results/IntersectionExample/PPO_MultiAgentIntersectionEnv-v0_0_2019-03-20_18-43-55ipg4b7cp -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 2.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-44-54\n",
      "  done: false\n",
      "  episode_len_mean: 481.7\n",
      "  episode_reward_max: 157.82593584178935\n",
      "  episode_reward_mean: 55.08088457653795\n",
      "  episode_reward_min: -146.92512367743936\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5043.743\n",
      "    load_time_ms: 151.02\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.417646884918213\n",
      "      kl: 0.0005095912492834032\n",
      "      policy_loss: -0.0011456786887720227\n",
      "      total_loss: 62.18684768676758\n",
      "      vf_explained_var: 0.05137001723051071\n",
      "      vf_loss: 62.187889099121094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4201960563659668\n",
      "      kl: 0.0012106711510568857\n",
      "      policy_loss: -0.0019190773600712419\n",
      "      total_loss: 57.88035202026367\n",
      "      vf_explained_var: 0.020150085911154747\n",
      "      vf_loss: 57.88203811645508\n",
      "    sample_time_ms: 21781.742\n",
      "    update_time_ms: 2141.111\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.38463204488147\n",
      "    rl_1: 25.69625253165653\n",
      "  time_since_restore: 29.250378608703613\n",
      "  time_this_iter_s: 29.250378608703613\n",
      "  time_total_s: 29.250378608703613\n",
      "  timestamp: 1553103894\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 29 s, 1 iter, 10000 ts, 55.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-45-20\n",
      "  done: false\n",
      "  episode_len_mean: 459.8809523809524\n",
      "  episode_reward_max: 293.37960882453683\n",
      "  episode_reward_mean: 52.51703754335908\n",
      "  episode_reward_min: -158.17230433684242\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 42\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4489.981\n",
      "    load_time_ms: 76.953\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4190887212753296\n",
      "      kl: 0.000412437308114022\n",
      "      policy_loss: -0.0007696931133978069\n",
      "      total_loss: 99.3851318359375\n",
      "      vf_explained_var: 0.10328243672847748\n",
      "      vf_loss: 99.3858642578125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.10000000149011612\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4177964925765991\n",
      "      kl: 0.0017524922732263803\n",
      "      policy_loss: -0.0018157243030145764\n",
      "      total_loss: 100.02275085449219\n",
      "      vf_explained_var: 0.05835813283920288\n",
      "      vf_loss: 100.0243911743164\n",
      "    sample_time_ms: 21520.846\n",
      "    update_time_ms: 1075.467\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.046669249900273\n",
      "    rl_1: 24.47036829345885\n",
      "  time_since_restore: 54.4771203994751\n",
      "  time_this_iter_s: 25.226741790771484\n",
      "  time_total_s: 54.4771203994751\n",
      "  timestamp: 1553103920\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 54 s, 2 iter, 20000 ts, 52.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-45-45\n",
      "  done: false\n",
      "  episode_len_mean: 454.0769230769231\n",
      "  episode_reward_max: 293.37960882453683\n",
      "  episode_reward_mean: 51.43755399555214\n",
      "  episode_reward_min: -158.17230433684242\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 65\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4307.029\n",
      "    load_time_ms: 52.065\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4209256172180176\n",
      "      kl: 0.00045682882773689926\n",
      "      policy_loss: -0.0004422502242960036\n",
      "      total_loss: 119.84720611572266\n",
      "      vf_explained_var: 0.17298904061317444\n",
      "      vf_loss: 119.84764099121094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.05000000074505806\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.417119026184082\n",
      "      kl: 0.002932310337200761\n",
      "      policy_loss: -0.0025549232959747314\n",
      "      total_loss: 115.14836120605469\n",
      "      vf_explained_var: 0.16096852719783783\n",
      "      vf_loss: 115.15076446533203\n",
      "    sample_time_ms: 21464.984\n",
      "    update_time_ms: 720.1\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 26.002104981825646\n",
      "    rl_1: 25.43544901372654\n",
      "  time_since_restore: 79.79939818382263\n",
      "  time_this_iter_s: 25.322277784347534\n",
      "  time_total_s: 79.79939818382263\n",
      "  timestamp: 1553103945\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 79 s, 3 iter, 30000 ts, 51.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-46-10\n",
      "  done: false\n",
      "  episode_len_mean: 460.9651162790698\n",
      "  episode_reward_max: 329.05035258097655\n",
      "  episode_reward_mean: 71.98906614929842\n",
      "  episode_reward_min: -158.17230433684242\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 86\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4217.002\n",
      "    load_time_ms: 39.596\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4277875423431396\n",
      "      kl: 0.0028233134653419256\n",
      "      policy_loss: -0.0030735821928828955\n",
      "      total_loss: 70.74600982666016\n",
      "      vf_explained_var: 0.32592231035232544\n",
      "      vf_loss: 70.74900817871094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.02500000037252903\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4123663902282715\n",
      "      kl: 0.0051951357163488865\n",
      "      policy_loss: -0.004090369213372469\n",
      "      total_loss: 67.32211303710938\n",
      "      vf_explained_var: 0.23228062689304352\n",
      "      vf_loss: 67.32608032226562\n",
      "    sample_time_ms: 21488.864\n",
      "    update_time_ms: 542.656\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.216033730373496\n",
      "    rl_1: 37.77303241892494\n",
      "  time_since_restore: 105.33421611785889\n",
      "  time_this_iter_s: 25.534817934036255\n",
      "  time_total_s: 105.33421611785889\n",
      "  timestamp: 1553103970\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 105 s, 4 iter, 40000 ts, 72 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-46-37\n",
      "  done: false\n",
      "  episode_len_mean: 453.67\n",
      "  episode_reward_max: 329.05035258097655\n",
      "  episode_reward_mean: 89.71178834960858\n",
      "  episode_reward_min: -158.17230433684242\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 109\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4164.82\n",
      "    load_time_ms: 32.191\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4128468036651611\n",
      "      kl: 0.007679771166294813\n",
      "      policy_loss: -0.003738576779142022\n",
      "      total_loss: 123.07553100585938\n",
      "      vf_explained_var: 0.28636640310287476\n",
      "      vf_loss: 123.07918548583984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.012500000186264515\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4362616539001465\n",
      "      kl: 0.007967169396579266\n",
      "      policy_loss: -0.004602141212671995\n",
      "      total_loss: 82.8777847290039\n",
      "      vf_explained_var: 0.163730651140213\n",
      "      vf_loss: 82.8823013305664\n",
      "    sample_time_ms: 21588.745\n",
      "    update_time_ms: 436.875\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.359925308522094\n",
      "    rl_1: 41.351863041086524\n",
      "  time_since_restore: 131.31219363212585\n",
      "  time_this_iter_s: 25.977977514266968\n",
      "  time_total_s: 131.31219363212585\n",
      "  timestamp: 1553103997\n",
      "  timesteps_since_restore: 50000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 5\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 131 s, 5 iter, 50000 ts, 89.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-47-02\n",
      "  done: false\n",
      "  episode_len_mean: 443.85\n",
      "  episode_reward_max: 329.05035258097655\n",
      "  episode_reward_mean: 126.39451862245124\n",
      "  episode_reward_min: -166.25860641018036\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 134\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4138.396\n",
      "    load_time_ms: 27.314\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4253075122833252\n",
      "      kl: 0.0054119364358484745\n",
      "      policy_loss: -0.0024171285331249237\n",
      "      total_loss: 154.82711791992188\n",
      "      vf_explained_var: 0.42322584986686707\n",
      "      vf_loss: 154.82948303222656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0062500000931322575\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.430298089981079\n",
      "      kl: 0.002427448285743594\n",
      "      policy_loss: -0.003286051796749234\n",
      "      total_loss: 79.94647979736328\n",
      "      vf_explained_var: 0.21752193570137024\n",
      "      vf_loss: 79.94975280761719\n",
      "    sample_time_ms: 21622.77\n",
      "    update_time_ms: 365.803\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 71.73522895908255\n",
      "    rl_1: 54.65928966336872\n",
      "  time_since_restore: 157.1410632133484\n",
      "  time_this_iter_s: 25.828869581222534\n",
      "  time_total_s: 157.1410632133484\n",
      "  timestamp: 1553104022\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 157 s, 6 iter, 60000 ts, 126 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-47-28\n",
      "  done: false\n",
      "  episode_len_mean: 417.24\n",
      "  episode_reward_max: 331.16123407467876\n",
      "  episode_reward_mean: 166.40698571843183\n",
      "  episode_reward_min: -166.25860641018036\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 161\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4112.781\n",
      "    load_time_ms: 23.738\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4107952117919922\n",
      "      kl: 0.008855474181473255\n",
      "      policy_loss: -0.004369709640741348\n",
      "      total_loss: 188.20582580566406\n",
      "      vf_explained_var: 0.4792636036872864\n",
      "      vf_loss: 188.2101287841797\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0031250000465661287\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.431036114692688\n",
      "      kl: 0.008933945558965206\n",
      "      policy_loss: -0.004550585988909006\n",
      "      total_loss: 67.55778503417969\n",
      "      vf_explained_var: 0.13983790576457977\n",
      "      vf_loss: 67.56230926513672\n",
      "    sample_time_ms: 21678.197\n",
      "    update_time_ms: 314.863\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 99.33660953383571\n",
      "    rl_1: 67.07037618459618\n",
      "  time_since_restore: 183.14048314094543\n",
      "  time_this_iter_s: 25.999419927597046\n",
      "  time_total_s: 183.14048314094543\n",
      "  timestamp: 1553104048\n",
      "  timesteps_since_restore: 70000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 183 s, 7 iter, 70000 ts, 166 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-47-54\n",
      "  done: false\n",
      "  episode_len_mean: 368.1\n",
      "  episode_reward_max: 360.6422051934095\n",
      "  episode_reward_mean: 201.13039505630115\n",
      "  episode_reward_min: -166.25860641018036\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 194\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4094.15\n",
      "    load_time_ms: 21.042\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4227317571640015\n",
      "      kl: 0.0055249580182135105\n",
      "      policy_loss: -0.002979020355269313\n",
      "      total_loss: 271.99896240234375\n",
      "      vf_explained_var: 0.4129941463470459\n",
      "      vf_loss: 272.0019226074219\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0015625000232830644\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4062740802764893\n",
      "      kl: 0.0038066834677010775\n",
      "      policy_loss: -0.002131868153810501\n",
      "      total_loss: 195.50381469726562\n",
      "      vf_explained_var: 0.21030403673648834\n",
      "      vf_loss: 195.5059356689453\n",
      "    sample_time_ms: 21653.212\n",
      "    update_time_ms: 276.809\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 119.6212062335537\n",
      "    rl_1: 81.50918882274748\n",
      "  time_since_restore: 208.61040472984314\n",
      "  time_this_iter_s: 25.469921588897705\n",
      "  time_total_s: 208.61040472984314\n",
      "  timestamp: 1553104074\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 208 s, 8 iter, 80000 ts, 201 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-48-20\n",
      "  done: false\n",
      "  episode_len_mean: 307.77\n",
      "  episode_reward_max: 389.73737098256055\n",
      "  episode_reward_mean: 211.88673039140906\n",
      "  episode_reward_min: -161.4407383781322\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 232\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4081.701\n",
      "    load_time_ms: 18.946\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4238823652267456\n",
      "      kl: 0.008949116803705692\n",
      "      policy_loss: -0.0032658707350492477\n",
      "      total_loss: 373.16046142578125\n",
      "      vf_explained_var: 0.3270212709903717\n",
      "      vf_loss: 373.1636657714844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0007812500116415322\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.429390788078308\n",
      "      kl: 0.001648127450607717\n",
      "      policy_loss: -0.0019176674541085958\n",
      "      total_loss: 268.46728515625\n",
      "      vf_explained_var: 0.23839600384235382\n",
      "      vf_loss: 268.4692077636719\n",
      "    sample_time_ms: 21717.491\n",
      "    update_time_ms: 246.95\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 124.25872021367908\n",
      "    rl_1: 87.62801017773003\n",
      "  time_since_restore: 234.850594997406\n",
      "  time_this_iter_s: 26.240190267562866\n",
      "  time_total_s: 234.850594997406\n",
      "  timestamp: 1553104100\n",
      "  timesteps_since_restore: 90000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 9\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 234 s, 9 iter, 90000 ts, 212 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-48-46\n",
      "  done: false\n",
      "  episode_len_mean: 253.02\n",
      "  episode_reward_max: 389.73737098256055\n",
      "  episode_reward_mean: 185.773258089166\n",
      "  episode_reward_min: -161.4407383781322\n",
      "  episodes_this_iter: 43\n",
      "  episodes_total: 275\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4077.173\n",
      "    load_time_ms: 17.367\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3957222700119019\n",
      "      kl: 0.006072198040783405\n",
      "      policy_loss: -0.001429747324436903\n",
      "      total_loss: 422.97406005859375\n",
      "      vf_explained_var: 0.3845478594303131\n",
      "      vf_loss: 422.97552490234375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.0003906250058207661\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4358688592910767\n",
      "      kl: 0.006098673213273287\n",
      "      policy_loss: -0.0012426840839907527\n",
      "      total_loss: 260.34088134765625\n",
      "      vf_explained_var: 0.39608076214790344\n",
      "      vf_loss: 260.3421325683594\n",
      "    sample_time_ms: 21731.673\n",
      "    update_time_ms: 223.087\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 111.38314379832272\n",
      "    rl_1: 74.39011429084333\n",
      "  time_since_restore: 260.77687788009644\n",
      "  time_this_iter_s: 25.92628288269043\n",
      "  time_total_s: 260.77687788009644\n",
      "  timestamp: 1553104126\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 260 s, 10 iter, 100000 ts, 186 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-49-12\n",
      "  done: false\n",
      "  episode_len_mean: 237.29\n",
      "  episode_reward_max: 369.03347909724823\n",
      "  episode_reward_mean: 212.93913059140905\n",
      "  episode_reward_min: -154.38780578566713\n",
      "  episodes_this_iter: 43\n",
      "  episodes_total: 318\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3969.898\n",
      "    load_time_ms: 2.549\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3658390045166016\n",
      "      kl: 0.010233321227133274\n",
      "      policy_loss: -0.003905637888237834\n",
      "      total_loss: 462.8759460449219\n",
      "      vf_explained_var: 0.28823962807655334\n",
      "      vf_loss: 462.87982177734375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4216594696044922\n",
      "      kl: 0.011354452930390835\n",
      "      policy_loss: -0.00460021011531353\n",
      "      total_loss: 195.362548828125\n",
      "      vf_explained_var: 0.36490941047668457\n",
      "      vf_loss: 195.3671417236328\n",
      "    sample_time_ms: 21764.886\n",
      "    update_time_ms: 10.145\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 134.79351797384507\n",
      "    rl_1: 78.14561261756398\n",
      "  time_since_restore: 286.8934075832367\n",
      "  time_this_iter_s: 26.11652970314026\n",
      "  time_total_s: 286.8934075832367\n",
      "  timestamp: 1553104152\n",
      "  timesteps_since_restore: 110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 11\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 286 s, 11 iter, 110000 ts, 213 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-49-38\n",
      "  done: false\n",
      "  episode_len_mean: 221.2\n",
      "  episode_reward_max: 343.26700275513264\n",
      "  episode_reward_mean: 230.63722363783316\n",
      "  episode_reward_min: -152.84483411113752\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 367\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3972.825\n",
      "    load_time_ms: 2.541\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3883036375045776\n",
      "      kl: 0.011511316522955894\n",
      "      policy_loss: -0.004897801671177149\n",
      "      total_loss: 603.812255859375\n",
      "      vf_explained_var: 0.3287436068058014\n",
      "      vf_loss: 603.8170776367188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4121794700622559\n",
      "      kl: 0.002415473572909832\n",
      "      policy_loss: -0.002168760634958744\n",
      "      total_loss: 265.11334228515625\n",
      "      vf_explained_var: 0.4287570118904114\n",
      "      vf_loss: 265.1154479980469\n",
      "    sample_time_ms: 21833.25\n",
      "    update_time_ms: 9.96\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 149.14610136411412\n",
      "    rl_1: 81.49112227371901\n",
      "  time_since_restore: 312.831964969635\n",
      "  time_this_iter_s: 25.938557386398315\n",
      "  time_total_s: 312.831964969635\n",
      "  timestamp: 1553104178\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 312 s, 12 iter, 120000 ts, 231 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-50-04\n",
      "  done: false\n",
      "  episode_len_mean: 198.13\n",
      "  episode_reward_max: 335.44271536109636\n",
      "  episode_reward_mean: 239.449230730454\n",
      "  episode_reward_min: -152.84483411113752\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 418\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3978.695\n",
      "    load_time_ms: 2.621\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00019531250291038305\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3532254695892334\n",
      "      kl: 0.004190858919173479\n",
      "      policy_loss: -0.0018129994859918952\n",
      "      total_loss: 611.5404663085938\n",
      "      vf_explained_var: 0.1836753487586975\n",
      "      vf_loss: 611.5423583984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3847070932388306\n",
      "      kl: 0.0038104031700640917\n",
      "      policy_loss: -0.002527160570025444\n",
      "      total_loss: 207.1494903564453\n",
      "      vf_explained_var: 0.3315557837486267\n",
      "      vf_loss: 207.15200805664062\n",
      "    sample_time_ms: 21868.028\n",
      "    update_time_ms: 10.391\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 156.39567808128115\n",
      "    rl_1: 83.05355264917287\n",
      "  time_since_restore: 338.56637692451477\n",
      "  time_this_iter_s: 25.73441195487976\n",
      "  time_total_s: 338.56637692451477\n",
      "  timestamp: 1553104204\n",
      "  timesteps_since_restore: 130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 13\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 338 s, 13 iter, 130000 ts, 239 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-50-30\n",
      "  done: false\n",
      "  episode_len_mean: 196.31\n",
      "  episode_reward_max: 335.44271536109636\n",
      "  episode_reward_mean: 269.7230543482586\n",
      "  episode_reward_min: -152.57196445152385\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 468\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3983.576\n",
      "    load_time_ms: 2.693\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3431257009506226\n",
      "      kl: 0.01313072256743908\n",
      "      policy_loss: -0.004216305445879698\n",
      "      total_loss: 727.5453491210938\n",
      "      vf_explained_var: 0.06926494836807251\n",
      "      vf_loss: 727.5496215820312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3787074089050293\n",
      "      kl: 0.0018209925619885325\n",
      "      policy_loss: -0.0011411623563617468\n",
      "      total_loss: 202.0120391845703\n",
      "      vf_explained_var: 0.20146887004375458\n",
      "      vf_loss: 202.01316833496094\n",
      "    sample_time_ms: 21899.722\n",
      "    update_time_ms: 10.287\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 176.69836520074637\n",
      "    rl_1: 93.02468914751222\n",
      "  time_since_restore: 364.4695863723755\n",
      "  time_this_iter_s: 25.903209447860718\n",
      "  time_total_s: 364.4695863723755\n",
      "  timestamp: 1553104230\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 14\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 364 s, 14 iter, 140000 ts, 270 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-50-56\n",
      "  done: false\n",
      "  episode_len_mean: 195.04\n",
      "  episode_reward_max: 318.46425023204347\n",
      "  episode_reward_mean: 286.4556144171527\n",
      "  episode_reward_min: -141.1893024718516\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 522\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3981.342\n",
      "    load_time_ms: 2.681\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.765625145519152e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3504679203033447\n",
      "      kl: 0.00482891546562314\n",
      "      policy_loss: -0.0024597151204943657\n",
      "      total_loss: 768.4486694335938\n",
      "      vf_explained_var: 0.002131971064954996\n",
      "      vf_loss: 768.4511108398438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.369530439376831\n",
      "      kl: 0.010296766646206379\n",
      "      policy_loss: -0.003040191950276494\n",
      "      total_loss: 176.8654327392578\n",
      "      vf_explained_var: 0.0945190042257309\n",
      "      vf_loss: 176.86846923828125\n",
      "    sample_time_ms: 21852.241\n",
      "    update_time_ms: 9.763\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.81696021607448\n",
      "    rl_1: 97.63865420107823\n",
      "  time_since_restore: 389.9461977481842\n",
      "  time_this_iter_s: 25.476611375808716\n",
      "  time_total_s: 389.9461977481842\n",
      "  timestamp: 1553104256\n",
      "  timesteps_since_restore: 150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 15\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 389 s, 15 iter, 150000 ts, 286 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-51-22\n",
      "  done: false\n",
      "  episode_len_mean: 184.58\n",
      "  episode_reward_max: 318.2745216160596\n",
      "  episode_reward_mean: 281.78558382108815\n",
      "  episode_reward_min: -147.6865207434331\n",
      "  episodes_this_iter: 55\n",
      "  episodes_total: 577\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3978.227\n",
      "    load_time_ms: 2.599\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.882812572759576e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3542187213897705\n",
      "      kl: 0.009582472033798695\n",
      "      policy_loss: -0.004358180798590183\n",
      "      total_loss: 793.5735473632812\n",
      "      vf_explained_var: 0.051544684916734695\n",
      "      vf_loss: 793.577880859375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.368811845779419\n",
      "      kl: 0.004662651568651199\n",
      "      policy_loss: -0.0028571633156389\n",
      "      total_loss: 235.13626098632812\n",
      "      vf_explained_var: 0.13047732412815094\n",
      "      vf_loss: 235.13917541503906\n",
      "    sample_time_ms: 21864.825\n",
      "    update_time_ms: 9.803\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 186.34840563771502\n",
      "    rl_1: 95.43717818337315\n",
      "  time_since_restore: 415.8689408302307\n",
      "  time_this_iter_s: 25.92274308204651\n",
      "  time_total_s: 415.8689408302307\n",
      "  timestamp: 1553104282\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 16\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 415 s, 16 iter, 160000 ts, 282 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-51-48\n",
      "  done: false\n",
      "  episode_len_mean: 180.27\n",
      "  episode_reward_max: 318.2745216160596\n",
      "  episode_reward_mean: 290.7837801250226\n",
      "  episode_reward_min: 264.9876809323066\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 633\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3982.444\n",
      "    load_time_ms: 2.611\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.441406286379788e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3599728345870972\n",
      "      kl: 0.00360445911064744\n",
      "      policy_loss: -0.0012921022716909647\n",
      "      total_loss: 785.8255004882812\n",
      "      vf_explained_var: 0.009690653532743454\n",
      "      vf_loss: 785.8268432617188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3445812463760376\n",
      "      kl: 0.009101868607103825\n",
      "      policy_loss: -0.0028571661096066236\n",
      "      total_loss: 192.34202575683594\n",
      "      vf_explained_var: 0.07361996173858643\n",
      "      vf_loss: 192.34487915039062\n",
      "    sample_time_ms: 21872.943\n",
      "    update_time_ms: 9.914\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.53062567252465\n",
      "    rl_1: 100.25315445249792\n",
      "  time_since_restore: 441.99227714538574\n",
      "  time_this_iter_s: 26.12333631515503\n",
      "  time_total_s: 441.99227714538574\n",
      "  timestamp: 1553104308\n",
      "  timesteps_since_restore: 170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 17\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 441 s, 17 iter, 170000 ts, 291 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-52-14\n",
      "  done: false\n",
      "  episode_len_mean: 175.45\n",
      "  episode_reward_max: 319.24071331708933\n",
      "  episode_reward_mean: 289.441418385412\n",
      "  episode_reward_min: -140.89180521016286\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 690\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3983.132\n",
      "    load_time_ms: 2.628\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.220703143189894e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.366287350654602\n",
      "      kl: 0.0037259997334331274\n",
      "      policy_loss: -0.0007467679097317159\n",
      "      total_loss: 819.6965942382812\n",
      "      vf_explained_var: 0.01220167800784111\n",
      "      vf_loss: 819.6973266601562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3323132991790771\n",
      "      kl: 0.004866354633122683\n",
      "      policy_loss: -0.00039528243360109627\n",
      "      total_loss: 221.96511840820312\n",
      "      vf_explained_var: 0.22692961990833282\n",
      "      vf_loss: 221.9654998779297\n",
      "    sample_time_ms: 21959.304\n",
      "    update_time_ms: 9.743\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.08759019618498\n",
      "    rl_1: 100.35382818922703\n",
      "  time_since_restore: 468.33339762687683\n",
      "  time_this_iter_s: 26.34112048149109\n",
      "  time_total_s: 468.33339762687683\n",
      "  timestamp: 1553104334\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 18\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 468 s, 18 iter, 180000 ts, 289 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-52-40\n",
      "  done: false\n",
      "  episode_len_mean: 174.53\n",
      "  episode_reward_max: 319.24071331708933\n",
      "  episode_reward_mean: 284.2224621633639\n",
      "  episode_reward_min: -162.2134245326086\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 747\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3982.537\n",
      "    load_time_ms: 2.734\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3480839729309082\n",
      "      kl: 0.0207850132137537\n",
      "      policy_loss: -0.005855133291333914\n",
      "      total_loss: 791.103271484375\n",
      "      vf_explained_var: 0.03362496197223663\n",
      "      vf_loss: 791.1092529296875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3431638479232788\n",
      "      kl: 0.008270476944744587\n",
      "      policy_loss: -0.003045635065063834\n",
      "      total_loss: 182.03842163085938\n",
      "      vf_explained_var: 0.3365172743797302\n",
      "      vf_loss: 182.04147338867188\n",
      "    sample_time_ms: 21943.851\n",
      "    update_time_ms: 9.758\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 186.06164503351403\n",
      "    rl_1: 98.16081712984992\n",
      "  time_since_restore: 494.41831946372986\n",
      "  time_this_iter_s: 26.084921836853027\n",
      "  time_total_s: 494.41831946372986\n",
      "  timestamp: 1553104360\n",
      "  timesteps_since_restore: 190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 19\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 494 s, 19 iter, 190000 ts, 284 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-53-06\n",
      "  done: false\n",
      "  episode_len_mean: 170.39\n",
      "  episode_reward_max: 316.67231133066366\n",
      "  episode_reward_mean: 283.61520014457415\n",
      "  episode_reward_min: -162.2134245326086\n",
      "  episodes_this_iter: 61\n",
      "  episodes_total: 808\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3973.565\n",
      "    load_time_ms: 2.631\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.10351571594947e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.346618890762329\n",
      "      kl: 0.0012807153398171067\n",
      "      policy_loss: -0.0005645955679938197\n",
      "      total_loss: 851.8440551757812\n",
      "      vf_explained_var: 0.029796184971928596\n",
      "      vf_loss: 851.8445434570312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.319851279258728\n",
      "      kl: 0.013942018151283264\n",
      "      policy_loss: -0.004633729346096516\n",
      "      total_loss: 173.08389282226562\n",
      "      vf_explained_var: 0.4064081609249115\n",
      "      vf_loss: 173.08851623535156\n",
      "    sample_time_ms: 21958.256\n",
      "    update_time_ms: 9.88\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 187.10955288435537\n",
      "    rl_1: 96.50564726021886\n",
      "  time_since_restore: 520.3959803581238\n",
      "  time_this_iter_s: 25.97766089439392\n",
      "  time_total_s: 520.3959803581238\n",
      "  timestamp: 1553104386\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 20\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 520 s, 20 iter, 200000 ts, 284 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-53-32\n",
      "  done: false\n",
      "  episode_len_mean: 164.92\n",
      "  episode_reward_max: 316.737645589838\n",
      "  episode_reward_mean: 285.14414075746816\n",
      "  episode_reward_min: -140.73688021849833\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 868\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3972.997\n",
      "    load_time_ms: 2.595\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3594346046447754\n",
      "      kl: 0.01193524431437254\n",
      "      policy_loss: -0.0054732272401452065\n",
      "      total_loss: 840.3240966796875\n",
      "      vf_explained_var: 0.11996109038591385\n",
      "      vf_loss: 840.3296508789062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.339450478553772\n",
      "      kl: 0.003499414771795273\n",
      "      policy_loss: -0.0035739142913371325\n",
      "      total_loss: 186.26437377929688\n",
      "      vf_explained_var: 0.460864394903183\n",
      "      vf_loss: 186.2679443359375\n",
      "    sample_time_ms: 21931.568\n",
      "    update_time_ms: 9.89\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 187.1473111469208\n",
      "    rl_1: 97.9968296105474\n",
      "  time_since_restore: 546.2389011383057\n",
      "  time_this_iter_s: 25.842920780181885\n",
      "  time_total_s: 546.2389011383057\n",
      "  timestamp: 1553104412\n",
      "  timesteps_since_restore: 210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 21\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 546 s, 21 iter, 210000 ts, 285 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-53-58\n",
      "  done: false\n",
      "  episode_len_mean: 162.33\n",
      "  episode_reward_max: 319.04676197548787\n",
      "  episode_reward_mean: 286.3228528904888\n",
      "  episode_reward_min: -140.73688021849833\n",
      "  episodes_this_iter: 63\n",
      "  episodes_total: 931\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3975.783\n",
      "    load_time_ms: 2.579\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.051757857974735e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3473856449127197\n",
      "      kl: 0.007866855710744858\n",
      "      policy_loss: -0.0023972077760845423\n",
      "      total_loss: 857.7191772460938\n",
      "      vf_explained_var: 0.1759367287158966\n",
      "      vf_loss: 857.7215576171875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.340937852859497\n",
      "      kl: 0.013977975584566593\n",
      "      policy_loss: -0.002417046343907714\n",
      "      total_loss: 180.08010864257812\n",
      "      vf_explained_var: 0.5016117095947266\n",
      "      vf_loss: 180.08255004882812\n",
      "    sample_time_ms: 21945.79\n",
      "    update_time_ms: 9.854\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 186.95870886189013\n",
      "    rl_1: 99.36414402859862\n",
      "  time_since_restore: 572.3434138298035\n",
      "  time_this_iter_s: 26.104512691497803\n",
      "  time_total_s: 572.3434138298035\n",
      "  timestamp: 1553104438\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 22\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 572 s, 22 iter, 220000 ts, 286 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-54-25\n",
      "  done: false\n",
      "  episode_len_mean: 157.93\n",
      "  episode_reward_max: 319.79790576703004\n",
      "  episode_reward_mean: 293.08651792297934\n",
      "  episode_reward_min: 266.16470660875524\n",
      "  episodes_this_iter: 63\n",
      "  episodes_total: 994\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3972.073\n",
      "    load_time_ms: 2.476\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.336913824081421\n",
      "      kl: 0.011404448188841343\n",
      "      policy_loss: -0.0034973518922924995\n",
      "      total_loss: 930.6494140625\n",
      "      vf_explained_var: 0.19321976602077484\n",
      "      vf_loss: 930.6530151367188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.330987572669983\n",
      "      kl: 0.007387317717075348\n",
      "      policy_loss: -0.0031332154758274555\n",
      "      total_loss: 188.20912170410156\n",
      "      vf_explained_var: 0.5202041268348694\n",
      "      vf_loss: 188.21224975585938\n",
      "    sample_time_ms: 21993.503\n",
      "    update_time_ms: 9.458\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.295557224631\n",
      "    rl_1: 101.79096069834831\n",
      "  time_since_restore: 598.5126719474792\n",
      "  time_this_iter_s: 26.16925811767578\n",
      "  time_total_s: 598.5126719474792\n",
      "  timestamp: 1553104465\n",
      "  timesteps_since_restore: 230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 23\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 598 s, 23 iter, 230000 ts, 293 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-54-51\n",
      "  done: false\n",
      "  episode_len_mean: 153.75\n",
      "  episode_reward_max: 319.79790576703004\n",
      "  episode_reward_mean: 292.5517037552802\n",
      "  episode_reward_min: 265.0907718385864\n",
      "  episodes_this_iter: 66\n",
      "  episodes_total: 1060\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3968.221\n",
      "    load_time_ms: 2.476\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5258789289873675e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3182460069656372\n",
      "      kl: 0.005166244227439165\n",
      "      policy_loss: -0.0013174032792448997\n",
      "      total_loss: 936.9827880859375\n",
      "      vf_explained_var: 0.229959174990654\n",
      "      vf_loss: 936.9840698242188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3250086307525635\n",
      "      kl: 0.019589753821492195\n",
      "      policy_loss: -0.00851091556251049\n",
      "      total_loss: 186.79075622558594\n",
      "      vf_explained_var: 0.5472866892814636\n",
      "      vf_loss: 186.7992706298828\n",
      "    sample_time_ms: 22018.771\n",
      "    update_time_ms: 9.4\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 192.0828623685603\n",
      "    rl_1: 100.46884138671989\n",
      "  time_since_restore: 624.6301827430725\n",
      "  time_this_iter_s: 26.11751079559326\n",
      "  time_total_s: 624.6301827430725\n",
      "  timestamp: 1553104491\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 24\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 624 s, 24 iter, 240000 ts, 293 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-55-17\n",
      "  done: false\n",
      "  episode_len_mean: 151.59\n",
      "  episode_reward_max: 319.9274829244105\n",
      "  episode_reward_mean: 291.85763393406916\n",
      "  episode_reward_min: 268.2487837672008\n",
      "  episodes_this_iter: 66\n",
      "  episodes_total: 1126\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3972.124\n",
      "    load_time_ms: 2.483\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.629394644936838e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.294982671737671\n",
      "      kl: 0.007186948321759701\n",
      "      policy_loss: -0.002544490620493889\n",
      "      total_loss: 917.7052001953125\n",
      "      vf_explained_var: 0.25053760409355164\n",
      "      vf_loss: 917.7078247070312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3302634954452515\n",
      "      kl: 0.009748841635882854\n",
      "      policy_loss: -0.0032037775963544846\n",
      "      total_loss: 192.56539916992188\n",
      "      vf_explained_var: 0.5503768920898438\n",
      "      vf_loss: 192.568603515625\n",
      "    sample_time_ms: 22131.929\n",
      "    update_time_ms: 9.372\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.11052457068317\n",
      "    rl_1: 100.747109363386\n",
      "  time_since_restore: 651.2801079750061\n",
      "  time_this_iter_s: 26.649925231933594\n",
      "  time_total_s: 651.2801079750061\n",
      "  timestamp: 1553104517\n",
      "  timesteps_since_restore: 250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 25\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 651 s, 25 iter, 250000 ts, 292 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-55-44\n",
      "  done: false\n",
      "  episode_len_mean: 148.91\n",
      "  episode_reward_max: 323.2489613170281\n",
      "  episode_reward_mean: 292.65014092777415\n",
      "  episode_reward_min: 266.8767751838198\n",
      "  episodes_this_iter: 68\n",
      "  episodes_total: 1194\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3971.538\n",
      "    load_time_ms: 2.479\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2840911149978638\n",
      "      kl: 0.012247628532350063\n",
      "      policy_loss: -0.004098152741789818\n",
      "      total_loss: 917.8673095703125\n",
      "      vf_explained_var: 0.27794259786605835\n",
      "      vf_loss: 917.8712768554688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3226594924926758\n",
      "      kl: 0.012079709209501743\n",
      "      policy_loss: -0.004643484018743038\n",
      "      total_loss: 211.7598114013672\n",
      "      vf_explained_var: 0.5420786142349243\n",
      "      vf_loss: 211.7644500732422\n",
      "    sample_time_ms: 22145.737\n",
      "    update_time_ms: 9.539\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.44680165533546\n",
      "    rl_1: 103.20333927243871\n",
      "  time_since_restore: 677.3386328220367\n",
      "  time_this_iter_s: 26.05852484703064\n",
      "  time_total_s: 677.3386328220367\n",
      "  timestamp: 1553104544\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 26\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 677 s, 26 iter, 260000 ts, 293 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-56-10\n",
      "  done: false\n",
      "  episode_len_mean: 144.98\n",
      "  episode_reward_max: 325.3405571843368\n",
      "  episode_reward_mean: 294.29680138558325\n",
      "  episode_reward_min: 266.8767751838198\n",
      "  episodes_this_iter: 69\n",
      "  episodes_total: 1263\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3967.331\n",
      "    load_time_ms: 2.525\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.276346206665039\n",
      "      kl: 0.01060949731618166\n",
      "      policy_loss: -0.004947609268128872\n",
      "      total_loss: 1002.1480102539062\n",
      "      vf_explained_var: 0.2793926000595093\n",
      "      vf_loss: 1002.1530151367188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3061755895614624\n",
      "      kl: 0.007755621336400509\n",
      "      policy_loss: -0.0013742876471951604\n",
      "      total_loss: 219.47085571289062\n",
      "      vf_explained_var: 0.5590773820877075\n",
      "      vf_loss: 219.47225952148438\n",
      "    sample_time_ms: 22176.948\n",
      "    update_time_ms: 9.528\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.9357278550699\n",
      "    rl_1: 104.36107353051348\n",
      "  time_since_restore: 703.7317292690277\n",
      "  time_this_iter_s: 26.393096446990967\n",
      "  time_total_s: 703.7317292690277\n",
      "  timestamp: 1553104570\n",
      "  timesteps_since_restore: 270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 27\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 703 s, 27 iter, 270000 ts, 294 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-56-36\n",
      "  done: false\n",
      "  episode_len_mean: 144.2\n",
      "  episode_reward_max: 326.93158125021205\n",
      "  episode_reward_mean: 295.27948932642755\n",
      "  episode_reward_min: 269.0410030697547\n",
      "  episodes_this_iter: 69\n",
      "  episodes_total: 1332\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3965.575\n",
      "    load_time_ms: 2.511\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2714643478393555\n",
      "      kl: 0.01331684086471796\n",
      "      policy_loss: -0.003223200561478734\n",
      "      total_loss: 935.7598266601562\n",
      "      vf_explained_var: 0.3253645598888397\n",
      "      vf_loss: 935.7631225585938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.297281265258789\n",
      "      kl: 0.013597945682704449\n",
      "      policy_loss: -0.0052255187183618546\n",
      "      total_loss: 219.75730895996094\n",
      "      vf_explained_var: 0.5724585056304932\n",
      "      vf_loss: 219.76258850097656\n",
      "    sample_time_ms: 22185.004\n",
      "    update_time_ms: 9.519\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.30899537422584\n",
      "    rl_1: 104.97049395220172\n",
      "  time_since_restore: 730.13707280159\n",
      "  time_this_iter_s: 26.405343532562256\n",
      "  time_total_s: 730.13707280159\n",
      "  timestamp: 1553104596\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 28\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 730 s, 28 iter, 280000 ts, 295 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-57-03\n",
      "  done: false\n",
      "  episode_len_mean: 144.62\n",
      "  episode_reward_max: 328.4058369479212\n",
      "  episode_reward_mean: 299.2123517710226\n",
      "  episode_reward_min: 273.3775029285497\n",
      "  episodes_this_iter: 70\n",
      "  episodes_total: 1402\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3964.13\n",
      "    load_time_ms: 2.408\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814697322468419e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2692779302597046\n",
      "      kl: 0.004817369394004345\n",
      "      policy_loss: -0.0006702814716845751\n",
      "      total_loss: 941.8818359375\n",
      "      vf_explained_var: 0.3378657400608063\n",
      "      vf_loss: 941.8825073242188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2876613140106201\n",
      "      kl: 0.011084000580012798\n",
      "      policy_loss: -0.004447202663868666\n",
      "      total_loss: 214.56866455078125\n",
      "      vf_explained_var: 0.6153473258018494\n",
      "      vf_loss: 214.57308959960938\n",
      "    sample_time_ms: 22228.669\n",
      "    update_time_ms: 10.261\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.14193198898505\n",
      "    rl_1: 108.07041978203755\n",
      "  time_since_restore: 756.6456799507141\n",
      "  time_this_iter_s: 26.508607149124146\n",
      "  time_total_s: 756.6456799507141\n",
      "  timestamp: 1553104623\n",
      "  timesteps_since_restore: 290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 29\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 756 s, 29 iter, 290000 ts, 299 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-57-29\n",
      "  done: false\n",
      "  episode_len_mean: 141.9\n",
      "  episode_reward_max: 323.8150314448601\n",
      "  episode_reward_mean: 294.2938026461126\n",
      "  episode_reward_min: -143.97634785214416\n",
      "  episodes_this_iter: 70\n",
      "  episodes_total: 1472\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3967.348\n",
      "    load_time_ms: 2.448\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9073486612342094e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2537062168121338\n",
      "      kl: 0.008926457725465298\n",
      "      policy_loss: -0.001633352367207408\n",
      "      total_loss: 975.4551391601562\n",
      "      vf_explained_var: 0.31776711344718933\n",
      "      vf_loss: 975.4566040039062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2821987867355347\n",
      "      kl: 0.005368183366954327\n",
      "      policy_loss: -0.0012350700562819839\n",
      "      total_loss: 245.18453979492188\n",
      "      vf_explained_var: 0.5695971846580505\n",
      "      vf_loss: 245.1857147216797\n",
      "    sample_time_ms: 22261.073\n",
      "    update_time_ms: 10.118\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.20957379577783\n",
      "    rl_1: 106.08422885033478\n",
      "  time_since_restore: 782.9795315265656\n",
      "  time_this_iter_s: 26.33385157585144\n",
      "  time_total_s: 782.9795315265656\n",
      "  timestamp: 1553104649\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 30\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 782 s, 30 iter, 300000 ts, 294 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-57-56\n",
      "  done: false\n",
      "  episode_len_mean: 138.69\n",
      "  episode_reward_max: 328.3383789244495\n",
      "  episode_reward_mean: 291.976539344841\n",
      "  episode_reward_min: -143.97634785214416\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 1544\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3966.287\n",
      "    load_time_ms: 2.478\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.536743306171047e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2550557851791382\n",
      "      kl: 0.009514255449175835\n",
      "      policy_loss: -0.0008178289281204343\n",
      "      total_loss: 968.23828125\n",
      "      vf_explained_var: 0.36258918046951294\n",
      "      vf_loss: 968.2390747070312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2677072286605835\n",
      "      kl: 0.004175866488367319\n",
      "      policy_loss: -0.001833529444411397\n",
      "      total_loss: 223.82411193847656\n",
      "      vf_explained_var: 0.6299880146980286\n",
      "      vf_loss: 223.8259735107422\n",
      "    sample_time_ms: 22322.625\n",
      "    update_time_ms: 9.674\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 186.2731135532927\n",
      "    rl_1: 105.70342579154827\n",
      "  time_since_restore: 809.4238746166229\n",
      "  time_this_iter_s: 26.444343090057373\n",
      "  time_total_s: 809.4238746166229\n",
      "  timestamp: 1553104676\n",
      "  timesteps_since_restore: 310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 31\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 809 s, 31 iter, 310000 ts, 292 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-58-22\n",
      "  done: false\n",
      "  episode_len_mean: 138.52\n",
      "  episode_reward_max: 324.10870079248656\n",
      "  episode_reward_mean: 298.05576311734444\n",
      "  episode_reward_min: 266.89220816423153\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 1617\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3961.557\n",
      "    load_time_ms: 2.462\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.7683716530855236e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2542518377304077\n",
      "      kl: 0.006613325793296099\n",
      "      policy_loss: -0.0017260528402402997\n",
      "      total_loss: 965.7178344726562\n",
      "      vf_explained_var: 0.38168519735336304\n",
      "      vf_loss: 965.7193603515625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2571064233779907\n",
      "      kl: 0.011612053029239178\n",
      "      policy_loss: -0.004052294883877039\n",
      "      total_loss: 235.030517578125\n",
      "      vf_explained_var: 0.6351875066757202\n",
      "      vf_loss: 235.03460693359375\n",
      "    sample_time_ms: 22345.238\n",
      "    update_time_ms: 9.719\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.92452336523417\n",
      "    rl_1: 108.13123975211029\n",
      "  time_since_restore: 835.7091679573059\n",
      "  time_this_iter_s: 26.285293340682983\n",
      "  time_total_s: 835.7091679573059\n",
      "  timestamp: 1553104702\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 32\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 835 s, 32 iter, 320000 ts, 298 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-58-48\n",
      "  done: false\n",
      "  episode_len_mean: 136.03\n",
      "  episode_reward_max: 330.9023799169206\n",
      "  episode_reward_mean: 297.34306868674184\n",
      "  episode_reward_min: 266.89220816423153\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 1691\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3959.849\n",
      "    load_time_ms: 2.477\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2216769456863403\n",
      "      kl: 0.014984233304858208\n",
      "      policy_loss: -0.003215595381334424\n",
      "      total_loss: 992.3871459960938\n",
      "      vf_explained_var: 0.38140302896499634\n",
      "      vf_loss: 992.3903198242188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.25454843044281\n",
      "      kl: 0.021551448851823807\n",
      "      policy_loss: -0.005222940351814032\n",
      "      total_loss: 228.56910705566406\n",
      "      vf_explained_var: 0.6400055289268494\n",
      "      vf_loss: 228.57432556152344\n",
      "    sample_time_ms: 22346.157\n",
      "    update_time_ms: 9.892\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.29571739693168\n",
      "    rl_1: 108.0473512898102\n",
      "  time_since_restore: 861.8731820583344\n",
      "  time_this_iter_s: 26.164014101028442\n",
      "  time_total_s: 861.8731820583344\n",
      "  timestamp: 1553104728\n",
      "  timesteps_since_restore: 330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 33\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 861 s, 33 iter, 330000 ts, 297 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-59-15\n",
      "  done: false\n",
      "  episode_len_mean: 133.52\n",
      "  episode_reward_max: 328.48274055435076\n",
      "  episode_reward_mean: 296.9397390354395\n",
      "  episode_reward_min: 266.1321898789706\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 1766\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3959.948\n",
      "    load_time_ms: 2.416\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2258265018463135\n",
      "      kl: 0.013048517517745495\n",
      "      policy_loss: -0.0038959281519055367\n",
      "      total_loss: 988.2620239257812\n",
      "      vf_explained_var: 0.3912937343120575\n",
      "      vf_loss: 988.2659301757812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2312570810317993\n",
      "      kl: 0.013024657033383846\n",
      "      policy_loss: -0.0025026954244822264\n",
      "      total_loss: 238.9442596435547\n",
      "      vf_explained_var: 0.6580354571342468\n",
      "      vf_loss: 238.94674682617188\n",
      "    sample_time_ms: 22388.95\n",
      "    update_time_ms: 10.175\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.95753306149808\n",
      "    rl_1: 107.98220597394149\n",
      "  time_since_restore: 888.4201364517212\n",
      "  time_this_iter_s: 26.54695439338684\n",
      "  time_total_s: 888.4201364517212\n",
      "  timestamp: 1553104755\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 34\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 888 s, 34 iter, 340000 ts, 297 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_18-59-42\n",
      "  done: false\n",
      "  episode_len_mean: 133.53\n",
      "  episode_reward_max: 326.440354313734\n",
      "  episode_reward_mean: 299.8713941891076\n",
      "  episode_reward_min: 266.1321898789706\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 1841\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3969.933\n",
      "    load_time_ms: 2.397\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2152904272079468\n",
      "      kl: 0.024016350507736206\n",
      "      policy_loss: -0.004219915252178907\n",
      "      total_loss: 999.3756713867188\n",
      "      vf_explained_var: 0.4032428562641144\n",
      "      vf_loss: 999.3800048828125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2242752313613892\n",
      "      kl: 0.009235206991434097\n",
      "      policy_loss: -0.003722947323694825\n",
      "      total_loss: 246.107177734375\n",
      "      vf_explained_var: 0.6583576202392578\n",
      "      vf_loss: 246.11094665527344\n",
      "    sample_time_ms: 22371.66\n",
      "    update_time_ms: 10.123\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.54534442508464\n",
      "    rl_1: 109.3260497640229\n",
      "  time_since_restore: 914.9917509555817\n",
      "  time_this_iter_s: 26.571614503860474\n",
      "  time_total_s: 914.9917509555817\n",
      "  timestamp: 1553104782\n",
      "  timesteps_since_restore: 350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 35\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 914 s, 35 iter, 350000 ts, 300 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-00-08\n",
      "  done: false\n",
      "  episode_len_mean: 131.14\n",
      "  episode_reward_max: 326.04796304427873\n",
      "  episode_reward_mean: 298.9563516787975\n",
      "  episode_reward_min: 265.29298617559095\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 1918\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3972.595\n",
      "    load_time_ms: 2.495\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2121872901916504\n",
      "      kl: 0.021715356037020683\n",
      "      policy_loss: -0.0037315392401069403\n",
      "      total_loss: 994.8798828125\n",
      "      vf_explained_var: 0.42568984627723694\n",
      "      vf_loss: 994.8836059570312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2182543277740479\n",
      "      kl: 0.005184201989322901\n",
      "      policy_loss: -0.0019090024288743734\n",
      "      total_loss: 243.6865692138672\n",
      "      vf_explained_var: 0.6808629035949707\n",
      "      vf_loss: 243.68850708007812\n",
      "    sample_time_ms: 22411.586\n",
      "    update_time_ms: 9.824\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.6898115894337\n",
      "    rl_1: 110.26654008936377\n",
      "  time_since_restore: 941.4745223522186\n",
      "  time_this_iter_s: 26.482771396636963\n",
      "  time_total_s: 941.4745223522186\n",
      "  timestamp: 1553104808\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 36\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 941 s, 36 iter, 360000 ts, 299 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-00-35\n",
      "  done: false\n",
      "  episode_len_mean: 130.04\n",
      "  episode_reward_max: 334.6232124683845\n",
      "  episode_reward_mean: 303.1537803117809\n",
      "  episode_reward_min: 271.83386526523196\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 1995\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3972.68\n",
      "    load_time_ms: 2.457\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2081758975982666\n",
      "      kl: 0.011167449876666069\n",
      "      policy_loss: -0.003585627069696784\n",
      "      total_loss: 1017.3037719726562\n",
      "      vf_explained_var: 0.4350035786628723\n",
      "      vf_loss: 1017.307373046875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2187366485595703\n",
      "      kl: 0.018734795972704887\n",
      "      policy_loss: -0.005833951756358147\n",
      "      total_loss: 252.7023162841797\n",
      "      vf_explained_var: 0.6869326829910278\n",
      "      vf_loss: 252.70816040039062\n",
      "    sample_time_ms: 22418.853\n",
      "    update_time_ms: 9.721\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.33383005256147\n",
      "    rl_1: 112.81995025921942\n",
      "  time_since_restore: 967.9421498775482\n",
      "  time_this_iter_s: 26.46762752532959\n",
      "  time_total_s: 967.9421498775482\n",
      "  timestamp: 1553104835\n",
      "  timesteps_since_restore: 370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 37\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 967 s, 37 iter, 370000 ts, 303 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 130.27\n",
      "  episode_reward_max: 338.57330851743126\n",
      "  episode_reward_mean: 304.1694878921062\n",
      "  episode_reward_min: 273.7145449677517\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 2071\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3973.406\n",
      "    load_time_ms: 2.524\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.224538803100586\n",
      "      kl: 0.016084980219602585\n",
      "      policy_loss: -0.0043078395538032055\n",
      "      total_loss: 1003.8587646484375\n",
      "      vf_explained_var: 0.4536494314670563\n",
      "      vf_loss: 1003.86328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2442626953125\n",
      "      kl: 0.029148954898118973\n",
      "      policy_loss: -0.005850998684763908\n",
      "      total_loss: 256.9443664550781\n",
      "      vf_explained_var: 0.6999927163124084\n",
      "      vf_loss: 256.9502258300781\n",
      "    sample_time_ms: 22419.95\n",
      "    update_time_ms: 9.73\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.70063032894996\n",
      "    rl_1: 114.46885756315628\n",
      "  time_since_restore: 994.3652651309967\n",
      "  time_this_iter_s: 26.423115253448486\n",
      "  time_total_s: 994.3652651309967\n",
      "  timestamp: 1553104861\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 38\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 994 s, 38 iter, 380000 ts, 304 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 129.57\n",
      "  episode_reward_max: 334.06170534132104\n",
      "  episode_reward_mean: 301.26195746863993\n",
      "  episode_reward_min: -119.91554465967937\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 2149\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3971.169\n",
      "    load_time_ms: 2.545\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2118734121322632\n",
      "      kl: 0.010706672444939613\n",
      "      policy_loss: -0.003254740731790662\n",
      "      total_loss: 1022.9122314453125\n",
      "      vf_explained_var: 0.4372557997703552\n",
      "      vf_loss: 1022.9154663085938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2451534271240234\n",
      "      kl: 0.0029918639920651913\n",
      "      policy_loss: -0.0007107551791705191\n",
      "      total_loss: 265.9335632324219\n",
      "      vf_explained_var: 0.6633917689323425\n",
      "      vf_loss: 265.934326171875\n",
      "    sample_time_ms: 22403.818\n",
      "    update_time_ms: 8.937\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.02798455006257\n",
      "    rl_1: 112.23397291857735\n",
      "  time_since_restore: 1020.68501496315\n",
      "  time_this_iter_s: 26.31974983215332\n",
      "  time_total_s: 1020.68501496315\n",
      "  timestamp: 1553104888\n",
      "  timesteps_since_restore: 390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 39\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1020 s, 39 iter, 390000 ts, 301 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-01-54\n",
      "  done: false\n",
      "  episode_len_mean: 128.61\n",
      "  episode_reward_max: 335.4744230892069\n",
      "  episode_reward_mean: 304.22473987737595\n",
      "  episode_reward_min: 279.5589996498334\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 2227\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3968.504\n",
      "    load_time_ms: 2.508\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1923147439956665\n",
      "      kl: 0.010978172533214092\n",
      "      policy_loss: -0.002688758773729205\n",
      "      total_loss: 999.02783203125\n",
      "      vf_explained_var: 0.48011475801467896\n",
      "      vf_loss: 999.0302734375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9802322831784522e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2392923831939697\n",
      "      kl: 0.003336076159030199\n",
      "      policy_loss: -0.0012234464520588517\n",
      "      total_loss: 243.2410888671875\n",
      "      vf_explained_var: 0.7282143831253052\n",
      "      vf_loss: 243.24229431152344\n",
      "    sample_time_ms: 22372.716\n",
      "    update_time_ms: 9.456\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.4362993646811\n",
      "    rl_1: 113.78844051269483\n",
      "  time_since_restore: 1046.686499595642\n",
      "  time_this_iter_s: 26.001484632492065\n",
      "  time_total_s: 1046.686499595642\n",
      "  timestamp: 1553104914\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 40\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1046 s, 40 iter, 400000 ts, 304 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-02-20\n",
      "  done: false\n",
      "  episode_len_mean: 126.88\n",
      "  episode_reward_max: 328.48270938249203\n",
      "  episode_reward_mean: 301.82502552564034\n",
      "  episode_reward_min: 276.25461401670253\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 2305\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3966.834\n",
      "    load_time_ms: 2.445\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841858265427618e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.175396203994751\n",
      "      kl: 0.004417592193931341\n",
      "      policy_loss: -0.0012525066267699003\n",
      "      total_loss: 1010.963623046875\n",
      "      vf_explained_var: 0.4872622787952423\n",
      "      vf_loss: 1010.9647216796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.235478162765503\n",
      "      kl: 0.008720247074961662\n",
      "      policy_loss: -0.0037873561959713697\n",
      "      total_loss: 227.948486328125\n",
      "      vf_explained_var: 0.7474292516708374\n",
      "      vf_loss: 227.95230102539062\n",
      "    sample_time_ms: 22348.996\n",
      "    update_time_ms: 9.57\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.74676637829256\n",
      "    rl_1: 112.07825914734777\n",
      "  time_since_restore: 1072.877253293991\n",
      "  time_this_iter_s: 26.190753698349\n",
      "  time_total_s: 1072.877253293991\n",
      "  timestamp: 1553104940\n",
      "  timesteps_since_restore: 410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 41\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1072 s, 41 iter, 410000 ts, 302 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-02-46\n",
      "  done: false\n",
      "  episode_len_mean: 126.37\n",
      "  episode_reward_max: 336.1248170181443\n",
      "  episode_reward_mean: 302.938596467877\n",
      "  episode_reward_min: 275.3667457778904\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 2385\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3966.578\n",
      "    load_time_ms: 2.426\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1920929132713809e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1666337251663208\n",
      "      kl: 0.0018607454840093851\n",
      "      policy_loss: -0.0010803536279127002\n",
      "      total_loss: 976.5455932617188\n",
      "      vf_explained_var: 0.5019891262054443\n",
      "      vf_loss: 976.5465698242188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2308672666549683\n",
      "      kl: 0.0026219591964036226\n",
      "      policy_loss: -0.0016548261046409607\n",
      "      total_loss: 232.71847534179688\n",
      "      vf_explained_var: 0.7399280667304993\n",
      "      vf_loss: 232.72012329101562\n",
      "    sample_time_ms: 22339.087\n",
      "    update_time_ms: 9.898\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.33009876796038\n",
      "    rl_1: 113.60849769991671\n",
      "  time_since_restore: 1099.063868522644\n",
      "  time_this_iter_s: 26.186615228652954\n",
      "  time_total_s: 1099.063868522644\n",
      "  timestamp: 1553104966\n",
      "  timesteps_since_restore: 420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 42\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1099 s, 42 iter, 420000 ts, 303 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-03-12\n",
      "  done: false\n",
      "  episode_len_mean: 126.56\n",
      "  episode_reward_max: 336.1248170181443\n",
      "  episode_reward_mean: 306.250020212503\n",
      "  episode_reward_min: 276.09734747382623\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 2464\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3989.261\n",
      "    load_time_ms: 2.435\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1791340112686157\n",
      "      kl: 0.02275978960096836\n",
      "      policy_loss: -0.004890022333711386\n",
      "      total_loss: 988.1986083984375\n",
      "      vf_explained_var: 0.511512279510498\n",
      "      vf_loss: 988.20361328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.210505723953247\n",
      "      kl: 0.018224788829684258\n",
      "      policy_loss: -0.0032339857425540686\n",
      "      total_loss: 235.4840545654297\n",
      "      vf_explained_var: 0.7481186985969543\n",
      "      vf_loss: 235.48727416992188\n",
      "    sample_time_ms: 22290.547\n",
      "    update_time_ms: 9.901\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.149519089516\n",
      "    rl_1: 115.10050112298696\n",
      "  time_since_restore: 1124.9710988998413\n",
      "  time_this_iter_s: 25.907230377197266\n",
      "  time_total_s: 1124.9710988998413\n",
      "  timestamp: 1553104992\n",
      "  timesteps_since_restore: 430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 43\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1124 s, 43 iter, 430000 ts, 306 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-03-38\n",
      "  done: false\n",
      "  episode_len_mean: 127.04\n",
      "  episode_reward_max: 338.1802558312737\n",
      "  episode_reward_mean: 307.4152671317697\n",
      "  episode_reward_min: 278.6232069611873\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 2543\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3993.222\n",
      "    load_time_ms: 2.531\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.9604645663569045e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1549773216247559\n",
      "      kl: 0.0057280901819467545\n",
      "      policy_loss: -0.0015229628188535571\n",
      "      total_loss: 978.0330810546875\n",
      "      vf_explained_var: 0.5222333669662476\n",
      "      vf_loss: 978.0345458984375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2241650819778442\n",
      "      kl: 0.01350589282810688\n",
      "      policy_loss: -0.00470610661432147\n",
      "      total_loss: 242.04559326171875\n",
      "      vf_explained_var: 0.745635449886322\n",
      "      vf_loss: 242.05027770996094\n",
      "    sample_time_ms: 22196.606\n",
      "    update_time_ms: 9.629\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.87914118462922\n",
      "    rl_1: 115.53612594714042\n",
      "  time_since_restore: 1150.6187236309052\n",
      "  time_this_iter_s: 25.647624731063843\n",
      "  time_total_s: 1150.6187236309052\n",
      "  timestamp: 1553105018\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 44\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1150 s, 44 iter, 440000 ts, 307 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-04-04\n",
      "  done: false\n",
      "  episode_len_mean: 126.87\n",
      "  episode_reward_max: 335.38664536305646\n",
      "  episode_reward_mean: 308.22175349108625\n",
      "  episode_reward_min: 277.66634458836796\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 2621\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3988.533\n",
      "    load_time_ms: 2.576\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9802322831784522e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1338139772415161\n",
      "      kl: 0.006261985283344984\n",
      "      policy_loss: -0.0012959465384483337\n",
      "      total_loss: 976.7520141601562\n",
      "      vf_explained_var: 0.5339890122413635\n",
      "      vf_loss: 976.75341796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2027699947357178\n",
      "      kl: 0.004505002871155739\n",
      "      policy_loss: -0.002759610302746296\n",
      "      total_loss: 233.50816345214844\n",
      "      vf_explained_var: 0.7633311152458191\n",
      "      vf_loss: 233.51097106933594\n",
      "    sample_time_ms: 22164.047\n",
      "    update_time_ms: 9.75\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.45488480792218\n",
      "    rl_1: 116.76686868316409\n",
      "  time_since_restore: 1176.8190364837646\n",
      "  time_this_iter_s: 26.200312852859497\n",
      "  time_total_s: 1176.8190364837646\n",
      "  timestamp: 1553105044\n",
      "  timesteps_since_restore: 450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 45\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1176 s, 45 iter, 450000 ts, 308 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-04-31\n",
      "  done: false\n",
      "  episode_len_mean: 126.61\n",
      "  episode_reward_max: 336.36283329759914\n",
      "  episode_reward_mean: 305.2342568681232\n",
      "  episode_reward_min: 274.1710242082341\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 2701\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3985.59\n",
      "    load_time_ms: 2.538\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1274667978286743\n",
      "      kl: 0.022387081757187843\n",
      "      policy_loss: -0.005354596301913261\n",
      "      total_loss: 926.3267211914062\n",
      "      vf_explained_var: 0.5545260906219482\n",
      "      vf_loss: 926.3321533203125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1972366571426392\n",
      "      kl: 0.008032325655221939\n",
      "      policy_loss: -0.002609158167615533\n",
      "      total_loss: 217.6526641845703\n",
      "      vf_explained_var: 0.7798534035682678\n",
      "      vf_loss: 217.6552734375\n",
      "    sample_time_ms: 22179.764\n",
      "    update_time_ms: 9.714\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.6303894282545\n",
      "    rl_1: 115.60386743986875\n",
      "  time_since_restore: 1203.4273912906647\n",
      "  time_this_iter_s: 26.608354806900024\n",
      "  time_total_s: 1203.4273912906647\n",
      "  timestamp: 1553105071\n",
      "  timesteps_since_restore: 460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 46\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1203 s, 46 iter, 460000 ts, 305 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-04-57\n",
      "  done: false\n",
      "  episode_len_mean: 127.21\n",
      "  episode_reward_max: 343.1430532406326\n",
      "  episode_reward_mean: 309.2837874944051\n",
      "  episode_reward_min: 280.0710428882969\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 2779\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3984.592\n",
      "    load_time_ms: 2.53\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4901161415892261e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.115157961845398\n",
      "      kl: 0.00662808446213603\n",
      "      policy_loss: -0.001591801643371582\n",
      "      total_loss: 920.01513671875\n",
      "      vf_explained_var: 0.5652101039886475\n",
      "      vf_loss: 920.0167236328125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.313225884932663e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1771328449249268\n",
      "      kl: 0.009717302396893501\n",
      "      policy_loss: -0.0016256493981927633\n",
      "      total_loss: 232.8313751220703\n",
      "      vf_explained_var: 0.7768756151199341\n",
      "      vf_loss: 232.83303833007812\n",
      "    sample_time_ms: 22127.335\n",
      "    update_time_ms: 9.832\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.01224451648977\n",
      "    rl_1: 119.27154297791536\n",
      "  time_since_restore: 1229.3596408367157\n",
      "  time_this_iter_s: 25.932249546051025\n",
      "  time_total_s: 1229.3596408367157\n",
      "  timestamp: 1553105097\n",
      "  timesteps_since_restore: 470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 47\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1229 s, 47 iter, 470000 ts, 309 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-05-23\n",
      "  done: false\n",
      "  episode_len_mean: 126.93\n",
      "  episode_reward_max: 343.1430532406326\n",
      "  episode_reward_mean: 308.4446796671154\n",
      "  episode_reward_min: 277.24049861474964\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 2858\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3980.388\n",
      "    load_time_ms: 2.446\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.450580707946131e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.118698239326477\n",
      "      kl: 0.0018251865403726697\n",
      "      policy_loss: -0.0006017433479428291\n",
      "      total_loss: 920.9926147460938\n",
      "      vf_explained_var: 0.5735657811164856\n",
      "      vf_loss: 920.9931030273438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1985950469970703\n",
      "      kl: 0.004656726028770208\n",
      "      policy_loss: -0.0010194381466135383\n",
      "      total_loss: 215.03225708007812\n",
      "      vf_explained_var: 0.7947254180908203\n",
      "      vf_loss: 215.0332794189453\n",
      "    sample_time_ms: 22129.271\n",
      "    update_time_ms: 10.061\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.8443344626795\n",
      "    rl_1: 117.60034520443591\n",
      "  time_since_restore: 1255.7612833976746\n",
      "  time_this_iter_s: 26.401642560958862\n",
      "  time_total_s: 1255.7612833976746\n",
      "  timestamp: 1553105123\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 48\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1255 s, 48 iter, 480000 ts, 308 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-05-50\n",
      "  done: false\n",
      "  episode_len_mean: 127.0\n",
      "  episode_reward_max: 340.41588920037077\n",
      "  episode_reward_mean: 307.3607449075629\n",
      "  episode_reward_min: 277.24049861474964\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 2937\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3980.014\n",
      "    load_time_ms: 2.558\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7252903539730653e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1134835481643677\n",
      "      kl: 0.007029965054243803\n",
      "      policy_loss: -0.0032709764782339334\n",
      "      total_loss: 901.4679565429688\n",
      "      vf_explained_var: 0.5849176049232483\n",
      "      vf_loss: 901.4713745117188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.193291187286377\n",
      "      kl: 0.0049394057132303715\n",
      "      policy_loss: -0.0029151462949812412\n",
      "      total_loss: 207.50286865234375\n",
      "      vf_explained_var: 0.7982660531997681\n",
      "      vf_loss: 207.50579833984375\n",
      "    sample_time_ms: 22149.711\n",
      "    update_time_ms: 10.277\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.3964390845889\n",
      "    rl_1: 116.96430582297403\n",
      "  time_since_restore: 1282.286685705185\n",
      "  time_this_iter_s: 26.525402307510376\n",
      "  time_total_s: 1282.286685705185\n",
      "  timestamp: 1553105150\n",
      "  timesteps_since_restore: 490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 49\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1282 s, 49 iter, 490000 ts, 307 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-06-16\n",
      "  done: false\n",
      "  episode_len_mean: 127.22\n",
      "  episode_reward_max: 339.52538810699366\n",
      "  episode_reward_mean: 307.756711164097\n",
      "  episode_reward_min: 280.1893425185198\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 3015\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3982.77\n",
      "    load_time_ms: 2.591\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8626451769865326e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1091989278793335\n",
      "      kl: 0.005224085412919521\n",
      "      policy_loss: -0.0016164240660145879\n",
      "      total_loss: 879.0474853515625\n",
      "      vf_explained_var: 0.5946645736694336\n",
      "      vf_loss: 879.0491943359375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1915910243988037\n",
      "      kl: 0.026832789182662964\n",
      "      policy_loss: -0.006877550855278969\n",
      "      total_loss: 207.5235595703125\n",
      "      vf_explained_var: 0.8054007887840271\n",
      "      vf_loss: 207.5304412841797\n",
      "    sample_time_ms: 22167.346\n",
      "    update_time_ms: 9.943\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.3139577129408\n",
      "    rl_1: 117.44275345115622\n",
      "  time_since_restore: 1308.4885773658752\n",
      "  time_this_iter_s: 26.201891660690308\n",
      "  time_total_s: 1308.4885773658752\n",
      "  timestamp: 1553105176\n",
      "  timesteps_since_restore: 500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 50\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1308 s, 50 iter, 500000 ts, 308 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-06-42\n",
      "  done: false\n",
      "  episode_len_mean: 126.4\n",
      "  episode_reward_max: 340.3413457687048\n",
      "  episode_reward_mean: 306.3216371789164\n",
      "  episode_reward_min: 279.5701902885424\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 3095\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3994.506\n",
      "    load_time_ms: 2.712\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.313225884932663e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0725431442260742\n",
      "      kl: 0.011718271300196648\n",
      "      policy_loss: -0.002081964397802949\n",
      "      total_loss: 881.0265502929688\n",
      "      vf_explained_var: 0.6056416034698486\n",
      "      vf_loss: 881.028564453125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1641532356165829e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.182939052581787\n",
      "      kl: 0.009033532813191414\n",
      "      policy_loss: -0.0020126597955822945\n",
      "      total_loss: 195.3861083984375\n",
      "      vf_explained_var: 0.8183439373970032\n",
      "      vf_loss: 195.38807678222656\n",
      "    sample_time_ms: 22174.603\n",
      "    update_time_ms: 10.149\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.8766877020553\n",
      "    rl_1: 116.44494947686103\n",
      "  time_since_restore: 1334.8742835521698\n",
      "  time_this_iter_s: 26.385706186294556\n",
      "  time_total_s: 1334.8742835521698\n",
      "  timestamp: 1553105202\n",
      "  timesteps_since_restore: 510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 51\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1334 s, 51 iter, 510000 ts, 306 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-07-09\n",
      "  done: false\n",
      "  episode_len_mean: 125.76\n",
      "  episode_reward_max: 338.69092141390416\n",
      "  episode_reward_mean: 305.72150468005367\n",
      "  episode_reward_min: 280.06953469898815\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 3174\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3994.352\n",
      "    load_time_ms: 2.75\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.313225884932663e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0693562030792236\n",
      "      kl: 0.005866308696568012\n",
      "      policy_loss: -0.0014810528373345733\n",
      "      total_loss: 873.6301879882812\n",
      "      vf_explained_var: 0.6085957288742065\n",
      "      vf_loss: 873.6317138671875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1885472536087036\n",
      "      kl: 0.029203597456216812\n",
      "      policy_loss: -0.006034535821527243\n",
      "      total_loss: 192.63107299804688\n",
      "      vf_explained_var: 0.8257535099983215\n",
      "      vf_loss: 192.63710021972656\n",
      "    sample_time_ms: 22186.356\n",
      "    update_time_ms: 10.093\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.25602866712416\n",
      "    rl_1: 115.46547601292949\n",
      "  time_since_restore: 1361.180585384369\n",
      "  time_this_iter_s: 26.306301832199097\n",
      "  time_total_s: 1361.180585384369\n",
      "  timestamp: 1553105229\n",
      "  timesteps_since_restore: 520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 52\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1361 s, 52 iter, 520000 ts, 306 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-07-35\n",
      "  done: false\n",
      "  episode_len_mean: 126.81\n",
      "  episode_reward_max: 338.69092141390416\n",
      "  episode_reward_mean: 304.0893864663619\n",
      "  episode_reward_min: 279.9558815326633\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 3253\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3976.556\n",
      "    load_time_ms: 2.766\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.103994369506836\n",
      "      kl: 0.011962488293647766\n",
      "      policy_loss: -0.0017127845203503966\n",
      "      total_loss: 858.5827026367188\n",
      "      vf_explained_var: 0.6262349486351013\n",
      "      vf_loss: 858.5845336914062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.207096815109253\n",
      "      kl: 0.017663458362221718\n",
      "      policy_loss: -0.005745315924286842\n",
      "      total_loss: 164.18997192382812\n",
      "      vf_explained_var: 0.850510835647583\n",
      "      vf_loss: 164.1957244873047\n",
      "    sample_time_ms: 22277.122\n",
      "    update_time_ms: 10.042\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.49548273037777\n",
      "    rl_1: 113.59390373598413\n",
      "  time_since_restore: 1387.8140788078308\n",
      "  time_this_iter_s: 26.633493423461914\n",
      "  time_total_s: 1387.8140788078308\n",
      "  timestamp: 1553105255\n",
      "  timesteps_since_restore: 530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 53\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1387 s, 53 iter, 530000 ts, 304 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-08-01\n",
      "  done: false\n",
      "  episode_len_mean: 127.44\n",
      "  episode_reward_max: 336.42123057873397\n",
      "  episode_reward_mean: 307.144249510815\n",
      "  episode_reward_min: 276.0240269627566\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 3331\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3974.38\n",
      "    load_time_ms: 2.666\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.110464096069336\n",
      "      kl: 0.02288733422756195\n",
      "      policy_loss: -0.006228006444871426\n",
      "      total_loss: 815.8233642578125\n",
      "      vf_explained_var: 0.6440245509147644\n",
      "      vf_loss: 815.8296508789062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.8207661780829145e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2089213132858276\n",
      "      kl: 0.002979565178975463\n",
      "      policy_loss: -0.0018499932484701276\n",
      "      total_loss: 185.24835205078125\n",
      "      vf_explained_var: 0.8377235531806946\n",
      "      vf_loss: 185.2501983642578\n",
      "    sample_time_ms: 22314.417\n",
      "    update_time_ms: 9.96\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.28540684245883\n",
      "    rl_1: 117.85884266835619\n",
      "  time_since_restore: 1413.809999704361\n",
      "  time_this_iter_s: 25.99592089653015\n",
      "  time_total_s: 1413.809999704361\n",
      "  timestamp: 1553105281\n",
      "  timesteps_since_restore: 540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 54\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1413 s, 54 iter, 540000 ts, 307 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-08-28\n",
      "  done: false\n",
      "  episode_len_mean: 128.36\n",
      "  episode_reward_max: 336.87518172015155\n",
      "  episode_reward_mean: 311.4263268044252\n",
      "  episode_reward_min: 282.79548390614434\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 3409\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3966.698\n",
      "    load_time_ms: 2.623\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0694153308868408\n",
      "      kl: 0.022866085171699524\n",
      "      policy_loss: -0.00474423635751009\n",
      "      total_loss: 832.820068359375\n",
      "      vf_explained_var: 0.6495965123176575\n",
      "      vf_loss: 832.8247680664062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.9103830890414573e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1863956451416016\n",
      "      kl: 0.004116615280508995\n",
      "      policy_loss: -0.0019885909277945757\n",
      "      total_loss: 189.13099670410156\n",
      "      vf_explained_var: 0.8376950025558472\n",
      "      vf_loss: 189.1329803466797\n",
      "    sample_time_ms: 22374.085\n",
      "    update_time_ms: 9.989\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.75178249247114\n",
      "    rl_1: 119.67454431195405\n",
      "  time_since_restore: 1440.5323243141174\n",
      "  time_this_iter_s: 26.72232460975647\n",
      "  time_total_s: 1440.5323243141174\n",
      "  timestamp: 1553105308\n",
      "  timesteps_since_restore: 550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 55\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1440 s, 55 iter, 550000 ts, 311 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-08-55\n",
      "  done: false\n",
      "  episode_len_mean: 129.24\n",
      "  episode_reward_max: 335.6924457124208\n",
      "  episode_reward_mean: 309.7126909195297\n",
      "  episode_reward_min: 284.61348623855594\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 3486\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3969.052\n",
      "    load_time_ms: 2.645\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.09678053855896\n",
      "      kl: 0.01045704260468483\n",
      "      policy_loss: -0.0027252451982349157\n",
      "      total_loss: 793.1541748046875\n",
      "      vf_explained_var: 0.6653013229370117\n",
      "      vf_loss: 793.1568603515625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2111310958862305\n",
      "      kl: 0.013152644038200378\n",
      "      policy_loss: -0.0036581121385097504\n",
      "      total_loss: 180.90530395507812\n",
      "      vf_explained_var: 0.847004234790802\n",
      "      vf_loss: 180.90895080566406\n",
      "    sample_time_ms: 22363.87\n",
      "    update_time_ms: 9.782\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.43484224223135\n",
      "    rl_1: 119.27784867729837\n",
      "  time_since_restore: 1467.0624604225159\n",
      "  time_this_iter_s: 26.530136108398438\n",
      "  time_total_s: 1467.0624604225159\n",
      "  timestamp: 1553105335\n",
      "  timesteps_since_restore: 560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 56\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1467 s, 56 iter, 560000 ts, 310 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-09-22\n",
      "  done: false\n",
      "  episode_len_mean: 129.65\n",
      "  episode_reward_max: 338.23029306701847\n",
      "  episode_reward_mean: 312.31046953086184\n",
      "  episode_reward_min: 287.60860067093574\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 3563\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3971.364\n",
      "    load_time_ms: 2.665\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.6566129424663316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.089120864868164\n",
      "      kl: 0.00687177199870348\n",
      "      policy_loss: -0.001796537428162992\n",
      "      total_loss: 759.8494262695312\n",
      "      vf_explained_var: 0.6834820508956909\n",
      "      vf_loss: 759.851318359375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1961910724639893\n",
      "      kl: 0.01636284776031971\n",
      "      policy_loss: -0.0033135737758129835\n",
      "      total_loss: 188.59776306152344\n",
      "      vf_explained_var: 0.8492547273635864\n",
      "      vf_loss: 188.60108947753906\n",
      "    sample_time_ms: 22435.525\n",
      "    update_time_ms: 9.647\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.5515417466484\n",
      "    rl_1: 122.75892778421334\n",
      "  time_since_restore: 1493.7334883213043\n",
      "  time_this_iter_s: 26.671027898788452\n",
      "  time_total_s: 1493.7334883213043\n",
      "  timestamp: 1553105362\n",
      "  timesteps_since_restore: 570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 57\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1493 s, 57 iter, 570000 ts, 312 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-09-48\n",
      "  done: false\n",
      "  episode_len_mean: 129.47\n",
      "  episode_reward_max: 334.31322570853524\n",
      "  episode_reward_mean: 309.5181639093311\n",
      "  episode_reward_min: 284.25301878307926\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 3641\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3979.305\n",
      "    load_time_ms: 2.754\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.072447419166565\n",
      "      kl: 0.017988359555602074\n",
      "      policy_loss: -0.003120673354715109\n",
      "      total_loss: 768.2107543945312\n",
      "      vf_explained_var: 0.6820073127746582\n",
      "      vf_loss: 768.2138671875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4551915445207286e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.203601360321045\n",
      "      kl: 0.004088259767740965\n",
      "      policy_loss: -0.0019121720688417554\n",
      "      total_loss: 167.25515747070312\n",
      "      vf_explained_var: 0.8628095984458923\n",
      "      vf_loss: 167.25706481933594\n",
      "    sample_time_ms: 22456.169\n",
      "    update_time_ms: 9.358\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.08802819662537\n",
      "    rl_1: 119.43013571270566\n",
      "  time_since_restore: 1520.420419216156\n",
      "  time_this_iter_s: 26.686930894851685\n",
      "  time_total_s: 1520.420419216156\n",
      "  timestamp: 1553105388\n",
      "  timesteps_since_restore: 580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 58\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1520 s, 58 iter, 580000 ts, 310 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-10-15\n",
      "  done: false\n",
      "  episode_len_mean: 128.2\n",
      "  episode_reward_max: 337.9544459263436\n",
      "  episode_reward_mean: 313.06792461983775\n",
      "  episode_reward_min: 285.99905318146176\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 3719\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3982.089\n",
      "    load_time_ms: 2.726\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283064712331658e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0328048467636108\n",
      "      kl: 0.05417289584875107\n",
      "      policy_loss: -0.008807465434074402\n",
      "      total_loss: 776.3706665039062\n",
      "      vf_explained_var: 0.6860947012901306\n",
      "      vf_loss: 776.3793334960938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.275957722603643e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1567981243133545\n",
      "      kl: 0.007321709301322699\n",
      "      policy_loss: -0.0021317265927791595\n",
      "      total_loss: 180.86912536621094\n",
      "      vf_explained_var: 0.8597378730773926\n",
      "      vf_loss: 180.8712615966797\n",
      "    sample_time_ms: 22475.462\n",
      "    update_time_ms: 9.339\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.52094581639204\n",
      "    rl_1: 122.54697880344567\n",
      "  time_since_restore: 1547.1651871204376\n",
      "  time_this_iter_s: 26.744767904281616\n",
      "  time_total_s: 1547.1651871204376\n",
      "  timestamp: 1553105415\n",
      "  timesteps_since_restore: 590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 59\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1547 s, 59 iter, 590000 ts, 313 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 126.75\n",
      "  episode_reward_max: 337.3335233162005\n",
      "  episode_reward_mean: 313.9677604160704\n",
      "  episode_reward_min: 285.9449847212062\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 3798\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3991.144\n",
      "    load_time_ms: 2.712\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.492458752751837e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.021341323852539\n",
      "      kl: 0.004757656715810299\n",
      "      policy_loss: -0.0011287376983091235\n",
      "      total_loss: 767.1556396484375\n",
      "      vf_explained_var: 0.6901593804359436\n",
      "      vf_loss: 767.1567993164062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.6379788613018216e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1298660039901733\n",
      "      kl: 0.009138384833931923\n",
      "      policy_loss: -0.001189831760711968\n",
      "      total_loss: 176.34323120117188\n",
      "      vf_explained_var: 0.8640519380569458\n",
      "      vf_loss: 176.3444061279297\n",
      "    sample_time_ms: 22465.586\n",
      "    update_time_ms: 9.059\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.5708174425017\n",
      "    rl_1: 123.39694297356867\n",
      "  time_since_restore: 1573.3571786880493\n",
      "  time_this_iter_s: 26.191991567611694\n",
      "  time_total_s: 1573.3571786880493\n",
      "  timestamp: 1553105441\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 60\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1573 s, 60 iter, 600000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-11-07\n",
      "  done: false\n",
      "  episode_len_mean: 126.14\n",
      "  episode_reward_max: 337.85489406792055\n",
      "  episode_reward_mean: 310.5385942665952\n",
      "  episode_reward_min: 287.2983352855186\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 3877\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3984.055\n",
      "    load_time_ms: 2.608\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7462293763759185e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0526221990585327\n",
      "      kl: 0.012383315712213516\n",
      "      policy_loss: -0.002246835734695196\n",
      "      total_loss: 748.3422241210938\n",
      "      vf_explained_var: 0.6992902755737305\n",
      "      vf_loss: 748.344482421875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8189894306509108e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.140453577041626\n",
      "      kl: 0.010356373153626919\n",
      "      policy_loss: -0.0029373308643698692\n",
      "      total_loss: 161.46685791015625\n",
      "      vf_explained_var: 0.8728078007698059\n",
      "      vf_loss: 161.46978759765625\n",
      "    sample_time_ms: 22442.939\n",
      "    update_time_ms: 8.91\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.72758170212785\n",
      "    rl_1: 120.8110125644673\n",
      "  time_since_restore: 1599.440212726593\n",
      "  time_this_iter_s: 26.0830340385437\n",
      "  time_total_s: 1599.440212726593\n",
      "  timestamp: 1553105467\n",
      "  timesteps_since_restore: 610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 61\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1599 s, 61 iter, 610000 ts, 311 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-11-34\n",
      "  done: false\n",
      "  episode_len_mean: 126.8\n",
      "  episode_reward_max: 332.83672680556793\n",
      "  episode_reward_mean: 308.79054726942695\n",
      "  episode_reward_min: 283.98603559062576\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 3956\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3988.223\n",
      "    load_time_ms: 2.562\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7462293763759185e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0647695064544678\n",
      "      kl: 0.00505832489579916\n",
      "      policy_loss: -0.0016677164239808917\n",
      "      total_loss: 702.4099731445312\n",
      "      vf_explained_var: 0.7151023745536804\n",
      "      vf_loss: 702.4115600585938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8189894306509108e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1372883319854736\n",
      "      kl: 0.007032718509435654\n",
      "      policy_loss: -0.0017431668238714337\n",
      "      total_loss: 154.66712951660156\n",
      "      vf_explained_var: 0.8793203830718994\n",
      "      vf_loss: 154.66883850097656\n",
      "    sample_time_ms: 22450.974\n",
      "    update_time_ms: 8.91\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.19981005203744\n",
      "    rl_1: 120.59073721738955\n",
      "  time_since_restore: 1625.8642332553864\n",
      "  time_this_iter_s: 26.424020528793335\n",
      "  time_total_s: 1625.8642332553864\n",
      "  timestamp: 1553105494\n",
      "  timesteps_since_restore: 620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 62\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1625 s, 62 iter, 620000 ts, 309 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-12-00\n",
      "  done: false\n",
      "  episode_len_mean: 126.84\n",
      "  episode_reward_max: 345.36234768683283\n",
      "  episode_reward_mean: 314.5623703682358\n",
      "  episode_reward_min: 283.98603559062576\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 4034\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3983.192\n",
      "    load_time_ms: 2.536\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.731146881879592e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0210177898406982\n",
      "      kl: 0.009045705199241638\n",
      "      policy_loss: -0.0014223086182028055\n",
      "      total_loss: 729.4405517578125\n",
      "      vf_explained_var: 0.7143068313598633\n",
      "      vf_loss: 729.44189453125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.073684811592102\n",
      "      kl: 0.016877466812729836\n",
      "      policy_loss: -0.0035749864764511585\n",
      "      total_loss: 170.19688415527344\n",
      "      vf_explained_var: 0.8749474883079529\n",
      "      vf_loss: 170.200439453125\n",
      "    sample_time_ms: 22383.742\n",
      "    update_time_ms: 8.658\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.3464226152573\n",
      "    rl_1: 124.2159477529785\n",
      "  time_since_restore: 1651.7741265296936\n",
      "  time_this_iter_s: 25.90989327430725\n",
      "  time_total_s: 1651.7741265296936\n",
      "  timestamp: 1553105520\n",
      "  timesteps_since_restore: 630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 63\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1651 s, 63 iter, 630000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-12-26\n",
      "  done: false\n",
      "  episode_len_mean: 127.59\n",
      "  episode_reward_max: 340.91605490126886\n",
      "  episode_reward_mean: 313.2545255896675\n",
      "  episode_reward_min: 280.33084688919394\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 4112\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3982.534\n",
      "    load_time_ms: 2.531\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.365573440939796e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0422446727752686\n",
      "      kl: 0.009168896824121475\n",
      "      policy_loss: -0.001770306145772338\n",
      "      total_loss: 705.412109375\n",
      "      vf_explained_var: 0.7261091470718384\n",
      "      vf_loss: 705.4139404296875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0853087902069092\n",
      "      kl: 0.015062008053064346\n",
      "      policy_loss: -0.0032635980751365423\n",
      "      total_loss: 154.861083984375\n",
      "      vf_explained_var: 0.8843480348587036\n",
      "      vf_loss: 154.86436462402344\n",
      "    sample_time_ms: 22359.27\n",
      "    update_time_ms: 8.87\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.94207714067798\n",
      "    rl_1: 123.31244844898949\n",
      "  time_since_restore: 1677.5199902057648\n",
      "  time_this_iter_s: 25.745863676071167\n",
      "  time_total_s: 1677.5199902057648\n",
      "  timestamp: 1553105546\n",
      "  timesteps_since_restore: 640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 64\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1677 s, 64 iter, 640000 ts, 313 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-12-52\n",
      "  done: false\n",
      "  episode_len_mean: 127.62\n",
      "  episode_reward_max: 343.84229607092345\n",
      "  episode_reward_mean: 314.59039749270573\n",
      "  episode_reward_min: 288.09913082354535\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 4191\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3983.722\n",
      "    load_time_ms: 2.505\n",
      "    num_steps_sampled: 650000\n",
      "    num_steps_trained: 650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.182786720469898e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0306626558303833\n",
      "      kl: 0.09084219485521317\n",
      "      policy_loss: -0.013080928474664688\n",
      "      total_loss: 715.3115234375\n",
      "      vf_explained_var: 0.7293534278869629\n",
      "      vf_loss: 715.3246459960938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.094947153254554e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0518789291381836\n",
      "      kl: 0.008711595088243484\n",
      "      policy_loss: -0.001720975385978818\n",
      "      total_loss: 145.17837524414062\n",
      "      vf_explained_var: 0.8921083211898804\n",
      "      vf_loss: 145.18008422851562\n",
      "    sample_time_ms: 22268.527\n",
      "    update_time_ms: 8.797\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.42351231379078\n",
      "    rl_1: 123.16688517891494\n",
      "  time_since_restore: 1703.3443093299866\n",
      "  time_this_iter_s: 25.8243191242218\n",
      "  time_total_s: 1703.3443093299866\n",
      "  timestamp: 1553105572\n",
      "  timesteps_since_restore: 650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 650000\n",
      "  training_iteration: 65\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1703 s, 65 iter, 650000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-13-18\n",
      "  done: false\n",
      "  episode_len_mean: 127.34\n",
      "  episode_reward_max: 345.4273704590814\n",
      "  episode_reward_mean: 316.66948958792636\n",
      "  episode_reward_min: 288.61485601531695\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 4270\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3984.432\n",
      "    load_time_ms: 2.42\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.2741814901676713e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0302737951278687\n",
      "      kl: 0.003883055876940489\n",
      "      policy_loss: -0.0014071252662688494\n",
      "      total_loss: 680.300048828125\n",
      "      vf_explained_var: 0.7408379912376404\n",
      "      vf_loss: 680.3014526367188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.547473576627277e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0351848602294922\n",
      "      kl: 0.007653973996639252\n",
      "      policy_loss: -0.0006089574308134615\n",
      "      total_loss: 159.841064453125\n",
      "      vf_explained_var: 0.8850058317184448\n",
      "      vf_loss: 159.84165954589844\n",
      "    sample_time_ms: 22208.85\n",
      "    update_time_ms: 9.041\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.05722010209072\n",
      "    rl_1: 125.6122694858357\n",
      "  time_since_restore: 1729.2863726615906\n",
      "  time_this_iter_s: 25.942063331604004\n",
      "  time_total_s: 1729.2863726615906\n",
      "  timestamp: 1553105598\n",
      "  timesteps_since_restore: 660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 66\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1729 s, 66 iter, 660000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-13-43\n",
      "  done: false\n",
      "  episode_len_mean: 126.89\n",
      "  episode_reward_max: 341.7704714349017\n",
      "  episode_reward_mean: 317.3160573474483\n",
      "  episode_reward_min: 291.41020978072817\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 4348\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4002.75\n",
      "    load_time_ms: 2.375\n",
      "    num_steps_sampled: 670000\n",
      "    num_steps_trained: 670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6370907450838357e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.975062370300293\n",
      "      kl: 0.015390725806355476\n",
      "      policy_loss: -0.0021535702981054783\n",
      "      total_loss: 691.7859497070312\n",
      "      vf_explained_var: 0.7420581579208374\n",
      "      vf_loss: 691.7881469726562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0342252254486084\n",
      "      kl: 0.022306395694613457\n",
      "      policy_loss: -0.003588977502658963\n",
      "      total_loss: 163.59814453125\n",
      "      vf_explained_var: 0.8865379095077515\n",
      "      vf_loss: 163.60174560546875\n",
      "    sample_time_ms: 22035.803\n",
      "    update_time_ms: 9.317\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.56841561748826\n",
      "    rl_1: 126.74764172996004\n",
      "  time_since_restore: 1754.4125518798828\n",
      "  time_this_iter_s: 25.126179218292236\n",
      "  time_total_s: 1754.4125518798828\n",
      "  timestamp: 1553105623\n",
      "  timesteps_since_restore: 670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 670000\n",
      "  training_iteration: 67\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1754 s, 67 iter, 670000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-14-08\n",
      "  done: false\n",
      "  episode_len_mean: 126.77\n",
      "  episode_reward_max: 343.6142338355848\n",
      "  episode_reward_mean: 315.6514498683866\n",
      "  episode_reward_min: 285.29818173739443\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 4428\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3993.061\n",
      "    load_time_ms: 2.363\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6370907450838357e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9992533922195435\n",
      "      kl: 0.008294581435620785\n",
      "      policy_loss: -0.0022529091220349073\n",
      "      total_loss: 648.4940795898438\n",
      "      vf_explained_var: 0.7549135684967041\n",
      "      vf_loss: 648.4963989257812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2737367883136385e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0546224117279053\n",
      "      kl: 0.005689437035471201\n",
      "      policy_loss: -0.001212551025673747\n",
      "      total_loss: 149.5336456298828\n",
      "      vf_explained_var: 0.8966547846794128\n",
      "      vf_loss: 149.5348663330078\n",
      "    sample_time_ms: 21942.785\n",
      "    update_time_ms: 9.616\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.43831711685172\n",
      "    rl_1: 126.21313275153484\n",
      "  time_since_restore: 1780.0749881267548\n",
      "  time_this_iter_s: 25.66243624687195\n",
      "  time_total_s: 1780.0749881267548\n",
      "  timestamp: 1553105648\n",
      "  timesteps_since_restore: 680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 68\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1780 s, 68 iter, 680000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-14-34\n",
      "  done: false\n",
      "  episode_len_mean: 126.84\n",
      "  episode_reward_max: 342.3293255679125\n",
      "  episode_reward_mean: 314.0228690534566\n",
      "  episode_reward_min: 285.29818173739443\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 4506\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3980.97\n",
      "    load_time_ms: 2.243\n",
      "    num_steps_sampled: 690000\n",
      "    num_steps_trained: 690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.185453725419178e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9833203554153442\n",
      "      kl: 0.0295490063726902\n",
      "      policy_loss: -0.00362197682261467\n",
      "      total_loss: 652.36328125\n",
      "      vf_explained_var: 0.7584177851676941\n",
      "      vf_loss: 652.366943359375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1368683941568192e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0379869937896729\n",
      "      kl: 0.007280520163476467\n",
      "      policy_loss: -0.0008538411348126829\n",
      "      total_loss: 142.71490478515625\n",
      "      vf_explained_var: 0.9012038111686707\n",
      "      vf_loss: 142.71572875976562\n",
      "    sample_time_ms: 21836.304\n",
      "    update_time_ms: 9.93\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.7974905718078\n",
      "    rl_1: 125.22537848164882\n",
      "  time_since_restore: 1805.6326401233673\n",
      "  time_this_iter_s: 25.55765199661255\n",
      "  time_total_s: 1805.6326401233673\n",
      "  timestamp: 1553105674\n",
      "  timesteps_since_restore: 690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 690000\n",
      "  training_iteration: 69\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1805 s, 69 iter, 690000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-15-00\n",
      "  done: false\n",
      "  episode_len_mean: 126.16\n",
      "  episode_reward_max: 343.86777322187356\n",
      "  episode_reward_mean: 314.95610407563726\n",
      "  episode_reward_min: 285.9071696508532\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 4586\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3960.61\n",
      "    load_time_ms: 2.245\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.185453725419178e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9801864624023438\n",
      "      kl: 0.017307456582784653\n",
      "      policy_loss: -0.002263941802084446\n",
      "      total_loss: 646.6139526367188\n",
      "      vf_explained_var: 0.7611085176467896\n",
      "      vf_loss: 646.6163330078125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.684341970784096e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0151680707931519\n",
      "      kl: 0.00587597256526351\n",
      "      policy_loss: -0.0008917826344259083\n",
      "      total_loss: 144.21981811523438\n",
      "      vf_explained_var: 0.9030485153198242\n",
      "      vf_loss: 144.22068786621094\n",
      "    sample_time_ms: 21821.57\n",
      "    update_time_ms: 10.634\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.10608856985422\n",
      "    rl_1: 125.850015505783\n",
      "  time_since_restore: 1831.4809906482697\n",
      "  time_this_iter_s: 25.848350524902344\n",
      "  time_total_s: 1831.4809906482697\n",
      "  timestamp: 1553105700\n",
      "  timesteps_since_restore: 700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 70\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1831 s, 70 iter, 700000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-15-25\n",
      "  done: false\n",
      "  episode_len_mean: 126.77\n",
      "  episode_reward_max: 342.53789189870247\n",
      "  episode_reward_mean: 317.4038750323026\n",
      "  episode_reward_min: 287.3983958653365\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 4664\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3939.033\n",
      "    load_time_ms: 2.213\n",
      "    num_steps_sampled: 710000\n",
      "    num_steps_trained: 710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.185453725419178e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9791789650917053\n",
      "      kl: 0.013049132190644741\n",
      "      policy_loss: -0.0037224632687866688\n",
      "      total_loss: 644.0180053710938\n",
      "      vf_explained_var: 0.7683518528938293\n",
      "      vf_loss: 644.021728515625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.842170985392048e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.00039541721344\n",
      "      kl: 0.00989860575646162\n",
      "      policy_loss: -0.0025503081269562244\n",
      "      total_loss: 135.53099060058594\n",
      "      vf_explained_var: 0.9084103107452393\n",
      "      vf_loss: 135.5335235595703\n",
      "    sample_time_ms: 21785.282\n",
      "    update_time_ms: 10.454\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.43792426094345\n",
      "    rl_1: 126.96595077135916\n",
      "  time_since_restore: 1856.98170876503\n",
      "  time_this_iter_s: 25.500718116760254\n",
      "  time_total_s: 1856.98170876503\n",
      "  timestamp: 1553105725\n",
      "  timesteps_since_restore: 710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 710000\n",
      "  training_iteration: 71\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1856 s, 71 iter, 710000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-15-51\n",
      "  done: false\n",
      "  episode_len_mean: 126.88\n",
      "  episode_reward_max: 347.7439192799159\n",
      "  episode_reward_mean: 317.34795673156617\n",
      "  episode_reward_min: 288.4566132743246\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 4743\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3918.369\n",
      "    load_time_ms: 2.252\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.185453725419178e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9956271052360535\n",
      "      kl: 0.026109224185347557\n",
      "      policy_loss: -0.0031516102608293295\n",
      "      total_loss: 622.0066528320312\n",
      "      vf_explained_var: 0.7758999466896057\n",
      "      vf_loss: 622.0097045898438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.421085492696024e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9971233010292053\n",
      "      kl: 0.0081614525988698\n",
      "      policy_loss: -0.0026511901523917913\n",
      "      total_loss: 125.21824645996094\n",
      "      vf_explained_var: 0.915554940700531\n",
      "      vf_loss: 125.22091674804688\n",
      "    sample_time_ms: 21721.171\n",
      "    update_time_ms: 10.61\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.8124563240284\n",
      "    rl_1: 126.53550040753777\n",
      "  time_since_restore: 1882.5613675117493\n",
      "  time_this_iter_s: 25.57965874671936\n",
      "  time_total_s: 1882.5613675117493\n",
      "  timestamp: 1553105751\n",
      "  timesteps_since_restore: 720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 72\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1882 s, 72 iter, 720000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-16-17\n",
      "  done: false\n",
      "  episode_len_mean: 127.17\n",
      "  episode_reward_max: 348.8531284574535\n",
      "  episode_reward_mean: 314.587306419396\n",
      "  episode_reward_min: 285.52248798639727\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 4822\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3900.634\n",
      "    load_time_ms: 2.24\n",
      "    num_steps_sampled: 730000\n",
      "    num_steps_trained: 730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.185453725419178e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9886384606361389\n",
      "      kl: 0.004670398775488138\n",
      "      policy_loss: -0.001529466942884028\n",
      "      total_loss: 617.9429931640625\n",
      "      vf_explained_var: 0.7815017104148865\n",
      "      vf_loss: 617.9445190429688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9909037947654724\n",
      "      kl: 0.015574329532682896\n",
      "      policy_loss: -0.00446545984596014\n",
      "      total_loss: 117.19215393066406\n",
      "      vf_explained_var: 0.9191647171974182\n",
      "      vf_loss: 117.19662475585938\n",
      "    sample_time_ms: 21716.782\n",
      "    update_time_ms: 10.686\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.72542982309747\n",
      "    rl_1: 123.86187659629859\n",
      "  time_since_restore: 1908.248080253601\n",
      "  time_this_iter_s: 25.686712741851807\n",
      "  time_total_s: 1908.248080253601\n",
      "  timestamp: 1553105777\n",
      "  timesteps_since_restore: 730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 730000\n",
      "  training_iteration: 73\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1908 s, 73 iter, 730000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-16-42\n",
      "  done: false\n",
      "  episode_len_mean: 127.26\n",
      "  episode_reward_max: 348.8531284574535\n",
      "  episode_reward_mean: 316.9411861129086\n",
      "  episode_reward_min: 285.52248798639727\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 4901\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3896.176\n",
      "    load_time_ms: 2.267\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.092726862709589e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9744232296943665\n",
      "      kl: 0.010836850851774216\n",
      "      policy_loss: -0.0014137778198346496\n",
      "      total_loss: 608.8489379882812\n",
      "      vf_explained_var: 0.7849606275558472\n",
      "      vf_loss: 608.8502807617188\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.10542746348012e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9826492667198181\n",
      "      kl: 0.008685082197189331\n",
      "      policy_loss: -0.0006253322353586555\n",
      "      total_loss: 119.93360900878906\n",
      "      vf_explained_var: 0.9186978340148926\n",
      "      vf_loss: 119.93421936035156\n",
      "    sample_time_ms: 21709.795\n",
      "    update_time_ms: 10.832\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.11813910191628\n",
      "    rl_1: 125.8230470109923\n",
      "  time_since_restore: 1933.8803052902222\n",
      "  time_this_iter_s: 25.632225036621094\n",
      "  time_total_s: 1933.8803052902222\n",
      "  timestamp: 1553105802\n",
      "  timesteps_since_restore: 740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 74\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1933 s, 74 iter, 740000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-17-08\n",
      "  done: false\n",
      "  episode_len_mean: 127.31\n",
      "  episode_reward_max: 343.36666107580265\n",
      "  episode_reward_mean: 316.4495050645326\n",
      "  episode_reward_min: 288.4124468469957\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 4979\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3874.219\n",
      "    load_time_ms: 2.266\n",
      "    num_steps_sampled: 750000\n",
      "    num_steps_trained: 750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.092726862709589e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0325734615325928\n",
      "      kl: 0.053923968225717545\n",
      "      policy_loss: -0.005850648041814566\n",
      "      total_loss: 585.8342895507812\n",
      "      vf_explained_var: 0.7947327494621277\n",
      "      vf_loss: 585.8402099609375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.55271373174006e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9819538593292236\n",
      "      kl: 0.008476168848574162\n",
      "      policy_loss: -0.0011680262396112084\n",
      "      total_loss: 109.20378875732422\n",
      "      vf_explained_var: 0.9265984892845154\n",
      "      vf_loss: 109.20496368408203\n",
      "    sample_time_ms: 21683.28\n",
      "    update_time_ms: 10.876\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.0274936635355\n",
      "    rl_1: 126.42201140099716\n",
      "  time_since_restore: 1959.2210626602173\n",
      "  time_this_iter_s: 25.340757369995117\n",
      "  time_total_s: 1959.2210626602173\n",
      "  timestamp: 1553105828\n",
      "  timesteps_since_restore: 750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 750000\n",
      "  training_iteration: 75\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1959 s, 75 iter, 750000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-17-33\n",
      "  done: false\n",
      "  episode_len_mean: 127.99\n",
      "  episode_reward_max: 345.31319134367624\n",
      "  episode_reward_mean: 316.0283413505412\n",
      "  episode_reward_min: 280.8097508986147\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5057\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3845.572\n",
      "    load_time_ms: 2.344\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.139089345387483e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0667099952697754\n",
      "      kl: 0.011702777817845345\n",
      "      policy_loss: -0.0031538857147097588\n",
      "      total_loss: 577.2225341796875\n",
      "      vf_explained_var: 0.7997558116912842\n",
      "      vf_loss: 577.2257080078125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.77635686587003e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9828675985336304\n",
      "      kl: 0.01641167141497135\n",
      "      policy_loss: -0.0012318401131778955\n",
      "      total_loss: 110.87156677246094\n",
      "      vf_explained_var: 0.9271536469459534\n",
      "      vf_loss: 110.87278747558594\n",
      "    sample_time_ms: 21604.466\n",
      "    update_time_ms: 11.012\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.10845403029325\n",
      "    rl_1: 125.91988732024798\n",
      "  time_since_restore: 1984.091790676117\n",
      "  time_this_iter_s: 24.870728015899658\n",
      "  time_total_s: 1984.091790676117\n",
      "  timestamp: 1553105853\n",
      "  timesteps_since_restore: 760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 76\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 1984 s, 76 iter, 760000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-17-58\n",
      "  done: false\n",
      "  episode_len_mean: 128.72\n",
      "  episode_reward_max: 351.29259715171906\n",
      "  episode_reward_mean: 318.6290013249161\n",
      "  episode_reward_min: 290.5471662613043\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 5134\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3801.479\n",
      "    load_time_ms: 2.342\n",
      "    num_steps_sampled: 770000\n",
      "    num_steps_trained: 770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.139089345387483e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.060210943222046\n",
      "      kl: 0.016657879576086998\n",
      "      policy_loss: -0.0021842767018824816\n",
      "      total_loss: 569.8680419921875\n",
      "      vf_explained_var: 0.8059468865394592\n",
      "      vf_loss: 569.8702392578125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.77635686587003e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.001487374305725\n",
      "      kl: 0.00671715522184968\n",
      "      policy_loss: -7.42335714676301e-06\n",
      "      total_loss: 101.27444458007812\n",
      "      vf_explained_var: 0.9331285357475281\n",
      "      vf_loss: 101.27445983886719\n",
      "    sample_time_ms: 21692.157\n",
      "    update_time_ms: 10.569\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.97598863471936\n",
      "    rl_1: 127.65301269019666\n",
      "  time_since_restore: 2009.6492810249329\n",
      "  time_this_iter_s: 25.557490348815918\n",
      "  time_total_s: 2009.6492810249329\n",
      "  timestamp: 1553105878\n",
      "  timesteps_since_restore: 770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 770000\n",
      "  training_iteration: 77\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2009 s, 77 iter, 770000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-18-24\n",
      "  done: false\n",
      "  episode_len_mean: 129.13\n",
      "  episode_reward_max: 351.29259715171906\n",
      "  episode_reward_mean: 322.6772867987166\n",
      "  episode_reward_min: 288.9044591144105\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5212\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3781.886\n",
      "    load_time_ms: 2.266\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.139089345387483e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0540289878845215\n",
      "      kl: 0.021496489644050598\n",
      "      policy_loss: -0.003372659208253026\n",
      "      total_loss: 548.2366333007812\n",
      "      vf_explained_var: 0.8126627206802368\n",
      "      vf_loss: 548.239990234375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.88178432935015e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9695313572883606\n",
      "      kl: 0.01609116978943348\n",
      "      policy_loss: -0.0026369441766291857\n",
      "      total_loss: 124.40662384033203\n",
      "      vf_explained_var: 0.9255422949790955\n",
      "      vf_loss: 124.40926361083984\n",
      "    sample_time_ms: 21673.149\n",
      "    update_time_ms: 10.411\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.9610473102987\n",
      "    rl_1: 131.7162394884178\n",
      "  time_since_restore: 2034.9208981990814\n",
      "  time_this_iter_s: 25.27161717414856\n",
      "  time_total_s: 2034.9208981990814\n",
      "  timestamp: 1553105904\n",
      "  timesteps_since_restore: 780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 78\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2034 s, 78 iter, 780000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-18-49\n",
      "  done: false\n",
      "  episode_len_mean: 128.87\n",
      "  episode_reward_max: 352.6569380195947\n",
      "  episode_reward_mean: 318.9678544444421\n",
      "  episode_reward_min: 283.986900164007\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5290\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3768.908\n",
      "    load_time_ms: 2.293\n",
      "    num_steps_sampled: 790000\n",
      "    num_steps_trained: 790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.139089345387483e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0475183725357056\n",
      "      kl: 0.010601375252008438\n",
      "      policy_loss: -0.0018829936161637306\n",
      "      total_loss: 540.51708984375\n",
      "      vf_explained_var: 0.8171579837799072\n",
      "      vf_loss: 540.51904296875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.88178432935015e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9629493355751038\n",
      "      kl: 0.009878241457045078\n",
      "      policy_loss: -0.0011199542786926031\n",
      "      total_loss: 109.10395050048828\n",
      "      vf_explained_var: 0.9312699437141418\n",
      "      vf_loss: 109.10505676269531\n",
      "    sample_time_ms: 21611.534\n",
      "    update_time_ms: 9.866\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.3929671594911\n",
      "    rl_1: 128.574887284951\n",
      "  time_since_restore: 2059.728912830353\n",
      "  time_this_iter_s: 24.808014631271362\n",
      "  time_total_s: 2059.728912830353\n",
      "  timestamp: 1553105929\n",
      "  timesteps_since_restore: 790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 790000\n",
      "  training_iteration: 79\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2059 s, 79 iter, 790000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-19-16\n",
      "  done: false\n",
      "  episode_len_mean: 128.86\n",
      "  episode_reward_max: 355.5098202940819\n",
      "  episode_reward_mean: 322.98591480389416\n",
      "  episode_reward_min: 283.986900164007\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 5367\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3786.329\n",
      "    load_time_ms: 2.3\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.139089345387483e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0430421829223633\n",
      "      kl: 0.003180508967489004\n",
      "      policy_loss: -0.0010115040931850672\n",
      "      total_loss: 536.8990478515625\n",
      "      vf_explained_var: 0.8205320835113525\n",
      "      vf_loss: 536.8999633789062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9000592231750488\n",
      "      kl: 0.02656206674873829\n",
      "      policy_loss: -0.006443576887249947\n",
      "      total_loss: 119.93939208984375\n",
      "      vf_explained_var: 0.9273295402526855\n",
      "      vf_loss: 119.9458236694336\n",
      "    sample_time_ms: 21788.26\n",
      "    update_time_ms: 9.217\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.38331873301547\n",
      "    rl_1: 131.60259607087875\n",
      "  time_since_restore: 2087.5102400779724\n",
      "  time_this_iter_s: 27.78132724761963\n",
      "  time_total_s: 2087.5102400779724\n",
      "  timestamp: 1553105956\n",
      "  timesteps_since_restore: 800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 80\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2087 s, 80 iter, 800000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-19-44\n",
      "  done: false\n",
      "  episode_len_mean: 128.29\n",
      "  episode_reward_max: 352.1070412465578\n",
      "  episode_reward_mean: 322.40774252304874\n",
      "  episode_reward_min: 288.7283312781909\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5445\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3804.768\n",
      "    load_time_ms: 2.326\n",
      "    num_steps_sampled: 810000\n",
      "    num_steps_trained: 810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.0695446726937414e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.05172598361969\n",
      "      kl: 0.03422408923506737\n",
      "      policy_loss: -0.005026785656809807\n",
      "      total_loss: 522.802490234375\n",
      "      vf_explained_var: 0.826635479927063\n",
      "      vf_loss: 522.8074951171875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9046171307563782\n",
      "      kl: 0.028497537598013878\n",
      "      policy_loss: -0.0039021270349621773\n",
      "      total_loss: 111.8741455078125\n",
      "      vf_explained_var: 0.9343598484992981\n",
      "      vf_loss: 111.8780517578125\n",
      "    sample_time_ms: 22011.839\n",
      "    update_time_ms: 9.389\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.17335139399788\n",
      "    rl_1: 132.23439112905083\n",
      "  time_since_restore: 2115.4355030059814\n",
      "  time_this_iter_s: 27.925262928009033\n",
      "  time_total_s: 2115.4355030059814\n",
      "  timestamp: 1553105984\n",
      "  timesteps_since_restore: 810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 810000\n",
      "  training_iteration: 81\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2115 s, 81 iter, 810000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-20-12\n",
      "  done: false\n",
      "  episode_len_mean: 127.92\n",
      "  episode_reward_max: 352.1070412465578\n",
      "  episode_reward_mean: 319.76043852272835\n",
      "  episode_reward_min: 288.052122909549\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 5524\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3940.335\n",
      "    load_time_ms: 2.383\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.0695446726937414e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0294488668441772\n",
      "      kl: 0.015455401502549648\n",
      "      policy_loss: -0.00234648073092103\n",
      "      total_loss: 513.9577026367188\n",
      "      vf_explained_var: 0.8300305604934692\n",
      "      vf_loss: 513.9600219726562\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8881618976593018\n",
      "      kl: 0.01714703068137169\n",
      "      policy_loss: -0.0008389197173528373\n",
      "      total_loss: 90.82024383544922\n",
      "      vf_explained_var: 0.9436154365539551\n",
      "      vf_loss: 90.82107543945312\n",
      "    sample_time_ms: 22055.318\n",
      "    update_time_ms: 9.01\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.04309254202133\n",
      "    rl_1: 129.71734598070702\n",
      "  time_since_restore: 2142.8007102012634\n",
      "  time_this_iter_s: 27.365207195281982\n",
      "  time_total_s: 2142.8007102012634\n",
      "  timestamp: 1553106012\n",
      "  timesteps_since_restore: 820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 82\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2142 s, 82 iter, 820000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-20-42\n",
      "  done: false\n",
      "  episode_len_mean: 128.76\n",
      "  episode_reward_max: 349.495871741274\n",
      "  episode_reward_mean: 319.76224836050216\n",
      "  episode_reward_min: 289.7625419465407\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 5601\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3967.329\n",
      "    load_time_ms: 2.586\n",
      "    num_steps_sampled: 830000\n",
      "    num_steps_trained: 830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.0695446726937414e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0347938537597656\n",
      "      kl: 0.00939225684851408\n",
      "      policy_loss: -0.0019007259979844093\n",
      "      total_loss: 508.3999328613281\n",
      "      vf_explained_var: 0.8354848027229309\n",
      "      vf_loss: 508.4020080566406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.440892164675075e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9374731779098511\n",
      "      kl: 0.05690198019146919\n",
      "      policy_loss: -0.009032312780618668\n",
      "      total_loss: 90.76277923583984\n",
      "      vf_explained_var: 0.9421313405036926\n",
      "      vf_loss: 90.77181243896484\n",
      "    sample_time_ms: 22484.434\n",
      "    update_time_ms: 8.991\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.0301721613289\n",
      "    rl_1: 128.7320761991733\n",
      "  time_since_restore: 2173.057977437973\n",
      "  time_this_iter_s: 30.257267236709595\n",
      "  time_total_s: 2173.057977437973\n",
      "  timestamp: 1553106042\n",
      "  timesteps_since_restore: 830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 830000\n",
      "  training_iteration: 83\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2173 s, 83 iter, 830000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-21-11\n",
      "  done: false\n",
      "  episode_len_mean: 128.07\n",
      "  episode_reward_max: 346.38304680998834\n",
      "  episode_reward_mean: 318.86593406237387\n",
      "  episode_reward_min: 289.19965762924016\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5679\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3963.525\n",
      "    load_time_ms: 2.588\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5347723363468707e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0216857194900513\n",
      "      kl: 0.017239360138773918\n",
      "      policy_loss: -0.0040576281026005745\n",
      "      total_loss: 488.88427734375\n",
      "      vf_explained_var: 0.8420204520225525\n",
      "      vf_loss: 488.8883361816406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.661336427215265e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9018533229827881\n",
      "      kl: 0.01406094990670681\n",
      "      policy_loss: -0.0027942124288529158\n",
      "      total_loss: 86.47940826416016\n",
      "      vf_explained_var: 0.94820636510849\n",
      "      vf_loss: 86.4822006225586\n",
      "    sample_time_ms: 22768.573\n",
      "    update_time_ms: 8.6\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.70741786945567\n",
      "    rl_1: 128.15851619291817\n",
      "  time_since_restore: 2201.4901084899902\n",
      "  time_this_iter_s: 28.432131052017212\n",
      "  time_total_s: 2201.4901084899902\n",
      "  timestamp: 1553106071\n",
      "  timesteps_since_restore: 840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 84\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2201 s, 84 iter, 840000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-21-38\n",
      "  done: false\n",
      "  episode_len_mean: 128.68\n",
      "  episode_reward_max: 344.7411777257442\n",
      "  episode_reward_mean: 319.50852447140466\n",
      "  episode_reward_min: 287.9140577442376\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 5756\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3997.915\n",
      "    load_time_ms: 2.838\n",
      "    num_steps_sampled: 850000\n",
      "    num_steps_trained: 850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5347723363468707e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9942646622657776\n",
      "      kl: 0.029206542298197746\n",
      "      policy_loss: -0.0036265396047383547\n",
      "      total_loss: 507.7945861816406\n",
      "      vf_explained_var: 0.8384379148483276\n",
      "      vf_loss: 507.79827880859375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.661336427215265e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8905438184738159\n",
      "      kl: 0.012065188959240913\n",
      "      policy_loss: -0.0006970511167310178\n",
      "      total_loss: 73.64726257324219\n",
      "      vf_explained_var: 0.9521159529685974\n",
      "      vf_loss: 73.64794158935547\n",
      "    sample_time_ms: 22969.316\n",
      "    update_time_ms: 8.644\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.6976082392955\n",
      "    rl_1: 127.8109162321092\n",
      "  time_since_restore: 2229.18874335289\n",
      "  time_this_iter_s: 27.69863486289978\n",
      "  time_total_s: 2229.18874335289\n",
      "  timestamp: 1553106098\n",
      "  timesteps_since_restore: 850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 850000\n",
      "  training_iteration: 85\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2229 s, 85 iter, 850000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-22-08\n",
      "  done: false\n",
      "  episode_len_mean: 128.8\n",
      "  episode_reward_max: 349.64525652832856\n",
      "  episode_reward_mean: 319.6033364625491\n",
      "  episode_reward_min: 287.24926693663434\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5834\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4053.765\n",
      "    load_time_ms: 2.756\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5347723363468707e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.018237829208374\n",
      "      kl: 0.013224933296442032\n",
      "      policy_loss: -0.0017082279082387686\n",
      "      total_loss: 473.26007080078125\n",
      "      vf_explained_var: 0.8503596782684326\n",
      "      vf_loss: 473.2618103027344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.661336427215265e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9228233098983765\n",
      "      kl: 0.05293664708733559\n",
      "      policy_loss: -0.009444613941013813\n",
      "      total_loss: 83.470458984375\n",
      "      vf_explained_var: 0.9491897821426392\n",
      "      vf_loss: 83.47989654541016\n",
      "    sample_time_ms: 23402.599\n",
      "    update_time_ms: 9.606\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.23946533661126\n",
      "    rl_1: 129.36387112593786\n",
      "  time_since_restore: 2258.964775800705\n",
      "  time_this_iter_s: 29.77603244781494\n",
      "  time_total_s: 2258.964775800705\n",
      "  timestamp: 1553106128\n",
      "  timesteps_since_restore: 860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 86\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2258 s, 86 iter, 860000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-22-39\n",
      "  done: false\n",
      "  episode_len_mean: 128.46\n",
      "  episode_reward_max: 349.64525652832856\n",
      "  episode_reward_mean: 317.28312783716245\n",
      "  episode_reward_min: 286.9629294066395\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5912\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4213.311\n",
      "    load_time_ms: 2.88\n",
      "    num_steps_sampled: 870000\n",
      "    num_steps_trained: 870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5347723363468707e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0179749727249146\n",
      "      kl: 0.03298410028219223\n",
      "      policy_loss: -0.002830612240359187\n",
      "      total_loss: 449.771728515625\n",
      "      vf_explained_var: 0.8554484844207764\n",
      "      vf_loss: 449.77459716796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9157795310020447\n",
      "      kl: 0.01184938382357359\n",
      "      policy_loss: -0.001087658223696053\n",
      "      total_loss: 67.04766082763672\n",
      "      vf_explained_var: 0.9582539796829224\n",
      "      vf_loss: 67.04875183105469\n",
      "    sample_time_ms: 23746.034\n",
      "    update_time_ms: 9.659\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.2924696329571\n",
      "    rl_1: 126.99065820420526\n",
      "  time_since_restore: 2289.556109189987\n",
      "  time_this_iter_s: 30.591333389282227\n",
      "  time_total_s: 2289.556109189987\n",
      "  timestamp: 1553106159\n",
      "  timesteps_since_restore: 870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 870000\n",
      "  training_iteration: 87\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2289 s, 87 iter, 870000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-23-12\n",
      "  done: false\n",
      "  episode_len_mean: 128.55\n",
      "  episode_reward_max: 370.24056002339665\n",
      "  episode_reward_mean: 314.4927990618715\n",
      "  episode_reward_min: 286.9629294066395\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5990\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4266.089\n",
      "    load_time_ms: 2.916\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5347723363468707e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0155857801437378\n",
      "      kl: 0.019191162660717964\n",
      "      policy_loss: -0.0018163000931963325\n",
      "      total_loss: 442.8800964355469\n",
      "      vf_explained_var: 0.860827624797821\n",
      "      vf_loss: 442.8819580078125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9182182550430298\n",
      "      kl: 0.012662549503147602\n",
      "      policy_loss: -0.00023994356160983443\n",
      "      total_loss: 61.2863883972168\n",
      "      vf_explained_var: 0.9595649838447571\n",
      "      vf_loss: 61.28662872314453\n",
      "    sample_time_ms: 24493.977\n",
      "    update_time_ms: 9.745\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.6910901927092\n",
      "    rl_1: 124.80170886916238\n",
      "  time_since_restore: 2322.8374865055084\n",
      "  time_this_iter_s: 33.28137731552124\n",
      "  time_total_s: 2322.8374865055084\n",
      "  timestamp: 1553106192\n",
      "  timesteps_since_restore: 880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 88\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2322 s, 88 iter, 880000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-23-40\n",
      "  done: false\n",
      "  episode_len_mean: 127.81\n",
      "  episode_reward_max: 341.58156520549215\n",
      "  episode_reward_mean: 315.537871649337\n",
      "  episode_reward_min: 284.48180633340047\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6068\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4332.774\n",
      "    load_time_ms: 2.966\n",
      "    num_steps_sampled: 890000\n",
      "    num_steps_trained: 890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5347723363468707e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9636369347572327\n",
      "      kl: 0.0070234062150120735\n",
      "      policy_loss: -0.00032348910463042557\n",
      "      total_loss: 447.1899108886719\n",
      "      vf_explained_var: 0.8612040877342224\n",
      "      vf_loss: 447.1902160644531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8791963458061218\n",
      "      kl: 0.012173637747764587\n",
      "      policy_loss: 9.22540421015583e-05\n",
      "      total_loss: 62.71208190917969\n",
      "      vf_explained_var: 0.9604853987693787\n",
      "      vf_loss: 62.71200180053711\n",
      "    sample_time_ms: 24745.499\n",
      "    update_time_ms: 9.848\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.09063905743199\n",
      "    rl_1: 125.44723259190496\n",
      "  time_since_restore: 2350.832074403763\n",
      "  time_this_iter_s: 27.994587898254395\n",
      "  time_total_s: 2350.832074403763\n",
      "  timestamp: 1553106220\n",
      "  timesteps_since_restore: 890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 890000\n",
      "  training_iteration: 89\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2350 s, 89 iter, 890000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-24-09\n",
      "  done: false\n",
      "  episode_len_mean: 128.11\n",
      "  episode_reward_max: 343.94588628869906\n",
      "  episode_reward_mean: 315.5294128588941\n",
      "  episode_reward_min: 284.48180633340047\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6146\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4339.884\n",
      "    load_time_ms: 2.985\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.673861681734354e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9858578443527222\n",
      "      kl: 0.017128022387623787\n",
      "      policy_loss: -0.0008773556328378618\n",
      "      total_loss: 434.83001708984375\n",
      "      vf_explained_var: 0.8654971718788147\n",
      "      vf_loss: 434.8307800292969\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8937881588935852\n",
      "      kl: 0.027246160432696342\n",
      "      policy_loss: -0.003514399053528905\n",
      "      total_loss: 49.72602081298828\n",
      "      vf_explained_var: 0.9677280783653259\n",
      "      vf_loss: 49.72953796386719\n",
      "    sample_time_ms: 24841.225\n",
      "    update_time_ms: 9.936\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.7122011623351\n",
      "    rl_1: 125.81721169655891\n",
      "  time_since_restore: 2379.6451013088226\n",
      "  time_this_iter_s: 28.813026905059814\n",
      "  time_total_s: 2379.6451013088226\n",
      "  timestamp: 1553106249\n",
      "  timesteps_since_restore: 900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 90\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2379 s, 90 iter, 900000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-24-39\n",
      "  done: false\n",
      "  episode_len_mean: 128.35\n",
      "  episode_reward_max: 346.0634786960251\n",
      "  episode_reward_mean: 313.8016126440472\n",
      "  episode_reward_min: 284.977229570139\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6224\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4355.925\n",
      "    load_time_ms: 2.982\n",
      "    num_steps_sampled: 910000\n",
      "    num_steps_trained: 910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.673861681734354e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9999783635139465\n",
      "      kl: 0.019010264426469803\n",
      "      policy_loss: -0.002166135236620903\n",
      "      total_loss: 415.2425231933594\n",
      "      vf_explained_var: 0.8721026182174683\n",
      "      vf_loss: 415.2446594238281\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8956882953643799\n",
      "      kl: 0.031472738832235336\n",
      "      policy_loss: 0.007200353313237429\n",
      "      total_loss: 46.795021057128906\n",
      "      vf_explained_var: 0.9689142107963562\n",
      "      vf_loss: 46.787818908691406\n",
      "    sample_time_ms: 24978.933\n",
      "    update_time_ms: 9.97\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.00524719055232\n",
      "    rl_1: 123.79636545349481\n",
      "  time_since_restore: 2409.1103122234344\n",
      "  time_this_iter_s: 29.465210914611816\n",
      "  time_total_s: 2409.1103122234344\n",
      "  timestamp: 1553106279\n",
      "  timesteps_since_restore: 910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 910000\n",
      "  training_iteration: 91\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2409 s, 91 iter, 910000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-25-09\n",
      "  done: false\n",
      "  episode_len_mean: 127.91\n",
      "  episode_reward_max: 351.0923957191823\n",
      "  episode_reward_mean: 317.92280994121495\n",
      "  episode_reward_min: 287.1671849515938\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6302\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4321.847\n",
      "    load_time_ms: 2.939\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.673861681734354e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9347731471061707\n",
      "      kl: 0.0178613793104887\n",
      "      policy_loss: -0.001283689052797854\n",
      "      total_loss: 419.2835998535156\n",
      "      vf_explained_var: 0.8722100853919983\n",
      "      vf_loss: 419.284912109375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8062590956687927\n",
      "      kl: 0.017883343622088432\n",
      "      policy_loss: -0.001337837427854538\n",
      "      total_loss: 53.811580657958984\n",
      "      vf_explained_var: 0.96714848279953\n",
      "      vf_loss: 53.81291198730469\n",
      "    sample_time_ms: 25265.887\n",
      "    update_time_ms: 9.86\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.81631259116784\n",
      "    rl_1: 127.10649735004714\n",
      "  time_since_restore: 2439.018445968628\n",
      "  time_this_iter_s: 29.90813374519348\n",
      "  time_total_s: 2439.018445968628\n",
      "  timestamp: 1553106309\n",
      "  timesteps_since_restore: 920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 92\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2439 s, 92 iter, 920000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-25-40\n",
      "  done: false\n",
      "  episode_len_mean: 128.27\n",
      "  episode_reward_max: 343.60586964289706\n",
      "  episode_reward_mean: 314.8112871695185\n",
      "  episode_reward_min: 285.56466498335794\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6380\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4304.063\n",
      "    load_time_ms: 2.779\n",
      "    num_steps_sampled: 930000\n",
      "    num_steps_trained: 930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.673861681734354e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0176966190338135\n",
      "      kl: 0.13714364171028137\n",
      "      policy_loss: -0.002831201534718275\n",
      "      total_loss: 391.1334533691406\n",
      "      vf_explained_var: 0.8806322813034058\n",
      "      vf_loss: 391.13623046875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8604620099067688\n",
      "      kl: 0.011851083487272263\n",
      "      policy_loss: 0.0007128671859391034\n",
      "      total_loss: 46.2606315612793\n",
      "      vf_explained_var: 0.9709236025810242\n",
      "      vf_loss: 46.259918212890625\n",
      "    sample_time_ms: 25418.573\n",
      "    update_time_ms: 10.64\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.61555572608438\n",
      "    rl_1: 126.19573144343413\n",
      "  time_since_restore: 2470.627836227417\n",
      "  time_this_iter_s: 31.609390258789062\n",
      "  time_total_s: 2470.627836227417\n",
      "  timestamp: 1553106340\n",
      "  timesteps_since_restore: 930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 930000\n",
      "  training_iteration: 93\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2470 s, 93 iter, 930000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-26-17\n",
      "  done: false\n",
      "  episode_len_mean: 129.72\n",
      "  episode_reward_max: 348.8433309265173\n",
      "  episode_reward_mean: 316.5892551577451\n",
      "  episode_reward_min: 284.081868565168\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6458\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4506.919\n",
      "    load_time_ms: 2.739\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1510788795656562e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.04917573928833\n",
      "      kl: 0.08110266178846359\n",
      "      policy_loss: -0.012076159939169884\n",
      "      total_loss: 375.4498596191406\n",
      "      vf_explained_var: 0.8861919045448303\n",
      "      vf_loss: 375.4619445800781\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9160397052764893\n",
      "      kl: 0.020508084446191788\n",
      "      policy_loss: -0.002804446965456009\n",
      "      total_loss: 46.97235107421875\n",
      "      vf_explained_var: 0.969445526599884\n",
      "      vf_loss: 46.97514724731445\n",
      "    sample_time_ms: 26072.287\n",
      "    update_time_ms: 10.593\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.97755510195665\n",
      "    rl_1: 126.61170005578833\n",
      "  time_since_restore: 2507.6267132759094\n",
      "  time_this_iter_s: 36.99887704849243\n",
      "  time_total_s: 2507.6267132759094\n",
      "  timestamp: 1553106377\n",
      "  timesteps_since_restore: 940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 94\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2507 s, 94 iter, 940000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-26-55\n",
      "  done: false\n",
      "  episode_len_mean: 129.53\n",
      "  episode_reward_max: 348.8433309265173\n",
      "  episode_reward_mean: 318.4045750568198\n",
      "  episode_reward_min: 291.5630529516937\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 6534\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4758.687\n",
      "    load_time_ms: 2.515\n",
      "    num_steps_sampled: 950000\n",
      "    num_steps_trained: 950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7266192002627495e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9967941641807556\n",
      "      kl: 0.02916465699672699\n",
      "      policy_loss: -0.0026028503198176622\n",
      "      total_loss: 385.6237487792969\n",
      "      vf_explained_var: 0.8831965327262878\n",
      "      vf_loss: 385.6263122558594\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8658483624458313\n",
      "      kl: 0.017075857147574425\n",
      "      policy_loss: 0.0011822510277852416\n",
      "      total_loss: 45.262115478515625\n",
      "      vf_explained_var: 0.9700127840042114\n",
      "      vf_loss: 45.26093292236328\n",
      "    sample_time_ms: 26849.542\n",
      "    update_time_ms: 10.869\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.46121352182948\n",
      "    rl_1: 127.94336153499034\n",
      "  time_since_restore: 2545.6135301589966\n",
      "  time_this_iter_s: 37.98681688308716\n",
      "  time_total_s: 2545.6135301589966\n",
      "  timestamp: 1553106415\n",
      "  timesteps_since_restore: 950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 950000\n",
      "  training_iteration: 95\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2545 s, 95 iter, 950000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-27-31\n",
      "  done: false\n",
      "  episode_len_mean: 128.92\n",
      "  episode_reward_max: 357.2063705375899\n",
      "  episode_reward_mean: 315.3804339675793\n",
      "  episode_reward_min: 284.6555595434959\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6612\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4921.031\n",
      "    load_time_ms: 2.549\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7266192002627495e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9638305902481079\n",
      "      kl: 0.034499991685152054\n",
      "      policy_loss: -0.004470860119909048\n",
      "      total_loss: 379.2623596191406\n",
      "      vf_explained_var: 0.8880763053894043\n",
      "      vf_loss: 379.2668151855469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.838608980178833\n",
      "      kl: 0.023090112954378128\n",
      "      policy_loss: -0.0006121605983935297\n",
      "      total_loss: 34.186912536621094\n",
      "      vf_explained_var: 0.9767608642578125\n",
      "      vf_loss: 34.18752670288086\n",
      "    sample_time_ms: 27273.837\n",
      "    update_time_ms: 9.883\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.54518208588254\n",
      "    rl_1: 124.8352518816967\n",
      "  time_since_restore: 2581.239767551422\n",
      "  time_this_iter_s: 35.62623739242554\n",
      "  time_total_s: 2581.239767551422\n",
      "  timestamp: 1553106451\n",
      "  timesteps_since_restore: 960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 96\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2581 s, 96 iter, 960000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-28-03\n",
      "  done: false\n",
      "  episode_len_mean: 129.65\n",
      "  episode_reward_max: 353.2086463532554\n",
      "  episode_reward_mean: 316.7360260125088\n",
      "  episode_reward_min: 282.54000666721197\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 6689\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4907.73\n",
      "    load_time_ms: 2.572\n",
      "    num_steps_sampled: 970000\n",
      "    num_steps_trained: 970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7266192002627495e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9909312129020691\n",
      "      kl: 0.020170653238892555\n",
      "      policy_loss: -0.0009012846858240664\n",
      "      total_loss: 371.8222961425781\n",
      "      vf_explained_var: 0.8901839852333069\n",
      "      vf_loss: 371.8232116699219\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8351612687110901\n",
      "      kl: 0.012794898822903633\n",
      "      policy_loss: 0.001050722086802125\n",
      "      total_loss: 39.726722717285156\n",
      "      vf_explained_var: 0.9728664755821228\n",
      "      vf_loss: 39.72566604614258\n",
      "    sample_time_ms: 27472.964\n",
      "    update_time_ms: 10.176\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.71619048652437\n",
      "    rl_1: 126.01983552598436\n",
      "  time_since_restore: 2613.6945872306824\n",
      "  time_this_iter_s: 32.454819679260254\n",
      "  time_total_s: 2613.6945872306824\n",
      "  timestamp: 1553106483\n",
      "  timesteps_since_restore: 970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 970000\n",
      "  training_iteration: 97\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2613 s, 97 iter, 970000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-28-38\n",
      "  done: false\n",
      "  episode_len_mean: 129.27\n",
      "  episode_reward_max: 345.1444276494159\n",
      "  episode_reward_mean: 318.34854506772115\n",
      "  episode_reward_min: 286.30305792149437\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6767\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4875.371\n",
      "    load_time_ms: 2.586\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7266192002627495e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9835379123687744\n",
      "      kl: 0.012506920844316483\n",
      "      policy_loss: 0.0006966032087802887\n",
      "      total_loss: 345.9796142578125\n",
      "      vf_explained_var: 0.8986846804618835\n",
      "      vf_loss: 345.9788818359375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8368415832519531\n",
      "      kl: 0.02003541588783264\n",
      "      policy_loss: 0.00846211425960064\n",
      "      total_loss: 40.29447937011719\n",
      "      vf_explained_var: 0.974950909614563\n",
      "      vf_loss: 40.286014556884766\n",
      "    sample_time_ms: 27665.925\n",
      "    update_time_ms: 10.244\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.79147476000492\n",
      "    rl_1: 128.55707030771623\n",
      "  time_since_restore: 2648.5821318626404\n",
      "  time_this_iter_s: 34.88754463195801\n",
      "  time_total_s: 2648.5821318626404\n",
      "  timestamp: 1553106518\n",
      "  timesteps_since_restore: 980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 98\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2648 s, 98 iter, 980000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-29-05\n",
      "  done: false\n",
      "  episode_len_mean: 128.58\n",
      "  episode_reward_max: 349.11643958108533\n",
      "  episode_reward_mean: 316.6943619278688\n",
      "  episode_reward_min: 286.30305792149437\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6845\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4813.986\n",
      "    load_time_ms: 2.506\n",
      "    num_steps_sampled: 990000\n",
      "    num_steps_trained: 990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7266192002627495e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9458675980567932\n",
      "      kl: 0.006366545334458351\n",
      "      policy_loss: 0.0009444311144761741\n",
      "      total_loss: 352.745849609375\n",
      "      vf_explained_var: 0.898148238658905\n",
      "      vf_loss: 352.7449035644531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7650318741798401\n",
      "      kl: 0.02666671760380268\n",
      "      policy_loss: 0.005874828901141882\n",
      "      total_loss: 33.85090255737305\n",
      "      vf_explained_var: 0.9775136113166809\n",
      "      vf_loss: 33.845027923583984\n",
      "    sample_time_ms: 27574.533\n",
      "    update_time_ms: 10.509\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.68678937898585\n",
      "    rl_1: 127.00757254888295\n",
      "  time_since_restore: 2675.0477306842804\n",
      "  time_this_iter_s: 26.465598821640015\n",
      "  time_total_s: 2675.0477306842804\n",
      "  timestamp: 1553106545\n",
      "  timesteps_since_restore: 990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 990000\n",
      "  training_iteration: 99\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2675 s, 99 iter, 990000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 128.71\n",
      "  episode_reward_max: 352.9137416256068\n",
      "  episode_reward_mean: 316.5973185232589\n",
      "  episode_reward_min: 285.5909001425004\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 6922\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4782.247\n",
      "    load_time_ms: 2.468\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.633096001313748e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9263541102409363\n",
      "      kl: 0.05226404219865799\n",
      "      policy_loss: -0.0072237527929246426\n",
      "      total_loss: 343.7176513671875\n",
      "      vf_explained_var: 0.901240348815918\n",
      "      vf_loss: 343.72491455078125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7484597563743591\n",
      "      kl: 0.023499073460698128\n",
      "      policy_loss: 0.005523023195564747\n",
      "      total_loss: 32.01626205444336\n",
      "      vf_explained_var: 0.978472888469696\n",
      "      vf_loss: 32.010738372802734\n",
      "    sample_time_ms: 27304.614\n",
      "    update_time_ms: 10.44\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.65294787526554\n",
      "    rl_1: 124.94437064799331\n",
      "  time_since_restore: 2700.8413047790527\n",
      "  time_this_iter_s: 25.79357409477234\n",
      "  time_total_s: 2700.8413047790527\n",
      "  timestamp: 1553106571\n",
      "  timesteps_since_restore: 1000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 100\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2700 s, 100 iter, 1000000 ts, 317 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-29-56\n",
      "  done: false\n",
      "  episode_len_mean: 128.55\n",
      "  episode_reward_max: 348.56951795798517\n",
      "  episode_reward_mean: 319.6725903580842\n",
      "  episode_reward_min: 278.7610476290652\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 6999\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4750.76\n",
      "    load_time_ms: 2.528\n",
      "    num_steps_sampled: 1010000\n",
      "    num_steps_trained: 1010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2949640275025653e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.904270350933075\n",
      "      kl: 0.03387286514043808\n",
      "      policy_loss: -0.003252130700275302\n",
      "      total_loss: 333.09393310546875\n",
      "      vf_explained_var: 0.9048740863800049\n",
      "      vf_loss: 333.09716796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7404077649116516\n",
      "      kl: 0.019070710986852646\n",
      "      policy_loss: 0.008664880879223347\n",
      "      total_loss: 39.61697006225586\n",
      "      vf_explained_var: 0.9746485948562622\n",
      "      vf_loss: 39.60830307006836\n",
      "    sample_time_ms: 26936.231\n",
      "    update_time_ms: 10.584\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.64406625520533\n",
      "    rl_1: 129.0285241028788\n",
      "  time_since_restore: 2726.3117678165436\n",
      "  time_this_iter_s: 25.470463037490845\n",
      "  time_total_s: 2726.3117678165436\n",
      "  timestamp: 1553106596\n",
      "  timesteps_since_restore: 1010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1010000\n",
      "  training_iteration: 101\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2726 s, 101 iter, 1010000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-30-26\n",
      "  done: false\n",
      "  episode_len_mean: 128.29\n",
      "  episode_reward_max: 350.55598449937236\n",
      "  episode_reward_mean: 317.9495189242369\n",
      "  episode_reward_min: 278.7610476290652\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7077\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4751.011\n",
      "    load_time_ms: 2.473\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2949640275025653e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8963989019393921\n",
      "      kl: 0.03103991597890854\n",
      "      policy_loss: 0.003945615608245134\n",
      "      total_loss: 324.2585144042969\n",
      "      vf_explained_var: 0.9071746468544006\n",
      "      vf_loss: 324.2545471191406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7143105864524841\n",
      "      kl: 0.03943327069282532\n",
      "      policy_loss: 0.012540976516902447\n",
      "      total_loss: 34.07169723510742\n",
      "      vf_explained_var: 0.9777829051017761\n",
      "      vf_loss: 34.05915832519531\n",
      "    sample_time_ms: 26916.354\n",
      "    update_time_ms: 10.824\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.430007422856\n",
      "    rl_1: 127.51951150138095\n",
      "  time_since_restore: 2756.0102038383484\n",
      "  time_this_iter_s: 29.69843602180481\n",
      "  time_total_s: 2756.0102038383484\n",
      "  timestamp: 1553106626\n",
      "  timesteps_since_restore: 1020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 102\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2756 s, 102 iter, 1020000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-30-54\n",
      "  done: false\n",
      "  episode_len_mean: 128.83\n",
      "  episode_reward_max: 354.2047723001722\n",
      "  episode_reward_mean: 318.85532190073764\n",
      "  episode_reward_min: 285.991911024254\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7155\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4800.771\n",
      "    load_time_ms: 2.421\n",
      "    num_steps_sampled: 1030000\n",
      "    num_steps_trained: 1030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2949640275025653e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9295740127563477\n",
      "      kl: 0.6298865079879761\n",
      "      policy_loss: -0.013138992711901665\n",
      "      total_loss: 324.2571716308594\n",
      "      vf_explained_var: 0.9103268384933472\n",
      "      vf_loss: 324.2702941894531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7082435488700867\n",
      "      kl: 0.03933257609605789\n",
      "      policy_loss: 0.013671774417161942\n",
      "      total_loss: 29.101869583129883\n",
      "      vf_explained_var: 0.9802024960517883\n",
      "      vf_loss: 29.088193893432617\n",
      "    sample_time_ms: 26524.829\n",
      "    update_time_ms: 10.205\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.67577595024133\n",
      "    rl_1: 128.17954595049625\n",
      "  time_since_restore: 2784.1954057216644\n",
      "  time_this_iter_s: 28.18520188331604\n",
      "  time_total_s: 2784.1954057216644\n",
      "  timestamp: 1553106654\n",
      "  timesteps_since_restore: 1030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1030000\n",
      "  training_iteration: 103\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2784 s, 103 iter, 1030000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-31-24\n",
      "  done: false\n",
      "  episode_len_mean: 130.07\n",
      "  episode_reward_max: 358.5262406879308\n",
      "  episode_reward_mean: 316.40498968799926\n",
      "  episode_reward_min: 285.991911024254\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 7232\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4641.224\n",
      "    load_time_ms: 2.411\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.942445770203305e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0007973909378052\n",
      "      kl: 0.014300164766609669\n",
      "      policy_loss: 0.00258735753595829\n",
      "      total_loss: 306.8474426269531\n",
      "      vf_explained_var: 0.9136599898338318\n",
      "      vf_loss: 306.8449401855469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8204774856567383\n",
      "      kl: 0.019513729959726334\n",
      "      policy_loss: 0.009845227003097534\n",
      "      total_loss: 24.12584114074707\n",
      "      vf_explained_var: 0.9837340116500854\n",
      "      vf_loss: 24.115991592407227\n",
      "    sample_time_ms: 25922.524\n",
      "    update_time_ms: 10.566\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.44261891261328\n",
      "    rl_1: 125.962370775386\n",
      "  time_since_restore: 2813.577743768692\n",
      "  time_this_iter_s: 29.382338047027588\n",
      "  time_total_s: 2813.577743768692\n",
      "  timestamp: 1553106684\n",
      "  timesteps_since_restore: 1040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 104\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2813 s, 104 iter, 1040000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-31-53\n",
      "  done: false\n",
      "  episode_len_mean: 130.24\n",
      "  episode_reward_max: 357.5697073244343\n",
      "  episode_reward_mean: 318.19832664439355\n",
      "  episode_reward_min: 285.70879047232376\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 7309\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4407.686\n",
      "    load_time_ms: 2.387\n",
      "    num_steps_sampled: 1050000\n",
      "    num_steps_trained: 1050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.942445770203305e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9897105097770691\n",
      "      kl: 0.013970807194709778\n",
      "      policy_loss: 0.001819548080675304\n",
      "      total_loss: 298.16021728515625\n",
      "      vf_explained_var: 0.9166557192802429\n",
      "      vf_loss: 298.158447265625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7754717469215393\n",
      "      kl: 0.02531832829117775\n",
      "      policy_loss: 0.01314717996865511\n",
      "      total_loss: 27.558609008789062\n",
      "      vf_explained_var: 0.982545793056488\n",
      "      vf_loss: 27.545467376708984\n",
      "    sample_time_ms: 25319.658\n",
      "    update_time_ms: 10.208\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.3945384986438\n",
      "    rl_1: 127.80378814574969\n",
      "  time_since_restore: 2843.201669216156\n",
      "  time_this_iter_s: 29.62392544746399\n",
      "  time_total_s: 2843.201669216156\n",
      "  timestamp: 1553106713\n",
      "  timesteps_since_restore: 1050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1050000\n",
      "  training_iteration: 105\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2843 s, 105 iter, 1050000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-32-23\n",
      "  done: false\n",
      "  episode_len_mean: 129.74\n",
      "  episode_reward_max: 357.5697073244343\n",
      "  episode_reward_mean: 320.3204604496648\n",
      "  episode_reward_min: 292.1176393918764\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 7386\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4261.618\n",
      "    load_time_ms: 2.442\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.942445770203305e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9486924409866333\n",
      "      kl: 0.03733593970537186\n",
      "      policy_loss: -0.0018712249584496021\n",
      "      total_loss: 290.68280029296875\n",
      "      vf_explained_var: 0.9201522469520569\n",
      "      vf_loss: 290.6846618652344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7156437039375305\n",
      "      kl: 0.030083918944001198\n",
      "      policy_loss: 0.011275474913418293\n",
      "      total_loss: 25.76048469543457\n",
      "      vf_explained_var: 0.984085202217102\n",
      "      vf_loss: 25.74920654296875\n",
      "    sample_time_ms: 24878.987\n",
      "    update_time_ms: 11.192\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.76172544447465\n",
      "    rl_1: 129.55873500519013\n",
      "  time_since_restore: 2872.973808526993\n",
      "  time_this_iter_s: 29.772139310836792\n",
      "  time_total_s: 2872.973808526993\n",
      "  timestamp: 1553106743\n",
      "  timesteps_since_restore: 1060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 106\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2872 s, 106 iter, 1060000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-32-52\n",
      "  done: false\n",
      "  episode_len_mean: 129.79\n",
      "  episode_reward_max: 351.4324288996352\n",
      "  episode_reward_mean: 320.06959974623715\n",
      "  episode_reward_min: 287.7703705523276\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 7463\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4166.901\n",
      "    load_time_ms: 2.28\n",
      "    num_steps_sampled: 1070000\n",
      "    num_steps_trained: 1070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.942445770203305e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9373811483383179\n",
      "      kl: 0.02628452703356743\n",
      "      policy_loss: 0.0005368575803004205\n",
      "      total_loss: 285.8760681152344\n",
      "      vf_explained_var: 0.9216924905776978\n",
      "      vf_loss: 285.8756408691406\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.992008942162083e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7191130518913269\n",
      "      kl: 0.04191337898373604\n",
      "      policy_loss: 0.01885524019598961\n",
      "      total_loss: 21.899734497070312\n",
      "      vf_explained_var: 0.9857267737388611\n",
      "      vf_loss: 21.88088035583496\n",
      "    sample_time_ms: 24572.455\n",
      "    update_time_ms: 11.056\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.015187147872\n",
      "    rl_1: 129.05441259836516\n",
      "  time_since_restore: 2901.4113688468933\n",
      "  time_this_iter_s: 28.437560319900513\n",
      "  time_total_s: 2901.4113688468933\n",
      "  timestamp: 1553106772\n",
      "  timesteps_since_restore: 1070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1070000\n",
      "  training_iteration: 107\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2901 s, 107 iter, 1070000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-33-20\n",
      "  done: false\n",
      "  episode_len_mean: 129.99\n",
      "  episode_reward_max: 356.47844785894074\n",
      "  episode_reward_mean: 322.1114323599431\n",
      "  episode_reward_min: 290.57775797473255\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 7540\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4192.532\n",
      "    load_time_ms: 2.301\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.942445770203305e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9793728590011597\n",
      "      kl: 0.019765475764870644\n",
      "      policy_loss: 0.004429791122674942\n",
      "      total_loss: 268.3567810058594\n",
      "      vf_explained_var: 0.9254632592201233\n",
      "      vf_loss: 268.3523864746094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.498801109713741e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7600051760673523\n",
      "      kl: 0.02813362516462803\n",
      "      policy_loss: 0.013624033890664577\n",
      "      total_loss: 24.021825790405273\n",
      "      vf_explained_var: 0.9851399064064026\n",
      "      vf_loss: 24.008207321166992\n",
      "    sample_time_ms: 23922.171\n",
      "    update_time_ms: 10.798\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.1273559663514\n",
      "    rl_1: 130.98407639359172\n",
      "  time_since_restore: 2930.0510878562927\n",
      "  time_this_iter_s: 28.639719009399414\n",
      "  time_total_s: 2930.0510878562927\n",
      "  timestamp: 1553106800\n",
      "  timesteps_since_restore: 1080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 108\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2930 s, 108 iter, 1080000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-33-49\n",
      "  done: false\n",
      "  episode_len_mean: 129.82\n",
      "  episode_reward_max: 348.51115444683285\n",
      "  episode_reward_mean: 317.7647976410716\n",
      "  episode_reward_min: 287.05610646393296\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 7616\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4230.634\n",
      "    load_time_ms: 2.31\n",
      "    num_steps_sampled: 1090000\n",
      "    num_steps_trained: 1090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.942445770203305e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.002131462097168\n",
      "      kl: 0.03289151191711426\n",
      "      policy_loss: 0.001698081847280264\n",
      "      total_loss: 253.09605407714844\n",
      "      vf_explained_var: 0.9300732016563416\n",
      "      vf_loss: 253.09429931640625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.498801109713741e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7659018635749817\n",
      "      kl: 0.030042262747883797\n",
      "      policy_loss: 0.010148818604648113\n",
      "      total_loss: 19.886825561523438\n",
      "      vf_explained_var: 0.9870905876159668\n",
      "      vf_loss: 19.876678466796875\n",
      "    sample_time_ms: 24095.337\n",
      "    update_time_ms: 10.996\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.88724700937996\n",
      "    rl_1: 128.8775506316916\n",
      "  time_since_restore: 2958.6321353912354\n",
      "  time_this_iter_s: 28.581047534942627\n",
      "  time_total_s: 2958.6321353912354\n",
      "  timestamp: 1553106829\n",
      "  timesteps_since_restore: 1090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1090000\n",
      "  training_iteration: 109\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2958 s, 109 iter, 1090000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-34-17\n",
      "  done: false\n",
      "  episode_len_mean: 129.88\n",
      "  episode_reward_max: 349.44062446455996\n",
      "  episode_reward_mean: 316.3569859460219\n",
      "  episode_reward_min: 287.05610646393296\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 7695\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4282.461\n",
      "    load_time_ms: 2.302\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.942445770203305e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.969071626663208\n",
      "      kl: 0.03832007199525833\n",
      "      policy_loss: 0.0044335476122796535\n",
      "      total_loss: 254.0138702392578\n",
      "      vf_explained_var: 0.930690348148346\n",
      "      vf_loss: 254.00941467285156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.498801109713741e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7214232087135315\n",
      "      kl: 0.04659142717719078\n",
      "      policy_loss: 0.02607158198952675\n",
      "      total_loss: 18.984338760375977\n",
      "      vf_explained_var: 0.9874410629272461\n",
      "      vf_loss: 18.95827293395996\n",
      "    sample_time_ms: 24299.989\n",
      "    update_time_ms: 11.297\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.55932721437756\n",
      "    rl_1: 126.79765873164432\n",
      "  time_since_restore: 2986.993476629257\n",
      "  time_this_iter_s: 28.36134123802185\n",
      "  time_total_s: 2986.993476629257\n",
      "  timestamp: 1553106857\n",
      "  timesteps_since_restore: 1100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 110\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 2986 s, 110 iter, 1100000 ts, 316 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-34-44\n",
      "  done: false\n",
      "  episode_len_mean: 129.54\n",
      "  episode_reward_max: 349.9416623947845\n",
      "  episode_reward_mean: 320.48252544318547\n",
      "  episode_reward_min: 289.4446785107948\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 7772\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4287.048\n",
      "    load_time_ms: 2.24\n",
      "    num_steps_sampled: 1110000\n",
      "    num_steps_trained: 1110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.942445770203305e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.897958517074585\n",
      "      kl: 0.04399406164884567\n",
      "      policy_loss: 0.004855417646467686\n",
      "      total_loss: 259.1968688964844\n",
      "      vf_explained_var: 0.9302089214324951\n",
      "      vf_loss: 259.1919860839844\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.2482009366516724e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6826672554016113\n",
      "      kl: 0.05947031080722809\n",
      "      policy_loss: 0.03451971337199211\n",
      "      total_loss: 17.571401596069336\n",
      "      vf_explained_var: 0.9891372919082642\n",
      "      vf_loss: 17.536884307861328\n",
      "    sample_time_ms: 24431.476\n",
      "    update_time_ms: 10.944\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.76526763765588\n",
      "    rl_1: 129.71725780552956\n",
      "  time_since_restore: 3013.8165378570557\n",
      "  time_this_iter_s: 26.823061227798462\n",
      "  time_total_s: 3013.8165378570557\n",
      "  timestamp: 1553106884\n",
      "  timesteps_since_restore: 1110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1110000\n",
      "  training_iteration: 111\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3013 s, 111 iter, 1110000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-35-10\n",
      "  done: false\n",
      "  episode_len_mean: 129.73\n",
      "  episode_reward_max: 352.60031459038385\n",
      "  episode_reward_mean: 318.5498425355651\n",
      "  episode_reward_min: 286.5682957164677\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 7848\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4194.104\n",
      "    load_time_ms: 2.271\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.913670010557673e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9048991203308105\n",
      "      kl: 0.04246900975704193\n",
      "      policy_loss: 0.0004343749606050551\n",
      "      total_loss: 248.96726989746094\n",
      "      vf_explained_var: 0.9330494403839111\n",
      "      vf_loss: 248.9668426513672\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.3723031255131826e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.64519864320755\n",
      "      kl: 0.030615292489528656\n",
      "      policy_loss: 0.010895638726651669\n",
      "      total_loss: 13.107589721679688\n",
      "      vf_explained_var: 0.9910827279090881\n",
      "      vf_loss: 13.096694946289062\n",
      "    sample_time_ms: 24161.413\n",
      "    update_time_ms: 10.704\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.8102838931272\n",
      "    rl_1: 127.73955864243788\n",
      "  time_since_restore: 3039.8879115581512\n",
      "  time_this_iter_s: 26.07137370109558\n",
      "  time_total_s: 3039.8879115581512\n",
      "  timestamp: 1553106910\n",
      "  timesteps_since_restore: 1120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 112\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3039 s, 112 iter, 1120000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-35-37\n",
      "  done: false\n",
      "  episode_len_mean: 130.02\n",
      "  episode_reward_max: 349.4074850142487\n",
      "  episode_reward_mean: 320.02593192815476\n",
      "  episode_reward_min: 287.7547166814017\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7926\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4138.808\n",
      "    load_time_ms: 2.289\n",
      "    num_steps_sampled: 1130000\n",
      "    num_steps_trained: 1130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.370503254007979e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8988310694694519\n",
      "      kl: 0.17143015563488007\n",
      "      policy_loss: 0.0022525617387145758\n",
      "      total_loss: 246.6378173828125\n",
      "      vf_explained_var: 0.9348289370536804\n",
      "      vf_loss: 246.63558959960938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.3723031255131826e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6666236519813538\n",
      "      kl: 0.04443315416574478\n",
      "      policy_loss: 0.01819738931953907\n",
      "      total_loss: 13.391318321228027\n",
      "      vf_explained_var: 0.9912632703781128\n",
      "      vf_loss: 13.373120307922363\n",
      "    sample_time_ms: 24044.128\n",
      "    update_time_ms: 10.599\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.47677623612236\n",
      "    rl_1: 128.54915569203246\n",
      "  time_since_restore: 3066.342886686325\n",
      "  time_this_iter_s: 26.454975128173828\n",
      "  time_total_s: 3066.342886686325\n",
      "  timestamp: 1553106937\n",
      "  timesteps_since_restore: 1130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1130000\n",
      "  training_iteration: 113\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3066 s, 113 iter, 1130000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-36-03\n",
      "  done: false\n",
      "  episode_len_mean: 129.55\n",
      "  episode_reward_max: 349.4074850142487\n",
      "  episode_reward_mean: 318.9352491982363\n",
      "  episode_reward_min: 284.446602348461\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 8003\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4092.578\n",
      "    load_time_ms: 2.325\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.55575582968887e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.909760594367981\n",
      "      kl: 0.02875913493335247\n",
      "      policy_loss: 0.005178063176572323\n",
      "      total_loss: 233.26071166992188\n",
      "      vf_explained_var: 0.9382772445678711\n",
      "      vf_loss: 233.2555389404297\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.058453232431896e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6515370011329651\n",
      "      kl: 0.03398444131016731\n",
      "      policy_loss: 0.02543380856513977\n",
      "      total_loss: 11.831912994384766\n",
      "      vf_explained_var: 0.9918984770774841\n",
      "      vf_loss: 11.806480407714844\n",
      "    sample_time_ms: 23759.756\n",
      "    update_time_ms: 10.382\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.16172517343736\n",
      "    rl_1: 127.77352402479893\n",
      "  time_since_restore: 3092.4178540706635\n",
      "  time_this_iter_s: 26.07496738433838\n",
      "  time_total_s: 3092.4178540706635\n",
      "  timestamp: 1553106963\n",
      "  timesteps_since_restore: 1140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 114\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3092 s, 114 iter, 1140000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-36-29\n",
      "  done: false\n",
      "  episode_len_mean: 129.82\n",
      "  episode_reward_max: 357.20607358667985\n",
      "  episode_reward_mean: 318.2465743290783\n",
      "  episode_reward_min: 287.712851657592\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 8080\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4049.946\n",
      "    load_time_ms: 2.363\n",
      "    num_steps_sampled: 1150000\n",
      "    num_steps_trained: 1150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.55575582968887e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9858455061912537\n",
      "      kl: 0.6027173399925232\n",
      "      policy_loss: 0.008352587930858135\n",
      "      total_loss: 218.1160430908203\n",
      "      vf_explained_var: 0.9418979287147522\n",
      "      vf_loss: 218.107666015625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.058453232431896e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6940911412239075\n",
      "      kl: 0.04576409235596657\n",
      "      policy_loss: 0.028507014736533165\n",
      "      total_loss: 13.400861740112305\n",
      "      vf_explained_var: 0.9911850094795227\n",
      "      vf_loss: 13.372353553771973\n",
      "    sample_time_ms: 23454.58\n",
      "    update_time_ms: 11.026\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.74118796413856\n",
      "    rl_1: 128.5053863649398\n",
      "  time_since_restore: 3118.564701318741\n",
      "  time_this_iter_s: 26.146847248077393\n",
      "  time_total_s: 3118.564701318741\n",
      "  timestamp: 1553106989\n",
      "  timesteps_since_restore: 1150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1150000\n",
      "  training_iteration: 115\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3118 s, 115 iter, 1150000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-36-55\n",
      "  done: false\n",
      "  episode_len_mean: 130.78\n",
      "  episode_reward_max: 365.15025749617797\n",
      "  episode_reward_mean: 320.99690248317586\n",
      "  episode_reward_min: 286.22420458647366\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 8156\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3989.939\n",
      "    load_time_ms: 2.321\n",
      "    num_steps_sampled: 1160000\n",
      "    num_steps_trained: 1160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.833636726089279e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.011609673500061\n",
      "      kl: 0.049424946308135986\n",
      "      policy_loss: 0.004913841374218464\n",
      "      total_loss: 209.94325256347656\n",
      "      vf_explained_var: 0.943967878818512\n",
      "      vf_loss: 209.9383544921875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.58767878985666e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7844946384429932\n",
      "      kl: 0.039283182471990585\n",
      "      policy_loss: 0.015344631858170033\n",
      "      total_loss: 20.72310447692871\n",
      "      vf_explained_var: 0.9876560568809509\n",
      "      vf_loss: 20.707759857177734\n",
      "    sample_time_ms: 23127.769\n",
      "    update_time_ms: 9.833\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.06762441737246\n",
      "    rl_1: 131.9292780658034\n",
      "  time_since_restore: 3144.4537835121155\n",
      "  time_this_iter_s: 25.889082193374634\n",
      "  time_total_s: 3144.4537835121155\n",
      "  timestamp: 1553107015\n",
      "  timesteps_since_restore: 1160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1160000\n",
      "  training_iteration: 116\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3144 s, 116 iter, 1160000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-37-21\n",
      "  done: false\n",
      "  episode_len_mean: 131.15\n",
      "  episode_reward_max: 358.4614075301768\n",
      "  episode_reward_mean: 318.1867181764996\n",
      "  episode_reward_min: 281.4312011440366\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 8232\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3948.117\n",
      "    load_time_ms: 2.402\n",
      "    num_steps_sampled: 1170000\n",
      "    num_steps_trained: 1170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4750448041819797e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.003233551979065\n",
      "      kl: 0.04802057519555092\n",
      "      policy_loss: 0.009504292160272598\n",
      "      total_loss: 209.52239990234375\n",
      "      vf_explained_var: 0.9456889033317566\n",
      "      vf_loss: 209.51287841796875\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.58767878985666e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7637004256248474\n",
      "      kl: 0.05645570531487465\n",
      "      policy_loss: 0.029676085337996483\n",
      "      total_loss: 11.94020938873291\n",
      "      vf_explained_var: 0.9915854334831238\n",
      "      vf_loss: 11.91053295135498\n",
      "    sample_time_ms: 22928.895\n",
      "    update_time_ms: 9.692\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.06065941723566\n",
      "    rl_1: 127.12605875926388\n",
      "  time_since_restore: 3170.4831788539886\n",
      "  time_this_iter_s: 26.02939534187317\n",
      "  time_total_s: 3170.4831788539886\n",
      "  timestamp: 1553107041\n",
      "  timesteps_since_restore: 1170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1170000\n",
      "  training_iteration: 117\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3170 s, 117 iter, 1170000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-37-48\n",
      "  done: false\n",
      "  episode_len_mean: 130.38\n",
      "  episode_reward_max: 355.5958974960726\n",
      "  episode_reward_mean: 319.0886355015158\n",
      "  episode_reward_min: 281.4312011440366\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 8308\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3957.551\n",
      "    load_time_ms: 2.336\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.212567152062861e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9927893280982971\n",
      "      kl: 0.07022204250097275\n",
      "      policy_loss: 0.026254625990986824\n",
      "      total_loss: 201.47518920898438\n",
      "      vf_explained_var: 0.9466967582702637\n",
      "      vf_loss: 201.44894409179688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.138152347874091e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6401888132095337\n",
      "      kl: 0.06862747669219971\n",
      "      policy_loss: 0.03392789140343666\n",
      "      total_loss: 10.507573127746582\n",
      "      vf_explained_var: 0.9930216670036316\n",
      "      vf_loss: 10.473645210266113\n",
      "    sample_time_ms: 22738.498\n",
      "    update_time_ms: 9.659\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.3080769077054\n",
      "    rl_1: 127.78055859381045\n",
      "  time_since_restore: 3197.3097784519196\n",
      "  time_this_iter_s: 26.826599597930908\n",
      "  time_total_s: 3197.3097784519196\n",
      "  timestamp: 1553107068\n",
      "  timesteps_since_restore: 1180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 118\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3197 s, 118 iter, 1180000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-38-18\n",
      "  done: false\n",
      "  episode_len_mean: 130.54\n",
      "  episode_reward_max: 353.8068041837738\n",
      "  episode_reward_mean: 314.1927202299978\n",
      "  episode_reward_min: -142.9761439762284\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 8385\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3987.428\n",
      "    load_time_ms: 2.358\n",
      "    num_steps_sampled: 1190000\n",
      "    num_steps_trained: 1190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.3188512701953776e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9627087116241455\n",
      "      kl: 0.0507570281624794\n",
      "      policy_loss: 0.020477676764130592\n",
      "      total_loss: 263.3103332519531\n",
      "      vf_explained_var: 0.9175136089324951\n",
      "      vf_loss: 263.2898254394531\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7072278335968668e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7392805814743042\n",
      "      kl: 0.022326692938804626\n",
      "      policy_loss: 0.0008171913214027882\n",
      "      total_loss: 47.439659118652344\n",
      "      vf_explained_var: 0.9680308699607849\n",
      "      vf_loss: 47.438846588134766\n",
      "    sample_time_ms: 22805.013\n",
      "    update_time_ms: 9.445\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.9459788271555\n",
      "    rl_1: 125.24674140284235\n",
      "  time_since_restore: 3226.8553273677826\n",
      "  time_this_iter_s: 29.545548915863037\n",
      "  time_total_s: 3226.8553273677826\n",
      "  timestamp: 1553107098\n",
      "  timesteps_since_restore: 1190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1190000\n",
      "  training_iteration: 119\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3226 s, 119 iter, 1190000 ts, 314 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-38-46\n",
      "  done: false\n",
      "  episode_len_mean: 131.29\n",
      "  episode_reward_max: 357.4026047830284\n",
      "  episode_reward_mean: 318.110139606049\n",
      "  episode_reward_min: 286.1292357325848\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 8461\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3988.011\n",
      "    load_time_ms: 2.34\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.9782769052930664e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9514537453651428\n",
      "      kl: 0.06857185065746307\n",
      "      policy_loss: 0.007316289935261011\n",
      "      total_loss: 193.62066650390625\n",
      "      vf_explained_var: 0.9492982625961304\n",
      "      vf_loss: 193.6133575439453\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.7072278335968668e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7470057010650635\n",
      "      kl: 0.15037928521633148\n",
      "      policy_loss: 0.06252433359622955\n",
      "      total_loss: 10.12525463104248\n",
      "      vf_explained_var: 0.9930002093315125\n",
      "      vf_loss: 10.062731742858887\n",
      "    sample_time_ms: 22840.282\n",
      "    update_time_ms: 9.117\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.87713314156824\n",
      "    rl_1: 126.23300646448078\n",
      "  time_since_restore: 3255.571215391159\n",
      "  time_this_iter_s: 28.715888023376465\n",
      "  time_total_s: 3255.571215391159\n",
      "  timestamp: 1553107126\n",
      "  timesteps_since_restore: 1200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 120\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3255 s, 120 iter, 1200000 ts, 318 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-39-15\n",
      "  done: false\n",
      "  episode_len_mean: 131.74\n",
      "  episode_reward_max: 358.588877128519\n",
      "  episode_reward_mean: 321.77670904455965\n",
      "  episode_reward_min: 286.1292357325848\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 8538\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4029.72\n",
      "    load_time_ms: 2.359\n",
      "    num_steps_sampled: 1210000\n",
      "    num_steps_trained: 1210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.467415574780034e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0615289211273193\n",
      "      kl: 0.0633847564458847\n",
      "      policy_loss: 0.013665158301591873\n",
      "      total_loss: 168.16200256347656\n",
      "      vf_explained_var: 0.956343412399292\n",
      "      vf_loss: 168.14833068847656\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.5608421209722147e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7995765209197998\n",
      "      kl: 0.037564970552921295\n",
      "      policy_loss: 0.021107537671923637\n",
      "      total_loss: 13.217267990112305\n",
      "      vf_explained_var: 0.991883397102356\n",
      "      vf_loss: 13.196159362792969\n",
      "    sample_time_ms: 22969.705\n",
      "    update_time_ms: 9.274\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.7950877833311\n",
      "    rl_1: 131.98162126122853\n",
      "  time_since_restore: 3284.107999563217\n",
      "  time_this_iter_s: 28.536784172058105\n",
      "  time_total_s: 3284.107999563217\n",
      "  timestamp: 1553107155\n",
      "  timesteps_since_restore: 1210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1210000\n",
      "  training_iteration: 121\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3284 s, 121 iter, 1210000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-39-46\n",
      "  done: false\n",
      "  episode_len_mean: 131.13\n",
      "  episode_reward_max: 353.8665352334605\n",
      "  episode_reward_mean: 321.13732714054225\n",
      "  episode_reward_min: 287.78799416137883\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 8614\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4062.402\n",
      "    load_time_ms: 2.552\n",
      "    num_steps_sampled: 1220000\n",
      "    num_steps_trained: 1220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1201125096893527e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.941180408000946\n",
      "      kl: 0.04979771375656128\n",
      "      policy_loss: 0.019175032153725624\n",
      "      total_loss: 179.689697265625\n",
      "      vf_explained_var: 0.9544148445129395\n",
      "      vf_loss: 179.67051696777344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.5608421209722147e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7217423319816589\n",
      "      kl: 0.4058213531970978\n",
      "      policy_loss: 0.08095304667949677\n",
      "      total_loss: 9.10640811920166\n",
      "      vf_explained_var: 0.9940693378448486\n",
      "      vf_loss: 9.025455474853516\n",
      "    sample_time_ms: 23377.236\n",
      "    update_time_ms: 9.355\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.93114947064157\n",
      "    rl_1: 130.2061776699007\n",
      "  time_since_restore: 3314.582186460495\n",
      "  time_this_iter_s: 30.474186897277832\n",
      "  time_total_s: 3314.582186460495\n",
      "  timestamp: 1553107186\n",
      "  timesteps_since_restore: 1220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1220000\n",
      "  training_iteration: 122\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3314 s, 122 iter, 1220000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-40-15\n",
      "  done: false\n",
      "  episode_len_mean: 130.78\n",
      "  episode_reward_max: 357.72668846746546\n",
      "  episode_reward_mean: 319.0790450343047\n",
      "  episode_reward_min: 289.85606092586767\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 8690\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4101.034\n",
      "    load_time_ms: 2.611\n",
      "    num_steps_sampled: 1230000\n",
      "    num_steps_trained: 1230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.68016833085316e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0127981901168823\n",
      "      kl: 1.2281644344329834\n",
      "      policy_loss: 0.015670254826545715\n",
      "      total_loss: 174.18861389160156\n",
      "      vf_explained_var: 0.9555918574333191\n",
      "      vf_loss: 174.17294311523438\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.8412643461286245e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7126407027244568\n",
      "      kl: 52.83858108520508\n",
      "      policy_loss: 0.2171078771352768\n",
      "      total_loss: 6.844710350036621\n",
      "      vf_explained_var: 0.995441734790802\n",
      "      vf_loss: 6.627602577209473\n",
      "    sample_time_ms: 23671.727\n",
      "    update_time_ms: 9.221\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.51298647662756\n",
      "    rl_1: 128.5660585576772\n",
      "  time_since_restore: 3344.3690202236176\n",
      "  time_this_iter_s: 29.78683376312256\n",
      "  time_total_s: 3344.3690202236176\n",
      "  timestamp: 1553107215\n",
      "  timesteps_since_restore: 1230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1230000\n",
      "  training_iteration: 123\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3344 s, 123 iter, 1230000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-40-44\n",
      "  done: false\n",
      "  episode_len_mean: 131.5\n",
      "  episode_reward_max: 358.7980224999674\n",
      "  episode_reward_mean: 319.4300284512339\n",
      "  episode_reward_min: 284.12892641104185\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 8765\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4142.3\n",
      "    load_time_ms: 2.598\n",
      "    num_steps_sampled: 1240000\n",
      "    num_steps_trained: 1240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.520252929960609e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0653209686279297\n",
      "      kl: 0.20232103765010834\n",
      "      policy_loss: 0.029026826843619347\n",
      "      total_loss: 158.33607482910156\n",
      "      vf_explained_var: 0.9587074518203735\n",
      "      vf_loss: 158.3070526123047\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.761893766335858e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.729334831237793\n",
      "      kl: 0.0997210368514061\n",
      "      policy_loss: 0.03398789092898369\n",
      "      total_loss: 9.227749824523926\n",
      "      vf_explained_var: 0.9936695694923401\n",
      "      vf_loss: 9.193761825561523\n",
      "    sample_time_ms: 23895.361\n",
      "    update_time_ms: 9.226\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.90740607473887\n",
      "    rl_1: 128.52262237649506\n",
      "  time_since_restore: 3373.091696023941\n",
      "  time_this_iter_s: 28.722675800323486\n",
      "  time_total_s: 3373.091696023941\n",
      "  timestamp: 1553107244\n",
      "  timesteps_since_restore: 1240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1240000\n",
      "  training_iteration: 124\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3373 s, 124 iter, 1240000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-41-12\n",
      "  done: false\n",
      "  episode_len_mean: 131.87\n",
      "  episode_reward_max: 353.61779540916535\n",
      "  episode_reward_mean: 320.5914727124063\n",
      "  episode_reward_min: 284.12892641104185\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 8841\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4168.102\n",
      "    load_time_ms: 2.666\n",
      "    num_steps_sampled: 1250000\n",
      "    num_steps_trained: 1250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.780378787787697e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.051352858543396\n",
      "      kl: 0.05802324414253235\n",
      "      policy_loss: 0.0265983734279871\n",
      "      total_loss: 147.65756225585938\n",
      "      vf_explained_var: 0.9619482755661011\n",
      "      vf_loss: 147.63096618652344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.64284043774555e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.733180582523346\n",
      "      kl: 0.08532124012708664\n",
      "      policy_loss: 0.039341289550065994\n",
      "      total_loss: 7.600612640380859\n",
      "      vf_explained_var: 0.9950292706489563\n",
      "      vf_loss: 7.561271667480469\n",
      "    sample_time_ms: 24079.834\n",
      "    update_time_ms: 8.474\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.27995166618672\n",
      "    rl_1: 130.31152104621964\n",
      "  time_since_restore: 3401.3334953784943\n",
      "  time_this_iter_s: 28.241799354553223\n",
      "  time_total_s: 3401.3334953784943\n",
      "  timestamp: 1553107272\n",
      "  timesteps_since_restore: 1250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1250000\n",
      "  training_iteration: 125\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3401 s, 125 iter, 1250000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-41-41\n",
      "  done: false\n",
      "  episode_len_mean: 131.87\n",
      "  episode_reward_max: 376.74098824187746\n",
      "  episode_reward_mean: 321.3380787545746\n",
      "  episode_reward_min: 277.5356684764285\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 8917\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4178.959\n",
      "    load_time_ms: 2.646\n",
      "    num_steps_sampled: 1260000\n",
      "    num_steps_trained: 1260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.6705695694603264e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.095213532447815\n",
      "      kl: 1.5627515316009521\n",
      "      policy_loss: 0.0213017575442791\n",
      "      total_loss: 151.5638427734375\n",
      "      vf_explained_var: 0.9621242880821228\n",
      "      vf_loss: 151.54254150390625\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2964262774200694e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7121873497962952\n",
      "      kl: 0.05324885621666908\n",
      "      policy_loss: 0.016926120966672897\n",
      "      total_loss: 14.38182544708252\n",
      "      vf_explained_var: 0.9924145936965942\n",
      "      vf_loss: 14.364896774291992\n",
      "    sample_time_ms: 24307.915\n",
      "    update_time_ms: 8.753\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.74570175224468\n",
      "    rl_1: 131.59237700232995\n",
      "  time_since_restore: 3429.6154339313507\n",
      "  time_this_iter_s: 28.281938552856445\n",
      "  time_total_s: 3429.6154339313507\n",
      "  timestamp: 1553107301\n",
      "  timesteps_since_restore: 1260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1260000\n",
      "  training_iteration: 126\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3429 s, 126 iter, 1260000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-42-11\n",
      "  done: false\n",
      "  episode_len_mean: 133.94\n",
      "  episode_reward_max: 376.74098824187746\n",
      "  episode_reward_mean: 320.9229835391741\n",
      "  episode_reward_min: 289.6837137906393\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 8992\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4197.457\n",
      "    load_time_ms: 2.564\n",
      "    num_steps_sampled: 1270000\n",
      "    num_steps_trained: 1270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.505852966411709e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2152729034423828\n",
      "      kl: 0.8240057826042175\n",
      "      policy_loss: 0.01809985190629959\n",
      "      total_loss: 134.40478515625\n",
      "      vf_explained_var: 0.9649220705032349\n",
      "      vf_loss: 134.38670349121094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.944639416130104e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.854284405708313\n",
      "      kl: 0.17462319135665894\n",
      "      policy_loss: 0.045471757650375366\n",
      "      total_loss: 8.826886177062988\n",
      "      vf_explained_var: 0.9939457774162292\n",
      "      vf_loss: 8.781414985656738\n",
      "    sample_time_ms: 24712.993\n",
      "    update_time_ms: 8.926\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.24829180067553\n",
      "    rl_1: 130.6746917384984\n",
      "  time_since_restore: 3459.881314754486\n",
      "  time_this_iter_s: 30.265880823135376\n",
      "  time_total_s: 3459.881314754486\n",
      "  timestamp: 1553107331\n",
      "  timesteps_since_restore: 1270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1270000\n",
      "  training_iteration: 127\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3459 s, 127 iter, 1270000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-42-40\n",
      "  done: false\n",
      "  episode_len_mean: 134.09\n",
      "  episode_reward_max: 358.0716440415382\n",
      "  episode_reward_mean: 319.7663578831767\n",
      "  episode_reward_min: 149.60863469317485\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 9066\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4187.87\n",
      "    load_time_ms: 2.588\n",
      "    num_steps_sampled: 1280000\n",
      "    num_steps_trained: 1280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2758781531285734e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2358580827713013\n",
      "      kl: 0.06313186883926392\n",
      "      policy_loss: 0.03194725140929222\n",
      "      total_loss: 138.32960510253906\n",
      "      vf_explained_var: 0.9648388624191284\n",
      "      vf_loss: 138.29763793945312\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.916959208898451e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8412500023841858\n",
      "      kl: 0.03577441722154617\n",
      "      policy_loss: 0.013143490068614483\n",
      "      total_loss: 16.85024070739746\n",
      "      vf_explained_var: 0.991735577583313\n",
      "      vf_loss: 16.83709716796875\n",
      "    sample_time_ms: 24944.742\n",
      "    update_time_ms: 9.174\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.1098273643055\n",
      "    rl_1: 131.65653051887122\n",
      "  time_since_restore: 3488.9354865550995\n",
      "  time_this_iter_s: 29.054171800613403\n",
      "  time_total_s: 3488.9354865550995\n",
      "  timestamp: 1553107360\n",
      "  timesteps_since_restore: 1280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1280000\n",
      "  training_iteration: 128\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3488 s, 128 iter, 1280000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-43-10\n",
      "  done: false\n",
      "  episode_len_mean: 132.77\n",
      "  episode_reward_max: 357.4087134447325\n",
      "  episode_reward_mean: 315.15852544848235\n",
      "  episode_reward_min: 149.60863469317485\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 9142\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4173.165\n",
      "    load_time_ms: 2.581\n",
      "    num_steps_sampled: 1290000\n",
      "    num_steps_trained: 1290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9138172990817992e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.160967230796814\n",
      "      kl: 0.08109159022569656\n",
      "      policy_loss: 0.04232405498623848\n",
      "      total_loss: 121.14191436767578\n",
      "      vf_explained_var: 0.9692069888114929\n",
      "      vf_loss: 121.099609375\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.916959208898451e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7647328972816467\n",
      "      kl: 0.06692112982273102\n",
      "      policy_loss: 0.02598682977259159\n",
      "      total_loss: 7.056890964508057\n",
      "      vf_explained_var: 0.9952058792114258\n",
      "      vf_loss: 7.030903339385986\n",
      "    sample_time_ms: 24963.245\n",
      "    update_time_ms: 9.196\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 187.99612585372918\n",
      "    rl_1: 127.16239959475317\n",
      "  time_since_restore: 3518.5151541233063\n",
      "  time_this_iter_s: 29.579667568206787\n",
      "  time_total_s: 3518.5151541233063\n",
      "  timestamp: 1553107390\n",
      "  timesteps_since_restore: 1290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1290000\n",
      "  training_iteration: 129\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3518 s, 129 iter, 1290000 ts, 315 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-43-40\n",
      "  done: false\n",
      "  episode_len_mean: 132.39\n",
      "  episode_reward_max: 354.8927913247348\n",
      "  episode_reward_mean: 320.93286567174147\n",
      "  episode_reward_min: 284.41337949951765\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 9217\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4137.466\n",
      "    load_time_ms: 2.631\n",
      "    num_steps_sampled: 1300000\n",
      "    num_steps_trained: 1300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.8707250465664913e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1752846240997314\n",
      "      kl: 1.948203682899475\n",
      "      policy_loss: 0.04155068099498749\n",
      "      total_loss: 118.64564514160156\n",
      "      vf_explained_var: 0.9695544838905334\n",
      "      vf_loss: 118.60408782958984\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.375439490974034e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7664782404899597\n",
      "      kl: 0.14705924689769745\n",
      "      policy_loss: 0.04280257597565651\n",
      "      total_loss: 9.2445068359375\n",
      "      vf_explained_var: 0.9940849542617798\n",
      "      vf_loss: 9.201705932617188\n",
      "    sample_time_ms: 25166.367\n",
      "    update_time_ms: 9.42\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.50909283616733\n",
      "    rl_1: 131.4237728355742\n",
      "  time_since_restore: 3548.910758495331\n",
      "  time_this_iter_s: 30.395604372024536\n",
      "  time_total_s: 3548.910758495331\n",
      "  timestamp: 1553107420\n",
      "  timesteps_since_restore: 1300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1300000\n",
      "  training_iteration: 130\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3548 s, 130 iter, 1300000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-44-08\n",
      "  done: false\n",
      "  episode_len_mean: 132.64\n",
      "  episode_reward_max: 355.8731085047801\n",
      "  episode_reward_mean: 321.3476860031727\n",
      "  episode_reward_min: 282.00597938913944\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 9293\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4115.765\n",
      "    load_time_ms: 2.625\n",
      "    num_steps_sampled: 1310000\n",
      "    num_steps_trained: 1310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.306089373962152e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1350605487823486\n",
      "      kl: 3.6186022758483887\n",
      "      policy_loss: 0.06275004893541336\n",
      "      total_loss: 119.79994201660156\n",
      "      vf_explained_var: 0.9702235460281372\n",
      "      vf_loss: 119.73719024658203\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.563157542395157e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7523854970932007\n",
      "      kl: 0.09346409887075424\n",
      "      policy_loss: 0.028049087151885033\n",
      "      total_loss: 6.76295280456543\n",
      "      vf_explained_var: 0.9956614375114441\n",
      "      vf_loss: 6.734903335571289\n",
      "    sample_time_ms: 25051.557\n",
      "    update_time_ms: 9.589\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.80940320003086\n",
      "    rl_1: 131.5382828031419\n",
      "  time_since_restore: 3576.0851736068726\n",
      "  time_this_iter_s: 27.174415111541748\n",
      "  time_total_s: 3576.0851736068726\n",
      "  timestamp: 1553107448\n",
      "  timesteps_since_restore: 1310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1310000\n",
      "  training_iteration: 131\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3576 s, 131 iter, 1310000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-44-37\n",
      "  done: false\n",
      "  episode_len_mean: 133.28\n",
      "  episode_reward_max: 366.9056458849418\n",
      "  episode_reward_mean: 321.84585729075263\n",
      "  episode_reward_min: 286.8613986598164\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 9368\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4094.794\n",
      "    load_time_ms: 2.388\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.459130452718398e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2135192155838013\n",
      "      kl: 0.16201625764369965\n",
      "      policy_loss: 0.07943229377269745\n",
      "      total_loss: 110.17457580566406\n",
      "      vf_explained_var: 0.9718495011329651\n",
      "      vf_loss: 110.09513854980469\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.84473800765863e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8006231188774109\n",
      "      kl: 1.5231468677520752\n",
      "      policy_loss: 0.05750277638435364\n",
      "      total_loss: 5.925138473510742\n",
      "      vf_explained_var: 0.995995044708252\n",
      "      vf_loss: 5.867635726928711\n",
      "    sample_time_ms: 24988.317\n",
      "    update_time_ms: 9.628\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.03325435366824\n",
      "    rl_1: 131.81260293708436\n",
      "  time_since_restore: 3605.7127220630646\n",
      "  time_this_iter_s: 29.627548456192017\n",
      "  time_total_s: 3605.7127220630646\n",
      "  timestamp: 1553107477\n",
      "  timesteps_since_restore: 1320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 132\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3605 s, 132 iter, 1320000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-45-14\n",
      "  done: false\n",
      "  episode_len_mean: 132.7\n",
      "  episode_reward_max: 361.1685694091095\n",
      "  episode_reward_mean: 321.6309000782565\n",
      "  episode_reward_min: 287.2486818419208\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 9443\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4199.191\n",
      "    load_time_ms: 2.456\n",
      "    num_steps_sampled: 1330000\n",
      "    num_steps_trained: 1330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.688696511744865e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1476393938064575\n",
      "      kl: 4.700308799743652\n",
      "      policy_loss: 0.0558016411960125\n",
      "      total_loss: 107.2808837890625\n",
      "      vf_explained_var: 0.9728057980537415\n",
      "      vf_loss: 107.22508239746094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4767104639795692e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7803736329078674\n",
      "      kl: 0.05585357919335365\n",
      "      policy_loss: 0.02329346537590027\n",
      "      total_loss: 6.055631160736084\n",
      "      vf_explained_var: 0.9959915280342102\n",
      "      vf_loss: 6.0323381423950195\n",
      "    sample_time_ms: 25581.923\n",
      "    update_time_ms: 9.549\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.3443387314686\n",
      "    rl_1: 131.28656134678786\n",
      "  time_since_restore: 3642.4842092990875\n",
      "  time_this_iter_s: 36.77148723602295\n",
      "  time_total_s: 3642.4842092990875\n",
      "  timestamp: 1553107514\n",
      "  timesteps_since_restore: 1330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1330000\n",
      "  training_iteration: 133\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3642 s, 133 iter, 1330000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-45-44\n",
      "  done: false\n",
      "  episode_len_mean: 132.85\n",
      "  episode_reward_max: 356.87812409374044\n",
      "  episode_reward_mean: 323.2072368200677\n",
      "  episode_reward_min: 290.86321589818635\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 9518\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4162.007\n",
      "    load_time_ms: 2.477\n",
      "    num_steps_sampled: 1340000\n",
      "    num_steps_trained: 1340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4533046988063347e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1607364416122437\n",
      "      kl: 0.09169536083936691\n",
      "      policy_loss: 0.04734920710325241\n",
      "      total_loss: 98.59815979003906\n",
      "      vf_explained_var: 0.9753482341766357\n",
      "      vf_loss: 98.55081176757812\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.21506623807044e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7933481931686401\n",
      "      kl: 0.11407463252544403\n",
      "      policy_loss: 0.034547608345746994\n",
      "      total_loss: 7.45245361328125\n",
      "      vf_explained_var: 0.9952465891838074\n",
      "      vf_loss: 7.417906761169434\n",
      "    sample_time_ms: 25727.475\n",
      "    update_time_ms: 9.701\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.07139178391853\n",
      "    rl_1: 134.1358450361493\n",
      "  time_since_restore: 3672.293934106827\n",
      "  time_this_iter_s: 29.809724807739258\n",
      "  time_total_s: 3672.293934106827\n",
      "  timestamp: 1553107544\n",
      "  timesteps_since_restore: 1340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1340000\n",
      "  training_iteration: 134\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3672 s, 134 iter, 1340000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-46-10\n",
      "  done: false\n",
      "  episode_len_mean: 133.82\n",
      "  episode_reward_max: 355.1348537333608\n",
      "  episode_reward_mean: 321.1550449536408\n",
      "  episode_reward_min: 284.46579027567714\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 9593\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4131.262\n",
      "    load_time_ms: 2.403\n",
      "    num_steps_sampled: 1350000\n",
      "    num_steps_trained: 1350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.179957547809863e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1955946683883667\n",
      "      kl: 0.8664892315864563\n",
      "      policy_loss: 0.04997507855296135\n",
      "      total_loss: 103.70620727539062\n",
      "      vf_explained_var: 0.9745034575462341\n",
      "      vf_loss: 103.65623474121094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.322598815004574e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.901912271976471\n",
      "      kl: 0.025029940530657768\n",
      "      policy_loss: 0.010004792362451553\n",
      "      total_loss: 7.086537837982178\n",
      "      vf_explained_var: 0.9952029585838318\n",
      "      vf_loss: 7.07653284072876\n",
      "    sample_time_ms: 25549.445\n",
      "    update_time_ms: 9.76\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.9377879871097\n",
      "    rl_1: 130.21725696653107\n",
      "  time_since_restore: 3698.44842505455\n",
      "  time_this_iter_s: 26.15449094772339\n",
      "  time_total_s: 3698.44842505455\n",
      "  timestamp: 1553107570\n",
      "  timesteps_since_restore: 1350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1350000\n",
      "  training_iteration: 135\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3698 s, 135 iter, 1350000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-46-36\n",
      "  done: false\n",
      "  episode_len_mean: 133.34\n",
      "  episode_reward_max: 353.1181114872998\n",
      "  episode_reward_mean: 319.5723970055778\n",
      "  episode_reward_min: 284.46579027567714\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 9669\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4118.413\n",
      "    load_time_ms: 2.401\n",
      "    num_steps_sampled: 1360000\n",
      "    num_steps_trained: 1360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.2699363217147948e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2863755226135254\n",
      "      kl: 0.5447577834129333\n",
      "      policy_loss: 0.047901131212711334\n",
      "      total_loss: 83.00965118408203\n",
      "      vf_explained_var: 0.978459358215332\n",
      "      vf_loss: 82.96176147460938\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.322598815004574e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.811546802520752\n",
      "      kl: 0.12697769701480865\n",
      "      policy_loss: 0.039102911949157715\n",
      "      total_loss: 6.123819828033447\n",
      "      vf_explained_var: 0.9961742758750916\n",
      "      vf_loss: 6.084716796875\n",
      "    sample_time_ms: 25319.233\n",
      "    update_time_ms: 9.556\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.73498054340791\n",
      "    rl_1: 130.8374164621698\n",
      "  time_since_restore: 3724.293946504593\n",
      "  time_this_iter_s: 25.845521450042725\n",
      "  time_total_s: 3724.293946504593\n",
      "  timestamp: 1553107596\n",
      "  timesteps_since_restore: 1360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1360000\n",
      "  training_iteration: 136\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3724 s, 136 iter, 1360000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-47-02\n",
      "  done: false\n",
      "  episode_len_mean: 133.41\n",
      "  episode_reward_max: 360.66662998694204\n",
      "  episode_reward_mean: 322.75937108206483\n",
      "  episode_reward_min: 287.1240408319906\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 9743\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4102.072\n",
      "    load_time_ms: 2.454\n",
      "    num_steps_sampled: 1370000\n",
      "    num_steps_trained: 1370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.904904038482982e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2145569324493408\n",
      "      kl: 3.3547985553741455\n",
      "      policy_loss: 0.07812599837779999\n",
      "      total_loss: 87.73990631103516\n",
      "      vf_explained_var: 0.9780279397964478\n",
      "      vf_loss: 87.66178131103516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.98389903565849e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7562840580940247\n",
      "      kl: 100.35657501220703\n",
      "      policy_loss: 0.13666465878486633\n",
      "      total_loss: 7.662917137145996\n",
      "      vf_explained_var: 0.9951440095901489\n",
      "      vf_loss: 7.526252269744873\n",
      "    sample_time_ms: 24887.06\n",
      "    update_time_ms: 9.284\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.102399119698\n",
      "    rl_1: 132.6569719623668\n",
      "  time_since_restore: 3750.0709030628204\n",
      "  time_this_iter_s: 25.77695655822754\n",
      "  time_total_s: 3750.0709030628204\n",
      "  timestamp: 1553107622\n",
      "  timesteps_since_restore: 1370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1370000\n",
      "  training_iteration: 137\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3750 s, 137 iter, 1370000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-47-28\n",
      "  done: false\n",
      "  episode_len_mean: 133.88\n",
      "  episode_reward_max: 360.66662998694204\n",
      "  episode_reward_mean: 324.2979494874528\n",
      "  episode_reward_min: 287.66838815834336\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 9818\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4070.215\n",
      "    load_time_ms: 2.505\n",
      "    num_steps_sampled: 1380000\n",
      "    num_steps_trained: 1380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.357356501813683e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2602436542510986\n",
      "      kl: 0.5578639507293701\n",
      "      policy_loss: 0.06933091580867767\n",
      "      total_loss: 83.10529327392578\n",
      "      vf_explained_var: 0.9788896441459656\n",
      "      vf_loss: 83.03595733642578\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.475848824538278e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.71857088804245\n",
      "      kl: 106.9632568359375\n",
      "      policy_loss: 0.09839273244142532\n",
      "      total_loss: 6.768154144287109\n",
      "      vf_explained_var: 0.9955552220344543\n",
      "      vf_loss: 6.669761657714844\n",
      "    sample_time_ms: 24607.969\n",
      "    update_time_ms: 9.115\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.8523284002792\n",
      "    rl_1: 133.44562108717358\n",
      "  time_since_restore: 3776.0175602436066\n",
      "  time_this_iter_s: 25.946657180786133\n",
      "  time_total_s: 3776.0175602436066\n",
      "  timestamp: 1553107648\n",
      "  timesteps_since_restore: 1380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1380000\n",
      "  training_iteration: 138\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3776 s, 138 iter, 1380000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-47-54\n",
      "  done: false\n",
      "  episode_len_mean: 133.78\n",
      "  episode_reward_max: 353.43416260760887\n",
      "  episode_reward_mean: 323.23887892125714\n",
      "  episode_reward_min: 292.3732828890159\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 9892\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4021.098\n",
      "    load_time_ms: 2.501\n",
      "    num_steps_sampled: 1390000\n",
      "    num_steps_trained: 1390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1036034308631315e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2402927875518799\n",
      "      kl: 2.9092655181884766\n",
      "      policy_loss: 0.08808202296495438\n",
      "      total_loss: 86.30113983154297\n",
      "      vf_explained_var: 0.9788278341293335\n",
      "      vf_loss: 86.21305847167969\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1213769713150357e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7785493731498718\n",
      "      kl: 1.3001736402511597\n",
      "      policy_loss: 0.038366250693798065\n",
      "      total_loss: 5.88627290725708\n",
      "      vf_explained_var: 0.9960957169532776\n",
      "      vf_loss: 5.84790563583374\n",
      "    sample_time_ms: 24328.964\n",
      "    update_time_ms: 9.0\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.56882044822544\n",
      "    rl_1: 132.67005847303165\n",
      "  time_since_restore: 3802.3168709278107\n",
      "  time_this_iter_s: 26.2993106842041\n",
      "  time_total_s: 3802.3168709278107\n",
      "  timestamp: 1553107674\n",
      "  timesteps_since_restore: 1390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1390000\n",
      "  training_iteration: 139\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3802 s, 139 iter, 1390000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-48-20\n",
      "  done: false\n",
      "  episode_len_mean: 133.66\n",
      "  episode_reward_max: 361.9929737938629\n",
      "  episode_reward_mean: 323.4147801924515\n",
      "  episode_reward_min: 292.95101870066685\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 9967\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4005.104\n",
      "    load_time_ms: 2.467\n",
      "    num_steps_sampled: 1400000\n",
      "    num_steps_trained: 1400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6554050574768553e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2452291250228882\n",
      "      kl: 0.1082029864192009\n",
      "      policy_loss: 0.055277615785598755\n",
      "      total_loss: 73.79908752441406\n",
      "      vf_explained_var: 0.9816266298294067\n",
      "      vf_loss: 73.74382019042969\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.6820661617039656e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8483431935310364\n",
      "      kl: 5.459957599639893\n",
      "      policy_loss: 0.0248078815639019\n",
      "      total_loss: 7.857594013214111\n",
      "      vf_explained_var: 0.9949493408203125\n",
      "      vf_loss: 7.832785606384277\n",
      "    sample_time_ms: 23916.071\n",
      "    update_time_ms: 8.936\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.13841793667504\n",
      "    rl_1: 133.2763622557764\n",
      "  time_since_restore: 3828.4196310043335\n",
      "  time_this_iter_s: 26.102760076522827\n",
      "  time_total_s: 3828.4196310043335\n",
      "  timestamp: 1553107700\n",
      "  timesteps_since_restore: 1400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1400000\n",
      "  training_iteration: 140\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3828 s, 140 iter, 1400000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-48-47\n",
      "  done: false\n",
      "  episode_len_mean: 134.81\n",
      "  episode_reward_max: 372.7962884287752\n",
      "  episode_reward_mean: 324.458605920906\n",
      "  episode_reward_min: 292.22089063510253\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 10042\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3979.399\n",
      "    load_time_ms: 2.495\n",
      "    num_steps_sampled: 1410000\n",
      "    num_steps_trained: 1410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.483107408579599e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2244576215744019\n",
      "      kl: 620.3554077148438\n",
      "      policy_loss: 0.42011550068855286\n",
      "      total_loss: 77.96672058105469\n",
      "      vf_explained_var: 0.9810278415679932\n",
      "      vf_loss: 77.54658508300781\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.523097833093124e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8497956395149231\n",
      "      kl: 0.09862560778856277\n",
      "      policy_loss: 0.006358680780977011\n",
      "      total_loss: 8.003263473510742\n",
      "      vf_explained_var: 0.9949327111244202\n",
      "      vf_loss: 7.996903419494629\n",
      "    sample_time_ms: 23835.593\n",
      "    update_time_ms: 8.898\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.5971283662501\n",
      "    rl_1: 132.8614775546559\n",
      "  time_since_restore: 3854.529631137848\n",
      "  time_this_iter_s: 26.110000133514404\n",
      "  time_total_s: 3854.529631137848\n",
      "  timestamp: 1553107727\n",
      "  timesteps_since_restore: 1410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1410000\n",
      "  training_iteration: 141\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3854 s, 141 iter, 1410000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-49-12\n",
      "  done: false\n",
      "  episode_len_mean: 134.59\n",
      "  episode_reward_max: 364.3879686495555\n",
      "  episode_reward_mean: 321.6850757896775\n",
      "  episode_reward_min: 291.2127134899496\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 10116\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3963.235\n",
      "    load_time_ms: 2.49\n",
      "    num_steps_sampled: 1420000\n",
      "    num_steps_trained: 1420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7246607575980306e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.321392297744751\n",
      "      kl: 0.46752187609672546\n",
      "      policy_loss: 0.05739475414156914\n",
      "      total_loss: 66.97937774658203\n",
      "      vf_explained_var: 0.9830141067504883\n",
      "      vf_loss: 66.9219970703125\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.784647074900338e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8775657415390015\n",
      "      kl: 0.06748790293931961\n",
      "      policy_loss: 0.013901074416935444\n",
      "      total_loss: 6.826861381530762\n",
      "      vf_explained_var: 0.9955863952636719\n",
      "      vf_loss: 6.812960147857666\n",
      "    sample_time_ms: 23477.665\n",
      "    update_time_ms: 8.854\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.6834001476122\n",
      "    rl_1: 132.00167564206535\n",
      "  time_since_restore: 3880.414316892624\n",
      "  time_this_iter_s: 25.884685754776\n",
      "  time_total_s: 3880.414316892624\n",
      "  timestamp: 1553107752\n",
      "  timesteps_since_restore: 1420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1420000\n",
      "  training_iteration: 142\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3880 s, 142 iter, 1420000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-49-38\n",
      "  done: false\n",
      "  episode_len_mean: 134.65\n",
      "  episode_reward_max: 362.03751743310704\n",
      "  episode_reward_mean: 320.02097791675516\n",
      "  episode_reward_min: 170.47973195523014\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 10190\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3819.267\n",
      "    load_time_ms: 2.347\n",
      "    num_steps_sampled: 1430000\n",
      "    num_steps_trained: 1430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.586991136397046e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3268940448760986\n",
      "      kl: 0.10818328708410263\n",
      "      policy_loss: 0.0473170280456543\n",
      "      total_loss: 68.58836364746094\n",
      "      vf_explained_var: 0.983170747756958\n",
      "      vf_loss: 68.54106140136719\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.676971479712245e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9147226214408875\n",
      "      kl: 0.04390517994761467\n",
      "      policy_loss: 0.0062902928330004215\n",
      "      total_loss: 14.419635772705078\n",
      "      vf_explained_var: 0.9927595853805542\n",
      "      vf_loss: 14.413344383239746\n",
      "    sample_time_ms: 22526.502\n",
      "    update_time_ms: 8.87\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.1657774999147\n",
      "    rl_1: 131.8552004168404\n",
      "  time_since_restore: 3906.227080821991\n",
      "  time_this_iter_s: 25.812763929367065\n",
      "  time_total_s: 3906.227080821991\n",
      "  timestamp: 1553107778\n",
      "  timesteps_since_restore: 1430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1430000\n",
      "  training_iteration: 143\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3906 s, 143 iter, 1430000 ts, 320 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-50-06\n",
      "  done: false\n",
      "  episode_len_mean: 133.96\n",
      "  episode_reward_max: 368.6738926139988\n",
      "  episode_reward_mean: 323.74776159271204\n",
      "  episode_reward_min: 285.733007321805\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 10264\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 3943.048\n",
      "    load_time_ms: 2.304\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.380488480952408e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2476820945739746\n",
      "      kl: 0.05634421482682228\n",
      "      policy_loss: 0.032046400010585785\n",
      "      total_loss: 61.28615951538086\n",
      "      vf_explained_var: 0.9846072196960449\n",
      "      vf_loss: 61.254112243652344\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.515459171132278e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7889289855957031\n",
      "      kl: 0.042813342064619064\n",
      "      policy_loss: 0.0063557433895766735\n",
      "      total_loss: 7.712926387786865\n",
      "      vf_explained_var: 0.9952766299247742\n",
      "      vf_loss: 7.706568717956543\n",
      "    sample_time_ms: 22215.675\n",
      "    update_time_ms: 8.555\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.19952984249122\n",
      "    rl_1: 133.5482317502208\n",
      "  time_since_restore: 3934.1635093688965\n",
      "  time_this_iter_s: 27.936428546905518\n",
      "  time_total_s: 3934.1635093688965\n",
      "  timestamp: 1553107806\n",
      "  timesteps_since_restore: 1440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 144\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3934 s, 144 iter, 1440000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-50-41\n",
      "  done: false\n",
      "  episode_len_mean: 133.97\n",
      "  episode_reward_max: 357.44382796766627\n",
      "  episode_reward_mean: 325.0266802015775\n",
      "  episode_reward_min: 288.02189747176305\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 10339\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4051.808\n",
      "    load_time_ms: 2.308\n",
      "    num_steps_sampled: 1450000\n",
      "    num_steps_trained: 1450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.257073307669998e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3699363470077515\n",
      "      kl: 362.6254577636719\n",
      "      policy_loss: 0.40337973833084106\n",
      "      total_loss: 45.36874771118164\n",
      "      vf_explained_var: 0.988387405872345\n",
      "      vf_loss: 44.9653205871582\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2773188756698417e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9251286387443542\n",
      "      kl: 6.031111717224121\n",
      "      policy_loss: 0.04586494714021683\n",
      "      total_loss: 8.194511413574219\n",
      "      vf_explained_var: 0.9950027465820312\n",
      "      vf_loss: 8.14864730834961\n",
      "    sample_time_ms: 22954.242\n",
      "    update_time_ms: 8.68\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.28062970733504\n",
      "    rl_1: 136.74605049424252\n",
      "  time_since_restore: 3968.7930274009705\n",
      "  time_this_iter_s: 34.629518032073975\n",
      "  time_total_s: 3968.7930274009705\n",
      "  timestamp: 1553107841\n",
      "  timesteps_since_restore: 1450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1450000\n",
      "  training_iteration: 145\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 3968 s, 145 iter, 1450000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-51-16\n",
      "  done: false\n",
      "  episode_len_mean: 134.04\n",
      "  episode_reward_max: 355.6751576730117\n",
      "  episode_reward_mean: 319.4626169917268\n",
      "  episode_reward_min: 101.33071795587807\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 10414\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4192.76\n",
      "    load_time_ms: 2.33\n",
      "    num_steps_sampled: 1460000\n",
      "    num_steps_trained: 1460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8856097483421763e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2895957231521606\n",
      "      kl: 123.1889877319336\n",
      "      policy_loss: 0.34836333990097046\n",
      "      total_loss: 70.11811065673828\n",
      "      vf_explained_var: 0.9830726385116577\n",
      "      vf_loss: 69.76972961425781\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.915978140032415e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8388860821723938\n",
      "      kl: 0.03379855677485466\n",
      "      policy_loss: 0.00544056948274374\n",
      "      total_loss: 16.384992599487305\n",
      "      vf_explained_var: 0.9925033450126648\n",
      "      vf_loss: 16.379552841186523\n",
      "    sample_time_ms: 23684.208\n",
      "    update_time_ms: 8.899\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 187.71922689955977\n",
      "    rl_1: 131.74339009216695\n",
      "  time_since_restore: 4003.3586032390594\n",
      "  time_this_iter_s: 34.56557583808899\n",
      "  time_total_s: 4003.3586032390594\n",
      "  timestamp: 1553107876\n",
      "  timesteps_since_restore: 1460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1460000\n",
      "  training_iteration: 146\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4003 s, 146 iter, 1460000 ts, 319 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-52-03\n",
      "  done: false\n",
      "  episode_len_mean: 134.05\n",
      "  episode_reward_max: 371.9042247794381\n",
      "  episode_reward_mean: 320.9518291983895\n",
      "  episode_reward_min: 286.1242525501508\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 10488\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4493.97\n",
      "    load_time_ms: 2.663\n",
      "    num_steps_sampled: 1470000\n",
      "    num_steps_trained: 1470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.828414835676085e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3166147470474243\n",
      "      kl: 107.41597747802734\n",
      "      policy_loss: 0.3932588994503021\n",
      "      total_loss: 50.98082733154297\n",
      "      vf_explained_var: 0.9872124791145325\n",
      "      vf_loss: 50.58753967285156\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.915978140032415e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8978757858276367\n",
      "      kl: 0.6376142501831055\n",
      "      policy_loss: 0.03149961307644844\n",
      "      total_loss: 5.768356800079346\n",
      "      vf_explained_var: 0.9961511492729187\n",
      "      vf_loss: 5.736856937408447\n",
      "    sample_time_ms: 25501.3\n",
      "    update_time_ms: 10.224\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.7582904026294\n",
      "    rl_1: 131.19353879576002\n",
      "  time_since_restore: 4050.3462460041046\n",
      "  time_this_iter_s: 46.987642765045166\n",
      "  time_total_s: 4050.3462460041046\n",
      "  timestamp: 1553107923\n",
      "  timesteps_since_restore: 1470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1470000\n",
      "  training_iteration: 147\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4050 s, 147 iter, 1470000 ts, 321 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-52-57\n",
      "  done: false\n",
      "  episode_len_mean: 134.06\n",
      "  episode_reward_max: 369.578420682303\n",
      "  episode_reward_mean: 322.320691421297\n",
      "  episode_reward_min: 287.42910387550677\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 10562\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4676.902\n",
      "    load_time_ms: 2.639\n",
      "    num_steps_sampled: 1480000\n",
      "    num_steps_trained: 1480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.2426211166457506e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3883997201919556\n",
      "      kl: 248.31019592285156\n",
      "      policy_loss: 0.39358216524124146\n",
      "      total_loss: 48.87697982788086\n",
      "      vf_explained_var: 0.9876866340637207\n",
      "      vf_loss: 48.483299255371094\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.87396738352097e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7803274989128113\n",
      "      kl: 0.1016106903553009\n",
      "      policy_loss: 0.012766006402671337\n",
      "      total_loss: 5.959307670593262\n",
      "      vf_explained_var: 0.9961580038070679\n",
      "      vf_loss: 5.9465413093566895\n",
      "    sample_time_ms: 28131.199\n",
      "    update_time_ms: 10.991\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.48304303857796\n",
      "    rl_1: 132.83764838271912\n",
      "  time_since_restore: 4104.427015304565\n",
      "  time_this_iter_s: 54.080769300460815\n",
      "  time_total_s: 4104.427015304565\n",
      "  timestamp: 1553107977\n",
      "  timesteps_since_restore: 1480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1480000\n",
      "  training_iteration: 148\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4104 s, 148 iter, 1480000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-53-42\n",
      "  done: false\n",
      "  episode_len_mean: 134.26\n",
      "  episode_reward_max: 374.14809659821873\n",
      "  episode_reward_mean: 325.3534765246348\n",
      "  episode_reward_min: 295.36021954528985\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 10638\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4936.114\n",
      "    load_time_ms: 2.738\n",
      "    num_steps_sampled: 1490000\n",
      "    num_steps_trained: 1490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.363932243402814e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2867627143859863\n",
      "      kl: 617.1904296875\n",
      "      policy_loss: 0.38746991753578186\n",
      "      total_loss: 47.12997817993164\n",
      "      vf_explained_var: 0.9882218837738037\n",
      "      vf_loss: 46.74210739135742\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.3109509018091074e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7739620208740234\n",
      "      kl: 0.05332952365279198\n",
      "      policy_loss: 0.014042273163795471\n",
      "      total_loss: 5.6154985427856445\n",
      "      vf_explained_var: 0.9964056611061096\n",
      "      vf_loss: 5.601456165313721\n",
      "    sample_time_ms: 29777.95\n",
      "    update_time_ms: 10.946\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.94477930377002\n",
      "    rl_1: 133.40869722086484\n",
      "  time_since_restore: 4149.790892362595\n",
      "  time_this_iter_s: 45.363877058029175\n",
      "  time_total_s: 4149.790892362595\n",
      "  timestamp: 1553108022\n",
      "  timesteps_since_restore: 1490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1490000\n",
      "  training_iteration: 149\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4149 s, 149 iter, 1490000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-54-28\n",
      "  done: false\n",
      "  episode_len_mean: 135.2\n",
      "  episode_reward_max: 361.2902715503085\n",
      "  episode_reward_mean: 323.06011220079296\n",
      "  episode_reward_min: 175.4995890002329\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 10711\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5290.545\n",
      "    load_time_ms: 2.948\n",
      "    num_steps_sampled: 1500000\n",
      "    num_steps_trained: 1500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.545901775709353e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.389479398727417\n",
      "      kl: 1100.154296875\n",
      "      policy_loss: 0.3395327925682068\n",
      "      total_loss: 51.345760345458984\n",
      "      vf_explained_var: 0.9872097373008728\n",
      "      vf_loss: 51.00516891479492\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.466426005768966e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.884446382522583\n",
      "      kl: 0.017699329182505608\n",
      "      policy_loss: 0.0015259790234267712\n",
      "      total_loss: 15.84020709991455\n",
      "      vf_explained_var: 0.9911556839942932\n",
      "      vf_loss: 15.8386812210083\n",
      "    sample_time_ms: 31398.9\n",
      "    update_time_ms: 11.331\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.42029462809222\n",
      "    rl_1: 133.63981757270068\n",
      "  time_since_restore: 4195.66303396225\n",
      "  time_this_iter_s: 45.87214159965515\n",
      "  time_total_s: 4195.66303396225\n",
      "  timestamp: 1553108068\n",
      "  timesteps_since_restore: 1500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1500000\n",
      "  training_iteration: 150\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4195 s, 150 iter, 1500000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-55-10\n",
      "  done: false\n",
      "  episode_len_mean: 136.59\n",
      "  episode_reward_max: 366.42565693709594\n",
      "  episode_reward_mean: 326.556362814353\n",
      "  episode_reward_min: 175.4995890002329\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 10785\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5453.623\n",
      "    load_time_ms: 2.906\n",
      "    num_steps_sampled: 1510000\n",
      "    num_steps_trained: 1510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4318850389827276e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.429623007774353\n",
      "      kl: 2047.4400634765625\n",
      "      policy_loss: 0.3854830861091614\n",
      "      total_loss: 39.01716995239258\n",
      "      vf_explained_var: 0.9902383089065552\n",
      "      vf_loss: 38.62875747680664\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 6.466426005768966e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8716786503791809\n",
      "      kl: 0.7018157839775085\n",
      "      policy_loss: 0.02904883399605751\n",
      "      total_loss: 8.228988647460938\n",
      "      vf_explained_var: 0.9949074983596802\n",
      "      vf_loss: 8.19994068145752\n",
      "    sample_time_ms: 32764.879\n",
      "    update_time_ms: 11.484\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.19461427947135\n",
      "    rl_1: 137.36174853488168\n",
      "  time_since_restore: 4237.067200899124\n",
      "  time_this_iter_s: 41.40416693687439\n",
      "  time_total_s: 4237.067200899124\n",
      "  timestamp: 1553108110\n",
      "  timesteps_since_restore: 1510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1510000\n",
      "  training_iteration: 151\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4237 s, 151 iter, 1510000 ts, 327 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-56-05\n",
      "  done: false\n",
      "  episode_len_mean: 136.95\n",
      "  episode_reward_max: 373.8539458764794\n",
      "  episode_reward_mean: 326.93552097430876\n",
      "  episode_reward_min: 293.76417089393016\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 10857\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5696.634\n",
      "    load_time_ms: 3.074\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.147827672160929e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4185723066329956\n",
      "      kl: 1155.6531982421875\n",
      "      policy_loss: 0.38572195172309875\n",
      "      total_loss: 36.98822021484375\n",
      "      vf_explained_var: 0.9907504320144653\n",
      "      vf_loss: 36.60001754760742\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 9.699638314764059e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.923431396484375\n",
      "      kl: 0.08162139356136322\n",
      "      policy_loss: 0.016315830871462822\n",
      "      total_loss: 8.877690315246582\n",
      "      vf_explained_var: 0.9943838715553284\n",
      "      vf_loss: 8.861373901367188\n",
      "    sample_time_ms: 35454.132\n",
      "    update_time_ms: 11.716\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.1811692322617\n",
      "    rl_1: 136.75435174204705\n",
      "  time_since_restore: 4292.288833618164\n",
      "  time_this_iter_s: 55.22163271903992\n",
      "  time_total_s: 4292.288833618164\n",
      "  timestamp: 1553108165\n",
      "  timesteps_since_restore: 1520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 152\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4292 s, 152 iter, 1520000 ts, 327 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-57-19\n",
      "  done: false\n",
      "  episode_len_mean: 136.37\n",
      "  episode_reward_max: 373.8539458764794\n",
      "  episode_reward_mean: 323.6726107381079\n",
      "  episode_reward_min: 173.21241245089118\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 10931\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6327.553\n",
      "    load_time_ms: 3.253\n",
      "    num_steps_sampled: 1530000\n",
      "    num_steps_trained: 1530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.221740598746692e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4037468433380127\n",
      "      kl: 165.8677978515625\n",
      "      policy_loss: 0.3144848048686981\n",
      "      total_loss: 40.587738037109375\n",
      "      vf_explained_var: 0.9909034371376038\n",
      "      vf_loss: 40.27271270751953\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4549456084367307e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8443886041641235\n",
      "      kl: 0.03778094798326492\n",
      "      policy_loss: 0.008357700891792774\n",
      "      total_loss: 10.906514167785645\n",
      "      vf_explained_var: 0.9953206181526184\n",
      "      vf_loss: 10.898158073425293\n",
      "    sample_time_ms: 39639.031\n",
      "    update_time_ms: 13.127\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.2903982725942\n",
      "    rl_1: 135.38221246551376\n",
      "  time_since_restore: 4366.300225973129\n",
      "  time_this_iter_s: 74.01139235496521\n",
      "  time_total_s: 4366.300225973129\n",
      "  timestamp: 1553108239\n",
      "  timesteps_since_restore: 1530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1530000\n",
      "  training_iteration: 153\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4366 s, 153 iter, 1530000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-58-15\n",
      "  done: false\n",
      "  episode_len_mean: 135.3\n",
      "  episode_reward_max: 361.69189765143375\n",
      "  episode_reward_mean: 325.32089945209657\n",
      "  episode_reward_min: 291.4910616063587\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 11005\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6666.477\n",
      "    load_time_ms: 3.579\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.8326110118068755e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3499393463134766\n",
      "      kl: 48.92643356323242\n",
      "      policy_loss: 0.37670376896858215\n",
      "      total_loss: 28.740219116210938\n",
      "      vf_explained_var: 0.992725670337677\n",
      "      vf_loss: 28.3632755279541\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.4549456084367307e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8095680475234985\n",
      "      kl: 0.0798831582069397\n",
      "      policy_loss: 0.014498050324618816\n",
      "      total_loss: 6.492529392242432\n",
      "      vf_explained_var: 0.9961153864860535\n",
      "      vf_loss: 6.478030681610107\n",
      "    sample_time_ms: 42065.814\n",
      "    update_time_ms: 14.876\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.29842977343534\n",
      "    rl_1: 136.02246967866114\n",
      "  time_since_restore: 4421.928354024887\n",
      "  time_this_iter_s: 55.62812805175781\n",
      "  time_total_s: 4421.928354024887\n",
      "  timestamp: 1553108295\n",
      "  timesteps_since_restore: 1540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 154\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4421 s, 154 iter, 1540000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-58-57\n",
      "  done: false\n",
      "  episode_len_mean: 136.05\n",
      "  episode_reward_max: 365.39965761360526\n",
      "  episode_reward_mean: 325.3923051364666\n",
      "  episode_reward_min: 292.1026151221282\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 11078\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6689.416\n",
      "    load_time_ms: 3.586\n",
      "    num_steps_sampled: 1550000\n",
      "    num_steps_trained: 1550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.2489151534682605e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4288119077682495\n",
      "      kl: 149.0294647216797\n",
      "      policy_loss: 0.36122432351112366\n",
      "      total_loss: 28.265853881835938\n",
      "      vf_explained_var: 0.9927458167076111\n",
      "      vf_loss: 27.90355110168457\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.182418412655096e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9100451469421387\n",
      "      kl: 0.04931405559182167\n",
      "      policy_loss: 0.009542658925056458\n",
      "      total_loss: 5.646399021148682\n",
      "      vf_explained_var: 0.9964231848716736\n",
      "      vf_loss: 5.636857032775879\n",
      "    sample_time_ms: 42802.5\n",
      "    update_time_ms: 15.438\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.97006287425765\n",
      "    rl_1: 135.4222422622089\n",
      "  time_since_restore: 4464.166792154312\n",
      "  time_this_iter_s: 42.23843812942505\n",
      "  time_total_s: 4464.166792154312\n",
      "  timestamp: 1553108337\n",
      "  timesteps_since_restore: 1550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1550000\n",
      "  training_iteration: 155\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4464 s, 155 iter, 1550000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_19-59-47\n",
      "  done: false\n",
      "  episode_len_mean: 136.45\n",
      "  episode_reward_max: 367.0318919067178\n",
      "  episode_reward_mean: 325.6188004234411\n",
      "  episode_reward_min: 293.3553321515099\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 11152\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7514.456\n",
      "    load_time_ms: 3.836\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0873373867070768e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4993634223937988\n",
      "      kl: 147.89474487304688\n",
      "      policy_loss: 0.35089150071144104\n",
      "      total_loss: 24.92141342163086\n",
      "      vf_explained_var: 0.9935950040817261\n",
      "      vf_loss: 24.568912506103516\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.2736283128720345e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8976684808731079\n",
      "      kl: 2.4965176582336426\n",
      "      policy_loss: 0.024717018008232117\n",
      "      total_loss: 6.3603596687316895\n",
      "      vf_explained_var: 0.9965725541114807\n",
      "      vf_loss: 6.335643291473389\n",
      "    sample_time_ms: 43463.424\n",
      "    update_time_ms: 15.587\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.57096861248704\n",
      "    rl_1: 136.04783181095402\n",
      "  time_since_restore: 4513.605752944946\n",
      "  time_this_iter_s: 49.438960790634155\n",
      "  time_total_s: 4513.605752944946\n",
      "  timestamp: 1553108387\n",
      "  timesteps_since_restore: 1560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 156\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4513 s, 156 iter, 1560000 ts, 326 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-01-11\n",
      "  done: false\n",
      "  episode_len_mean: 137.4\n",
      "  episode_reward_max: 367.0318919067178\n",
      "  episode_reward_mean: 322.2736766398838\n",
      "  episode_reward_min: 161.77856267656762\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 11224\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8454.65\n",
      "    load_time_ms: 3.842\n",
      "    num_steps_sampled: 1570000\n",
      "    num_steps_trained: 1570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.63100594363641e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.5795691013336182\n",
      "      kl: 1827.6102294921875\n",
      "      policy_loss: 0.28125154972076416\n",
      "      total_loss: 29.69074249267578\n",
      "      vf_explained_var: 0.9923246502876282\n",
      "      vf_loss: 29.379682540893555\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.91044260808593e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9817091226577759\n",
      "      kl: 0.03600699082016945\n",
      "      policy_loss: 0.0024191008415073156\n",
      "      total_loss: 13.28734302520752\n",
      "      vf_explained_var: 0.994111955165863\n",
      "      vf_loss: 13.28492259979248\n",
      "    sample_time_ms: 46283.559\n",
      "    update_time_ms: 15.951\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.08267496433\n",
      "    rl_1: 134.19100167555374\n",
      "  time_since_restore: 4598.223312139511\n",
      "  time_this_iter_s: 84.61755919456482\n",
      "  time_total_s: 4598.223312139511\n",
      "  timestamp: 1553108471\n",
      "  timesteps_since_restore: 1570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1570000\n",
      "  training_iteration: 157\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4598 s, 157 iter, 1570000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-02-17\n",
      "  done: false\n",
      "  episode_len_mean: 137.02\n",
      "  episode_reward_max: 367.048725141883\n",
      "  episode_reward_mean: 322.5613984437673\n",
      "  episode_reward_min: 126.39226595441494\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 11298\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8594.6\n",
      "    load_time_ms: 3.985\n",
      "    num_steps_sampled: 1580000\n",
      "    num_steps_trained: 1580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4465101887471974e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.5039347410202026\n",
      "      kl: 3984.03125\n",
      "      policy_loss: 0.2760457396507263\n",
      "      total_loss: 31.684444427490234\n",
      "      vf_explained_var: 0.992097795009613\n",
      "      vf_loss: 31.31093406677246\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.91044260808593e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9472178816795349\n",
      "      kl: 24.404380798339844\n",
      "      policy_loss: 0.030287541449069977\n",
      "      total_loss: 13.235783576965332\n",
      "      vf_explained_var: 0.993262767791748\n",
      "      vf_loss: 13.205495834350586\n",
      "    sample_time_ms: 47335.562\n",
      "    update_time_ms: 16.84\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 187.82682166014393\n",
      "    rl_1: 134.7345767836233\n",
      "  time_since_restore: 4664.236510276794\n",
      "  time_this_iter_s: 66.01319813728333\n",
      "  time_total_s: 4664.236510276794\n",
      "  timestamp: 1553108537\n",
      "  timesteps_since_restore: 1580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1580000\n",
      "  training_iteration: 158\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4664 s, 158 iter, 1580000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-02-59\n",
      "  done: false\n",
      "  episode_len_mean: 138.63\n",
      "  episode_reward_max: 383.09491621233127\n",
      "  episode_reward_mean: 324.5279475122866\n",
      "  episode_reward_min: 126.39226595441494\n",
      "  episodes_this_iter: 71\n",
      "  episodes_total: 11369\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8460.512\n",
      "    load_time_ms: 4.006\n",
      "    num_steps_sampled: 1590000\n",
      "    num_steps_trained: 1590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.669763827929273e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.5200055837631226\n",
      "      kl: 63.8046989440918\n",
      "      policy_loss: 0.2955513298511505\n",
      "      total_loss: 25.905385971069336\n",
      "      vf_explained_var: 0.9931333661079407\n",
      "      vf_loss: 25.607494354248047\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 7.365663079461626e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9811139106750488\n",
      "      kl: 11.685673713684082\n",
      "      policy_loss: 0.011700384318828583\n",
      "      total_loss: 10.092829704284668\n",
      "      vf_explained_var: 0.9943716526031494\n",
      "      vf_loss: 10.081128120422363\n",
      "    sample_time_ms: 47072.933\n",
      "    update_time_ms: 16.81\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 187.890777175672\n",
      "    rl_1: 136.63717033661453\n",
      "  time_since_restore: 4705.629544973373\n",
      "  time_this_iter_s: 41.39303469657898\n",
      "  time_total_s: 4705.629544973373\n",
      "  timestamp: 1553108579\n",
      "  timesteps_since_restore: 1590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1590000\n",
      "  training_iteration: 159\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4705 s, 159 iter, 1590000 ts, 325 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-03-44\n",
      "  done: false\n",
      "  episode_len_mean: 139.72\n",
      "  episode_reward_max: 383.09491621233127\n",
      "  episode_reward_mean: 326.4128368254589\n",
      "  episode_reward_min: 130.87510551995848\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 11441\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8502.193\n",
      "    load_time_ms: 3.948\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.504645378096029e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.571427822113037\n",
      "      kl: 60.51083755493164\n",
      "      policy_loss: 0.30252689123153687\n",
      "      total_loss: 16.968944549560547\n",
      "      vf_explained_var: 0.9955395460128784\n",
      "      vf_loss: 16.663089752197266\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.1048495451859708e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9836756587028503\n",
      "      kl: 0.05270807072520256\n",
      "      policy_loss: 0.0020465748384594917\n",
      "      total_loss: 10.940248489379883\n",
      "      vf_explained_var: 0.9940295219421387\n",
      "      vf_loss: 10.938200950622559\n",
      "    sample_time_ms: 46962.417\n",
      "    update_time_ms: 16.337\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.87848150140223\n",
      "    rl_1: 137.53435532405666\n",
      "  time_since_restore: 4750.804202795029\n",
      "  time_this_iter_s: 45.17465782165527\n",
      "  time_total_s: 4750.804202795029\n",
      "  timestamp: 1553108624\n",
      "  timesteps_since_restore: 1600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 160\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4750 s, 160 iter, 1600000 ts, 326 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-04-34\n",
      "  done: false\n",
      "  episode_len_mean: 139.34\n",
      "  episode_reward_max: 372.1943984742273\n",
      "  episode_reward_mean: 327.14153604244365\n",
      "  episode_reward_min: 293.96794827707873\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 11513\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8584.769\n",
      "    load_time_ms: 4.245\n",
      "    num_steps_sampled: 1610000\n",
      "    num_steps_trained: 1610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.256970613729209e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.518681287765503\n",
      "      kl: 21.341567993164062\n",
      "      policy_loss: 0.33680596947669983\n",
      "      total_loss: 16.052881240844727\n",
      "      vf_explained_var: 0.9957535862922668\n",
      "      vf_loss: 15.714311599731445\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.6572738736897463e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9485241770744324\n",
      "      kl: 0.5796784162521362\n",
      "      policy_loss: 0.022225698456168175\n",
      "      total_loss: 6.438746452331543\n",
      "      vf_explained_var: 0.9959037899971008\n",
      "      vf_loss: 6.416520118713379\n",
      "    sample_time_ms: 47770.26\n",
      "    update_time_ms: 16.356\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.4884074389216\n",
      "    rl_1: 136.653128603522\n",
      "  time_since_restore: 4801.122562170029\n",
      "  time_this_iter_s: 50.318359375\n",
      "  time_total_s: 4801.122562170029\n",
      "  timestamp: 1553108674\n",
      "  timesteps_since_restore: 1610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1610000\n",
      "  training_iteration: 161\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4801 s, 161 iter, 1610000 ts, 327 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-05-18\n",
      "  done: false\n",
      "  episode_len_mean: 139.52\n",
      "  episode_reward_max: 366.869771681552\n",
      "  episode_reward_mean: 326.2857515565781\n",
      "  episode_reward_min: 149.4319386659484\n",
      "  episodes_this_iter: 70\n",
      "  episodes_total: 11583\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8455.136\n",
      "    load_time_ms: 4.142\n",
      "    num_steps_sampled: 1620000\n",
      "    num_steps_trained: 1620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00012385456648189574\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.6008802652359009\n",
      "      kl: 25.318262100219727\n",
      "      policy_loss: 0.168111190199852\n",
      "      total_loss: 24.417509078979492\n",
      "      vf_explained_var: 0.9940975904464722\n",
      "      vf_loss: 24.246261596679688\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.4859110325792244e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9852880835533142\n",
      "      kl: 0.043531887233257294\n",
      "      policy_loss: 0.00752058532088995\n",
      "      total_loss: 13.404399871826172\n",
      "      vf_explained_var: 0.9939915537834167\n",
      "      vf_loss: 13.396880149841309\n",
      "    sample_time_ms: 46759.503\n",
      "    update_time_ms: 16.225\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.64854066309366\n",
      "    rl_1: 137.63721089348437\n",
      "  time_since_restore: 4844.93097615242\n",
      "  time_this_iter_s: 43.80841398239136\n",
      "  time_total_s: 4844.93097615242\n",
      "  timestamp: 1553108718\n",
      "  timesteps_since_restore: 1620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1620000\n",
      "  training_iteration: 162\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4844 s, 162 iter, 1620000 ts, 326 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-06-08\n",
      "  done: false\n",
      "  episode_len_mean: 140.44\n",
      "  episode_reward_max: 367.27338122133636\n",
      "  episode_reward_mean: 324.05480643228225\n",
      "  episode_reward_min: -140.78317696493392\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 11656\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8076.598\n",
      "    load_time_ms: 4.329\n",
      "    num_steps_sampled: 1630000\n",
      "    num_steps_trained: 1630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00018578185699880123\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.5142759084701538\n",
      "      kl: 77.12279510498047\n",
      "      policy_loss: 0.13415081799030304\n",
      "      total_loss: 110.58843994140625\n",
      "      vf_explained_var: 0.9664265513420105\n",
      "      vf_loss: 110.4399642944336\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.728867881136466e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0048710107803345\n",
      "      kl: 0.023929668590426445\n",
      "      policy_loss: -0.002743422519415617\n",
      "      total_loss: 48.13610076904297\n",
      "      vf_explained_var: 0.9748626947402954\n",
      "      vf_loss: 48.13884735107422\n",
      "    sample_time_ms: 44721.12\n",
      "    update_time_ms: 15.522\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 187.77292210474337\n",
      "    rl_1: 136.28188432753893\n",
      "  time_since_restore: 4894.755047082901\n",
      "  time_this_iter_s: 49.82407093048096\n",
      "  time_total_s: 4894.755047082901\n",
      "  timestamp: 1553108768\n",
      "  timesteps_since_restore: 1630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1630000\n",
      "  training_iteration: 163\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4894 s, 163 iter, 1630000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-07-08\n",
      "  done: false\n",
      "  episode_len_mean: 139.01\n",
      "  episode_reward_max: 373.34109011565835\n",
      "  episode_reward_mean: 323.64441366806176\n",
      "  episode_reward_min: -140.78317696493392\n",
      "  episodes_this_iter: 71\n",
      "  episodes_total: 11727\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8256.086\n",
      "    load_time_ms: 4.343\n",
      "    num_steps_sampled: 1640000\n",
      "    num_steps_trained: 1640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00027867266908288\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.555202841758728\n",
      "      kl: 25.812259674072266\n",
      "      policy_loss: 0.28457579016685486\n",
      "      total_loss: 13.653935432434082\n",
      "      vf_explained_var: 0.9965279698371887\n",
      "      vf_loss: 13.362165451049805\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 3.728867881136466e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9643874764442444\n",
      "      kl: 356.95220947265625\n",
      "      policy_loss: 0.06345418840646744\n",
      "      total_loss: 13.797449111938477\n",
      "      vf_explained_var: 0.993748664855957\n",
      "      vf_loss: 13.733993530273438\n",
      "    sample_time_ms: 44976.254\n",
      "    update_time_ms: 14.482\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.6750624723853\n",
      "    rl_1: 134.96935119567647\n",
      "  time_since_restore: 4954.717412948608\n",
      "  time_this_iter_s: 59.9623658657074\n",
      "  time_total_s: 4954.717412948608\n",
      "  timestamp: 1553108828\n",
      "  timesteps_since_restore: 1640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1640000\n",
      "  training_iteration: 164\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 4954 s, 164 iter, 1640000 ts, 324 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-07-56\n",
      "  done: false\n",
      "  episode_len_mean: 138.82\n",
      "  episode_reward_max: 367.7990810222922\n",
      "  episode_reward_mean: 323.2182565767242\n",
      "  episode_reward_min: 292.6989462584217\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 11799\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8472.181\n",
      "    load_time_ms: 4.397\n",
      "    num_steps_sampled: 1650000\n",
      "    num_steps_trained: 1650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00041800900362432003\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.5089560747146606\n",
      "      kl: 0.06328733265399933\n",
      "      policy_loss: 0.020327607169747353\n",
      "      total_loss: 10.427841186523438\n",
      "      vf_explained_var: 0.9971020221710205\n",
      "      vf_loss: 10.407485961914062\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 5.5933009335262796e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.146907925605774\n",
      "      kl: 128.35755920410156\n",
      "      policy_loss: 0.04304537549614906\n",
      "      total_loss: 7.734753608703613\n",
      "      vf_explained_var: 0.9952380061149597\n",
      "      vf_loss: 7.691707611083984\n",
      "    sample_time_ms: 45279.997\n",
      "    update_time_ms: 14.49\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.33211059300305\n",
      "    rl_1: 133.88614598372112\n",
      "  time_since_restore: 5002.153913497925\n",
      "  time_this_iter_s: 47.436500549316406\n",
      "  time_total_s: 5002.153913497925\n",
      "  timestamp: 1553108876\n",
      "  timesteps_since_restore: 1650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1650000\n",
      "  training_iteration: 165\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 5002 s, 165 iter, 1650000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-08-26\n",
      "  done: false\n",
      "  episode_len_mean: 137.89\n",
      "  episode_reward_max: 372.4601350510491\n",
      "  episode_reward_mean: 327.6486103004407\n",
      "  episode_reward_min: 293.4867375845707\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 11872\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7507.203\n",
      "    load_time_ms: 4.113\n",
      "    num_steps_sampled: 1660000\n",
      "    num_steps_trained: 1660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0006270137382671237\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3640145063400269\n",
      "      kl: 2.46799635887146\n",
      "      policy_loss: 0.0170731283724308\n",
      "      total_loss: 12.732128143310547\n",
      "      vf_explained_var: 0.9966305494308472\n",
      "      vf_loss: 12.713506698608398\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 8.38995184437863e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2210962772369385\n",
      "      kl: 74.71367645263672\n",
      "      policy_loss: -0.008038041181862354\n",
      "      total_loss: 12.578990936279297\n",
      "      vf_explained_var: 0.9934628009796143\n",
      "      vf_loss: 12.587029457092285\n",
      "    sample_time_ms: 44308.91\n",
      "    update_time_ms: 14.245\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 191.15452932778217\n",
      "    rl_1: 136.49408097265848\n",
      "  time_since_restore: 5032.210757493973\n",
      "  time_this_iter_s: 30.056843996047974\n",
      "  time_total_s: 5032.210757493973\n",
      "  timestamp: 1553108906\n",
      "  timesteps_since_restore: 1660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1660000\n",
      "  training_iteration: 166\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 5032 s, 166 iter, 1660000 ts, 328 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-08-52\n",
      "  done: false\n",
      "  episode_len_mean: 137.69\n",
      "  episode_reward_max: 371.0607216465699\n",
      "  episode_reward_mean: 327.49236375619864\n",
      "  episode_reward_min: 288.1923992559058\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 11944\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6263.423\n",
      "    load_time_ms: 3.779\n",
      "    num_steps_sampled: 1670000\n",
      "    num_steps_trained: 1670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0009405207238160074\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.482796311378479\n",
      "      kl: 0.491472989320755\n",
      "      policy_loss: 0.032543398439884186\n",
      "      total_loss: 8.246466636657715\n",
      "      vf_explained_var: 0.9976872205734253\n",
      "      vf_loss: 8.213459968566895\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.2584924213854265e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1485776901245117\n",
      "      kl: 0.980372965335846\n",
      "      policy_loss: 0.000476744316983968\n",
      "      total_loss: 7.609363079071045\n",
      "      vf_explained_var: 0.9951731562614441\n",
      "      vf_loss: 7.608885765075684\n",
      "    sample_time_ms: 39705.884\n",
      "    update_time_ms: 12.749\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.82482173082747\n",
      "    rl_1: 136.66754202537123\n",
      "  time_since_restore: 5058.3071212768555\n",
      "  time_this_iter_s: 26.09636378288269\n",
      "  time_total_s: 5058.3071212768555\n",
      "  timestamp: 1553108932\n",
      "  timesteps_since_restore: 1670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1670000\n",
      "  training_iteration: 167\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 5058 s, 167 iter, 1670000 ts, 327 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-09-18\n",
      "  done: false\n",
      "  episode_len_mean: 137.49\n",
      "  episode_reward_max: 360.75005843525025\n",
      "  episode_reward_mean: 322.34409763365323\n",
      "  episode_reward_min: 172.93691200021266\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 12017\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5934.845\n",
      "    load_time_ms: 3.605\n",
      "    num_steps_sampled: 1680000\n",
      "    num_steps_trained: 1680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0014107807073742151\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4840155839920044\n",
      "      kl: 0.04644385725259781\n",
      "      policy_loss: 0.01023581251502037\n",
      "      total_loss: 16.548065185546875\n",
      "      vf_explained_var: 0.9961212277412415\n",
      "      vf_loss: 16.537761688232422\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8877386764870607e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1442137956619263\n",
      "      kl: 0.02297958731651306\n",
      "      policy_loss: 0.0029269878286868334\n",
      "      total_loss: 13.723014831542969\n",
      "      vf_explained_var: 0.9940938949584961\n",
      "      vf_loss: 13.720088005065918\n",
      "    sample_time_ms: 36050.118\n",
      "    update_time_ms: 11.086\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 187.60040773882585\n",
      "    rl_1: 134.7436898948273\n",
      "  time_since_restore: 5084.452714204788\n",
      "  time_this_iter_s: 26.14559292793274\n",
      "  time_total_s: 5084.452714204788\n",
      "  timestamp: 1553108958\n",
      "  timesteps_since_restore: 1680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1680000\n",
      "  training_iteration: 168\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 5084 s, 168 iter, 1680000 ts, 322 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-09-45\n",
      "  done: false\n",
      "  episode_len_mean: 136.76\n",
      "  episode_reward_max: 363.64913238076105\n",
      "  episode_reward_mean: 322.9488933323588\n",
      "  episode_reward_min: 172.93691200021266\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 12090\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5811.808\n",
      "    load_time_ms: 3.47\n",
      "    num_steps_sampled: 1690000\n",
      "    num_steps_trained: 1690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0021161711774766445\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.409257173538208\n",
      "      kl: 0.03933471441268921\n",
      "      policy_loss: 0.019283514469861984\n",
      "      total_loss: 6.432026386260986\n",
      "      vf_explained_var: 0.9981878995895386\n",
      "      vf_loss: 6.412659645080566\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8877386764870607e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0491276979446411\n",
      "      kl: 0.020512891933321953\n",
      "      policy_loss: 0.0015735799679532647\n",
      "      total_loss: 6.462813377380371\n",
      "      vf_explained_var: 0.9959907531738281\n",
      "      vf_loss: 6.461240291595459\n",
      "    sample_time_ms: 34676.634\n",
      "    update_time_ms: 11.078\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 188.85247197557572\n",
      "    rl_1: 134.09642135678303\n",
      "  time_since_restore: 5110.877132892609\n",
      "  time_this_iter_s: 26.424418687820435\n",
      "  time_total_s: 5110.877132892609\n",
      "  timestamp: 1553108985\n",
      "  timesteps_since_restore: 1690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1690000\n",
      "  training_iteration: 169\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 5110 s, 169 iter, 1690000 ts, 323 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-10-11\n",
      "  done: false\n",
      "  episode_len_mean: 136.6\n",
      "  episode_reward_max: 380.4360744755644\n",
      "  episode_reward_mean: 326.44772270785273\n",
      "  episode_reward_min: 294.8658171489943\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 12163\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5417.15\n",
      "    load_time_ms: 3.324\n",
      "    num_steps_sampled: 1700000\n",
      "    num_steps_trained: 1700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0021161711774766445\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4829151630401611\n",
      "      kl: 0.058438241481781006\n",
      "      policy_loss: 0.01931939460337162\n",
      "      total_loss: 5.510708332061768\n",
      "      vf_explained_var: 0.9984333515167236\n",
      "      vf_loss: 5.491265296936035\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 1.8877386764870607e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1150157451629639\n",
      "      kl: 13.821828842163086\n",
      "      policy_loss: 0.015156994573771954\n",
      "      total_loss: 6.70307731628418\n",
      "      vf_explained_var: 0.9959881901741028\n",
      "      vf_loss: 6.68792200088501\n",
      "    sample_time_ms: 33160.484\n",
      "    update_time_ms: 11.421\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 189.68042188994386\n",
      "    rl_1: 136.76730081790888\n",
      "  time_since_restore: 5136.940749168396\n",
      "  time_this_iter_s: 26.063616275787354\n",
      "  time_total_s: 5136.940749168396\n",
      "  timestamp: 1553109011\n",
      "  timesteps_since_restore: 1700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1700000\n",
      "  training_iteration: 170\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 5136 s, 170 iter, 1700000 ts, 326 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-10-37\n",
      "  done: false\n",
      "  episode_len_mean: 137.55\n",
      "  episode_reward_max: 380.4360744755644\n",
      "  episode_reward_mean: 328.6792035878301\n",
      "  episode_reward_min: 292.4084407450593\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 12236\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5170.529\n",
      "    load_time_ms: 3.131\n",
      "    num_steps_sampled: 1710000\n",
      "    num_steps_trained: 1710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031742567662149668\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4093267917633057\n",
      "      kl: 0.4360252916812897\n",
      "      policy_loss: 0.04570325091481209\n",
      "      total_loss: 5.695718765258789\n",
      "      vf_explained_var: 0.9983903169631958\n",
      "      vf_loss: 5.648630619049072\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 2.8316074818235393e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1275441646575928\n",
      "      kl: 35.550235748291016\n",
      "      policy_loss: 0.030339907854795456\n",
      "      total_loss: 7.782416820526123\n",
      "      vf_explained_var: 0.9952938556671143\n",
      "      vf_loss: 7.752076625823975\n",
      "    sample_time_ms: 31021.021\n",
      "    update_time_ms: 11.064\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.77185622182827\n",
      "    rl_1: 137.90734736600186\n",
      "  time_since_restore: 5163.387147188187\n",
      "  time_this_iter_s: 26.44639801979065\n",
      "  time_total_s: 5163.387147188187\n",
      "  timestamp: 1553109037\n",
      "  timesteps_since_restore: 1710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1710000\n",
      "  training_iteration: 171\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 5163 s, 171 iter, 1710000 ts, 329 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-03-20_20-11-06\n",
      "  done: false\n",
      "  episode_len_mean: 138.49\n",
      "  episode_reward_max: 363.9505475773587\n",
      "  episode_reward_mean: 327.3420470999497\n",
      "  episode_reward_min: 295.1473940270921\n",
      "  episodes_this_iter: 72\n",
      "  episodes_total: 12308\n",
      "  experiment_id: d4702f73002545e9a3fbdb44c083d921\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5081.485\n",
      "    load_time_ms: 3.082\n",
      "    num_steps_sampled: 1720000\n",
      "    num_steps_trained: 1720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.004761385731399059\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4799929857254028\n",
      "      kl: 2.3707468509674072\n",
      "      policy_loss: 0.07062294334173203\n",
      "      total_loss: 4.68271017074585\n",
      "      vf_explained_var: 0.9986810684204102\n",
      "      vf_loss: 4.600798606872559\n",
      "    rl_1:\n",
      "      cur_kl_coeff: 4.247411666824519e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2659525871276855\n",
      "      kl: 13.221657752990723\n",
      "      policy_loss: 0.03672413155436516\n",
      "      total_loss: 7.711525917053223\n",
      "      vf_explained_var: 0.9952718019485474\n",
      "      vf_loss: 7.674802303314209\n",
      "    sample_time_ms: 29579.501\n",
      "    update_time_ms: 11.06\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.2.102\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 5225\n",
      "  policy_reward_mean:\n",
      "    rl_0: 190.46550729564896\n",
      "    rl_1: 136.87653980430068\n",
      "  time_since_restore: 5191.887795209885\n",
      "  time_this_iter_s: 28.500648021697998\n",
      "  time_total_s: 5191.887795209885\n",
      "  timestamp: 1553109066\n",
      "  timesteps_since_restore: 1720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1720000\n",
      "  training_iteration: 172\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv-v0_0:\tRUNNING [pid=5225], 5191 s, 172 iter, 1720000 ts, 327 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,  # RL algorithm to run\n",
    "        \"env\": gym_name,  # environment name generated earlier\n",
    "        \"config\": {  # configuration params (must match \"run\" value)\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1000,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow_2)",
   "language": "python",
   "name": "flow_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
