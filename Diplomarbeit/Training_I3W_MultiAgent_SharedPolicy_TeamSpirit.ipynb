{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING I3W\n",
    "\n",
    "\n",
    "# A) Create Envorinment, Vehicles etc\n",
    "\n",
    "### General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scenarios:\n",
      "['Scenario', 'BayBridgeScenario', 'BayBridgeTollScenario', 'BottleneckScenario', 'Figure8Scenario', 'SimpleGridScenario', 'HighwayScenario', 'LoopScenario', 'MergeScenario', 'TwoLoopsOneMergingScenario', 'MultiLoopScenario', 'IntersectionScenarioTW']\n",
      "\n",
      "Available environments:\n",
      "['MultiEnv', 'MultiAgentAccelEnv', 'MultiWaveAttenuationPOEnv', 'MultiAgentIntersectionEnv', 'MultiAgentTeamSpiritIntersectionEnv', 'MultiAgentIntersectionEnv_baseline_1', 'MultiAgentIntersectionEnv_baseline_2', 'MultiAgentIntersectionEnv_baseline_3', 'MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit']\n"
     ]
    }
   ],
   "source": [
    "# Define horizon as a variable to ensure consistent use across notebook (length of one rollout)\n",
    "HORIZON=500                                 #103 max Horizon, wenn es vor verlassen abbrechen soll!, default war 500\n",
    "\n",
    "# name of the experiment\n",
    "experiment_name = \"IntersectionExample\"\n",
    "\n",
    "# scenario class\n",
    "import flow.scenarios as scenarios\n",
    "print(\"Available scenarios:\")\n",
    "print(scenarios.__all__)\n",
    "scenario_name = \"IntersectionTWScenario\"\n",
    "\n",
    "# environment class\n",
    "import flow.multiagent_envs as flowenvs\n",
    "print(\"\\nAvailable environments:\")\n",
    "print(flowenvs.__all__)\n",
    "env_name = \"MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "from flow.scenarios.intersection import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "additionalNetParams = {\n",
    "            \"edge_length\": 40,\n",
    "            \"lanes\": 1,\n",
    "            \"speed_limit\": 30\n",
    "        }\n",
    "\n",
    "net_params = NetParams( no_internal_links=False,                  #default: True   !! damit Kreuzungen nicht Ã¼berspr. werden\n",
    "                        inflows=None,                             #default: None\n",
    "                        osm_path=None,                            #default: None\n",
    "                        netfile=None,                             #default: None\n",
    "                        additional_params=additionalNetParams     #default: None   !!\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InitialConfig Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "\n",
    "initial_config = InitialConfig( shuffle=True,                            #default: False         !!\n",
    "                                spacing=\"custom\",                        #default: \"uniform\"     !!\n",
    "                                min_gap=10,                              #default: 0\n",
    "                                perturbation=29.99,                      #default: 0.0            !!        \n",
    "                                x0=0,                                    #default: 0\n",
    "                                bunching=0,                              #default: 0\n",
    "                                lanes_distribution=float(\"inf\"),         #default: float(\"inf\")\n",
    "                                edges_distribution=\"all\",                #default: \"all\"\n",
    "                                additional_params=None )                 #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams( port = None,                  #default: None\n",
    "                          sim_step=0.1,                 #default: 0.1\n",
    "                          emission_path=None,           #default: None\n",
    "                          lateral_resolution=None,      #default: None\n",
    "                          no_step_log=True,             #default: True\n",
    "                          render=False,                 #default: False\n",
    "                          save_render=False,            #default: False\n",
    "                          sight_radius=25,              #default: 25\n",
    "                          show_radius=False,            #default: False\n",
    "                          pxpm=2,                       #default: 2\n",
    "                          overtake_right=False,         #default: False    \n",
    "                          seed=None,                    #default: None\n",
    "                          restart_instance=False,       #default: False\n",
    "                          print_warnings=True,          #default: True\n",
    "                          teleport_time=-1,             #default: -1\n",
    "                          num_clients=1,                #default: 1\n",
    "                          sumo_binary=None )            #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "additionalEnvParams = {\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 3,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 3,\n",
    "        # desired velocity for all vehicles in the network, in m/s\n",
    "        \"target_velocity\": 30,\n",
    "        # initial teamspirit\n",
    "        \"ap_teamspirit_0\": -1,\n",
    "        \"ap_teamspirit_1\": -1,\n",
    "        # shuffle teamspirit?\n",
    "        \"ap_teamspirit_shuffle\": False    \n",
    "    }\n",
    "\n",
    "env_params = EnvParams( additional_params=additionalEnvParams, #default: None    !!\n",
    "                        horizon=HORIZON,                       #default: 500     !!\n",
    "                        warmup_steps=0,                        #default: 0       \n",
    "                        sims_per_step=1,                       #default: 1\n",
    "                        evaluate=False )                       #default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# import vehicles dynamics models\n",
    "#from flow.controllers import SumoCarFollowingController\n",
    "from flow.controllers import ContinuousRouter\n",
    "#from flow.controllers.lane_change_controllers import SumoLaneChangeController\n",
    "from flow.controllers.lane_change_controllers import StaticLaneChanger\n",
    "from flow.controllers import RLController\n",
    "from flow.core.params import SumoLaneChangeParams\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "from random import *\n",
    "\n",
    "vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RL-Agent controlled vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car following parameters, default: None\n",
    "cf_parameter = SumoCarFollowingParams(\n",
    "                speed_mode=\"aggressive\")\n",
    "# lane change parameters, default: None\n",
    "lc_parameter =  None\n",
    "\n",
    "vehicles.add( # name of the vehicle\n",
    "                veh_id = \"rl\",\n",
    "              # acceleration controller, default: (SumoCarFollowingController, {})\n",
    "                acceleration_controller=(RLController, {}),\n",
    "              # lane_change_controller, default: (SumoLaneChangeController, {})\n",
    "                lane_change_controller=(StaticLaneChanger,{}),\n",
    "              # routing controller, default: None\n",
    "                routing_controller=(ContinuousRouter, {}),\n",
    "              # initial speed, default: 0\n",
    "                initial_speed=0,\n",
    "              # number of vehicles, default: 1 \n",
    "                num_vehicles=2,\n",
    "                \n",
    "                car_following_params=cf_parameter\n",
    "              # speed mode, default: \"right_of_way\"\n",
    "                #speed_mode=\"aggressive\",\n",
    "              # lane change mode, default: \"no_lat_collide\"\n",
    "                #lane_change_mode=\"aggressive\", \n",
    "              # car following parameter, default: None\n",
    "                #sumo_car_following_params=cf_parameter,\n",
    "              # lane change parameter, default: None\n",
    "                #sumo_lc_params=lc_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict( # name of the experiment\n",
    "                      exp_tag=experiment_name,\n",
    "                    # name of the flow environment the experiment is running on\n",
    "                      env_name=env_name,\n",
    "                    # name of the scenario class the experiment uses\n",
    "                      scenario=scenario_name,\n",
    "                    # simulator that is used by the experiment\n",
    "                      simulator='traci',\n",
    "                    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "                      sim=sumo_params,\n",
    "                    # environment related parameters (see flow.core.params.EnvParams)\n",
    "                      env=env_params,\n",
    "                    # network-related parameters (see flow.core.params.NetParams and\n",
    "                    # the scenario's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "                      net=net_params,\n",
    "                    # vehicles to be placed in the network at the start of a rollout \n",
    "                    # (see flow.core.vehicles.Vehicles)\n",
    "                      veh=vehicles,\n",
    "                   # (optional) parameters affecting the positioning of vehicles upon \n",
    "                   # initialization/reset (see flow.core.params.InitialConfig)\n",
    "                      initial=initial_config\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-04-09_13-11-00_30240/logs.\n",
      "Waiting for redis server at 127.0.0.1:24259 to respond...\n",
      "Waiting for redis server at 127.0.0.1:62896 to respond...\n",
      "Starting the Plasma object store with 6.554658406 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=1c71038f05007dc71caeb68d84db6ade856d9270dd1ec1a0\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.16.123.117',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-04-09_13-11-00_30240/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-04-09_13-11-00_30240/sockets/raylet'],\n",
       " 'redis_address': '172.16.123.117:24259',\n",
       " 'webui_url': 'http://localhost:8889/notebooks/ray_ui.ipynb?token=1c71038f05007dc71caeb68d84db6ade856d9270dd1ec1a0'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "\n",
    "ray.init(redirect_output=True, num_cpus=N_CPUS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate default 0.999\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [100, 50, 25]})  # size of hidden layers in network defaule 64 32\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "#config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "#config[\"sample_batch_size\"] = config[\"train_batch_size\"]/config[\"num_workers\"] # 200 default, trotzdem zu hoch?\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 52973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Teamspirit:\n",
      "0.976419485470094\n",
      "-0.4442784097254695\n"
     ]
    }
   ],
   "source": [
    "# multi agent policy mapping\n",
    "test_env = create_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "\n",
    "def gen_policy():\n",
    "    return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "# Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "policy_graphs = {'rl_0': gen_policy()}\n",
    "    \n",
    "def policy_mapping_fn(agent_id):\n",
    "    return 'rl_0'\n",
    "\n",
    "config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn),\n",
    "            'policies_to_train': ['rl_0']\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "\n",
      "Created LogSyncer for /home/thorsten/ray_results/IntersectionExample/PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0_2019-04-09_13-11-02cdaml41i -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-11-57\n",
      "  done: false\n",
      "  episode_len_mean: 485.95\n",
      "  episode_reward_max: 123.0041521056342\n",
      "  episode_reward_mean: -8.499962313751194\n",
      "  episode_reward_min: -200.90302216972958\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6407.958\n",
      "    load_time_ms: 54.135\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.20000004768371582\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4233288764953613\n",
      "      kl: 0.0008070566109381616\n",
      "      policy_loss: -0.0017539882101118565\n",
      "      total_loss: 51.7591438293457\n",
      "      vf_explained_var: 0.10599080473184586\n",
      "      vf_loss: 51.76073455810547\n",
      "    sample_time_ms: 20352.314\n",
      "    update_time_ms: 586.967\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -4.249981156875593\n",
      "  time_since_restore: 27.47398614883423\n",
      "  time_this_iter_s: 27.47398614883423\n",
      "  time_total_s: 27.47398614883423\n",
      "  timestamp: 1554808317\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 27 s, 1 iter, 10000 ts, -8.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-12-24\n",
      "  done: false\n",
      "  episode_len_mean: 466.23809523809524\n",
      "  episode_reward_max: 123.0041521056342\n",
      "  episode_reward_mean: -32.22888522573097\n",
      "  episode_reward_min: -212.94198867973563\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 42\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5775.597\n",
      "    load_time_ms: 27.781\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.10000002384185791\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4183303117752075\n",
      "      kl: 0.0026181384455412626\n",
      "      policy_loss: -0.0022669066675007343\n",
      "      total_loss: 109.3974609375\n",
      "      vf_explained_var: 0.14223776757717133\n",
      "      vf_loss: 109.39945220947266\n",
      "    sample_time_ms: 21332.533\n",
      "    update_time_ms: 297.843\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -16.114442612865485\n",
      "  time_since_restore: 54.95775604248047\n",
      "  time_this_iter_s: 27.48376989364624\n",
      "  time_total_s: 54.95775604248047\n",
      "  timestamp: 1554808344\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 54 s, 2 iter, 20000 ts, -32.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-12-52\n",
      "  done: false\n",
      "  episode_len_mean: 446.04545454545456\n",
      "  episode_reward_max: 145.03232374845246\n",
      "  episode_reward_mean: -39.6564319836885\n",
      "  episode_reward_min: -212.94198867973563\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 66\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5369.552\n",
      "    load_time_ms: 19.028\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.050000011920928955\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4095711708068848\n",
      "      kl: 0.006126821972429752\n",
      "      policy_loss: -0.003037484595552087\n",
      "      total_loss: 183.893798828125\n",
      "      vf_explained_var: 0.14314278960227966\n",
      "      vf_loss: 183.8965301513672\n",
      "    sample_time_ms: 21701.448\n",
      "    update_time_ms: 200.417\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -19.82821599184425\n",
      "  time_since_restore: 81.97865891456604\n",
      "  time_this_iter_s: 27.02090287208557\n",
      "  time_total_s: 81.97865891456604\n",
      "  timestamp: 1554808372\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 81 s, 3 iter, 30000 ts, -39.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-13-19\n",
      "  done: false\n",
      "  episode_len_mean: 422.02127659574467\n",
      "  episode_reward_max: 145.03232374845246\n",
      "  episode_reward_mean: -51.57156180840915\n",
      "  episode_reward_min: -212.94198867973563\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 94\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5095.594\n",
      "    load_time_ms: 14.877\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.025000005960464478\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4060282707214355\n",
      "      kl: 0.006029496900737286\n",
      "      policy_loss: -0.0030528700444847345\n",
      "      total_loss: 227.60992431640625\n",
      "      vf_explained_var: 0.31428438425064087\n",
      "      vf_loss: 227.61282348632812\n",
      "    sample_time_ms: 22108.38\n",
      "    update_time_ms: 151.736\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -25.78578090420456\n",
      "  time_since_restore: 109.60885882377625\n",
      "  time_this_iter_s: 27.630199909210205\n",
      "  time_total_s: 109.60885882377625\n",
      "  timestamp: 1554808399\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 109 s, 4 iter, 40000 ts, -51.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-13-45\n",
      "  done: false\n",
      "  episode_len_mean: 373.68\n",
      "  episode_reward_max: 145.03232374845246\n",
      "  episode_reward_mean: -50.67686437721194\n",
      "  episode_reward_min: -212.94198867973563\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 126\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4930.521\n",
      "    load_time_ms: 12.172\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.012500002980232239\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3864383697509766\n",
      "      kl: 0.010883291251957417\n",
      "      policy_loss: -0.0033636880107223988\n",
      "      total_loss: 260.7073974609375\n",
      "      vf_explained_var: 0.2709121108055115\n",
      "      vf_loss: 260.71063232421875\n",
      "    sample_time_ms: 21918.929\n",
      "    update_time_ms: 122.492\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -25.338432188605957\n",
      "  time_since_restore: 135.0646378993988\n",
      "  time_this_iter_s: 25.45577907562256\n",
      "  time_total_s: 135.0646378993988\n",
      "  timestamp: 1554808425\n",
      "  timesteps_since_restore: 50000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 5\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 135 s, 5 iter, 50000 ts, -50.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-14-11\n",
      "  done: false\n",
      "  episode_len_mean: 297.34\n",
      "  episode_reward_max: 157.88026843530776\n",
      "  episode_reward_mean: -50.07914276241572\n",
      "  episode_reward_min: -212.38164665177027\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 167\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4877.544\n",
      "    load_time_ms: 10.471\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.012500002980232239\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.380710482597351\n",
      "      kl: 0.008414514362812042\n",
      "      policy_loss: -0.002822562586516142\n",
      "      total_loss: 448.85748291015625\n",
      "      vf_explained_var: 0.30932188034057617\n",
      "      vf_loss: 448.86016845703125\n",
      "    sample_time_ms: 21878.048\n",
      "    update_time_ms: 102.884\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -25.03957138120786\n",
      "  time_since_restore: 161.37684178352356\n",
      "  time_this_iter_s: 26.312203884124756\n",
      "  time_total_s: 161.37684178352356\n",
      "  timestamp: 1554808451\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 161 s, 6 iter, 60000 ts, -50.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-14-36\n",
      "  done: false\n",
      "  episode_len_mean: 227.39\n",
      "  episode_reward_max: 157.88026843530776\n",
      "  episode_reward_mean: -54.41776850779584\n",
      "  episode_reward_min: -212.38164665177027\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 217\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4779.989\n",
      "    load_time_ms: 9.167\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.006250001490116119\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.383337378501892\n",
      "      kl: 0.007434678263962269\n",
      "      policy_loss: -0.0020860556978732347\n",
      "      total_loss: 528.5306396484375\n",
      "      vf_explained_var: 0.3559248149394989\n",
      "      vf_loss: 528.53271484375\n",
      "    sample_time_ms: 21706.737\n",
      "    update_time_ms: 88.936\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -27.20888425389792\n",
      "  time_since_restore: 186.27342343330383\n",
      "  time_this_iter_s: 24.896581649780273\n",
      "  time_total_s: 186.27342343330383\n",
      "  timestamp: 1554808476\n",
      "  timesteps_since_restore: 70000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 186 s, 7 iter, 70000 ts, -54.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-15-01\n",
      "  done: false\n",
      "  episode_len_mean: 201.94\n",
      "  episode_reward_max: 132.27795643782466\n",
      "  episode_reward_mean: -48.18022049683726\n",
      "  episode_reward_min: -211.43873573865267\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 267\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4702.266\n",
      "    load_time_ms: 8.235\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031250007450580597\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3817377090454102\n",
      "      kl: 0.005358195398002863\n",
      "      policy_loss: -0.0018956523854285479\n",
      "      total_loss: 556.4226684570312\n",
      "      vf_explained_var: 0.3770095705986023\n",
      "      vf_loss: 556.424560546875\n",
      "    sample_time_ms: 21543.367\n",
      "    update_time_ms: 78.359\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -24.090110248418622\n",
      "  time_since_restore: 210.85710954666138\n",
      "  time_this_iter_s: 24.583686113357544\n",
      "  time_total_s: 210.85710954666138\n",
      "  timestamp: 1554808501\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 210 s, 8 iter, 80000 ts, -48.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-15-25\n",
      "  done: false\n",
      "  episode_len_mean: 186.36\n",
      "  episode_reward_max: 122.58905587066599\n",
      "  episode_reward_mean: -47.306093749149916\n",
      "  episode_reward_min: -210.74163418412127\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 323\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4649.273\n",
      "    load_time_ms: 7.489\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0015625003725290298\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3719691038131714\n",
      "      kl: 0.008195644244551659\n",
      "      policy_loss: -0.0030661923810839653\n",
      "      total_loss: 656.5288696289062\n",
      "      vf_explained_var: 0.3790294826030731\n",
      "      vf_loss: 656.5318603515625\n",
      "    sample_time_ms: 21413.285\n",
      "    update_time_ms: 70.447\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -23.653046874574958\n",
      "  time_since_restore: 235.48394751548767\n",
      "  time_this_iter_s: 24.626837968826294\n",
      "  time_total_s: 235.48394751548767\n",
      "  timestamp: 1554808525\n",
      "  timesteps_since_restore: 90000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 9\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 235 s, 9 iter, 90000 ts, -47.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-15-52\n",
      "  done: false\n",
      "  episode_len_mean: 169.58\n",
      "  episode_reward_max: 106.33157897953784\n",
      "  episode_reward_mean: -55.52900732583796\n",
      "  episode_reward_min: -213.2367595468922\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 383\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4642.131\n",
      "    load_time_ms: 6.917\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0007812501862645149\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3677352666854858\n",
      "      kl: 0.007755328435450792\n",
      "      policy_loss: -0.0019807612989097834\n",
      "      total_loss: 657.2618408203125\n",
      "      vf_explained_var: 0.3767036199569702\n",
      "      vf_loss: 657.2637939453125\n",
      "    sample_time_ms: 21431.64\n",
      "    update_time_ms: 63.958\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -27.76450366291898\n",
      "  time_since_restore: 261.68231105804443\n",
      "  time_this_iter_s: 26.198363542556763\n",
      "  time_total_s: 261.68231105804443\n",
      "  timestamp: 1554808552\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 261 s, 10 iter, 100000 ts, -55.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-16-21\n",
      "  done: false\n",
      "  episode_len_mean: 161.61\n",
      "  episode_reward_max: 100.78826665440077\n",
      "  episode_reward_mean: -52.440004880118934\n",
      "  episode_reward_min: -208.23114062178718\n",
      "  episodes_this_iter: 64\n",
      "  episodes_total: 447\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4445.614\n",
      "    load_time_ms: 1.73\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00039062509313225746\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3647818565368652\n",
      "      kl: 0.00386752188205719\n",
      "      policy_loss: -0.0005341600044630468\n",
      "      total_loss: 734.5462646484375\n",
      "      vf_explained_var: 0.3651774525642395\n",
      "      vf_loss: 734.5467529296875\n",
      "    sample_time_ms: 21887.172\n",
      "    update_time_ms: 5.91\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -26.220002440059474\n",
      "  time_since_restore: 291.06344175338745\n",
      "  time_this_iter_s: 29.381130695343018\n",
      "  time_total_s: 291.06344175338745\n",
      "  timestamp: 1554808581\n",
      "  timesteps_since_restore: 110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 11\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 291 s, 11 iter, 110000 ts, -52.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-16-48\n",
      "  done: false\n",
      "  episode_len_mean: 166.75\n",
      "  episode_reward_max: 87.01025356992736\n",
      "  episode_reward_mean: -42.394484607244785\n",
      "  episode_reward_min: -206.2418071304367\n",
      "  episodes_this_iter: 58\n",
      "  episodes_total: 505\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4402.868\n",
      "    load_time_ms: 1.738\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00019531254656612873\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3685646057128906\n",
      "      kl: 0.005091942846775055\n",
      "      policy_loss: -0.0008750286651775241\n",
      "      total_loss: 712.6026611328125\n",
      "      vf_explained_var: 0.4160993993282318\n",
      "      vf_loss: 712.6035766601562\n",
      "    sample_time_ms: 21925.715\n",
      "    update_time_ms: 5.985\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -21.19724230362239\n",
      "  time_since_restore: 318.5089681148529\n",
      "  time_this_iter_s: 27.445526361465454\n",
      "  time_total_s: 318.5089681148529\n",
      "  timestamp: 1554808608\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 318 s, 12 iter, 120000 ts, -42.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-17-16\n",
      "  done: false\n",
      "  episode_len_mean: 159.83\n",
      "  episode_reward_max: 80.85883310207157\n",
      "  episode_reward_mean: -59.070067515164936\n",
      "  episode_reward_min: -208.29209884730744\n",
      "  episodes_this_iter: 65\n",
      "  episodes_total: 570\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4368.006\n",
      "    load_time_ms: 1.741\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.765627328306437e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3616195917129517\n",
      "      kl: 0.00641414150595665\n",
      "      policy_loss: -0.0026010721921920776\n",
      "      total_loss: 814.6236572265625\n",
      "      vf_explained_var: 0.4199068248271942\n",
      "      vf_loss: 814.626220703125\n",
      "    sample_time_ms: 21985.674\n",
      "    update_time_ms: 6.266\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -29.535033757582465\n",
      "  time_since_restore: 345.78589272499084\n",
      "  time_this_iter_s: 27.27692461013794\n",
      "  time_total_s: 345.78589272499084\n",
      "  timestamp: 1554808636\n",
      "  timesteps_since_restore: 130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 13\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 345 s, 13 iter, 130000 ts, -59.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-17-41\n",
      "  done: false\n",
      "  episode_len_mean: 150.74\n",
      "  episode_reward_max: 92.20003891637805\n",
      "  episode_reward_mean: -69.63181494168148\n",
      "  episode_reward_min: -211.42155494767243\n",
      "  episodes_this_iter: 67\n",
      "  episodes_total: 637\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4357.564\n",
      "    load_time_ms: 1.697\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.882813664153218e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.347786545753479\n",
      "      kl: 0.006554834544658661\n",
      "      policy_loss: -0.0014410542789846659\n",
      "      total_loss: 826.16845703125\n",
      "      vf_explained_var: 0.4496823251247406\n",
      "      vf_loss: 826.169921875\n",
      "    sample_time_ms: 21756.869\n",
      "    update_time_ms: 6.19\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -34.81590747084075\n",
      "  time_since_restore: 371.0233371257782\n",
      "  time_this_iter_s: 25.237444400787354\n",
      "  time_total_s: 371.0233371257782\n",
      "  timestamp: 1554808661\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 14\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 371 s, 14 iter, 140000 ts, -69.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-18-06\n",
      "  done: false\n",
      "  episode_len_mean: 135.25\n",
      "  episode_reward_max: 80.32868234441437\n",
      "  episode_reward_mean: -84.21884477650369\n",
      "  episode_reward_min: -207.48194571321483\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 712\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4351.227\n",
      "    load_time_ms: 1.747\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.441406832076609e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3555657863616943\n",
      "      kl: 0.00427269795909524\n",
      "      policy_loss: -0.0014626368647441268\n",
      "      total_loss: 877.4373168945312\n",
      "      vf_explained_var: 0.5110371708869934\n",
      "      vf_loss: 877.4389038085938\n",
      "    sample_time_ms: 21702.716\n",
      "    update_time_ms: 6.254\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -42.10942238825184\n",
      "  time_since_restore: 395.8757493495941\n",
      "  time_this_iter_s: 24.852412223815918\n",
      "  time_total_s: 395.8757493495941\n",
      "  timestamp: 1554808686\n",
      "  timesteps_since_restore: 150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 15\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 395 s, 15 iter, 150000 ts, -84.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-18-31\n",
      "  done: false\n",
      "  episode_len_mean: 134.67\n",
      "  episode_reward_max: 70.82583127941187\n",
      "  episode_reward_mean: -74.77613441819334\n",
      "  episode_reward_min: -210.37661887154917\n",
      "  episodes_this_iter: 74\n",
      "  episodes_total: 786\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4304.536\n",
      "    load_time_ms: 1.74\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2207034160383046e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3485230207443237\n",
      "      kl: 0.00677295820787549\n",
      "      policy_loss: -0.001733770128339529\n",
      "      total_loss: 889.4120483398438\n",
      "      vf_explained_var: 0.4686715602874756\n",
      "      vf_loss: 889.4137573242188\n",
      "    sample_time_ms: 21619.432\n",
      "    update_time_ms: 6.301\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -37.38806720909666\n",
      "  time_since_restore: 420.88715410232544\n",
      "  time_this_iter_s: 25.011404752731323\n",
      "  time_total_s: 420.88715410232544\n",
      "  timestamp: 1554808711\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 16\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 420 s, 16 iter, 160000 ts, -74.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-18-56\n",
      "  done: false\n",
      "  episode_len_mean: 131.66\n",
      "  episode_reward_max: 70.42767800017589\n",
      "  episode_reward_mean: -75.5627476129271\n",
      "  episode_reward_min: -205.24244449241382\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 862\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4299.872\n",
      "    load_time_ms: 1.774\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.103517080191523e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3468657732009888\n",
      "      kl: 0.004225405398756266\n",
      "      policy_loss: -0.0024400283582508564\n",
      "      total_loss: 876.1751098632812\n",
      "      vf_explained_var: 0.47006484866142273\n",
      "      vf_loss: 876.1776733398438\n",
      "    sample_time_ms: 21591.03\n",
      "    update_time_ms: 6.31\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -37.78137380646354\n",
      "  time_since_restore: 445.4550528526306\n",
      "  time_this_iter_s: 24.567898750305176\n",
      "  time_total_s: 445.4550528526306\n",
      "  timestamp: 1554808736\n",
      "  timesteps_since_restore: 170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 17\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 445 s, 17 iter, 170000 ts, -75.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-19-21\n",
      "  done: false\n",
      "  episode_len_mean: 128.27\n",
      "  episode_reward_max: 74.68561202316242\n",
      "  episode_reward_mean: -77.55967695286567\n",
      "  episode_reward_min: -207.48201026013686\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 939\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4305.085\n",
      "    load_time_ms: 1.759\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.0517585400957614e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3433126211166382\n",
      "      kl: 0.004785922355949879\n",
      "      policy_loss: -0.0012616569874808192\n",
      "      total_loss: 946.9166870117188\n",
      "      vf_explained_var: 0.47409674525260925\n",
      "      vf_loss: 946.9180908203125\n",
      "    sample_time_ms: 21657.785\n",
      "    update_time_ms: 6.49\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -38.779838476432836\n",
      "  time_since_restore: 470.76004672050476\n",
      "  time_this_iter_s: 25.304993867874146\n",
      "  time_total_s: 470.76004672050476\n",
      "  timestamp: 1554808761\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 18\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 470 s, 18 iter, 180000 ts, -77.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-19-52\n",
      "  done: false\n",
      "  episode_len_mean: 125.51\n",
      "  episode_reward_max: 60.51282815681031\n",
      "  episode_reward_mean: -78.06204958638274\n",
      "  episode_reward_min: -206.63077320738176\n",
      "  episodes_this_iter: 80\n",
      "  episodes_total: 1019\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4467.029\n",
      "    load_time_ms: 1.827\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5258792700478807e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3435450792312622\n",
      "      kl: 0.006778412032872438\n",
      "      policy_loss: -0.0025315135717391968\n",
      "      total_loss: 968.9403076171875\n",
      "      vf_explained_var: 0.5691983103752136\n",
      "      vf_loss: 968.9429321289062\n",
      "    sample_time_ms: 22108.598\n",
      "    update_time_ms: 6.276\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -39.03102479319137\n",
      "  time_since_restore: 501.5142822265625\n",
      "  time_this_iter_s: 30.75423550605774\n",
      "  time_total_s: 501.5142822265625\n",
      "  timestamp: 1554808792\n",
      "  timesteps_since_restore: 190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 19\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 501 s, 19 iter, 190000 ts, -78.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-20-23\n",
      "  done: false\n",
      "  episode_len_mean: 127.96\n",
      "  episode_reward_max: 59.49925130721364\n",
      "  episode_reward_mean: -65.31493611345824\n",
      "  episode_reward_min: -203.66386075799875\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 1097\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4689.144\n",
      "    load_time_ms: 1.856\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.629396350239404e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3339611291885376\n",
      "      kl: 0.006954931188374758\n",
      "      policy_loss: -0.0026593278162181377\n",
      "      total_loss: 994.2201538085938\n",
      "      vf_explained_var: 0.53030925989151\n",
      "      vf_loss: 994.2227783203125\n",
      "    sample_time_ms: 22370.605\n",
      "    update_time_ms: 6.481\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -32.65746805672912\n",
      "  time_since_restore: 532.5604288578033\n",
      "  time_this_iter_s: 31.046146631240845\n",
      "  time_total_s: 532.5604288578033\n",
      "  timestamp: 1554808823\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 20\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 532 s, 20 iter, 200000 ts, -65.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-20-57\n",
      "  done: false\n",
      "  episode_len_mean: 123.27\n",
      "  episode_reward_max: 45.511679134814166\n",
      "  episode_reward_mean: -67.57139626000044\n",
      "  episode_reward_min: -204.554295878169\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1178\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4683.735\n",
      "    load_time_ms: 1.783\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814698175119702e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3358968496322632\n",
      "      kl: 0.007030341774225235\n",
      "      policy_loss: -0.002907942747697234\n",
      "      total_loss: 963.73779296875\n",
      "      vf_explained_var: 0.4280337393283844\n",
      "      vf_loss: 963.7406005859375\n",
      "    sample_time_ms: 22817.569\n",
      "    update_time_ms: 6.759\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -33.78569813000023\n",
      "  time_since_restore: 566.3550233840942\n",
      "  time_this_iter_s: 33.794594526290894\n",
      "  time_total_s: 566.3550233840942\n",
      "  timestamp: 1554808857\n",
      "  timesteps_since_restore: 210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 21\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 566 s, 21 iter, 210000 ts, -67.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-21-21\n",
      "  done: false\n",
      "  episode_len_mean: 117.63\n",
      "  episode_reward_max: 62.73081598320505\n",
      "  episode_reward_mean: -71.7889069665056\n",
      "  episode_reward_min: -206.84527056337703\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1259\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4631.72\n",
      "    load_time_ms: 1.819\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.907349087559851e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.329553484916687\n",
      "      kl: 0.0080961799249053\n",
      "      policy_loss: -0.0028062909841537476\n",
      "      total_loss: 991.7139282226562\n",
      "      vf_explained_var: 0.45929795503616333\n",
      "      vf_loss: 991.7167358398438\n",
      "    sample_time_ms: 22530.555\n",
      "    update_time_ms: 6.415\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -35.89445348325279\n",
      "  time_since_restore: 590.4074075222015\n",
      "  time_this_iter_s: 24.0523841381073\n",
      "  time_total_s: 590.4074075222015\n",
      "  timestamp: 1554808881\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 22\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 590 s, 22 iter, 220000 ts, -71.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-21-48\n",
      "  done: false\n",
      "  episode_len_mean: 117.31\n",
      "  episode_reward_max: 57.47674114286373\n",
      "  episode_reward_mean: -65.95592695123185\n",
      "  episode_reward_min: -204.47302819650196\n",
      "  episodes_this_iter: 86\n",
      "  episodes_total: 1345\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4767.5\n",
      "    load_time_ms: 1.804\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.536745437799254e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3195055723190308\n",
      "      kl: 0.008901417255401611\n",
      "      policy_loss: -0.0035670858342200518\n",
      "      total_loss: 1066.1038818359375\n",
      "      vf_explained_var: 0.4819578528404236\n",
      "      vf_loss: 1066.107421875\n",
      "    sample_time_ms: 22337.447\n",
      "    update_time_ms: 6.141\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -32.97796347561592\n",
      "  time_since_restore: 617.1147272586823\n",
      "  time_this_iter_s: 26.707319736480713\n",
      "  time_total_s: 617.1147272586823\n",
      "  timestamp: 1554808908\n",
      "  timesteps_since_restore: 230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 23\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 617 s, 23 iter, 230000 ts, -66 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-22-12\n",
      "  done: false\n",
      "  episode_len_mean: 104.61\n",
      "  episode_reward_max: 48.58869970544772\n",
      "  episode_reward_mean: -90.47337937454\n",
      "  episode_reward_min: -203.37411957080235\n",
      "  episodes_this_iter: 94\n",
      "  episodes_total: 1439\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4769.643\n",
      "    load_time_ms: 1.811\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.768372718899627e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.304598331451416\n",
      "      kl: 0.006355709861963987\n",
      "      policy_loss: -0.002509327372536063\n",
      "      total_loss: 1119.1629638671875\n",
      "      vf_explained_var: 0.41703614592552185\n",
      "      vf_loss: 1119.1654052734375\n",
      "    sample_time_ms: 22270.746\n",
      "    update_time_ms: 6.228\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -45.23668968727001\n",
      "  time_since_restore: 641.7065110206604\n",
      "  time_this_iter_s: 24.59178376197815\n",
      "  time_total_s: 641.7065110206604\n",
      "  timestamp: 1554808932\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 24\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 641 s, 24 iter, 240000 ts, -90.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-22-37\n",
      "  done: false\n",
      "  episode_len_mean: 105.74\n",
      "  episode_reward_max: 43.390052238224875\n",
      "  episode_reward_mean: -83.2998219631665\n",
      "  episode_reward_min: -203.41583488221875\n",
      "  episodes_this_iter: 93\n",
      "  episodes_total: 1532\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4769.322\n",
      "    load_time_ms: 1.786\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841863594498136e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2974714040756226\n",
      "      kl: 0.006321268621832132\n",
      "      policy_loss: -0.002198399743065238\n",
      "      total_loss: 1081.716064453125\n",
      "      vf_explained_var: 0.4642568826675415\n",
      "      vf_loss: 1081.7181396484375\n",
      "    sample_time_ms: 22311.849\n",
      "    update_time_ms: 6.317\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -41.649910981583254\n",
      "  time_since_restore: 666.9685273170471\n",
      "  time_this_iter_s: 25.26201629638672\n",
      "  time_total_s: 666.9685273170471\n",
      "  timestamp: 1554808957\n",
      "  timesteps_since_restore: 250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 25\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 666 s, 25 iter, 250000 ts, -83.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-23-09\n",
      "  done: false\n",
      "  episode_len_mean: 110.58\n",
      "  episode_reward_max: 51.004695417862266\n",
      "  episode_reward_mean: -71.82947398628986\n",
      "  episode_reward_min: -204.3580894248651\n",
      "  episodes_this_iter: 92\n",
      "  episodes_total: 1624\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4937.286\n",
      "    load_time_ms: 1.763\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1920931797249068e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2975622415542603\n",
      "      kl: 0.006903012748807669\n",
      "      policy_loss: -0.0027583427727222443\n",
      "      total_loss: 1142.688720703125\n",
      "      vf_explained_var: 0.4039919972419739\n",
      "      vf_loss: 1142.6915283203125\n",
      "    sample_time_ms: 22757.039\n",
      "    update_time_ms: 6.332\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -35.91473699314493\n",
      "  time_since_restore: 698.114185333252\n",
      "  time_this_iter_s: 31.145658016204834\n",
      "  time_total_s: 698.114185333252\n",
      "  timestamp: 1554808989\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 26\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 698 s, 26 iter, 260000 ts, -71.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-23-40\n",
      "  done: false\n",
      "  episode_len_mean: 109.3\n",
      "  episode_reward_max: 55.608738389056924\n",
      "  episode_reward_mean: -69.860339611886\n",
      "  episode_reward_min: -205.8295560922513\n",
      "  episodes_this_iter: 92\n",
      "  episodes_total: 1716\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5014.029\n",
      "    load_time_ms: 1.745\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.960465898624534e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2907048463821411\n",
      "      kl: 0.004938818514347076\n",
      "      policy_loss: -0.0005261412588879466\n",
      "      total_loss: 1136.8912353515625\n",
      "      vf_explained_var: 0.4498477280139923\n",
      "      vf_loss: 1136.8917236328125\n",
      "    sample_time_ms: 23328.593\n",
      "    update_time_ms: 6.466\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -34.930169805943\n",
      "  time_since_restore: 729.1667757034302\n",
      "  time_this_iter_s: 31.052590370178223\n",
      "  time_total_s: 729.1667757034302\n",
      "  timestamp: 1554809020\n",
      "  timesteps_since_restore: 270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 27\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 729 s, 27 iter, 270000 ts, -69.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-24-05\n",
      "  done: false\n",
      "  episode_len_mean: 110.98\n",
      "  episode_reward_max: 52.018286963408\n",
      "  episode_reward_mean: -63.471653389786134\n",
      "  episode_reward_min: -203.09993689942326\n",
      "  episodes_this_iter: 90\n",
      "  episodes_total: 1806\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5012.303\n",
      "    load_time_ms: 1.784\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.980232949312267e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2882546186447144\n",
      "      kl: 0.00465485779568553\n",
      "      policy_loss: -0.0006573972059413791\n",
      "      total_loss: 1144.738037109375\n",
      "      vf_explained_var: 0.4369441270828247\n",
      "      vf_loss: 1144.738525390625\n",
      "    sample_time_ms: 23342.331\n",
      "    update_time_ms: 6.461\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -31.735826694893056\n",
      "  time_since_restore: 754.593514919281\n",
      "  time_this_iter_s: 25.42673921585083\n",
      "  time_total_s: 754.593514919281\n",
      "  timestamp: 1554809045\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 28\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 754 s, 28 iter, 280000 ts, -63.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-24-31\n",
      "  done: false\n",
      "  episode_len_mean: 111.24\n",
      "  episode_reward_max: 65.17390019190145\n",
      "  episode_reward_mean: -53.31208762725389\n",
      "  episode_reward_min: -204.59304849108452\n",
      "  episodes_this_iter: 91\n",
      "  episodes_total: 1897\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4848.059\n",
      "    load_time_ms: 1.722\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4901164746561335e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.29396653175354\n",
      "      kl: 0.0030895136296749115\n",
      "      policy_loss: -0.0010674095246940851\n",
      "      total_loss: 1124.1844482421875\n",
      "      vf_explained_var: 0.4008503258228302\n",
      "      vf_loss: 1124.185546875\n",
      "    sample_time_ms: 22973.154\n",
      "    update_time_ms: 6.485\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -26.656043813626948\n",
      "  time_since_restore: 780.0107982158661\n",
      "  time_this_iter_s: 25.417283296585083\n",
      "  time_total_s: 780.0107982158661\n",
      "  timestamp: 1554809071\n",
      "  timesteps_since_restore: 290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 29\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 780 s, 29 iter, 290000 ts, -53.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-24-56\n",
      "  done: false\n",
      "  episode_len_mean: 98.41584158415841\n",
      "  episode_reward_max: 45.73141404955655\n",
      "  episode_reward_mean: -84.89934989667434\n",
      "  episode_reward_min: -203.5124840425217\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 1998\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4601.483\n",
      "    load_time_ms: 1.68\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.450582373280668e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.289655089378357\n",
      "      kl: 0.004909326788038015\n",
      "      policy_loss: -0.0015784796560183167\n",
      "      total_loss: 1101.2779541015625\n",
      "      vf_explained_var: 0.4773902893066406\n",
      "      vf_loss: 1101.2796630859375\n",
      "    sample_time_ms: 22613.476\n",
      "    update_time_ms: 6.299\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -42.449674948337176\n",
      "  time_since_restore: 804.990213394165\n",
      "  time_this_iter_s: 24.97941517829895\n",
      "  time_total_s: 804.990213394165\n",
      "  timestamp: 1554809096\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 30\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 804 s, 30 iter, 300000 ts, -84.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-25-25\n",
      "  done: false\n",
      "  episode_len_mean: 106.91\n",
      "  episode_reward_max: 57.64779866614255\n",
      "  episode_reward_mean: -70.18505259960922\n",
      "  episode_reward_min: -202.99739084102183\n",
      "  episodes_this_iter: 91\n",
      "  episodes_total: 2089\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4618.167\n",
      "    load_time_ms: 1.678\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.725291186640334e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2649966478347778\n",
      "      kl: 0.0045312843285501\n",
      "      policy_loss: -0.0015549001982435584\n",
      "      total_loss: 1195.597412109375\n",
      "      vf_explained_var: 0.45412102341651917\n",
      "      vf_loss: 1195.598876953125\n",
      "    sample_time_ms: 22152.198\n",
      "    update_time_ms: 6.065\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -35.09252629980461\n",
      "  time_since_restore: 834.3397467136383\n",
      "  time_this_iter_s: 29.349533319473267\n",
      "  time_total_s: 834.3397467136383\n",
      "  timestamp: 1554809125\n",
      "  timesteps_since_restore: 310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 31\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 834 s, 31 iter, 310000 ts, -70.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-25-51\n",
      "  done: false\n",
      "  episode_len_mean: 111.9\n",
      "  episode_reward_max: 43.63411609370023\n",
      "  episode_reward_mean: -56.06343748036691\n",
      "  episode_reward_min: -202.10423560275\n",
      "  episodes_this_iter: 92\n",
      "  episodes_total: 2181\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4622.075\n",
      "    load_time_ms: 1.751\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.862645593320167e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2507855892181396\n",
      "      kl: 0.006957800127565861\n",
      "      policy_loss: -0.003338832873851061\n",
      "      total_loss: 1208.757568359375\n",
      "      vf_explained_var: 0.44480469822883606\n",
      "      vf_loss: 1208.7608642578125\n",
      "    sample_time_ms: 22372.176\n",
      "    update_time_ms: 6.016\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -28.031718740183454\n",
      "  time_since_restore: 860.6327633857727\n",
      "  time_this_iter_s: 26.2930166721344\n",
      "  time_total_s: 860.6327633857727\n",
      "  timestamp: 1554809151\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 32\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 860 s, 32 iter, 320000 ts, -56.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-26-16\n",
      "  done: false\n",
      "  episode_len_mean: 103.78\n",
      "  episode_reward_max: 60.768172368314126\n",
      "  episode_reward_mean: -67.23992727357248\n",
      "  episode_reward_min: -202.27829937848736\n",
      "  episodes_this_iter: 96\n",
      "  episodes_total: 2277\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4503.441\n",
      "    load_time_ms: 1.766\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.313227966600834e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2451201677322388\n",
      "      kl: 0.0038572573103010654\n",
      "      policy_loss: -0.0020013332832604647\n",
      "      total_loss: 1148.1849365234375\n",
      "      vf_explained_var: 0.523297905921936\n",
      "      vf_loss: 1148.18701171875\n",
      "    sample_time_ms: 22277.217\n",
      "    update_time_ms: 5.962\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -33.61996363678625\n",
      "  time_since_restore: 885.1959662437439\n",
      "  time_this_iter_s: 24.56320285797119\n",
      "  time_total_s: 885.1959662437439\n",
      "  timestamp: 1554809176\n",
      "  timesteps_since_restore: 330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 33\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 885 s, 33 iter, 330000 ts, -67.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-26-44\n",
      "  done: false\n",
      "  episode_len_mean: 108.97\n",
      "  episode_reward_max: 67.9229266400995\n",
      "  episode_reward_mean: -51.76841310213804\n",
      "  episode_reward_min: -203.0265757714992\n",
      "  episodes_this_iter: 92\n",
      "  episodes_total: 2369\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4508.401\n",
      "    load_time_ms: 1.705\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.656613983300417e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2400134801864624\n",
      "      kl: 0.005268986336886883\n",
      "      policy_loss: -0.0010529498104006052\n",
      "      total_loss: 1132.91064453125\n",
      "      vf_explained_var: 0.5628798007965088\n",
      "      vf_loss: 1132.91162109375\n",
      "    sample_time_ms: 22613.406\n",
      "    update_time_ms: 6.151\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -25.88420655106901\n",
      "  time_since_restore: 913.2011964321136\n",
      "  time_this_iter_s: 28.00523018836975\n",
      "  time_total_s: 913.2011964321136\n",
      "  timestamp: 1554809204\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 34\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 913 s, 34 iter, 340000 ts, -51.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-27-09\n",
      "  done: false\n",
      "  episode_len_mean: 107.26\n",
      "  episode_reward_max: 68.23759737071515\n",
      "  episode_reward_mean: -50.65353719139456\n",
      "  episode_reward_min: -201.0435431099029\n",
      "  episodes_this_iter: 94\n",
      "  episodes_total: 2463\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4505.127\n",
      "    load_time_ms: 1.738\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283069916502086e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2376315593719482\n",
      "      kl: 0.004306452348828316\n",
      "      policy_loss: -0.00179478176869452\n",
      "      total_loss: 1197.87353515625\n",
      "      vf_explained_var: 0.5675925016403198\n",
      "      vf_loss: 1197.8753662109375\n",
      "    sample_time_ms: 22575.804\n",
      "    update_time_ms: 6.24\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -25.326768595697285\n",
      "  time_since_restore: 938.0551209449768\n",
      "  time_this_iter_s: 24.85392451286316\n",
      "  time_total_s: 938.0551209449768\n",
      "  timestamp: 1554809229\n",
      "  timesteps_since_restore: 350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 35\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 938 s, 35 iter, 350000 ts, -50.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-27-35\n",
      "  done: false\n",
      "  episode_len_mean: 102.49\n",
      "  episode_reward_max: 60.223126775184056\n",
      "  episode_reward_mean: -63.40286349029906\n",
      "  episode_reward_min: -203.29756608068988\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 2562\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4473.656\n",
      "    load_time_ms: 1.771\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1641534958251043e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2253849506378174\n",
      "      kl: 0.004911523777991533\n",
      "      policy_loss: -0.0013248316245153546\n",
      "      total_loss: 1228.49365234375\n",
      "      vf_explained_var: 0.5490255951881409\n",
      "      vf_loss: 1228.4949951171875\n",
      "    sample_time_ms: 22101.035\n",
      "    update_time_ms: 6.317\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -31.701431745149534\n",
      "  time_since_restore: 964.139830827713\n",
      "  time_this_iter_s: 26.084709882736206\n",
      "  time_total_s: 964.139830827713\n",
      "  timestamp: 1554809255\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 36\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 964 s, 36 iter, 360000 ts, -63.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-28-11\n",
      "  done: false\n",
      "  episode_len_mean: 96.66019417475728\n",
      "  episode_reward_max: 66.95338404408187\n",
      "  episode_reward_mean: -68.28731944602062\n",
      "  episode_reward_min: -202.3324528189178\n",
      "  episodes_this_iter: 103\n",
      "  episodes_total: 2665\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4662.466\n",
      "    load_time_ms: 1.767\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.8207674791255215e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2205922603607178\n",
      "      kl: 0.006010645534843206\n",
      "      policy_loss: -0.0022299564443528652\n",
      "      total_loss: 1118.504638671875\n",
      "      vf_explained_var: 0.585049033164978\n",
      "      vf_loss: 1118.5068359375\n",
      "    sample_time_ms: 22378.453\n",
      "    update_time_ms: 6.249\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -34.14365972301031\n",
      "  time_since_restore: 999.8537287712097\n",
      "  time_this_iter_s: 35.713897943496704\n",
      "  time_total_s: 999.8537287712097\n",
      "  timestamp: 1554809291\n",
      "  timesteps_since_restore: 370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 37\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 999 s, 37 iter, 370000 ts, -68.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-28-39\n",
      "  done: false\n",
      "  episode_len_mean: 102.52\n",
      "  episode_reward_max: 78.71508191967652\n",
      "  episode_reward_mean: -57.793969353536106\n",
      "  episode_reward_min: -203.0652460025433\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 2762\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4684.964\n",
      "    load_time_ms: 1.736\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9103837395627608e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1977487802505493\n",
      "      kl: 0.0037991260178387165\n",
      "      policy_loss: -0.000947968102991581\n",
      "      total_loss: 1169.00537109375\n",
      "      vf_explained_var: 0.5368849039077759\n",
      "      vf_loss: 1169.0062255859375\n",
      "    sample_time_ms: 22613.48\n",
      "    update_time_ms: 6.187\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -28.896984676768056\n",
      "  time_since_restore: 1027.8535509109497\n",
      "  time_this_iter_s: 27.99982213973999\n",
      "  time_total_s: 1027.8535509109497\n",
      "  timestamp: 1554809319\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 38\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1027 s, 38 iter, 380000 ts, -57.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-29-04\n",
      "  done: false\n",
      "  episode_len_mean: 106.39\n",
      "  episode_reward_max: 73.30477179951754\n",
      "  episode_reward_mean: -53.15155035871268\n",
      "  episode_reward_min: -203.53502362976653\n",
      "  episodes_this_iter: 94\n",
      "  episodes_total: 2856\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4732.046\n",
      "    load_time_ms: 1.752\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4551918697813804e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.196708083152771\n",
      "      kl: 0.004398655146360397\n",
      "      policy_loss: -0.0019461807096377015\n",
      "      total_loss: 1200.6314697265625\n",
      "      vf_explained_var: 0.48067453503608704\n",
      "      vf_loss: 1200.63330078125\n",
      "    sample_time_ms: 22559.692\n",
      "    update_time_ms: 6.192\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -26.575775179356334\n",
      "  time_since_restore: 1053.203274011612\n",
      "  time_this_iter_s: 25.34972310066223\n",
      "  time_total_s: 1053.203274011612\n",
      "  timestamp: 1554809344\n",
      "  timesteps_since_restore: 390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 39\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1053 s, 39 iter, 390000 ts, -53.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-29-30\n",
      "  done: false\n",
      "  episode_len_mean: 102.05\n",
      "  episode_reward_max: 63.8630667381359\n",
      "  episode_reward_mean: -63.460557549864355\n",
      "  episode_reward_min: -203.36107699531362\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 2954\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4725.701\n",
      "    load_time_ms: 1.739\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.275959348906902e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.178621768951416\n",
      "      kl: 0.005117684602737427\n",
      "      policy_loss: -0.001443394343368709\n",
      "      total_loss: 1239.70458984375\n",
      "      vf_explained_var: 0.4979853928089142\n",
      "      vf_loss: 1239.7061767578125\n",
      "    sample_time_ms: 22672.679\n",
      "    update_time_ms: 6.191\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -31.730278774932184\n",
      "  time_since_restore: 1079.2528948783875\n",
      "  time_this_iter_s: 26.049620866775513\n",
      "  time_total_s: 1079.2528948783875\n",
      "  timestamp: 1554809370\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 40\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1079 s, 40 iter, 400000 ts, -63.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-29-56\n",
      "  done: false\n",
      "  episode_len_mean: 106.27\n",
      "  episode_reward_max: 69.14657059603846\n",
      "  episode_reward_mean: -44.018249193452355\n",
      "  episode_reward_min: -203.42689467708527\n",
      "  episodes_this_iter: 93\n",
      "  episodes_total: 3047\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4692.758\n",
      "    load_time_ms: 1.749\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.637979674453451e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.163641095161438\n",
      "      kl: 0.004667460918426514\n",
      "      policy_loss: -0.001217645243741572\n",
      "      total_loss: 1229.4774169921875\n",
      "      vf_explained_var: 0.44677019119262695\n",
      "      vf_loss: 1229.478515625\n",
      "    sample_time_ms: 22322.048\n",
      "    update_time_ms: 6.373\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -22.00912459672619\n",
      "  time_since_restore: 1104.7661247253418\n",
      "  time_this_iter_s: 25.513229846954346\n",
      "  time_total_s: 1104.7661247253418\n",
      "  timestamp: 1554809396\n",
      "  timesteps_since_restore: 410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 41\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1104 s, 41 iter, 410000 ts, -44 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-30-22\n",
      "  done: false\n",
      "  episode_len_mean: 105.67\n",
      "  episode_reward_max: 71.63814830501313\n",
      "  episode_reward_mean: -44.066445781584534\n",
      "  episode_reward_min: -203.98625510875183\n",
      "  episodes_this_iter: 95\n",
      "  episodes_total: 3142\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4693.391\n",
      "    load_time_ms: 1.63\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8189898372267255e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1564209461212158\n",
      "      kl: 0.0052231838926672935\n",
      "      policy_loss: -0.0011887536384165287\n",
      "      total_loss: 1255.217529296875\n",
      "      vf_explained_var: 0.41320112347602844\n",
      "      vf_loss: 1255.21875\n",
      "    sample_time_ms: 22291.297\n",
      "    update_time_ms: 6.403\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -22.033222890792263\n",
      "  time_since_restore: 1130.7525763511658\n",
      "  time_this_iter_s: 25.986451625823975\n",
      "  time_total_s: 1130.7525763511658\n",
      "  timestamp: 1554809422\n",
      "  timesteps_since_restore: 420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 42\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1130 s, 42 iter, 420000 ts, -44.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-30-47\n",
      "  done: false\n",
      "  episode_len_mean: 110.51\n",
      "  episode_reward_max: 74.11721362137237\n",
      "  episode_reward_mean: -30.39734910079314\n",
      "  episode_reward_min: -204.6558612998017\n",
      "  episodes_this_iter: 91\n",
      "  episodes_total: 3233\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4687.325\n",
      "    load_time_ms: 1.652\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.094949186133627e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.159204125404358\n",
      "      kl: 0.004188602324575186\n",
      "      policy_loss: -0.0015504962066188455\n",
      "      total_loss: 1162.385986328125\n",
      "      vf_explained_var: 0.40149250626564026\n",
      "      vf_loss: 1162.3875732421875\n",
      "    sample_time_ms: 22310.904\n",
      "    update_time_ms: 6.486\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -15.198674550396575\n",
      "  time_since_restore: 1155.4559273719788\n",
      "  time_this_iter_s: 24.70335102081299\n",
      "  time_total_s: 1155.4559273719788\n",
      "  timestamp: 1554809447\n",
      "  timesteps_since_restore: 430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 43\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1155 s, 43 iter, 430000 ts, -30.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-31-13\n",
      "  done: false\n",
      "  episode_len_mean: 107.98\n",
      "  episode_reward_max: 78.88430261738985\n",
      "  episode_reward_mean: -40.688788269471154\n",
      "  episode_reward_min: -205.90763808229485\n",
      "  episodes_this_iter: 93\n",
      "  episodes_total: 3326\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4696.059\n",
      "    load_time_ms: 1.772\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.5474745930668137e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1462059020996094\n",
      "      kl: 0.004547902848571539\n",
      "      policy_loss: -0.0016135374316945672\n",
      "      total_loss: 1210.069091796875\n",
      "      vf_explained_var: 0.4250567555427551\n",
      "      vf_loss: 1210.0706787109375\n",
      "    sample_time_ms: 22085.148\n",
      "    update_time_ms: 6.299\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -20.34439413473557\n",
      "  time_since_restore: 1181.29208445549\n",
      "  time_this_iter_s: 25.836157083511353\n",
      "  time_total_s: 1181.29208445549\n",
      "  timestamp: 1554809473\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 44\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1181 s, 44 iter, 440000 ts, -40.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.65\n",
      "  episode_reward_max: 69.71867971347858\n",
      "  episode_reward_mean: -58.05889508311407\n",
      "  episode_reward_min: -203.89129731715929\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 3425\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4728.104\n",
      "    load_time_ms: 1.718\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2737372965334068e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1356183290481567\n",
      "      kl: 0.0052560195326805115\n",
      "      policy_loss: -0.002097527962177992\n",
      "      total_loss: 1179.47900390625\n",
      "      vf_explained_var: 0.4725320339202881\n",
      "      vf_loss: 1179.481201171875\n",
      "    sample_time_ms: 22127.306\n",
      "    update_time_ms: 5.964\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -29.029447541557033\n",
      "  time_since_restore: 1206.8843212127686\n",
      "  time_this_iter_s: 25.592236757278442\n",
      "  time_total_s: 1206.8843212127686\n",
      "  timestamp: 1554809498\n",
      "  timesteps_since_restore: 450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 45\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1206 s, 45 iter, 450000 ts, -58.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-32-10\n",
      "  done: false\n",
      "  episode_len_mean: 106.72\n",
      "  episode_reward_max: 72.76087679295682\n",
      "  episode_reward_mean: -49.1412309768456\n",
      "  episode_reward_min: -204.83842419818183\n",
      "  episodes_this_iter: 93\n",
      "  episodes_total: 3518\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4606.97\n",
      "    load_time_ms: 1.738\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1368686482667034e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.129170298576355\n",
      "      kl: 0.004947388079017401\n",
      "      policy_loss: -0.0017705151112750173\n",
      "      total_loss: 1214.802734375\n",
      "      vf_explained_var: 0.48780402541160583\n",
      "      vf_loss: 1214.8045654296875\n",
      "    sample_time_ms: 22818.758\n",
      "    update_time_ms: 5.858\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -24.570615488422803\n",
      "  time_since_restore: 1238.6680924892426\n",
      "  time_this_iter_s: 31.783771276474\n",
      "  time_total_s: 1238.6680924892426\n",
      "  timestamp: 1554809530\n",
      "  timesteps_since_restore: 460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 46\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1238 s, 46 iter, 460000 ts, -49.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-32-36\n",
      "  done: false\n",
      "  episode_len_mean: 99.0\n",
      "  episode_reward_max: 81.24165954442248\n",
      "  episode_reward_mean: -59.08215462437685\n",
      "  episode_reward_min: -203.74522977003613\n",
      "  episodes_this_iter: 102\n",
      "  episodes_total: 3620\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4375.154\n",
      "    load_time_ms: 1.743\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.684343241333517e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1145966053009033\n",
      "      kl: 0.0035188905894756317\n",
      "      policy_loss: -0.0005853166803717613\n",
      "      total_loss: 1139.4896240234375\n",
      "      vf_explained_var: 0.548133909702301\n",
      "      vf_loss: 1139.490234375\n",
      "    sample_time_ms: 22091.622\n",
      "    update_time_ms: 5.736\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -29.541077312188417\n",
      "  time_since_restore: 1264.79168343544\n",
      "  time_this_iter_s: 26.12359094619751\n",
      "  time_total_s: 1264.79168343544\n",
      "  timestamp: 1554809556\n",
      "  timesteps_since_restore: 470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 47\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1264 s, 47 iter, 470000 ts, -59.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-33-02\n",
      "  done: false\n",
      "  episode_len_mean: 103.96\n",
      "  episode_reward_max: 77.65952187322004\n",
      "  episode_reward_mean: -46.81642391622668\n",
      "  episode_reward_min: -203.21842095430884\n",
      "  episodes_this_iter: 96\n",
      "  episodes_total: 3716\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4352.062\n",
      "    load_time_ms: 1.719\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.8421716206667585e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1082245111465454\n",
      "      kl: 0.005106916185468435\n",
      "      policy_loss: -0.0011437362991273403\n",
      "      total_loss: 1154.5582275390625\n",
      "      vf_explained_var: 0.5021228790283203\n",
      "      vf_loss: 1154.559326171875\n",
      "    sample_time_ms: 21904.318\n",
      "    update_time_ms: 5.622\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -23.40821195811334\n",
      "  time_since_restore: 1290.688705444336\n",
      "  time_this_iter_s: 25.897022008895874\n",
      "  time_total_s: 1290.688705444336\n",
      "  timestamp: 1554809582\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 48\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1290 s, 48 iter, 480000 ts, -46.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-33-28\n",
      "  done: false\n",
      "  episode_len_mean: 104.14\n",
      "  episode_reward_max: 66.77489971035352\n",
      "  episode_reward_mean: -52.07672470273396\n",
      "  episode_reward_min: -204.43909642628654\n",
      "  episodes_this_iter: 96\n",
      "  episodes_total: 3812\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4329.013\n",
      "    load_time_ms: 1.695\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4210858103333793e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0945690870285034\n",
      "      kl: 0.0065459259785711765\n",
      "      policy_loss: -0.0017890852177515626\n",
      "      total_loss: 1253.650146484375\n",
      "      vf_explained_var: 0.5213577747344971\n",
      "      vf_loss: 1253.65185546875\n",
      "    sample_time_ms: 22013.494\n",
      "    update_time_ms: 5.603\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -26.03836235136698\n",
      "  time_since_restore: 1316.9037187099457\n",
      "  time_this_iter_s: 26.21501326560974\n",
      "  time_total_s: 1316.9037187099457\n",
      "  timestamp: 1554809608\n",
      "  timesteps_since_restore: 490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 49\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1316 s, 49 iter, 490000 ts, -52.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-33-54\n",
      "  done: false\n",
      "  episode_len_mean: 103.3\n",
      "  episode_reward_max: 63.46602818085465\n",
      "  episode_reward_mean: -59.91273278871914\n",
      "  episode_reward_min: -204.18160275508856\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 3910\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4342.332\n",
      "    load_time_ms: 1.821\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.105429051666896e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0888195037841797\n",
      "      kl: 0.006299504078924656\n",
      "      policy_loss: -0.0015314498450607061\n",
      "      total_loss: 1278.7410888671875\n",
      "      vf_explained_var: 0.5387841463088989\n",
      "      vf_loss: 1278.7425537109375\n",
      "    sample_time_ms: 21978.861\n",
      "    update_time_ms: 5.735\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -29.956366394359566\n",
      "  time_since_restore: 1342.7438702583313\n",
      "  time_this_iter_s: 25.84015154838562\n",
      "  time_total_s: 1342.7438702583313\n",
      "  timestamp: 1554809634\n",
      "  timesteps_since_restore: 500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 50\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1342 s, 50 iter, 500000 ts, -59.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-34-20\n",
      "  done: false\n",
      "  episode_len_mean: 97.06862745098039\n",
      "  episode_reward_max: 53.05825814277717\n",
      "  episode_reward_mean: -66.77478152406987\n",
      "  episode_reward_min: -204.45437654862926\n",
      "  episodes_this_iter: 102\n",
      "  episodes_total: 4012\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4349.335\n",
      "    load_time_ms: 1.819\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.552714525833448e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.081869125366211\n",
      "      kl: 0.003704096656292677\n",
      "      policy_loss: -0.0009256770135834813\n",
      "      total_loss: 1238.5379638671875\n",
      "      vf_explained_var: 0.5537271499633789\n",
      "      vf_loss: 1238.538818359375\n",
      "    sample_time_ms: 21996.462\n",
      "    update_time_ms: 5.38\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -33.38739076203493\n",
      "  time_since_restore: 1368.4987840652466\n",
      "  time_this_iter_s: 25.754913806915283\n",
      "  time_total_s: 1368.4987840652466\n",
      "  timestamp: 1554809660\n",
      "  timesteps_since_restore: 510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 51\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1368 s, 51 iter, 510000 ts, -66.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-34-47\n",
      "  done: false\n",
      "  episode_len_mean: 99.07\n",
      "  episode_reward_max: 55.55652159494887\n",
      "  episode_reward_mean: -57.0712551594569\n",
      "  episode_reward_min: -201.73297965711294\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 4112\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4357.556\n",
      "    load_time_ms: 1.852\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.776357262916724e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0902695655822754\n",
      "      kl: 0.0023428332060575485\n",
      "      policy_loss: -0.0004585163842421025\n",
      "      total_loss: 1272.8707275390625\n",
      "      vf_explained_var: 0.4950397312641144\n",
      "      vf_loss: 1272.8712158203125\n",
      "    sample_time_ms: 22034.734\n",
      "    update_time_ms: 5.344\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -28.53562757972845\n",
      "  time_since_restore: 1394.951821565628\n",
      "  time_this_iter_s: 26.45303750038147\n",
      "  time_total_s: 1394.951821565628\n",
      "  timestamp: 1554809687\n",
      "  timesteps_since_restore: 520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 52\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1394 s, 52 iter, 520000 ts, -57.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-35-13\n",
      "  done: false\n",
      "  episode_len_mean: 102.67\n",
      "  episode_reward_max: 51.15563736785044\n",
      "  episode_reward_mean: -52.04644680958611\n",
      "  episode_reward_min: -203.39999796497176\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 4211\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4374.467\n",
      "    load_time_ms: 1.843\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.88178631458362e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0846991539001465\n",
      "      kl: 0.004587765317410231\n",
      "      policy_loss: -0.0015780386747792363\n",
      "      total_loss: 1244.845947265625\n",
      "      vf_explained_var: 0.5400010943412781\n",
      "      vf_loss: 1244.8475341796875\n",
      "    sample_time_ms: 22211.795\n",
      "    update_time_ms: 5.401\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -26.023223404793043\n",
      "  time_since_restore: 1421.5930571556091\n",
      "  time_this_iter_s: 26.64123558998108\n",
      "  time_total_s: 1421.5930571556091\n",
      "  timestamp: 1554809713\n",
      "  timesteps_since_restore: 530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 53\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1421 s, 53 iter, 530000 ts, -52 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-35-38\n",
      "  done: false\n",
      "  episode_len_mean: 101.16\n",
      "  episode_reward_max: 46.7854455209598\n",
      "  episode_reward_mean: -55.068187087004524\n",
      "  episode_reward_min: -203.305278736526\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 4310\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4358.106\n",
      "    load_time_ms: 1.715\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.44089315729181e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.058762550354004\n",
      "      kl: 0.005550812464207411\n",
      "      policy_loss: -0.0026634098030626774\n",
      "      total_loss: 1308.7996826171875\n",
      "      vf_explained_var: 0.5241267681121826\n",
      "      vf_loss: 1308.80224609375\n",
      "    sample_time_ms: 22164.079\n",
      "    update_time_ms: 5.437\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -27.53409354350226\n",
      "  time_since_restore: 1446.7841360569\n",
      "  time_this_iter_s: 25.191078901290894\n",
      "  time_total_s: 1446.7841360569\n",
      "  timestamp: 1554809738\n",
      "  timesteps_since_restore: 540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 54\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1446 s, 54 iter, 540000 ts, -55.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-36-07\n",
      "  done: false\n",
      "  episode_len_mean: 103.97\n",
      "  episode_reward_max: 49.343749531203066\n",
      "  episode_reward_mean: -43.081170708716535\n",
      "  episode_reward_min: -203.6717731693624\n",
      "  episodes_this_iter: 96\n",
      "  episodes_total: 4406\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4333.217\n",
      "    load_time_ms: 1.74\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.220446578645905e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.054858684539795\n",
      "      kl: 0.005582212004810572\n",
      "      policy_loss: -0.0005996286636218429\n",
      "      total_loss: 1320.48681640625\n",
      "      vf_explained_var: 0.4547688066959381\n",
      "      vf_loss: 1320.4874267578125\n",
      "    sample_time_ms: 22498.957\n",
      "    update_time_ms: 5.466\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -21.54058535435827\n",
      "  time_since_restore: 1475.476755142212\n",
      "  time_this_iter_s: 28.69261908531189\n",
      "  time_total_s: 1475.476755142212\n",
      "  timestamp: 1554809767\n",
      "  timesteps_since_restore: 550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 55\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1475 s, 55 iter, 550000 ts, -43.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-36-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.36\n",
      "  episode_reward_max: 46.282717551064856\n",
      "  episode_reward_mean: -55.06509889675568\n",
      "  episode_reward_min: -204.8575438112579\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 4505\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4321.459\n",
      "    load_time_ms: 1.699\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1102232893229526e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0537713766098022\n",
      "      kl: 0.0034554775338619947\n",
      "      policy_loss: -0.000931258371565491\n",
      "      total_loss: 1335.4134521484375\n",
      "      vf_explained_var: 0.44364821910858154\n",
      "      vf_loss: 1335.4144287109375\n",
      "    sample_time_ms: 22055.534\n",
      "    update_time_ms: 5.591\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -27.53254944837784\n",
      "  time_since_restore: 1502.7112836837769\n",
      "  time_this_iter_s: 27.23452854156494\n",
      "  time_total_s: 1502.7112836837769\n",
      "  timestamp: 1554809794\n",
      "  timesteps_since_restore: 560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 56\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1502 s, 56 iter, 560000 ts, -55.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-36-59\n",
      "  done: false\n",
      "  episode_len_mean: 106.73\n",
      "  episode_reward_max: 40.89575939668461\n",
      "  episode_reward_mean: -36.78857620913079\n",
      "  episode_reward_min: -202.7375725668848\n",
      "  episodes_this_iter: 94\n",
      "  episodes_total: 4599\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4288.546\n",
      "    load_time_ms: 1.735\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.551116446614763e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0469647645950317\n",
      "      kl: 0.0065439739264547825\n",
      "      policy_loss: -0.0026392831932753325\n",
      "      total_loss: 1374.4473876953125\n",
      "      vf_explained_var: 0.3817480504512787\n",
      "      vf_loss: 1374.4500732421875\n",
      "    sample_time_ms: 21948.641\n",
      "    update_time_ms: 5.709\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -18.394288104565398\n",
      "  time_since_restore: 1527.4414467811584\n",
      "  time_this_iter_s: 24.730163097381592\n",
      "  time_total_s: 1527.4414467811584\n",
      "  timestamp: 1554809819\n",
      "  timesteps_since_restore: 570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 57\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1527 s, 57 iter, 570000 ts, -36.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-37-24\n",
      "  done: false\n",
      "  episode_len_mean: 104.25\n",
      "  episode_reward_max: 44.913327456712906\n",
      "  episode_reward_mean: -39.78985926290644\n",
      "  episode_reward_min: -203.8391783540257\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 4696\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4298.575\n",
      "    load_time_ms: 1.744\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.7755582233073814e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0408486127853394\n",
      "      kl: 0.00636352738365531\n",
      "      policy_loss: -0.0017018956132233143\n",
      "      total_loss: 1306.16015625\n",
      "      vf_explained_var: 0.3249404728412628\n",
      "      vf_loss: 1306.16162109375\n",
      "    sample_time_ms: 21863.768\n",
      "    update_time_ms: 5.703\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -19.89492963145322\n",
      "  time_since_restore: 1552.5870735645294\n",
      "  time_this_iter_s: 25.14562678337097\n",
      "  time_total_s: 1552.5870735645294\n",
      "  timestamp: 1554809844\n",
      "  timesteps_since_restore: 580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 58\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1552 s, 58 iter, 580000 ts, -39.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-37-54\n",
      "  done: false\n",
      "  episode_len_mean: 98.5049504950495\n",
      "  episode_reward_max: 37.66257056638576\n",
      "  episode_reward_mean: -58.766669954067254\n",
      "  episode_reward_min: -205.03821325947655\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 4797\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4389.849\n",
      "    load_time_ms: 1.75\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3877791116536907e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0168306827545166\n",
      "      kl: 0.006387719418853521\n",
      "      policy_loss: -0.0025130051653832197\n",
      "      total_loss: 1342.90380859375\n",
      "      vf_explained_var: 0.3833464980125427\n",
      "      vf_loss: 1342.9063720703125\n",
      "    sample_time_ms: 22098.177\n",
      "    update_time_ms: 5.903\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -29.38333497703363\n",
      "  time_since_restore: 1582.0589230060577\n",
      "  time_this_iter_s: 29.47184944152832\n",
      "  time_total_s: 1582.0589230060577\n",
      "  timestamp: 1554809874\n",
      "  timesteps_since_restore: 590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 59\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1582 s, 59 iter, 590000 ts, -58.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-38-26\n",
      "  done: false\n",
      "  episode_len_mean: 102.07\n",
      "  episode_reward_max: 40.32881267518755\n",
      "  episode_reward_mean: -45.73957874744926\n",
      "  episode_reward_min: -203.70298285893472\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 4895\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4409.674\n",
      "    load_time_ms: 1.614\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.9388955582684535e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9975562691688538\n",
      "      kl: 0.004720120690762997\n",
      "      policy_loss: -0.0013190933968871832\n",
      "      total_loss: 1426.6063232421875\n",
      "      vf_explained_var: 0.3034715950489044\n",
      "      vf_loss: 1426.6077880859375\n",
      "    sample_time_ms: 22718.685\n",
      "    update_time_ms: 6.339\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -22.869789373724625\n",
      "  time_since_restore: 1614.3001759052277\n",
      "  time_this_iter_s: 32.24125289916992\n",
      "  time_total_s: 1614.3001759052277\n",
      "  timestamp: 1554809906\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 60\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1614 s, 60 iter, 600000 ts, -45.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-38-53\n",
      "  done: false\n",
      "  episode_len_mean: 99.15841584158416\n",
      "  episode_reward_max: 45.143505880240504\n",
      "  episode_reward_mean: -54.198587135743416\n",
      "  episode_reward_min: -203.49129793454688\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 4996\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4491.573\n",
      "    load_time_ms: 1.595\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.4694477791342267e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9955657720565796\n",
      "      kl: 0.00418449379503727\n",
      "      policy_loss: -0.00023663960746489465\n",
      "      total_loss: 1383.4898681640625\n",
      "      vf_explained_var: 0.3240918815135956\n",
      "      vf_loss: 1383.4901123046875\n",
      "    sample_time_ms: 22695.562\n",
      "    update_time_ms: 6.424\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -27.099293567871715\n",
      "  time_since_restore: 1640.64572930336\n",
      "  time_this_iter_s: 26.345553398132324\n",
      "  time_total_s: 1640.64572930336\n",
      "  timestamp: 1554809933\n",
      "  timesteps_since_restore: 610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 61\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1640 s, 61 iter, 610000 ts, -54.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-39-22\n",
      "  done: false\n",
      "  episode_len_mean: 99.61386138613861\n",
      "  episode_reward_max: 39.330234168094094\n",
      "  episode_reward_mean: -53.86892435841506\n",
      "  episode_reward_min: -202.61218696622313\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 5097\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4539.398\n",
      "    load_time_ms: 1.588\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7347238895671134e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9978402853012085\n",
      "      kl: 0.0047828396782279015\n",
      "      policy_loss: -0.000920521910302341\n",
      "      total_loss: 1351.4925537109375\n",
      "      vf_explained_var: 0.3559649884700775\n",
      "      vf_loss: 1351.493408203125\n",
      "    sample_time_ms: 22941.718\n",
      "    update_time_ms: 6.465\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -26.934462179207525\n",
      "  time_since_restore: 1670.0380511283875\n",
      "  time_this_iter_s: 29.392321825027466\n",
      "  time_total_s: 1670.0380511283875\n",
      "  timestamp: 1554809962\n",
      "  timesteps_since_restore: 620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 62\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1670 s, 62 iter, 620000 ts, -53.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-39-48\n",
      "  done: false\n",
      "  episode_len_mean: 97.44117647058823\n",
      "  episode_reward_max: 29.300727807244645\n",
      "  episode_reward_mean: -62.454242828762894\n",
      "  episode_reward_min: -201.85215166572203\n",
      "  episodes_this_iter: 102\n",
      "  episodes_total: 5199\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4533.742\n",
      "    load_time_ms: 1.625\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.673619447835567e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9652265906333923\n",
      "      kl: 0.0058530112728476524\n",
      "      policy_loss: -0.0016215518116950989\n",
      "      total_loss: 1391.2762451171875\n",
      "      vf_explained_var: 0.3960295021533966\n",
      "      vf_loss: 1391.27783203125\n",
      "    sample_time_ms: 22903.695\n",
      "    update_time_ms: 6.298\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -31.227121414381447\n",
      "  time_since_restore: 1696.2425045967102\n",
      "  time_this_iter_s: 26.204453468322754\n",
      "  time_total_s: 1696.2425045967102\n",
      "  timestamp: 1554809988\n",
      "  timesteps_since_restore: 630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 63\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1696 s, 63 iter, 630000 ts, -62.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-40-13\n",
      "  done: false\n",
      "  episode_len_mean: 94.76190476190476\n",
      "  episode_reward_max: 31.22641916618795\n",
      "  episode_reward_mean: -65.89913897127977\n",
      "  episode_reward_min: -201.91770875700874\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 5304\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4538.4\n",
      "    load_time_ms: 1.636\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.3368097239177834e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9472418427467346\n",
      "      kl: 0.0062034414149820805\n",
      "      policy_loss: -0.0019048789981752634\n",
      "      total_loss: 1344.26220703125\n",
      "      vf_explained_var: 0.4140503704547882\n",
      "      vf_loss: 1344.26416015625\n",
      "    sample_time_ms: 22862.835\n",
      "    update_time_ms: 6.171\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -32.94956948563989\n",
      "  time_since_restore: 1721.0718569755554\n",
      "  time_this_iter_s: 24.829352378845215\n",
      "  time_total_s: 1721.0718569755554\n",
      "  timestamp: 1554810013\n",
      "  timesteps_since_restore: 640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 64\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1721 s, 64 iter, 640000 ts, -65.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-40-37\n",
      "  done: false\n",
      "  episode_len_mean: 94.61682242990655\n",
      "  episode_reward_max: 29.84653895418417\n",
      "  episode_reward_mean: -66.44567658547358\n",
      "  episode_reward_min: -201.55386058459317\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 5411\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4541.107\n",
      "    load_time_ms: 1.615\n",
      "    num_steps_sampled: 650000\n",
      "    num_steps_trained: 650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.1684048619588917e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9430149793624878\n",
      "      kl: 0.005454009864479303\n",
      "      policy_loss: -0.0020120511762797832\n",
      "      total_loss: 1330.6217041015625\n",
      "      vf_explained_var: 0.4276930093765259\n",
      "      vf_loss: 1330.623779296875\n",
      "    sample_time_ms: 22358.367\n",
      "    update_time_ms: 6.274\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -33.22283829273679\n",
      "  time_since_restore: 1744.7480494976044\n",
      "  time_this_iter_s: 23.67619252204895\n",
      "  time_total_s: 1744.7480494976044\n",
      "  timestamp: 1554810037\n",
      "  timesteps_since_restore: 650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 650000\n",
      "  training_iteration: 65\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1744 s, 65 iter, 650000 ts, -66.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-41-03\n",
      "  done: false\n",
      "  episode_len_mean: 94.97115384615384\n",
      "  episode_reward_max: 31.830785021328516\n",
      "  episode_reward_mean: -68.80393622429686\n",
      "  episode_reward_min: -203.68874930578795\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 5515\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4541.04\n",
      "    load_time_ms: 1.583\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842024309794459e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9410213828086853\n",
      "      kl: 0.0038620501291006804\n",
      "      policy_loss: -0.0009526694193482399\n",
      "      total_loss: 1386.31005859375\n",
      "      vf_explained_var: 0.47105103731155396\n",
      "      vf_loss: 1386.3111572265625\n",
      "    sample_time_ms: 22194.964\n",
      "    update_time_ms: 6.238\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -34.40196811214843\n",
      "  time_since_restore: 1770.3475167751312\n",
      "  time_this_iter_s: 25.599467277526855\n",
      "  time_total_s: 1770.3475167751312\n",
      "  timestamp: 1554810063\n",
      "  timesteps_since_restore: 660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 66\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1770 s, 66 iter, 660000 ts, -68.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-41-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.48\n",
      "  episode_reward_max: 33.11301329275618\n",
      "  episode_reward_mean: -50.24565918063581\n",
      "  episode_reward_min: -203.16922738258535\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 5615\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4549.059\n",
      "    load_time_ms: 1.58\n",
      "    num_steps_sampled: 670000\n",
      "    num_steps_trained: 670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.421012154897229e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9358865022659302\n",
      "      kl: 0.006858435925096273\n",
      "      policy_loss: -0.0021816319786012173\n",
      "      total_loss: 1377.8106689453125\n",
      "      vf_explained_var: 0.4216375946998596\n",
      "      vf_loss: 1377.8128662109375\n",
      "    sample_time_ms: 22238.485\n",
      "    update_time_ms: 6.142\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -25.122829590317906\n",
      "  time_since_restore: 1795.5893020629883\n",
      "  time_this_iter_s: 25.241785287857056\n",
      "  time_total_s: 1795.5893020629883\n",
      "  timestamp: 1554810088\n",
      "  timesteps_since_restore: 670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 670000\n",
      "  training_iteration: 67\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1795 s, 67 iter, 670000 ts, -50.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 95.12380952380953\n",
      "  episode_reward_max: 30.98985932388397\n",
      "  episode_reward_mean: -67.26480807122762\n",
      "  episode_reward_min: -202.12687975323084\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 5720\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4535.715\n",
      "    load_time_ms: 1.593\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.7105060774486146e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9239476323127747\n",
      "      kl: 0.005924254190176725\n",
      "      policy_loss: -0.0018883009906858206\n",
      "      total_loss: 1322.6593017578125\n",
      "      vf_explained_var: 0.46661484241485596\n",
      "      vf_loss: 1322.6610107421875\n",
      "    sample_time_ms: 22190.149\n",
      "    update_time_ms: 6.262\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -33.63240403561381\n",
      "  time_since_restore: 1820.1202511787415\n",
      "  time_this_iter_s: 24.530949115753174\n",
      "  time_total_s: 1820.1202511787415\n",
      "  timestamp: 1554810112\n",
      "  timesteps_since_restore: 680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 68\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1820 s, 68 iter, 680000 ts, -67.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-42-17\n",
      "  done: false\n",
      "  episode_len_mean: 96.06666666666666\n",
      "  episode_reward_max: 31.010516234651828\n",
      "  episode_reward_mean: -64.36850279049541\n",
      "  episode_reward_min: -202.390591834996\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 5825\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4420.717\n",
      "    load_time_ms: 1.618\n",
      "    num_steps_sampled: 690000\n",
      "    num_steps_trained: 690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3552530387243073e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9232110381126404\n",
      "      kl: 0.005886986386030912\n",
      "      policy_loss: -0.001425471855327487\n",
      "      total_loss: 1349.8262939453125\n",
      "      vf_explained_var: 0.4712238907814026\n",
      "      vf_loss: 1349.8277587890625\n",
      "    sample_time_ms: 21781.076\n",
      "    update_time_ms: 6.215\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -32.1842513952477\n",
      "  time_since_restore: 1844.3494744300842\n",
      "  time_this_iter_s: 24.229223251342773\n",
      "  time_total_s: 1844.3494744300842\n",
      "  timestamp: 1554810137\n",
      "  timesteps_since_restore: 690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 690000\n",
      "  training_iteration: 69\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1844 s, 69 iter, 690000 ts, -64.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-42-41\n",
      "  done: false\n",
      "  episode_len_mean: 95.74038461538461\n",
      "  episode_reward_max: 33.01763255150917\n",
      "  episode_reward_mean: -63.95376203037816\n",
      "  episode_reward_min: -202.39440620446595\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 5929\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4380.492\n",
      "    load_time_ms: 1.615\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.776265193621537e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9309754967689514\n",
      "      kl: 0.004281153436750174\n",
      "      policy_loss: -0.0015133768320083618\n",
      "      total_loss: 1309.98681640625\n",
      "      vf_explained_var: 0.45925405621528625\n",
      "      vf_loss: 1309.98828125\n",
      "    sample_time_ms: 21072.501\n",
      "    update_time_ms: 5.795\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -31.976881015189072\n",
      "  time_since_restore: 1869.0991559028625\n",
      "  time_this_iter_s: 24.74968147277832\n",
      "  time_total_s: 1869.0991559028625\n",
      "  timestamp: 1554810161\n",
      "  timesteps_since_restore: 700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 70\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1869 s, 70 iter, 700000 ts, -64 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-43-06\n",
      "  done: false\n",
      "  episode_len_mean: 94.65094339622641\n",
      "  episode_reward_max: 30.22451712628606\n",
      "  episode_reward_mean: -65.2384583380912\n",
      "  episode_reward_min: -202.51230409914288\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 6035\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4285.764\n",
      "    load_time_ms: 1.613\n",
      "    num_steps_sampled: 710000\n",
      "    num_steps_trained: 710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.3881325968107683e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9223096966743469\n",
      "      kl: 0.005640849936753511\n",
      "      policy_loss: -0.0015313720796257257\n",
      "      total_loss: 1391.4886474609375\n",
      "      vf_explained_var: 0.42455512285232544\n",
      "      vf_loss: 1391.490234375\n",
      "    sample_time_ms: 21025.348\n",
      "    update_time_ms: 5.675\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -32.61922916904559\n",
      "  time_since_restore: 1894.0253276824951\n",
      "  time_this_iter_s: 24.92617177963257\n",
      "  time_total_s: 1894.0253276824951\n",
      "  timestamp: 1554810186\n",
      "  timesteps_since_restore: 710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 710000\n",
      "  training_iteration: 71\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1894 s, 71 iter, 710000 ts, -65.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-43-32\n",
      "  done: false\n",
      "  episode_len_mean: 94.5\n",
      "  episode_reward_max: 26.23425882429872\n",
      "  episode_reward_mean: -68.66893861776144\n",
      "  episode_reward_min: -201.99571281157378\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 6139\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4239.172\n",
      "    load_time_ms: 1.587\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6940662984053842e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9059934616088867\n",
      "      kl: 0.006988817360252142\n",
      "      policy_loss: -0.0021148251835256815\n",
      "      total_loss: 1402.1085205078125\n",
      "      vf_explained_var: 0.42968496680259705\n",
      "      vf_loss: 1402.1107177734375\n",
      "    sample_time_ms: 20683.03\n",
      "    update_time_ms: 5.741\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -34.334469308880735\n",
      "  time_since_restore: 1919.5290968418121\n",
      "  time_this_iter_s: 25.503769159317017\n",
      "  time_total_s: 1919.5290968418121\n",
      "  timestamp: 1554810212\n",
      "  timesteps_since_restore: 720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 72\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1919 s, 72 iter, 720000 ts, -68.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-43-58\n",
      "  done: false\n",
      "  episode_len_mean: 95.11320754716981\n",
      "  episode_reward_max: 27.456152918548323\n",
      "  episode_reward_mean: -66.04524845445368\n",
      "  episode_reward_min: -202.46609605414704\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 6245\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4231.324\n",
      "    load_time_ms: 1.629\n",
      "    num_steps_sampled: 730000\n",
      "    num_steps_trained: 730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.470331492026921e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8968561887741089\n",
      "      kl: 0.004797164350748062\n",
      "      policy_loss: -0.0009607416577637196\n",
      "      total_loss: 1351.832275390625\n",
      "      vf_explained_var: 0.445841521024704\n",
      "      vf_loss: 1351.8333740234375\n",
      "    sample_time_ms: 20666.948\n",
      "    update_time_ms: 5.917\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -33.02262422722684\n",
      "  time_since_restore: 1945.4924635887146\n",
      "  time_this_iter_s: 25.963366746902466\n",
      "  time_total_s: 1945.4924635887146\n",
      "  timestamp: 1554810238\n",
      "  timesteps_since_restore: 730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 730000\n",
      "  training_iteration: 73\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1945 s, 73 iter, 730000 ts, -66 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-44-23\n",
      "  done: false\n",
      "  episode_len_mean: 94.41509433962264\n",
      "  episode_reward_max: 22.96914292703882\n",
      "  episode_reward_mean: -66.31799829243269\n",
      "  episode_reward_min: -202.67337802556438\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 6351\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4257.412\n",
      "    load_time_ms: 1.684\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.2351657460134604e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8862440586090088\n",
      "      kl: 0.005029506515711546\n",
      "      policy_loss: -0.0015850032214075327\n",
      "      total_loss: 1330.22998046875\n",
      "      vf_explained_var: 0.4187515377998352\n",
      "      vf_loss: 1330.2315673828125\n",
      "    sample_time_ms: 20681.317\n",
      "    update_time_ms: 5.912\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -33.158999146216345\n",
      "  time_since_restore: 1970.728890657425\n",
      "  time_this_iter_s: 25.236427068710327\n",
      "  time_total_s: 1970.728890657425\n",
      "  timestamp: 1554810263\n",
      "  timesteps_since_restore: 740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 74\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1970 s, 74 iter, 740000 ts, -66.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-44-48\n",
      "  done: false\n",
      "  episode_len_mean: 96.02884615384616\n",
      "  episode_reward_max: 19.531246423390783\n",
      "  episode_reward_mean: -63.947057286626766\n",
      "  episode_reward_min: -202.63130881811108\n",
      "  episodes_this_iter: 104\n",
      "  episodes_total: 6455\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4243.935\n",
      "    load_time_ms: 1.706\n",
      "    num_steps_sampled: 750000\n",
      "    num_steps_trained: 750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.1175828730067302e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8632673621177673\n",
      "      kl: 0.005001331213861704\n",
      "      policy_loss: -0.0011024981504306197\n",
      "      total_loss: 1433.231201171875\n",
      "      vf_explained_var: 0.3914385437965393\n",
      "      vf_loss: 1433.232177734375\n",
      "    sample_time_ms: 20805.578\n",
      "    update_time_ms: 5.837\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -31.973528643313387\n",
      "  time_since_restore: 1995.5130410194397\n",
      "  time_this_iter_s: 24.78415036201477\n",
      "  time_total_s: 1995.5130410194397\n",
      "  timestamp: 1554810288\n",
      "  timesteps_since_restore: 750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 750000\n",
      "  training_iteration: 75\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 1995 s, 75 iter, 750000 ts, -63.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-45-13\n",
      "  done: false\n",
      "  episode_len_mean: 90.30630630630631\n",
      "  episode_reward_max: 21.481946675030343\n",
      "  episode_reward_mean: -77.54213193085874\n",
      "  episode_reward_min: -202.51573355551307\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 6566\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4247.679\n",
      "    load_time_ms: 1.706\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0587914365033651e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8571481704711914\n",
      "      kl: 0.006015871185809374\n",
      "      policy_loss: -0.002281009452417493\n",
      "      total_loss: 1361.908447265625\n",
      "      vf_explained_var: 0.43070924282073975\n",
      "      vf_loss: 1361.91064453125\n",
      "    sample_time_ms: 20710.498\n",
      "    update_time_ms: 5.884\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -38.77106596542936\n",
      "  time_since_restore: 2020.2020931243896\n",
      "  time_this_iter_s: 24.68905210494995\n",
      "  time_total_s: 2020.2020931243896\n",
      "  timestamp: 1554810313\n",
      "  timesteps_since_restore: 760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 76\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2020 s, 76 iter, 760000 ts, -77.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-45-38\n",
      "  done: false\n",
      "  episode_len_mean: 93.4392523364486\n",
      "  episode_reward_max: 20.174919777410935\n",
      "  episode_reward_mean: -68.14959590190334\n",
      "  episode_reward_min: -202.37220308616924\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 6673\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4247.27\n",
      "    load_time_ms: 1.667\n",
      "    num_steps_sampled: 770000\n",
      "    num_steps_trained: 770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.2939571825168255e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8554137945175171\n",
      "      kl: 0.005289954133331776\n",
      "      policy_loss: -0.0010674090590327978\n",
      "      total_loss: 1313.54150390625\n",
      "      vf_explained_var: 0.43004393577575684\n",
      "      vf_loss: 1313.5426025390625\n",
      "    sample_time_ms: 20654.656\n",
      "    update_time_ms: 5.83\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -34.074797950951684\n",
      "  time_since_restore: 2044.8802733421326\n",
      "  time_this_iter_s: 24.67818021774292\n",
      "  time_total_s: 2044.8802733421326\n",
      "  timestamp: 1554810338\n",
      "  timesteps_since_restore: 770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 770000\n",
      "  training_iteration: 77\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2044 s, 77 iter, 770000 ts, -68.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-46-02\n",
      "  done: false\n",
      "  episode_len_mean: 88.69026548672566\n",
      "  episode_reward_max: 18.914854599808983\n",
      "  episode_reward_mean: -85.67498328145057\n",
      "  episode_reward_min: -203.01072989558463\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 6786\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4250.819\n",
      "    load_time_ms: 1.709\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.6469785912584127e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8383052349090576\n",
      "      kl: 0.005945699289441109\n",
      "      policy_loss: -0.0011876631760969758\n",
      "      total_loss: 1358.2125244140625\n",
      "      vf_explained_var: 0.46158188581466675\n",
      "      vf_loss: 1358.2137451171875\n",
      "    sample_time_ms: 20681.556\n",
      "    update_time_ms: 5.784\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -42.83749164072528\n",
      "  time_since_restore: 2069.7137200832367\n",
      "  time_this_iter_s: 24.833446741104126\n",
      "  time_total_s: 2069.7137200832367\n",
      "  timestamp: 1554810362\n",
      "  timesteps_since_restore: 780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 78\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2069 s, 78 iter, 780000 ts, -85.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-46-29\n",
      "  done: false\n",
      "  episode_len_mean: 93.32710280373831\n",
      "  episode_reward_max: 18.877238313654903\n",
      "  episode_reward_mean: -69.25224201999147\n",
      "  episode_reward_min: -201.82996223218652\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 6893\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4319.756\n",
      "    load_time_ms: 1.725\n",
      "    num_steps_sampled: 790000\n",
      "    num_steps_trained: 790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3234892956292064e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8425794839859009\n",
      "      kl: 0.005578184965997934\n",
      "      policy_loss: -0.001282021403312683\n",
      "      total_loss: 1371.1158447265625\n",
      "      vf_explained_var: 0.3943439722061157\n",
      "      vf_loss: 1371.116943359375\n",
      "    sample_time_ms: 20831.851\n",
      "    update_time_ms: 5.684\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -34.62612100999575\n",
      "  time_since_restore: 2096.1382071971893\n",
      "  time_this_iter_s: 26.424487113952637\n",
      "  time_total_s: 2096.1382071971893\n",
      "  timestamp: 1554810389\n",
      "  timesteps_since_restore: 790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 790000\n",
      "  training_iteration: 79\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2096 s, 79 iter, 790000 ts, -69.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-46-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.62\n",
      "  episode_reward_max: 20.34271561929551\n",
      "  episode_reward_mean: -53.077561489537835\n",
      "  episode_reward_min: -202.2868146677725\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 6992\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4335.667\n",
      "    load_time_ms: 1.725\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.617446478146032e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8427311778068542\n",
      "      kl: 0.004145412240177393\n",
      "      policy_loss: -0.0005150377983227372\n",
      "      total_loss: 1428.3497314453125\n",
      "      vf_explained_var: 0.34653303027153015\n",
      "      vf_loss: 1428.3502197265625\n",
      "    sample_time_ms: 21135.025\n",
      "    update_time_ms: 6.892\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -26.538780744768918\n",
      "  time_since_restore: 2124.0892901420593\n",
      "  time_this_iter_s: 27.951082944869995\n",
      "  time_total_s: 2124.0892901420593\n",
      "  timestamp: 1554810417\n",
      "  timesteps_since_restore: 800000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 80\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2124 s, 80 iter, 800000 ts, -53.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-47-23\n",
      "  done: false\n",
      "  episode_len_mean: 92.62037037037037\n",
      "  episode_reward_max: 21.28163795717896\n",
      "  episode_reward_mean: -75.08059353466325\n",
      "  episode_reward_min: -201.93975139433005\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 7100\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4475.108\n",
      "    load_time_ms: 1.794\n",
      "    num_steps_sampled: 810000\n",
      "    num_steps_trained: 810000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.308723239073016e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8406602144241333\n",
      "      kl: 0.005424166563898325\n",
      "      policy_loss: -0.0010715308599174023\n",
      "      total_loss: 1247.646728515625\n",
      "      vf_explained_var: 0.48554727435112\n",
      "      vf_loss: 1247.6478271484375\n",
      "    sample_time_ms: 21157.943\n",
      "    update_time_ms: 7.099\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -37.54029676733163\n",
      "  time_since_restore: 2150.644957780838\n",
      "  time_this_iter_s: 26.555667638778687\n",
      "  time_total_s: 2150.644957780838\n",
      "  timestamp: 1554810443\n",
      "  timesteps_since_restore: 810000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 810000\n",
      "  training_iteration: 81\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2150 s, 81 iter, 810000 ts, -75.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-47-49\n",
      "  done: false\n",
      "  episode_len_mean: 97.7843137254902\n",
      "  episode_reward_max: 23.786416796915425\n",
      "  episode_reward_mean: -58.363244948499194\n",
      "  episode_reward_min: -203.70146304790006\n",
      "  episodes_this_iter: 102\n",
      "  episodes_total: 7202\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4463.251\n",
      "    load_time_ms: 1.792\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.654361619536508e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8472520709037781\n",
      "      kl: 0.0048398603685200214\n",
      "      policy_loss: -0.0009178169420920312\n",
      "      total_loss: 1396.052978515625\n",
      "      vf_explained_var: 0.37273576855659485\n",
      "      vf_loss: 1396.053955078125\n",
      "    sample_time_ms: 21145.761\n",
      "    update_time_ms: 7.027\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -29.181622474249597\n",
      "  time_since_restore: 2175.9058887958527\n",
      "  time_this_iter_s: 25.26093101501465\n",
      "  time_total_s: 2175.9058887958527\n",
      "  timestamp: 1554810469\n",
      "  timesteps_since_restore: 820000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 82\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2175 s, 82 iter, 820000 ts, -58.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-48-15\n",
      "  done: false\n",
      "  episode_len_mean: 91.97247706422019\n",
      "  episode_reward_max: 25.310804770614173\n",
      "  episode_reward_mean: -75.18780807326915\n",
      "  episode_reward_min: -202.2905760913917\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 7311\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4476.321\n",
      "    load_time_ms: 1.72\n",
      "    num_steps_sampled: 830000\n",
      "    num_steps_trained: 830000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.27180809768254e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8408868312835693\n",
      "      kl: 0.005488715134561062\n",
      "      policy_loss: -0.0010671118507161736\n",
      "      total_loss: 1295.2255859375\n",
      "      vf_explained_var: 0.46409422159194946\n",
      "      vf_loss: 1295.226806640625\n",
      "    sample_time_ms: 21106.361\n",
      "    update_time_ms: 7.65\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -37.59390403663458\n",
      "  time_since_restore: 2201.6130516529083\n",
      "  time_this_iter_s: 25.707162857055664\n",
      "  time_total_s: 2201.6130516529083\n",
      "  timestamp: 1554810495\n",
      "  timesteps_since_restore: 830000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 830000\n",
      "  training_iteration: 83\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2201 s, 83 iter, 830000 ts, -75.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-48-40\n",
      "  done: false\n",
      "  episode_len_mean: 96.46601941747574\n",
      "  episode_reward_max: 24.44659982924847\n",
      "  episode_reward_mean: -60.343967399374044\n",
      "  episode_reward_min: -203.27332255398227\n",
      "  episodes_this_iter: 103\n",
      "  episodes_total: 7414\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4479.537\n",
      "    load_time_ms: 1.66\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.13590404884127e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8498907089233398\n",
      "      kl: 0.004289453383535147\n",
      "      policy_loss: -0.0014322430361062288\n",
      "      total_loss: 1392.9044189453125\n",
      "      vf_explained_var: 0.36669614911079407\n",
      "      vf_loss: 1392.9058837890625\n",
      "    sample_time_ms: 21104.402\n",
      "    update_time_ms: 7.7\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -30.17198369968703\n",
      "  time_since_restore: 2226.856554031372\n",
      "  time_this_iter_s: 25.243502378463745\n",
      "  time_total_s: 2226.856554031372\n",
      "  timestamp: 1554810520\n",
      "  timesteps_since_restore: 840000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 84\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2226 s, 84 iter, 840000 ts, -60.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-49-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.34653465346534\n",
      "  episode_reward_max: 23.52268786981621\n",
      "  episode_reward_mean: -52.44167031960741\n",
      "  episode_reward_min: -202.08349387323432\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 7515\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4580.139\n",
      "    load_time_ms: 1.659\n",
      "    num_steps_sampled: 850000\n",
      "    num_steps_trained: 850000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.067952024420635e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8435885906219482\n",
      "      kl: 0.0056764609180390835\n",
      "      policy_loss: -0.002085037063807249\n",
      "      total_loss: 1392.4512939453125\n",
      "      vf_explained_var: 0.3590649366378784\n",
      "      vf_loss: 1392.453369140625\n",
      "    sample_time_ms: 21399.139\n",
      "    update_time_ms: 7.657\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -26.220835159803716\n",
      "  time_since_restore: 2255.6044466495514\n",
      "  time_this_iter_s: 28.74789261817932\n",
      "  time_total_s: 2255.6044466495514\n",
      "  timestamp: 1554810549\n",
      "  timesteps_since_restore: 850000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 850000\n",
      "  training_iteration: 85\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2255 s, 85 iter, 850000 ts, -52.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-49-39\n",
      "  done: false\n",
      "  episode_len_mean: 99.62\n",
      "  episode_reward_max: 24.588349775724723\n",
      "  episode_reward_mean: -54.60825109398371\n",
      "  episode_reward_min: -202.57857002694175\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 7615\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4590.067\n",
      "    load_time_ms: 1.68\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0339760122103175e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8330186605453491\n",
      "      kl: 0.005717768333852291\n",
      "      policy_loss: -0.001063407282344997\n",
      "      total_loss: 1420.8212890625\n",
      "      vf_explained_var: 0.36431047320365906\n",
      "      vf_loss: 1420.822265625\n",
      "    sample_time_ms: 21983.474\n",
      "    update_time_ms: 7.795\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -27.30412554699186\n",
      "  time_since_restore: 2286.2357409000397\n",
      "  time_this_iter_s: 30.63129425048828\n",
      "  time_total_s: 2286.2357409000397\n",
      "  timestamp: 1554810579\n",
      "  timesteps_since_restore: 860000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 86\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2286 s, 86 iter, 860000 ts, -54.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-50-07\n",
      "  done: false\n",
      "  episode_len_mean: 92.70370370370371\n",
      "  episode_reward_max: 26.391072902676996\n",
      "  episode_reward_mean: -72.28958742809962\n",
      "  episode_reward_min: -202.8307724919642\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 7723\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4590.027\n",
      "    load_time_ms: 1.693\n",
      "    num_steps_sampled: 870000\n",
      "    num_steps_trained: 870000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.1698800610515874e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8248167634010315\n",
      "      kl: 0.004779214505106211\n",
      "      policy_loss: -0.0021821013651788235\n",
      "      total_loss: 1322.5924072265625\n",
      "      vf_explained_var: 0.4541299343109131\n",
      "      vf_loss: 1322.5946044921875\n",
      "    sample_time_ms: 22250.036\n",
      "    update_time_ms: 7.906\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -36.144793714049804\n",
      "  time_since_restore: 2313.581480741501\n",
      "  time_this_iter_s: 27.34573984146118\n",
      "  time_total_s: 2313.581480741501\n",
      "  timestamp: 1554810607\n",
      "  timesteps_since_restore: 870000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 870000\n",
      "  training_iteration: 87\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2313 s, 87 iter, 870000 ts, -72.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-50-37\n",
      "  done: false\n",
      "  episode_len_mean: 89.84821428571429\n",
      "  episode_reward_max: 22.68885308197593\n",
      "  episode_reward_mean: -81.87764846887914\n",
      "  episode_reward_min: -202.88804404512916\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 7835\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4652.612\n",
      "    load_time_ms: 1.623\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5849400305257937e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8167470097541809\n",
      "      kl: 0.003662643488496542\n",
      "      policy_loss: -0.0004845750518143177\n",
      "      total_loss: 1319.4681396484375\n",
      "      vf_explained_var: 0.4450209140777588\n",
      "      vf_loss: 1319.468505859375\n",
      "    sample_time_ms: 22689.907\n",
      "    update_time_ms: 8.109\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -40.93882423443957\n",
      "  time_since_restore: 2343.44290971756\n",
      "  time_this_iter_s: 29.86142897605896\n",
      "  time_total_s: 2343.44290971756\n",
      "  timestamp: 1554810637\n",
      "  timesteps_since_restore: 880000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 88\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2343 s, 88 iter, 880000 ts, -81.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-51-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.11818181818182\n",
      "  episode_reward_max: 20.77449338049013\n",
      "  episode_reward_mean: -80.13434883137374\n",
      "  episode_reward_min: -202.10872666996391\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 7945\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4587.717\n",
      "    load_time_ms: 1.585\n",
      "    num_steps_sampled: 890000\n",
      "    num_steps_trained: 890000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2924700152628968e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.809503436088562\n",
      "      kl: 0.006607650779187679\n",
      "      policy_loss: -0.002368329092860222\n",
      "      total_loss: 1333.42529296875\n",
      "      vf_explained_var: 0.46741151809692383\n",
      "      vf_loss: 1333.427734375\n",
      "    sample_time_ms: 22557.159\n",
      "    update_time_ms: 8.037\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -40.06717441568686\n",
      "  time_since_restore: 2367.885939359665\n",
      "  time_this_iter_s: 24.443029642105103\n",
      "  time_total_s: 2367.885939359665\n",
      "  timestamp: 1554810661\n",
      "  timesteps_since_restore: 890000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 890000\n",
      "  training_iteration: 89\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2367 s, 89 iter, 890000 ts, -80.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-51-25\n",
      "  done: false\n",
      "  episode_len_mean: 102.43\n",
      "  episode_reward_max: 23.135169807976446\n",
      "  episode_reward_mean: -47.32570961926411\n",
      "  episode_reward_min: -202.49758978030565\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 8042\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4568.872\n",
      "    load_time_ms: 1.588\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.462350076314484e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8138663172721863\n",
      "      kl: 0.0039367713034152985\n",
      "      policy_loss: -0.0006271511083468795\n",
      "      total_loss: 1430.0350341796875\n",
      "      vf_explained_var: 0.3636220097541809\n",
      "      vf_loss: 1430.0355224609375\n",
      "    sample_time_ms: 22219.889\n",
      "    update_time_ms: 6.606\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -23.662854809632044\n",
      "  time_since_restore: 2392.2616176605225\n",
      "  time_this_iter_s: 24.375678300857544\n",
      "  time_total_s: 2392.2616176605225\n",
      "  timestamp: 1554810685\n",
      "  timesteps_since_restore: 900000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 90\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2392 s, 90 iter, 900000 ts, -47.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-51-49\n",
      "  done: false\n",
      "  episode_len_mean: 101.62\n",
      "  episode_reward_max: 21.207782656562678\n",
      "  episode_reward_mean: -47.26063801333388\n",
      "  episode_reward_min: -202.21931817577695\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 8140\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4426.678\n",
      "    load_time_ms: 1.526\n",
      "    num_steps_sampled: 910000\n",
      "    num_steps_trained: 910000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.231175038157242e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.809399425983429\n",
      "      kl: 0.004713853355497122\n",
      "      policy_loss: -0.001095946179702878\n",
      "      total_loss: 1499.9993896484375\n",
      "      vf_explained_var: 0.3293927311897278\n",
      "      vf_loss: 1500.0003662109375\n",
      "    sample_time_ms: 22031.678\n",
      "    update_time_ms: 6.368\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -23.630319006666944\n",
      "  time_since_restore: 2415.5045926570892\n",
      "  time_this_iter_s: 23.242974996566772\n",
      "  time_total_s: 2415.5045926570892\n",
      "  timestamp: 1554810709\n",
      "  timesteps_since_restore: 910000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 910000\n",
      "  training_iteration: 91\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2415 s, 91 iter, 910000 ts, -47.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-52-13\n",
      "  done: false\n",
      "  episode_len_mean: 98.35643564356435\n",
      "  episode_reward_max: 20.016298608138015\n",
      "  episode_reward_mean: -58.95792497326795\n",
      "  episode_reward_min: -201.7654345886114\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 8241\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4418.211\n",
      "    load_time_ms: 1.537\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.615587519078621e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8104059100151062\n",
      "      kl: 0.004135179799050093\n",
      "      policy_loss: 9.69012689893134e-05\n",
      "      total_loss: 1458.9598388671875\n",
      "      vf_explained_var: 0.41385072469711304\n",
      "      vf_loss: 1458.9595947265625\n",
      "    sample_time_ms: 21945.435\n",
      "    update_time_ms: 6.523\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -29.478962486633986\n",
      "  time_since_restore: 2439.8222539424896\n",
      "  time_this_iter_s: 24.31766128540039\n",
      "  time_total_s: 2439.8222539424896\n",
      "  timestamp: 1554810733\n",
      "  timesteps_since_restore: 920000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 92\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2439 s, 92 iter, 920000 ts, -59 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-52-37\n",
      "  done: false\n",
      "  episode_len_mean: 94.93396226415095\n",
      "  episode_reward_max: 20.871463203140422\n",
      "  episode_reward_mean: -66.91782162817799\n",
      "  episode_reward_min: -201.6255191237148\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 8347\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4383.603\n",
      "    load_time_ms: 1.56\n",
      "    num_steps_sampled: 930000\n",
      "    num_steps_trained: 930000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.077937595393105e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8112401962280273\n",
      "      kl: 0.005716491024941206\n",
      "      policy_loss: -0.0021885570604354143\n",
      "      total_loss: 1435.714111328125\n",
      "      vf_explained_var: 0.4409423768520355\n",
      "      vf_loss: 1435.71630859375\n",
      "    sample_time_ms: 21787.241\n",
      "    update_time_ms: 5.803\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -33.458910814089\n",
      "  time_since_restore: 2463.5967831611633\n",
      "  time_this_iter_s: 23.774529218673706\n",
      "  time_total_s: 2463.5967831611633\n",
      "  timestamp: 1554810757\n",
      "  timesteps_since_restore: 930000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 930000\n",
      "  training_iteration: 93\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2463 s, 93 iter, 930000 ts, -66.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-53-01\n",
      "  done: false\n",
      "  episode_len_mean: 93.48113207547169\n",
      "  episode_reward_max: 19.19911800398603\n",
      "  episode_reward_mean: -72.71155183096764\n",
      "  episode_reward_min: -201.70402695453993\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 8453\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4344.758\n",
      "    load_time_ms: 1.598\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.0389687976965526e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7976839542388916\n",
      "      kl: 0.005232976749539375\n",
      "      policy_loss: -0.0013582942774519324\n",
      "      total_loss: 1429.2200927734375\n",
      "      vf_explained_var: 0.46782028675079346\n",
      "      vf_loss: 1429.2213134765625\n",
      "    sample_time_ms: 21712.45\n",
      "    update_time_ms: 5.749\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -36.35577591548381\n",
      "  time_since_restore: 2487.705851793289\n",
      "  time_this_iter_s: 24.109068632125854\n",
      "  time_total_s: 2487.705851793289\n",
      "  timestamp: 1554810781\n",
      "  timesteps_since_restore: 940000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 94\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2487 s, 94 iter, 940000 ts, -72.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-53-25\n",
      "  done: false\n",
      "  episode_len_mean: 91.15315315315316\n",
      "  episode_reward_max: 19.15639301244613\n",
      "  episode_reward_mean: -76.16872860695027\n",
      "  episode_reward_min: -202.1852327248344\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 8564\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4249.843\n",
      "    load_time_ms: 1.587\n",
      "    num_steps_sampled: 950000\n",
      "    num_steps_trained: 950000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.0194843988482763e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7847568392753601\n",
      "      kl: 0.003804944222792983\n",
      "      policy_loss: -0.0010086694965139031\n",
      "      total_loss: 1368.461669921875\n",
      "      vf_explained_var: 0.4861702024936676\n",
      "      vf_loss: 1368.462646484375\n",
      "    sample_time_ms: 21361.334\n",
      "    update_time_ms: 5.781\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -38.08436430347513\n",
      "  time_since_restore: 2511.9848957061768\n",
      "  time_this_iter_s: 24.279043912887573\n",
      "  time_total_s: 2511.9848957061768\n",
      "  timestamp: 1554810805\n",
      "  timesteps_since_restore: 950000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 950000\n",
      "  training_iteration: 95\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2511 s, 95 iter, 950000 ts, -76.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-53-54\n",
      "  done: false\n",
      "  episode_len_mean: 93.17757009345794\n",
      "  episode_reward_max: 18.049660931550495\n",
      "  episode_reward_mean: -69.82959320234734\n",
      "  episode_reward_min: -202.28399713763906\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 8671\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4361.459\n",
      "    load_time_ms: 1.675\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0097421994241382e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7903825640678406\n",
      "      kl: 0.006343415006995201\n",
      "      policy_loss: -0.0020004999823868275\n",
      "      total_loss: 1361.89404296875\n",
      "      vf_explained_var: 0.47008559107780457\n",
      "      vf_loss: 1361.89599609375\n",
      "    sample_time_ms: 21069.165\n",
      "    update_time_ms: 5.559\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -34.914796601173656\n",
      "  time_since_restore: 2540.809586048126\n",
      "  time_this_iter_s: 28.824690341949463\n",
      "  time_total_s: 2540.809586048126\n",
      "  timestamp: 1554810834\n",
      "  timesteps_since_restore: 960000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 96\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2540 s, 96 iter, 960000 ts, -69.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-54-18\n",
      "  done: false\n",
      "  episode_len_mean: 98.84158415841584\n",
      "  episode_reward_max: 18.236984345885325\n",
      "  episode_reward_mean: -56.54315831742678\n",
      "  episode_reward_min: -201.99517222701826\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 8772\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4364.79\n",
      "    load_time_ms: 1.659\n",
      "    num_steps_sampled: 970000\n",
      "    num_steps_trained: 970000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.048710997120691e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7807376384735107\n",
      "      kl: 0.005368008743971586\n",
      "      policy_loss: -0.0009596557938493788\n",
      "      total_loss: 1500.340576171875\n",
      "      vf_explained_var: 0.38170483708381653\n",
      "      vf_loss: 1500.3414306640625\n",
      "    sample_time_ms: 20744.125\n",
      "    update_time_ms: 5.567\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -28.271579158713394\n",
      "  time_since_restore: 2564.937420129776\n",
      "  time_this_iter_s: 24.12783408164978\n",
      "  time_total_s: 2564.937420129776\n",
      "  timestamp: 1554810858\n",
      "  timesteps_since_restore: 970000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 970000\n",
      "  training_iteration: 97\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2564 s, 97 iter, 970000 ts, -56.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-54-43\n",
      "  done: false\n",
      "  episode_len_mean: 90.35454545454546\n",
      "  episode_reward_max: 15.335283187907388\n",
      "  episode_reward_mean: -77.65931072168493\n",
      "  episode_reward_min: -202.18043071138104\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 8882\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4297.831\n",
      "    load_time_ms: 1.669\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5243554985603454e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7727394104003906\n",
      "      kl: 0.005413609091192484\n",
      "      policy_loss: -0.001954768318682909\n",
      "      total_loss: 1461.2982177734375\n",
      "      vf_explained_var: 0.41504189372062683\n",
      "      vf_loss: 1461.300537109375\n",
      "    sample_time_ms: 20295.61\n",
      "    update_time_ms: 6.647\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -38.82965536084248\n",
      "  time_since_restore: 2589.653036594391\n",
      "  time_this_iter_s: 24.715616464614868\n",
      "  time_total_s: 2589.653036594391\n",
      "  timestamp: 1554810883\n",
      "  timesteps_since_restore: 980000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 98\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2589 s, 98 iter, 980000 ts, -77.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-55-08\n",
      "  done: false\n",
      "  episode_len_mean: 87.13793103448276\n",
      "  episode_reward_max: 14.923458044732934\n",
      "  episode_reward_mean: -88.89090505849583\n",
      "  episode_reward_min: -201.59687398396633\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 8998\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4288.924\n",
      "    load_time_ms: 1.675\n",
      "    num_steps_sampled: 990000\n",
      "    num_steps_trained: 990000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2621777492801727e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7608442306518555\n",
      "      kl: 0.005272991955280304\n",
      "      policy_loss: -0.0016012376872822642\n",
      "      total_loss: 1288.5115966796875\n",
      "      vf_explained_var: 0.5016191005706787\n",
      "      vf_loss: 1288.51318359375\n",
      "    sample_time_ms: 20320.729\n",
      "    update_time_ms: 6.756\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -44.44545252924793\n",
      "  time_since_restore: 2614.262196779251\n",
      "  time_this_iter_s: 24.60916018486023\n",
      "  time_total_s: 2614.262196779251\n",
      "  timestamp: 1554810908\n",
      "  timesteps_since_restore: 990000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 990000\n",
      "  training_iteration: 99\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2614 s, 99 iter, 990000 ts, -88.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-55-34\n",
      "  done: false\n",
      "  episode_len_mean: 91.2\n",
      "  episode_reward_max: 14.083311083676136\n",
      "  episode_reward_mean: -75.86682830912966\n",
      "  episode_reward_min: -201.23993386493157\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 9108\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4404.143\n",
      "    load_time_ms: 1.716\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.3108887464008635e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7535067796707153\n",
      "      kl: 0.004181527066975832\n",
      "      policy_loss: -0.00027323493850417435\n",
      "      total_loss: 1392.6075439453125\n",
      "      vf_explained_var: 0.4708426892757416\n",
      "      vf_loss: 1392.6080322265625\n",
      "    sample_time_ms: 20390.074\n",
      "    update_time_ms: 6.913\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -37.93341415456484\n",
      "  time_since_restore: 2640.487959623337\n",
      "  time_this_iter_s: 26.225762844085693\n",
      "  time_total_s: 2640.487959623337\n",
      "  timestamp: 1554810934\n",
      "  timesteps_since_restore: 1000000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 100\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2640 s, 100 iter, 1000000 ts, -75.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-56-02\n",
      "  done: false\n",
      "  episode_len_mean: 93.44339622641509\n",
      "  episode_reward_max: 13.544955837195678\n",
      "  episode_reward_mean: -70.02026120741583\n",
      "  episode_reward_min: -201.32040822122963\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 9214\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4410.447\n",
      "    load_time_ms: 1.717\n",
      "    num_steps_sampled: 1010000\n",
      "    num_steps_trained: 1010000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.1554443732004317e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7452145218849182\n",
      "      kl: 0.005241906736046076\n",
      "      policy_loss: -0.001215618452988565\n",
      "      total_loss: 1372.108154296875\n",
      "      vf_explained_var: 0.4692055284976959\n",
      "      vf_loss: 1372.109375\n",
      "    sample_time_ms: 20873.925\n",
      "    update_time_ms: 7.068\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -35.01013060370793\n",
      "  time_since_restore: 2668.6347863674164\n",
      "  time_this_iter_s: 28.14682674407959\n",
      "  time_total_s: 2668.6347863674164\n",
      "  timestamp: 1554810962\n",
      "  timesteps_since_restore: 1010000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1010000\n",
      "  training_iteration: 101\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2668 s, 101 iter, 1010000 ts, -70 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-56-29\n",
      "  done: false\n",
      "  episode_len_mean: 90.09009009009009\n",
      "  episode_reward_max: 14.106218429517654\n",
      "  episode_reward_mean: -79.72859604968409\n",
      "  episode_reward_min: -201.15683118651054\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 9325\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4542.518\n",
      "    load_time_ms: 1.707\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5777221866002159e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7356601357460022\n",
      "      kl: 0.005261532962322235\n",
      "      policy_loss: -0.0007602327968925238\n",
      "      total_loss: 1341.1383056640625\n",
      "      vf_explained_var: 0.4933713376522064\n",
      "      vf_loss: 1341.13916015625\n",
      "    sample_time_ms: 20965.989\n",
      "    update_time_ms: 6.914\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -39.86429802484204\n",
      "  time_since_restore: 2695.1995463371277\n",
      "  time_this_iter_s: 26.564759969711304\n",
      "  time_total_s: 2695.1995463371277\n",
      "  timestamp: 1554810989\n",
      "  timesteps_since_restore: 1020000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 102\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2695 s, 102 iter, 1020000 ts, -79.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-56-56\n",
      "  done: false\n",
      "  episode_len_mean: 94.84761904761905\n",
      "  episode_reward_max: 17.29877167097174\n",
      "  episode_reward_mean: -67.0170028654003\n",
      "  episode_reward_min: -201.17333283759478\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 9430\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4639.564\n",
      "    load_time_ms: 1.644\n",
      "    num_steps_sampled: 1030000\n",
      "    num_steps_trained: 1030000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.888610933001079e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7307636141777039\n",
      "      kl: 0.004998000804334879\n",
      "      policy_loss: -0.0006135260919108987\n",
      "      total_loss: 1418.1708984375\n",
      "      vf_explained_var: 0.44998258352279663\n",
      "      vf_loss: 1418.171630859375\n",
      "    sample_time_ms: 21186.633\n",
      "    update_time_ms: 6.871\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -33.50850143270015\n",
      "  time_since_restore: 2722.147851705551\n",
      "  time_this_iter_s: 26.948305368423462\n",
      "  time_total_s: 2722.147851705551\n",
      "  timestamp: 1554811016\n",
      "  timesteps_since_restore: 1030000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1030000\n",
      "  training_iteration: 103\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2722 s, 103 iter, 1030000 ts, -67 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-57-23\n",
      "  done: false\n",
      "  episode_len_mean: 94.25233644859813\n",
      "  episode_reward_max: 16.211275892775944\n",
      "  episode_reward_mean: -66.9781045684831\n",
      "  episode_reward_min: -201.43490230944857\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 9537\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4852.004\n",
      "    load_time_ms: 1.61\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.9443054665005397e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7240200638771057\n",
      "      kl: 0.005560462828725576\n",
      "      policy_loss: -0.0015806066803634167\n",
      "      total_loss: 1461.16650390625\n",
      "      vf_explained_var: 0.4161209762096405\n",
      "      vf_loss: 1461.1680908203125\n",
      "    sample_time_ms: 21316.664\n",
      "    update_time_ms: 6.998\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -33.489052284241545\n",
      "  time_since_restore: 2749.6839797496796\n",
      "  time_this_iter_s: 27.536128044128418\n",
      "  time_total_s: 2749.6839797496796\n",
      "  timestamp: 1554811043\n",
      "  timesteps_since_restore: 1040000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 104\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2749 s, 104 iter, 1040000 ts, -67 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-57-53\n",
      "  done: false\n",
      "  episode_len_mean: 94.20560747663552\n",
      "  episode_reward_max: 14.524800957246455\n",
      "  episode_reward_mean: -68.03063646888234\n",
      "  episode_reward_min: -201.04506852584575\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 9644\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4853.163\n",
      "    load_time_ms: 1.612\n",
      "    num_steps_sampled: 1050000\n",
      "    num_steps_trained: 1050000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9721527332502698e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7148193717002869\n",
      "      kl: 0.006158728618174791\n",
      "      policy_loss: -0.0016652628546580672\n",
      "      total_loss: 1394.7625732421875\n",
      "      vf_explained_var: 0.45522767305374146\n",
      "      vf_loss: 1394.7642822265625\n",
      "    sample_time_ms: 21841.249\n",
      "    update_time_ms: 7.117\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -34.01531823444116\n",
      "  time_since_restore: 2779.221018075943\n",
      "  time_this_iter_s: 29.537038326263428\n",
      "  time_total_s: 2779.221018075943\n",
      "  timestamp: 1554811073\n",
      "  timesteps_since_restore: 1050000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1050000\n",
      "  training_iteration: 105\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2779 s, 105 iter, 1050000 ts, -68 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-58-22\n",
      "  done: false\n",
      "  episode_len_mean: 92.14018691588785\n",
      "  episode_reward_max: 14.851853442473562\n",
      "  episode_reward_mean: -73.03962332133149\n",
      "  episode_reward_min: -201.20916389145452\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 9751\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4737.179\n",
      "    load_time_ms: 1.579\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1060000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.860763666251349e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7063418030738831\n",
      "      kl: 0.007378404028713703\n",
      "      policy_loss: -0.0020358574111014605\n",
      "      total_loss: 1369.300048828125\n",
      "      vf_explained_var: 0.4700121283531189\n",
      "      vf_loss: 1369.3021240234375\n",
      "    sample_time_ms: 21957.681\n",
      "    update_time_ms: 7.139\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -36.519811660665745\n",
      "  time_since_restore: 2808.0486183166504\n",
      "  time_this_iter_s: 28.827600240707397\n",
      "  time_total_s: 2808.0486183166504\n",
      "  timestamp: 1554811102\n",
      "  timesteps_since_restore: 1060000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 106\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2808 s, 106 iter, 1060000 ts, -73 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-58-48\n",
      "  done: false\n",
      "  episode_len_mean: 94.98113207547169\n",
      "  episode_reward_max: 13.212754320273039\n",
      "  episode_reward_mean: -66.63885047570619\n",
      "  episode_reward_min: -201.46848582390584\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 9857\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4840.23\n",
      "    load_time_ms: 1.683\n",
      "    num_steps_sampled: 1070000\n",
      "    num_steps_trained: 1070000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.9303818331256746e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.702870786190033\n",
      "      kl: 0.004763740114867687\n",
      "      policy_loss: -0.0012839249102398753\n",
      "      total_loss: 1382.637451171875\n",
      "      vf_explained_var: 0.45435217022895813\n",
      "      vf_loss: 1382.638671875\n",
      "    sample_time_ms: 22031.683\n",
      "    update_time_ms: 7.011\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -33.31942523785309\n",
      "  time_since_restore: 2833.949125766754\n",
      "  time_this_iter_s: 25.90050745010376\n",
      "  time_total_s: 2833.949125766754\n",
      "  timestamp: 1554811128\n",
      "  timesteps_since_restore: 1070000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1070000\n",
      "  training_iteration: 107\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2833 s, 107 iter, 1070000 ts, -66.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-59-12\n",
      "  done: false\n",
      "  episode_len_mean: 83.18333333333334\n",
      "  episode_reward_max: 13.203823118985351\n",
      "  episode_reward_mean: -98.4742426921904\n",
      "  episode_reward_min: -200.8741510004479\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 9977\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4846.49\n",
      "    load_time_ms: 1.691\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4651909165628373e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7001216411590576\n",
      "      kl: 0.004361668601632118\n",
      "      policy_loss: -0.0008928672759793699\n",
      "      total_loss: 1312.91748046875\n",
      "      vf_explained_var: 0.5259613394737244\n",
      "      vf_loss: 1312.91845703125\n",
      "    sample_time_ms: 21982.822\n",
      "    update_time_ms: 5.762\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -49.23712134609518\n",
      "  time_since_restore: 2858.2288019657135\n",
      "  time_this_iter_s: 24.27967619895935\n",
      "  time_total_s: 2858.2288019657135\n",
      "  timestamp: 1554811152\n",
      "  timesteps_since_restore: 1080000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 108\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2858 s, 108 iter, 1080000 ts, -98.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_13-59-36\n",
      "  done: false\n",
      "  episode_len_mean: 91.85321100917432\n",
      "  episode_reward_max: 12.887953478583832\n",
      "  episode_reward_mean: -75.56262462262679\n",
      "  episode_reward_min: -201.09537817012693\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 10086\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4849.177\n",
      "    load_time_ms: 1.674\n",
      "    num_steps_sampled: 1090000\n",
      "    num_steps_trained: 1090000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2325954582814187e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6948843002319336\n",
      "      kl: 0.004746666178107262\n",
      "      policy_loss: -0.00019881052139680833\n",
      "      total_loss: 1453.90478515625\n",
      "      vf_explained_var: 0.44305986166000366\n",
      "      vf_loss: 1453.9049072265625\n",
      "    sample_time_ms: 21909.44\n",
      "    update_time_ms: 5.867\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -37.78131231131339\n",
      "  time_since_restore: 2882.128294467926\n",
      "  time_this_iter_s: 23.899492502212524\n",
      "  time_total_s: 2882.128294467926\n",
      "  timestamp: 1554811176\n",
      "  timesteps_since_restore: 1090000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1090000\n",
      "  training_iteration: 109\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2882 s, 109 iter, 1090000 ts, -75.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-00-01\n",
      "  done: false\n",
      "  episode_len_mean: 92.25\n",
      "  episode_reward_max: 10.638865122192843\n",
      "  episode_reward_mean: -72.45650105302485\n",
      "  episode_reward_min: -200.99637221002686\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 10194\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4730.22\n",
      "    load_time_ms: 1.694\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.162977291407093e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6865118741989136\n",
      "      kl: 0.0056592789478600025\n",
      "      policy_loss: -0.002412928966805339\n",
      "      total_loss: 1420.468505859375\n",
      "      vf_explained_var: 0.4571181535720825\n",
      "      vf_loss: 1420.470947265625\n",
      "    sample_time_ms: 21875.674\n",
      "    update_time_ms: 5.706\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -36.228250526512426\n",
      "  time_since_restore: 2906.8249344825745\n",
      "  time_this_iter_s: 24.696640014648438\n",
      "  time_total_s: 2906.8249344825745\n",
      "  timestamp: 1554811201\n",
      "  timesteps_since_restore: 1100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 110\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2906 s, 110 iter, 1100000 ts, -72.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-00-26\n",
      "  done: false\n",
      "  episode_len_mean: 92.35185185185185\n",
      "  episode_reward_max: 10.126430522627935\n",
      "  episode_reward_mean: -73.06752990636221\n",
      "  episode_reward_min: -201.32301554234076\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 10302\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4725.019\n",
      "    load_time_ms: 1.692\n",
      "    num_steps_sampled: 1110000\n",
      "    num_steps_trained: 1110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.0814886457035466e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6726891994476318\n",
      "      kl: 0.008999135345220566\n",
      "      policy_loss: -0.0025880178436636925\n",
      "      total_loss: 1445.6876220703125\n",
      "      vf_explained_var: 0.45318514108657837\n",
      "      vf_loss: 1445.6903076171875\n",
      "    sample_time_ms: 21547.713\n",
      "    update_time_ms: 5.705\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -36.53376495318111\n",
      "  time_since_restore: 2931.6407222747803\n",
      "  time_this_iter_s: 24.81578779220581\n",
      "  time_total_s: 2931.6407222747803\n",
      "  timestamp: 1554811226\n",
      "  timesteps_since_restore: 1110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1110000\n",
      "  training_iteration: 111\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2931 s, 111 iter, 1110000 ts, -73.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-00-51\n",
      "  done: false\n",
      "  episode_len_mean: 91.72727272727273\n",
      "  episode_reward_max: 10.320626612479083\n",
      "  episode_reward_mean: -74.32031163060043\n",
      "  episode_reward_min: -201.0028638161682\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 10412\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4596.489\n",
      "    load_time_ms: 1.731\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5407443228517733e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6785162091255188\n",
      "      kl: 0.004756656941026449\n",
      "      policy_loss: -0.0018933732062578201\n",
      "      total_loss: 1507.730712890625\n",
      "      vf_explained_var: 0.4212597608566284\n",
      "      vf_loss: 1507.7327880859375\n",
      "    sample_time_ms: 21549.568\n",
      "    update_time_ms: 5.598\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -37.16015581530022\n",
      "  time_since_restore: 2956.9328966140747\n",
      "  time_this_iter_s: 25.292174339294434\n",
      "  time_total_s: 2956.9328966140747\n",
      "  timestamp: 1554811251\n",
      "  timesteps_since_restore: 1120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 112\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2956 s, 112 iter, 1120000 ts, -74.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-01-15\n",
      "  done: false\n",
      "  episode_len_mean: 88.31858407079646\n",
      "  episode_reward_max: 8.974797597101727\n",
      "  episode_reward_mean: -85.60346331206931\n",
      "  episode_reward_min: -200.9446361473236\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 10525\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4500.615\n",
      "    load_time_ms: 1.733\n",
      "    num_steps_sampled: 1130000\n",
      "    num_steps_trained: 1130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.703721614258867e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6684615612030029\n",
      "      kl: 0.0049285865388810635\n",
      "      policy_loss: -0.0011198555584996939\n",
      "      total_loss: 1386.939697265625\n",
      "      vf_explained_var: 0.48801979422569275\n",
      "      vf_loss: 1386.9407958984375\n",
      "    sample_time_ms: 21296.104\n",
      "    update_time_ms: 5.624\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -42.801731656034654\n",
      "  time_since_restore: 2980.387311935425\n",
      "  time_this_iter_s: 23.454415321350098\n",
      "  time_total_s: 2980.387311935425\n",
      "  timestamp: 1554811275\n",
      "  timesteps_since_restore: 1130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1130000\n",
      "  training_iteration: 113\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 2980 s, 113 iter, 1130000 ts, -85.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-01-38\n",
      "  done: false\n",
      "  episode_len_mean: 89.54954954954955\n",
      "  episode_reward_max: 10.073178500607742\n",
      "  episode_reward_mean: -79.03862179997014\n",
      "  episode_reward_min: -200.9305255220726\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 10636\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4291.066\n",
      "    load_time_ms: 1.784\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.8518608071294333e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6684631109237671\n",
      "      kl: 0.004773502703756094\n",
      "      policy_loss: -0.0015488710487261415\n",
      "      total_loss: 1410.7508544921875\n",
      "      vf_explained_var: 0.4593009054660797\n",
      "      vf_loss: 1410.7523193359375\n",
      "    sample_time_ms: 21130.437\n",
      "    update_time_ms: 5.584\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -39.519310899985065\n",
      "  time_since_restore: 3004.1711049079895\n",
      "  time_this_iter_s: 23.783792972564697\n",
      "  time_total_s: 3004.1711049079895\n",
      "  timestamp: 1554811298\n",
      "  timesteps_since_restore: 1140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 114\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3004 s, 114 iter, 1140000 ts, -79 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-02-02\n",
      "  done: false\n",
      "  episode_len_mean: 90.61818181818182\n",
      "  episode_reward_max: 9.967566887979107\n",
      "  episode_reward_mean: -79.23509168915716\n",
      "  episode_reward_min: -201.04336419517898\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 10746\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4305.658\n",
      "    load_time_ms: 1.82\n",
      "    num_steps_sampled: 1150000\n",
      "    num_steps_trained: 1150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9259304035647166e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6574600338935852\n",
      "      kl: 0.003993293270468712\n",
      "      policy_loss: -0.0009722694521769881\n",
      "      total_loss: 1403.0242919921875\n",
      "      vf_explained_var: 0.44806405901908875\n",
      "      vf_loss: 1403.0252685546875\n",
      "    sample_time_ms: 20542.736\n",
      "    update_time_ms: 5.617\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -39.6175458445786\n",
      "  time_since_restore: 3027.9741218090057\n",
      "  time_this_iter_s: 23.803016901016235\n",
      "  time_total_s: 3027.9741218090057\n",
      "  timestamp: 1554811322\n",
      "  timesteps_since_restore: 1150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1150000\n",
      "  training_iteration: 115\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3027 s, 115 iter, 1150000 ts, -79.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-02-31\n",
      "  done: false\n",
      "  episode_len_mean: 92.87962962962963\n",
      "  episode_reward_max: 11.773926690028972\n",
      "  episode_reward_mean: -72.28571432815535\n",
      "  episode_reward_min: -201.4421616627027\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 10854\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4293.724\n",
      "    load_time_ms: 1.818\n",
      "    num_steps_sampled: 1160000\n",
      "    num_steps_trained: 1160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.629652017823583e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.641748309135437\n",
      "      kl: 0.006262464448809624\n",
      "      policy_loss: -0.0016866514924913645\n",
      "      total_loss: 1476.253662109375\n",
      "      vf_explained_var: 0.41198328137397766\n",
      "      vf_loss: 1476.25537109375\n",
      "    sample_time_ms: 20508.813\n",
      "    update_time_ms: 5.539\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -36.142857164077675\n",
      "  time_since_restore: 3056.34233045578\n",
      "  time_this_iter_s: 28.368208646774292\n",
      "  time_total_s: 3056.34233045578\n",
      "  timestamp: 1554811351\n",
      "  timesteps_since_restore: 1160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1160000\n",
      "  training_iteration: 116\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3056 s, 116 iter, 1160000 ts, -72.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-02-54\n",
      "  done: false\n",
      "  episode_len_mean: 93.85046728971963\n",
      "  episode_reward_max: 10.079716014473215\n",
      "  episode_reward_mean: -70.23512878197421\n",
      "  episode_reward_min: -201.25751657734418\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 10961\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4180.288\n",
      "    load_time_ms: 1.711\n",
      "    num_steps_sampled: 1170000\n",
      "    num_steps_trained: 1170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.8148260089117916e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6436289548873901\n",
      "      kl: 0.004443586803972721\n",
      "      policy_loss: -0.00152000249363482\n",
      "      total_loss: 1476.440673828125\n",
      "      vf_explained_var: 0.3923780024051666\n",
      "      vf_loss: 1476.4422607421875\n",
      "    sample_time_ms: 20407.708\n",
      "    update_time_ms: 5.578\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -35.117564390987106\n",
      "  time_since_restore: 3080.0947601795197\n",
      "  time_this_iter_s: 23.752429723739624\n",
      "  time_total_s: 3080.0947601795197\n",
      "  timestamp: 1554811374\n",
      "  timesteps_since_restore: 1170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1170000\n",
      "  training_iteration: 117\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3080 s, 117 iter, 1170000 ts, -70.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-03-18\n",
      "  done: false\n",
      "  episode_len_mean: 90.53636363636363\n",
      "  episode_reward_max: 12.470980368967838\n",
      "  episode_reward_mean: -76.63550749221304\n",
      "  episode_reward_min: -200.58100006744047\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 11071\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4175.971\n",
      "    load_time_ms: 1.703\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4074130044558958e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6376120448112488\n",
      "      kl: 0.008367870934307575\n",
      "      policy_loss: -0.003405024530366063\n",
      "      total_loss: 1466.2882080078125\n",
      "      vf_explained_var: 0.3871391713619232\n",
      "      vf_loss: 1466.2918701171875\n",
      "    sample_time_ms: 20333.758\n",
      "    update_time_ms: 5.517\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -38.317753746106526\n",
      "  time_since_restore: 3103.590049982071\n",
      "  time_this_iter_s: 23.49528980255127\n",
      "  time_total_s: 3103.590049982071\n",
      "  timestamp: 1554811398\n",
      "  timesteps_since_restore: 1180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 118\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3103 s, 118 iter, 1180000 ts, -76.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-03-42\n",
      "  done: false\n",
      "  episode_len_mean: 90.74774774774775\n",
      "  episode_reward_max: 12.25806354058453\n",
      "  episode_reward_mean: -79.86433540043275\n",
      "  episode_reward_min: -200.96411593953837\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 11182\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4179.566\n",
      "    load_time_ms: 1.774\n",
      "    num_steps_sampled: 1190000\n",
      "    num_steps_trained: 1190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037065022279479e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6238279938697815\n",
      "      kl: 0.0076059880666434765\n",
      "      policy_loss: -0.0027227981481701136\n",
      "      total_loss: 1441.398193359375\n",
      "      vf_explained_var: 0.43320879340171814\n",
      "      vf_loss: 1441.4010009765625\n",
      "    sample_time_ms: 20300.883\n",
      "    update_time_ms: 5.361\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -39.93216770021637\n",
      "  time_since_restore: 3127.1998541355133\n",
      "  time_this_iter_s: 23.609804153442383\n",
      "  time_total_s: 3127.1998541355133\n",
      "  timestamp: 1554811422\n",
      "  timesteps_since_restore: 1190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1190000\n",
      "  training_iteration: 119\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3127 s, 119 iter, 1190000 ts, -79.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-04-06\n",
      "  done: false\n",
      "  episode_len_mean: 90.46363636363637\n",
      "  episode_reward_max: 10.520507504619285\n",
      "  episode_reward_mean: -79.73284138893487\n",
      "  episode_reward_min: -201.25338287228325\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 11292\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4177.501\n",
      "    load_time_ms: 1.747\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.0185325111397395e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6085778474807739\n",
      "      kl: 0.007076913956552744\n",
      "      policy_loss: -0.0016099050408229232\n",
      "      total_loss: 1385.6744384765625\n",
      "      vf_explained_var: 0.44420018792152405\n",
      "      vf_loss: 1385.6761474609375\n",
      "    sample_time_ms: 20285.775\n",
      "    update_time_ms: 5.306\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -39.86642069446744\n",
      "  time_since_restore: 3151.7224435806274\n",
      "  time_this_iter_s: 24.522589445114136\n",
      "  time_total_s: 3151.7224435806274\n",
      "  timestamp: 1554811446\n",
      "  timesteps_since_restore: 1200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 120\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3151 s, 120 iter, 1200000 ts, -79.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-04-30\n",
      "  done: false\n",
      "  episode_len_mean: 86.9396551724138\n",
      "  episode_reward_max: 10.543910174312362\n",
      "  episode_reward_mean: -89.52818074823477\n",
      "  episode_reward_min: -201.1740065404848\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 11408\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4175.276\n",
      "    load_time_ms: 1.809\n",
      "    num_steps_sampled: 1210000\n",
      "    num_steps_trained: 1210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.0092662555698698e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6085827350616455\n",
      "      kl: 0.006973233073949814\n",
      "      policy_loss: -0.002883837791159749\n",
      "      total_loss: 1322.2642822265625\n",
      "      vf_explained_var: 0.49343156814575195\n",
      "      vf_loss: 1322.26708984375\n",
      "    sample_time_ms: 20194.154\n",
      "    update_time_ms: 5.366\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -44.7640903741174\n",
      "  time_since_restore: 3175.600809812546\n",
      "  time_this_iter_s: 23.878366231918335\n",
      "  time_total_s: 3175.600809812546\n",
      "  timestamp: 1554811470\n",
      "  timesteps_since_restore: 1210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1210000\n",
      "  training_iteration: 121\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3175 s, 121 iter, 1210000 ts, -89.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-04-53\n",
      "  done: false\n",
      "  episode_len_mean: 91.02752293577981\n",
      "  episode_reward_max: 9.73517125856017\n",
      "  episode_reward_mean: -76.60189202808108\n",
      "  episode_reward_min: -200.68220977723766\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 11517\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4173.912\n",
      "    load_time_ms: 1.768\n",
      "    num_steps_sampled: 1220000\n",
      "    num_steps_trained: 1220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5046331277849349e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5928190350532532\n",
      "      kl: 0.007278570905327797\n",
      "      policy_loss: -0.0027300615329295397\n",
      "      total_loss: 1396.46533203125\n",
      "      vf_explained_var: 0.454641729593277\n",
      "      vf_loss: 1396.4681396484375\n",
      "    sample_time_ms: 19999.343\n",
      "    update_time_ms: 5.352\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -38.30094601404055\n",
      "  time_since_restore: 3198.928158760071\n",
      "  time_this_iter_s: 23.327348947525024\n",
      "  time_total_s: 3198.928158760071\n",
      "  timestamp: 1554811493\n",
      "  timesteps_since_restore: 1220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1220000\n",
      "  training_iteration: 122\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3198 s, 122 iter, 1220000 ts, -76.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-05-18\n",
      "  done: false\n",
      "  episode_len_mean: 94.54285714285714\n",
      "  episode_reward_max: 8.696356852358972\n",
      "  episode_reward_mean: -69.17423010287679\n",
      "  episode_reward_min: -201.00567599628408\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 11622\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4173.926\n",
      "    load_time_ms: 1.801\n",
      "    num_steps_sampled: 1230000\n",
      "    num_steps_trained: 1230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.523165638924674e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5864006280899048\n",
      "      kl: 0.005352736450731754\n",
      "      policy_loss: -0.001257753581739962\n",
      "      total_loss: 1491.5322265625\n",
      "      vf_explained_var: 0.4243628680706024\n",
      "      vf_loss: 1491.5335693359375\n",
      "    sample_time_ms: 20085.755\n",
      "    update_time_ms: 5.425\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -34.58711505143839\n",
      "  time_since_restore: 3223.2497091293335\n",
      "  time_this_iter_s: 24.321550369262695\n",
      "  time_total_s: 3223.2497091293335\n",
      "  timestamp: 1554811518\n",
      "  timesteps_since_restore: 1230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1230000\n",
      "  training_iteration: 123\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3223 s, 123 iter, 1230000 ts, -69.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-05-42\n",
      "  done: false\n",
      "  episode_len_mean: 89.19469026548673\n",
      "  episode_reward_max: 8.905845322412418\n",
      "  episode_reward_mean: -82.83973063948339\n",
      "  episode_reward_min: -200.57095371952303\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 11735\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4172.982\n",
      "    load_time_ms: 1.75\n",
      "    num_steps_sampled: 1240000\n",
      "    num_steps_trained: 1240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.761582819462337e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5763477087020874\n",
      "      kl: 0.0042583835311234\n",
      "      policy_loss: -0.0005129331257194281\n",
      "      total_loss: 1424.371337890625\n",
      "      vf_explained_var: 0.46207037568092346\n",
      "      vf_loss: 1424.371826171875\n",
      "    sample_time_ms: 20118.67\n",
      "    update_time_ms: 5.395\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -41.41986531974169\n",
      "  time_since_restore: 3247.3503065109253\n",
      "  time_this_iter_s: 24.100597381591797\n",
      "  time_total_s: 3247.3503065109253\n",
      "  timestamp: 1554811542\n",
      "  timesteps_since_restore: 1240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1240000\n",
      "  training_iteration: 124\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3247 s, 124 iter, 1240000 ts, -82.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-06-06\n",
      "  done: false\n",
      "  episode_len_mean: 89.67857142857143\n",
      "  episode_reward_max: 8.099607070386373\n",
      "  episode_reward_mean: -80.56820629857555\n",
      "  episode_reward_min: -200.6270679038837\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 11847\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4152.794\n",
      "    load_time_ms: 1.74\n",
      "    num_steps_sampled: 1250000\n",
      "    num_steps_trained: 1250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8807914097311686e-38\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5796895027160645\n",
      "      kl: 0.005059698596596718\n",
      "      policy_loss: -0.0016704683657735586\n",
      "      total_loss: 1374.5018310546875\n",
      "      vf_explained_var: 0.4581191837787628\n",
      "      vf_loss: 1374.5032958984375\n",
      "    sample_time_ms: 20189.466\n",
      "    update_time_ms: 5.205\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -40.284103149287766\n",
      "  time_since_restore: 3271.6610219478607\n",
      "  time_this_iter_s: 24.310715436935425\n",
      "  time_total_s: 3271.6610219478607\n",
      "  timestamp: 1554811566\n",
      "  timesteps_since_restore: 1250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1250000\n",
      "  training_iteration: 125\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3271 s, 125 iter, 1250000 ts, -80.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-06-30\n",
      "  done: false\n",
      "  episode_len_mean: 89.1875\n",
      "  episode_reward_max: 8.204192255749941\n",
      "  episode_reward_mean: -81.58688026364045\n",
      "  episode_reward_min: -200.5879473139545\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 11959\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4150.331\n",
      "    load_time_ms: 1.66\n",
      "    num_steps_sampled: 1260000\n",
      "    num_steps_trained: 1260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.403955647357379e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.576238214969635\n",
      "      kl: 0.005156892351806164\n",
      "      policy_loss: -0.0019432008266448975\n",
      "      total_loss: 1393.2659912109375\n",
      "      vf_explained_var: 0.44839906692504883\n",
      "      vf_loss: 1393.2679443359375\n",
      "    sample_time_ms: 19755.047\n",
      "    update_time_ms: 5.419\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -40.79344013182024\n",
      "  time_since_restore: 3295.662501811981\n",
      "  time_this_iter_s: 24.001479864120483\n",
      "  time_total_s: 3295.662501811981\n",
      "  timestamp: 1554811590\n",
      "  timesteps_since_restore: 1260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1260000\n",
      "  training_iteration: 126\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3295 s, 126 iter, 1260000 ts, -81.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-06-53\n",
      "  done: false\n",
      "  episode_len_mean: 89.72072072072072\n",
      "  episode_reward_max: 8.235544137456897\n",
      "  episode_reward_mean: -81.033761603594\n",
      "  episode_reward_min: -200.70810966595616\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 12070\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4144.377\n",
      "    load_time_ms: 1.667\n",
      "    num_steps_sampled: 1270000\n",
      "    num_steps_trained: 1270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.701977123029457e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.575576663017273\n",
      "      kl: 0.0062291440553963184\n",
      "      policy_loss: -0.002444624900817871\n",
      "      total_loss: 1398.8634033203125\n",
      "      vf_explained_var: 0.460130900144577\n",
      "      vf_loss: 1398.86572265625\n",
      "    sample_time_ms: 19660.931\n",
      "    update_time_ms: 5.501\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -40.516880801797\n",
      "  time_since_restore: 3318.4145834445953\n",
      "  time_this_iter_s: 22.752081632614136\n",
      "  time_total_s: 3318.4145834445953\n",
      "  timestamp: 1554811613\n",
      "  timesteps_since_restore: 1270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1270000\n",
      "  training_iteration: 127\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3318 s, 127 iter, 1270000 ts, -81 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-07-18\n",
      "  done: false\n",
      "  episode_len_mean: 82.82644628099173\n",
      "  episode_reward_max: 7.495489375582039\n",
      "  episode_reward_mean: -99.14331190983748\n",
      "  episode_reward_min: -200.7613853429547\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 12191\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4238.121\n",
      "    load_time_ms: 1.712\n",
      "    num_steps_sampled: 1280000\n",
      "    num_steps_trained: 1280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3509892621639607e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5655191540718079\n",
      "      kl: 0.006191942375153303\n",
      "      policy_loss: -0.0014650978846475482\n",
      "      total_loss: 1362.7467041015625\n",
      "      vf_explained_var: 0.5288986563682556\n",
      "      vf_loss: 1362.7481689453125\n",
      "    sample_time_ms: 19721.497\n",
      "    update_time_ms: 5.55\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -49.57165595491874\n",
      "  time_since_restore: 3343.4570546150208\n",
      "  time_this_iter_s: 25.042471170425415\n",
      "  time_total_s: 3343.4570546150208\n",
      "  timestamp: 1554811638\n",
      "  timesteps_since_restore: 1280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1280000\n",
      "  training_iteration: 128\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3343 s, 128 iter, 1280000 ts, -99.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-07-43\n",
      "  done: false\n",
      "  episode_len_mean: 99.43\n",
      "  episode_reward_max: 7.2591075337985735\n",
      "  episode_reward_mean: -57.04698642384553\n",
      "  episode_reward_min: -200.7021090440061\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 12291\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4227.969\n",
      "    load_time_ms: 1.636\n",
      "    num_steps_sampled: 1290000\n",
      "    num_steps_trained: 1290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1754946310819804e-39\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5695739388465881\n",
      "      kl: 0.007411550264805555\n",
      "      policy_loss: -0.0025158654898405075\n",
      "      total_loss: 1437.813720703125\n",
      "      vf_explained_var: 0.41832923889160156\n",
      "      vf_loss: 1437.816162109375\n",
      "    sample_time_ms: 19802.261\n",
      "    update_time_ms: 5.67\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -28.523493211922766\n",
      "  time_since_restore: 3367.7697200775146\n",
      "  time_this_iter_s: 24.312665462493896\n",
      "  time_total_s: 3367.7697200775146\n",
      "  timestamp: 1554811663\n",
      "  timesteps_since_restore: 1290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1290000\n",
      "  training_iteration: 129\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3367 s, 129 iter, 1290000 ts, -57 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-08-07\n",
      "  done: false\n",
      "  episode_len_mean: 90.41964285714286\n",
      "  episode_reward_max: 7.3302416739118375\n",
      "  episode_reward_mean: -80.632633777633\n",
      "  episode_reward_min: -200.4827146694494\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 12403\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4231.076\n",
      "    load_time_ms: 1.604\n",
      "    num_steps_sampled: 1300000\n",
      "    num_steps_trained: 1300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.87746614891758e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5495852828025818\n",
      "      kl: 0.006260564085096121\n",
      "      policy_loss: -0.0016730546485632658\n",
      "      total_loss: 1382.7652587890625\n",
      "      vf_explained_var: 0.4796208143234253\n",
      "      vf_loss: 1382.766845703125\n",
      "    sample_time_ms: 19737.383\n",
      "    update_time_ms: 5.803\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -40.3163168888165\n",
      "  time_since_restore: 3391.678850412369\n",
      "  time_this_iter_s: 23.909130334854126\n",
      "  time_total_s: 3391.678850412369\n",
      "  timestamp: 1554811687\n",
      "  timesteps_since_restore: 1300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1300000\n",
      "  training_iteration: 130\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3391 s, 130 iter, 1300000 ts, -80.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-08-31\n",
      "  done: false\n",
      "  episode_len_mean: 92.90566037735849\n",
      "  episode_reward_max: 8.011723978459024\n",
      "  episode_reward_mean: -71.39815571047389\n",
      "  episode_reward_min: -200.49654771886014\n",
      "  episodes_this_iter: 106\n",
      "  episodes_total: 12509\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4234.614\n",
      "    load_time_ms: 1.616\n",
      "    num_steps_sampled: 1310000\n",
      "    num_steps_trained: 1310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.93873307445879e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5400283932685852\n",
      "      kl: 0.0051874807104468346\n",
      "      policy_loss: -0.0013749463250860572\n",
      "      total_loss: 1472.529296875\n",
      "      vf_explained_var: 0.4223276674747467\n",
      "      vf_loss: 1472.5306396484375\n",
      "    sample_time_ms: 19770.278\n",
      "    update_time_ms: 5.589\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -35.69907785523696\n",
      "  time_since_restore: 3415.9212040901184\n",
      "  time_this_iter_s: 24.242353677749634\n",
      "  time_total_s: 3415.9212040901184\n",
      "  timestamp: 1554811711\n",
      "  timesteps_since_restore: 1310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1310000\n",
      "  training_iteration: 131\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3415 s, 131 iter, 1310000 ts, -71.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-08-56\n",
      "  done: false\n",
      "  episode_len_mean: 90.22321428571429\n",
      "  episode_reward_max: 6.830491861021536\n",
      "  episode_reward_mean: -80.05070688717119\n",
      "  episode_reward_min: -200.4498964882058\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 12621\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4311.876\n",
      "    load_time_ms: 1.687\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4693735437217167e-40\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5382491946220398\n",
      "      kl: 0.005343020427972078\n",
      "      policy_loss: -0.0010946488473564386\n",
      "      total_loss: 1423.9947509765625\n",
      "      vf_explained_var: 0.43741264939308167\n",
      "      vf_loss: 1423.995849609375\n",
      "    sample_time_ms: 19852.565\n",
      "    update_time_ms: 5.806\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -40.025353443585594\n",
      "  time_since_restore: 3440.8478276729584\n",
      "  time_this_iter_s: 24.926623582839966\n",
      "  time_total_s: 3440.8478276729584\n",
      "  timestamp: 1554811736\n",
      "  timesteps_since_restore: 1320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 132\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3440 s, 132 iter, 1320000 ts, -80.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-09-20\n",
      "  done: false\n",
      "  episode_len_mean: 83.60504201680672\n",
      "  episode_reward_max: 7.762989973244586\n",
      "  episode_reward_mean: -99.39233646667148\n",
      "  episode_reward_min: -200.62609025563614\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 12740\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4312.051\n",
      "    load_time_ms: 1.678\n",
      "    num_steps_sampled: 1330000\n",
      "    num_steps_trained: 1330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.346867718608583e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5276400446891785\n",
      "      kl: 0.004985780455172062\n",
      "      policy_loss: -0.0013144296826794744\n",
      "      total_loss: 1336.8446044921875\n",
      "      vf_explained_var: 0.5064560174942017\n",
      "      vf_loss: 1336.8458251953125\n",
      "    sample_time_ms: 19876.909\n",
      "    update_time_ms: 5.687\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -49.696168233335754\n",
      "  time_since_restore: 3465.4145057201385\n",
      "  time_this_iter_s: 24.566678047180176\n",
      "  time_total_s: 3465.4145057201385\n",
      "  timestamp: 1554811760\n",
      "  timesteps_since_restore: 1330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1330000\n",
      "  training_iteration: 133\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3465 s, 133 iter, 1330000 ts, -99.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-09-45\n",
      "  done: false\n",
      "  episode_len_mean: 89.75\n",
      "  episode_reward_max: 7.461549153822463\n",
      "  episode_reward_mean: -83.86948904324979\n",
      "  episode_reward_min: -200.6686016920333\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 12852\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4319.073\n",
      "    load_time_ms: 1.711\n",
      "    num_steps_sampled: 1340000\n",
      "    num_steps_trained: 1340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6733637943810755e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5185015797615051\n",
      "      kl: 0.008375751785933971\n",
      "      policy_loss: -0.0020104916766285896\n",
      "      total_loss: 1385.0006103515625\n",
      "      vf_explained_var: 0.49867844581604004\n",
      "      vf_loss: 1385.0025634765625\n",
      "    sample_time_ms: 19911.089\n",
      "    update_time_ms: 5.69\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -41.93474452162489\n",
      "  time_since_restore: 3489.9290356636047\n",
      "  time_this_iter_s: 24.514529943466187\n",
      "  time_total_s: 3489.9290356636047\n",
      "  timestamp: 1554811785\n",
      "  timesteps_since_restore: 1340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1340000\n",
      "  training_iteration: 134\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3489 s, 134 iter, 1340000 ts, -83.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-10-12\n",
      "  done: false\n",
      "  episode_len_mean: 92.1743119266055\n",
      "  episode_reward_max: 7.988025274376085\n",
      "  episode_reward_mean: -73.00808400832524\n",
      "  episode_reward_min: -200.32959172270756\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 12961\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4352.762\n",
      "    load_time_ms: 1.673\n",
      "    num_steps_sampled: 1350000\n",
      "    num_steps_trained: 1350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8366818971905377e-41\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5168645977973938\n",
      "      kl: 0.006562141235917807\n",
      "      policy_loss: -0.0028194456826895475\n",
      "      total_loss: 1426.4217529296875\n",
      "      vf_explained_var: 0.4511100649833679\n",
      "      vf_loss: 1426.424560546875\n",
      "    sample_time_ms: 20128.04\n",
      "    update_time_ms: 5.903\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -36.50404200416262\n",
      "  time_since_restore: 3516.7451713085175\n",
      "  time_this_iter_s: 26.81613564491272\n",
      "  time_total_s: 3516.7451713085175\n",
      "  timestamp: 1554811812\n",
      "  timesteps_since_restore: 1350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1350000\n",
      "  training_iteration: 135\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3516 s, 135 iter, 1350000 ts, -73 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-10-37\n",
      "  done: false\n",
      "  episode_len_mean: 88.49549549549549\n",
      "  episode_reward_max: 6.474143341527744\n",
      "  episode_reward_mean: -86.42182669281783\n",
      "  episode_reward_min: -200.35153464386977\n",
      "  episodes_this_iter: 111\n",
      "  episodes_total: 13072\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4355.025\n",
      "    load_time_ms: 1.68\n",
      "    num_steps_sampled: 1360000\n",
      "    num_steps_trained: 1360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.184110135184851e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5054977536201477\n",
      "      kl: 0.00635699275881052\n",
      "      policy_loss: -0.0010775598930194974\n",
      "      total_loss: 1360.6470947265625\n",
      "      vf_explained_var: 0.48752066493034363\n",
      "      vf_loss: 1360.64794921875\n",
      "    sample_time_ms: 20222.66\n",
      "    update_time_ms: 5.856\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -43.21091334640893\n",
      "  time_since_restore: 3541.713198900223\n",
      "  time_this_iter_s: 24.968027591705322\n",
      "  time_total_s: 3541.713198900223\n",
      "  timestamp: 1554811837\n",
      "  timesteps_since_restore: 1360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1360000\n",
      "  training_iteration: 136\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3541 s, 136 iter, 1360000 ts, -86.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-11-03\n",
      "  done: false\n",
      "  episode_len_mean: 89.97321428571429\n",
      "  episode_reward_max: 6.777318531260474\n",
      "  episode_reward_mean: -81.60403577119233\n",
      "  episode_reward_min: -200.26667924060945\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 13184\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4361.442\n",
      "    load_time_ms: 1.7\n",
      "    num_steps_sampled: 1370000\n",
      "    num_steps_trained: 1370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.5920550675924255e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5074676871299744\n",
      "      kl: 0.005777306854724884\n",
      "      policy_loss: -0.0021570497192442417\n",
      "      total_loss: 1403.5753173828125\n",
      "      vf_explained_var: 0.48116132616996765\n",
      "      vf_loss: 1403.5772705078125\n",
      "    sample_time_ms: 20505.006\n",
      "    update_time_ms: 5.795\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -40.80201788559617\n",
      "  time_since_restore: 3567.3537945747375\n",
      "  time_this_iter_s: 25.64059567451477\n",
      "  time_total_s: 3567.3537945747375\n",
      "  timestamp: 1554811863\n",
      "  timesteps_since_restore: 1370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1370000\n",
      "  training_iteration: 137\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3567 s, 137 iter, 1370000 ts, -81.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-11-28\n",
      "  done: false\n",
      "  episode_len_mean: 87.75438596491227\n",
      "  episode_reward_max: 7.229439832467568\n",
      "  episode_reward_mean: -87.8863339946974\n",
      "  episode_reward_min: -200.52098734270572\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 13298\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4286.443\n",
      "    load_time_ms: 1.664\n",
      "    num_steps_sampled: 1380000\n",
      "    num_steps_trained: 1380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2953268845640504e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5062771439552307\n",
      "      kl: 0.006511659361422062\n",
      "      policy_loss: -0.0010620775865390897\n",
      "      total_loss: 1393.0106201171875\n",
      "      vf_explained_var: 0.4612603187561035\n",
      "      vf_loss: 1393.011474609375\n",
      "    sample_time_ms: 20574.964\n",
      "    update_time_ms: 5.729\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -43.94316699734868\n",
      "  time_since_restore: 3592.3410840034485\n",
      "  time_this_iter_s: 24.987289428710938\n",
      "  time_total_s: 3592.3410840034485\n",
      "  timestamp: 1554811888\n",
      "  timesteps_since_restore: 1380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1380000\n",
      "  training_iteration: 138\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3592 s, 138 iter, 1380000 ts, -87.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-11-53\n",
      "  done: false\n",
      "  episode_len_mean: 87.47787610619469\n",
      "  episode_reward_max: 6.395915911162001\n",
      "  episode_reward_mean: -86.6573874128552\n",
      "  episode_reward_min: -200.44436003375625\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 13411\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4323.436\n",
      "    load_time_ms: 1.712\n",
      "    num_steps_sampled: 1390000\n",
      "    num_steps_trained: 1390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1476634422820252e-42\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5002812147140503\n",
      "      kl: 0.0072896708734333515\n",
      "      policy_loss: -0.0013852196279913187\n",
      "      total_loss: 1365.01806640625\n",
      "      vf_explained_var: 0.4897818863391876\n",
      "      vf_loss: 1365.01953125\n",
      "    sample_time_ms: 20658.437\n",
      "    update_time_ms: 5.559\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -43.3286937064276\n",
      "  time_since_restore: 3617.8575191497803\n",
      "  time_this_iter_s: 25.516435146331787\n",
      "  time_total_s: 3617.8575191497803\n",
      "  timestamp: 1554811913\n",
      "  timesteps_since_restore: 1390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1390000\n",
      "  training_iteration: 139\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3617 s, 139 iter, 1390000 ts, -86.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-12-17\n",
      "  done: false\n",
      "  episode_len_mean: 86.94017094017094\n",
      "  episode_reward_max: 6.9403164476752295\n",
      "  episode_reward_mean: -88.98763519562172\n",
      "  episode_reward_min: -200.317571291991\n",
      "  episodes_this_iter: 117\n",
      "  episodes_total: 13528\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4322.889\n",
      "    load_time_ms: 1.775\n",
      "    num_steps_sampled: 1400000\n",
      "    num_steps_trained: 1400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.74532370373175e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.48776695132255554\n",
      "      kl: 0.006457045208662748\n",
      "      policy_loss: -0.001490337192080915\n",
      "      total_loss: 1307.9779052734375\n",
      "      vf_explained_var: 0.5386092066764832\n",
      "      vf_loss: 1307.9796142578125\n",
      "    sample_time_ms: 20675.698\n",
      "    update_time_ms: 5.861\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -44.49381759781086\n",
      "  time_since_restore: 3641.938529253006\n",
      "  time_this_iter_s: 24.081010103225708\n",
      "  time_total_s: 3641.938529253006\n",
      "  timestamp: 1554811937\n",
      "  timesteps_since_restore: 1400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1400000\n",
      "  training_iteration: 140\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3641 s, 140 iter, 1400000 ts, -89 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-12-41\n",
      "  done: false\n",
      "  episode_len_mean: 86.46551724137932\n",
      "  episode_reward_max: 6.323600731319942\n",
      "  episode_reward_mean: -91.30756062756946\n",
      "  episode_reward_min: -200.40456319354\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 13644\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4320.159\n",
      "    load_time_ms: 1.694\n",
      "    num_steps_sampled: 1410000\n",
      "    num_steps_trained: 1410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.872661851865875e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.48148491978645325\n",
      "      kl: 0.005870245397090912\n",
      "      policy_loss: -0.0013852509437128901\n",
      "      total_loss: 1369.4903564453125\n",
      "      vf_explained_var: 0.5179959535598755\n",
      "      vf_loss: 1369.4918212890625\n",
      "    sample_time_ms: 20656.295\n",
      "    update_time_ms: 6.068\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -45.653780313784736\n",
      "  time_since_restore: 3665.959460735321\n",
      "  time_this_iter_s: 24.020931482315063\n",
      "  time_total_s: 3665.959460735321\n",
      "  timestamp: 1554811961\n",
      "  timesteps_since_restore: 1410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1410000\n",
      "  training_iteration: 141\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3665 s, 141 iter, 1410000 ts, -91.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-13-06\n",
      "  done: false\n",
      "  episode_len_mean: 90.91743119266054\n",
      "  episode_reward_max: 7.047811471117021\n",
      "  episode_reward_mean: -77.1447757956751\n",
      "  episode_reward_min: -200.27862091533206\n",
      "  episodes_this_iter: 109\n",
      "  episodes_total: 13753\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4241.913\n",
      "    load_time_ms: 1.626\n",
      "    num_steps_sampled: 1420000\n",
      "    num_steps_trained: 1420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4293244336113134e-43\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.47442689538002014\n",
      "      kl: 0.006880016066133976\n",
      "      policy_loss: -0.0018740525702014565\n",
      "      total_loss: 1454.7193603515625\n",
      "      vf_explained_var: 0.45555025339126587\n",
      "      vf_loss: 1454.7213134765625\n",
      "    sample_time_ms: 20749.174\n",
      "    update_time_ms: 5.996\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -38.572387897837565\n",
      "  time_since_restore: 3691.030976295471\n",
      "  time_this_iter_s: 25.071515560150146\n",
      "  time_total_s: 3691.030976295471\n",
      "  timestamp: 1554811986\n",
      "  timesteps_since_restore: 1420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1420000\n",
      "  training_iteration: 142\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3691 s, 142 iter, 1420000 ts, -77.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-13-30\n",
      "  done: false\n",
      "  episode_len_mean: 86.99137931034483\n",
      "  episode_reward_max: 8.208340648519501\n",
      "  episode_reward_mean: -87.6472776373393\n",
      "  episode_reward_min: -200.30369027582114\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 13869\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4239.752\n",
      "    load_time_ms: 1.608\n",
      "    num_steps_sampled: 1430000\n",
      "    num_steps_trained: 1430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.146622168056567e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4631726145744324\n",
      "      kl: 0.005029669962823391\n",
      "      policy_loss: -0.0008021270041354001\n",
      "      total_loss: 1412.86279296875\n",
      "      vf_explained_var: 0.4812152683734894\n",
      "      vf_loss: 1412.863525390625\n",
      "    sample_time_ms: 20696.047\n",
      "    update_time_ms: 6.039\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -43.82363881866965\n",
      "  time_since_restore: 3715.0417580604553\n",
      "  time_this_iter_s: 24.01078176498413\n",
      "  time_total_s: 3715.0417580604553\n",
      "  timestamp: 1554812010\n",
      "  timesteps_since_restore: 1430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1430000\n",
      "  training_iteration: 143\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3715 s, 143 iter, 1430000 ts, -87.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-13-55\n",
      "  done: false\n",
      "  episode_len_mean: 88.22321428571429\n",
      "  episode_reward_max: 7.7717779009665655\n",
      "  episode_reward_mean: -87.8042868997941\n",
      "  episode_reward_min: -200.22823049114388\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 13981\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4235.631\n",
      "    load_time_ms: 1.57\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6433760072445244e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4535863399505615\n",
      "      kl: 0.0066632782109081745\n",
      "      policy_loss: -0.0019299000268802047\n",
      "      total_loss: 1406.975341796875\n",
      "      vf_explained_var: 0.49583113193511963\n",
      "      vf_loss: 1406.9771728515625\n",
      "    sample_time_ms: 20654.549\n",
      "    update_time_ms: 5.997\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -43.90214344989706\n",
      "  time_since_restore: 3739.1018676757812\n",
      "  time_this_iter_s: 24.060109615325928\n",
      "  time_total_s: 3739.1018676757812\n",
      "  timestamp: 1554812035\n",
      "  timesteps_since_restore: 1440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 144\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3739 s, 144 iter, 1440000 ts, -87.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-14-21\n",
      "  done: false\n",
      "  episode_len_mean: 94.19626168224299\n",
      "  episode_reward_max: 7.744306652348826\n",
      "  episode_reward_mean: -68.74162317977482\n",
      "  episode_reward_min: -200.22193907564665\n",
      "  episodes_this_iter: 107\n",
      "  episodes_total: 14088\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4217.123\n",
      "    load_time_ms: 1.58\n",
      "    num_steps_sampled: 1450000\n",
      "    num_steps_trained: 1450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8216880036222622e-44\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.45043256878852844\n",
      "      kl: 0.006161588244140148\n",
      "      policy_loss: -0.0012231034925207496\n",
      "      total_loss: 1424.445068359375\n",
      "      vf_explained_var: 0.4335927665233612\n",
      "      vf_loss: 1424.4462890625\n",
      "    sample_time_ms: 20620.765\n",
      "    update_time_ms: 5.847\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -34.37081158988741\n",
      "  time_since_restore: 3765.3953268527985\n",
      "  time_this_iter_s: 26.293459177017212\n",
      "  time_total_s: 3765.3953268527985\n",
      "  timestamp: 1554812061\n",
      "  timesteps_since_restore: 1450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1450000\n",
      "  training_iteration: 145\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3765 s, 145 iter, 1450000 ts, -68.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-14-45\n",
      "  done: false\n",
      "  episode_len_mean: 92.41666666666667\n",
      "  episode_reward_max: 6.535632525940627\n",
      "  episode_reward_mean: -76.47738347020116\n",
      "  episode_reward_min: -200.41517482177238\n",
      "  episodes_this_iter: 108\n",
      "  episodes_total: 14196\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4214.343\n",
      "    load_time_ms: 1.618\n",
      "    num_steps_sampled: 1460000\n",
      "    num_steps_trained: 1460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.407790785948902e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4379827678203583\n",
      "      kl: 0.005962305702269077\n",
      "      policy_loss: -0.0020644045434892178\n",
      "      total_loss: 1386.6781005859375\n",
      "      vf_explained_var: 0.4430955648422241\n",
      "      vf_loss: 1386.68017578125\n",
      "    sample_time_ms: 20526.858\n",
      "    update_time_ms: 5.826\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -38.238691735100595\n",
      "  time_since_restore: 3789.398455142975\n",
      "  time_this_iter_s: 24.00312829017639\n",
      "  time_total_s: 3789.398455142975\n",
      "  timestamp: 1554812085\n",
      "  timesteps_since_restore: 1460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1460000\n",
      "  training_iteration: 146\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3789 s, 146 iter, 1460000 ts, -76.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-15-17\n",
      "  done: false\n",
      "  episode_len_mean: 82.46280991735537\n",
      "  episode_reward_max: 7.542172922612817\n",
      "  episode_reward_mean: -101.06527683130601\n",
      "  episode_reward_min: -200.27122005957017\n",
      "  episodes_this_iter: 121\n",
      "  episodes_total: 14317\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4380.239\n",
      "    load_time_ms: 1.609\n",
      "    num_steps_sampled: 1470000\n",
      "    num_steps_trained: 1470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.203895392974451e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4249061048030853\n",
      "      kl: 0.003982802387326956\n",
      "      policy_loss: -0.00023448982392437756\n",
      "      total_loss: 1301.830810546875\n",
      "      vf_explained_var: 0.5321295261383057\n",
      "      vf_loss: 1301.8311767578125\n",
      "    sample_time_ms: 20949.96\n",
      "    update_time_ms: 5.847\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -50.532638415653004\n",
      "  time_since_restore: 3820.9274015426636\n",
      "  time_this_iter_s: 31.52894639968872\n",
      "  time_total_s: 3820.9274015426636\n",
      "  timestamp: 1554812117\n",
      "  timesteps_since_restore: 1470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1470000\n",
      "  training_iteration: 147\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3820 s, 147 iter, 1470000 ts, -101 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-15-43\n",
      "  done: false\n",
      "  episode_len_mean: 87.74782608695652\n",
      "  episode_reward_max: 6.99584410889326\n",
      "  episode_reward_mean: -86.80764264158809\n",
      "  episode_reward_min: -200.29037391580258\n",
      "  episodes_this_iter: 115\n",
      "  episodes_total: 14432\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4358.162\n",
      "    load_time_ms: 1.602\n",
      "    num_steps_sampled: 1480000\n",
      "    num_steps_trained: 1480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.802596928649634e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.430248498916626\n",
      "      kl: 0.0055666216649115086\n",
      "      policy_loss: -0.0019630242604762316\n",
      "      total_loss: 1381.160400390625\n",
      "      vf_explained_var: 0.4923362135887146\n",
      "      vf_loss: 1381.1622314453125\n",
      "    sample_time_ms: 21086.377\n",
      "    update_time_ms: 5.991\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -43.40382132079404\n",
      "  time_since_restore: 3847.061018228531\n",
      "  time_this_iter_s: 26.13361668586731\n",
      "  time_total_s: 3847.061018228531\n",
      "  timestamp: 1554812143\n",
      "  timesteps_since_restore: 1480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1480000\n",
      "  training_iteration: 148\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3847 s, 148 iter, 1480000 ts, -86.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-16-07\n",
      "  done: false\n",
      "  episode_len_mean: 90.06363636363636\n",
      "  episode_reward_max: 8.410645928128996\n",
      "  episode_reward_mean: -80.35777355040165\n",
      "  episode_reward_min: -200.1318464391199\n",
      "  episodes_this_iter: 110\n",
      "  episodes_total: 14542\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4327.025\n",
      "    load_time_ms: 1.588\n",
      "    num_steps_sampled: 1490000\n",
      "    num_steps_trained: 1490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.401298464324817e-45\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4198397994041443\n",
      "      kl: 0.006536674685776234\n",
      "      policy_loss: -0.0011830800212919712\n",
      "      total_loss: 1436.310546875\n",
      "      vf_explained_var: 0.4697219133377075\n",
      "      vf_loss: 1436.3116455078125\n",
      "    sample_time_ms: 21011.077\n",
      "    update_time_ms: 5.999\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -40.17888677520082\n",
      "  time_since_restore: 3871.5139014720917\n",
      "  time_this_iter_s: 24.45288324356079\n",
      "  time_total_s: 3871.5139014720917\n",
      "  timestamp: 1554812167\n",
      "  timesteps_since_restore: 1490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1490000\n",
      "  training_iteration: 149\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3871 s, 149 iter, 1490000 ts, -80.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-16-32\n",
      "  done: false\n",
      "  episode_len_mean: 83.13333333333334\n",
      "  episode_reward_max: 7.904371470771968\n",
      "  episode_reward_mean: -98.1322138507479\n",
      "  episode_reward_min: -200.18534041383816\n",
      "  episodes_this_iter: 120\n",
      "  episodes_total: 14662\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4329.632\n",
      "    load_time_ms: 1.567\n",
      "    num_steps_sampled: 1500000\n",
      "    num_steps_trained: 1500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.40174999833106995\n",
      "      kl: 0.006670753005892038\n",
      "      policy_loss: -0.0017988604959100485\n",
      "      total_loss: 1337.3604736328125\n",
      "      vf_explained_var: 0.5389527082443237\n",
      "      vf_loss: 1337.3624267578125\n",
      "    sample_time_ms: 21090.339\n",
      "    update_time_ms: 5.694\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -49.06610692537395\n",
      "  time_since_restore: 3896.406997203827\n",
      "  time_this_iter_s: 24.89309573173523\n",
      "  time_total_s: 3896.406997203827\n",
      "  timestamp: 1554812192\n",
      "  timesteps_since_restore: 1500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1500000\n",
      "  training_iteration: 150\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3896 s, 150 iter, 1500000 ts, -98.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-16-56\n",
      "  done: false\n",
      "  episode_len_mean: 96.87378640776699\n",
      "  episode_reward_max: 8.828299412320671\n",
      "  episode_reward_mean: -63.827285429506325\n",
      "  episode_reward_min: -200.10380904706668\n",
      "  episodes_this_iter: 103\n",
      "  episodes_total: 14765\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4332.763\n",
      "    load_time_ms: 1.639\n",
      "    num_steps_sampled: 1510000\n",
      "    num_steps_trained: 1510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3975749611854553\n",
      "      kl: 0.007491372991353273\n",
      "      policy_loss: -0.002335286233574152\n",
      "      total_loss: 1432.0850830078125\n",
      "      vf_explained_var: 0.4188728332519531\n",
      "      vf_loss: 1432.0875244140625\n",
      "    sample_time_ms: 21068.518\n",
      "    update_time_ms: 5.766\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -31.913642714753163\n",
      "  time_since_restore: 3920.2449893951416\n",
      "  time_this_iter_s: 23.837992191314697\n",
      "  time_total_s: 3920.2449893951416\n",
      "  timestamp: 1554812216\n",
      "  timesteps_since_restore: 1510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1510000\n",
      "  training_iteration: 151\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3920 s, 151 iter, 1510000 ts, -63.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-17-21\n",
      "  done: false\n",
      "  episode_len_mean: 87.30434782608695\n",
      "  episode_reward_max: 8.680045569941662\n",
      "  episode_reward_mean: -87.02592671426044\n",
      "  episode_reward_min: -200.28356513186742\n",
      "  episodes_this_iter: 115\n",
      "  episodes_total: 14880\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4336.497\n",
      "    load_time_ms: 1.638\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3919532001018524\n",
      "      kl: 0.005742210429161787\n",
      "      policy_loss: -0.0009836700046434999\n",
      "      total_loss: 1355.0010986328125\n",
      "      vf_explained_var: 0.49982061982154846\n",
      "      vf_loss: 1355.002197265625\n",
      "    sample_time_ms: 21010.46\n",
      "    update_time_ms: 5.746\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -43.51296335713023\n",
      "  time_since_restore: 3944.7743802070618\n",
      "  time_this_iter_s: 24.529390811920166\n",
      "  time_total_s: 3944.7743802070618\n",
      "  timestamp: 1554812241\n",
      "  timesteps_since_restore: 1520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 152\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3944 s, 152 iter, 1520000 ts, -87 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-17-46\n",
      "  done: false\n",
      "  episode_len_mean: 88.88495575221239\n",
      "  episode_reward_max: 7.6555696015517025\n",
      "  episode_reward_mean: -83.40773584144442\n",
      "  episode_reward_min: -200.17930836242311\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 14993\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4337.664\n",
      "    load_time_ms: 1.65\n",
      "    num_steps_sampled: 1530000\n",
      "    num_steps_trained: 1530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3908700942993164\n",
      "      kl: 0.004375812131911516\n",
      "      policy_loss: -0.0006910321535542607\n",
      "      total_loss: 1416.2095947265625\n",
      "      vf_explained_var: 0.4531577527523041\n",
      "      vf_loss: 1416.2103271484375\n",
      "    sample_time_ms: 21172.717\n",
      "    update_time_ms: 5.74\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -41.70386792072222\n",
      "  time_since_restore: 3970.421573162079\n",
      "  time_this_iter_s: 25.64719295501709\n",
      "  time_total_s: 3970.421573162079\n",
      "  timestamp: 1554812266\n",
      "  timesteps_since_restore: 1530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1530000\n",
      "  training_iteration: 153\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3970 s, 153 iter, 1530000 ts, -83.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 87.39473684210526\n",
      "  episode_reward_max: 7.642701963597329\n",
      "  episode_reward_mean: -86.231102769284\n",
      "  episode_reward_min: -200.20665225004265\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 15107\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4338.855\n",
      "    load_time_ms: 1.655\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3766547441482544\n",
      "      kl: 0.007009952329099178\n",
      "      policy_loss: -0.0014377370243892074\n",
      "      total_loss: 1355.1287841796875\n",
      "      vf_explained_var: 0.499641090631485\n",
      "      vf_loss: 1355.1302490234375\n",
      "    sample_time_ms: 21176.203\n",
      "    update_time_ms: 5.759\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -43.115551384641996\n",
      "  time_since_restore: 3994.52419090271\n",
      "  time_this_iter_s: 24.102617740631104\n",
      "  time_total_s: 3994.52419090271\n",
      "  timestamp: 1554812290\n",
      "  timesteps_since_restore: 1540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 154\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 3994 s, 154 iter, 1540000 ts, -86.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-18-34\n",
      "  done: false\n",
      "  episode_len_mean: 89.23214285714286\n",
      "  episode_reward_max: 7.228048956325394\n",
      "  episode_reward_mean: -82.66931904659292\n",
      "  episode_reward_min: -200.21733781737663\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 15219\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4327.182\n",
      "    load_time_ms: 1.686\n",
      "    num_steps_sampled: 1550000\n",
      "    num_steps_trained: 1550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3707539141178131\n",
      "      kl: 0.00557762011885643\n",
      "      policy_loss: -0.0013121970696374774\n",
      "      total_loss: 1370.0377197265625\n",
      "      vf_explained_var: 0.5012130737304688\n",
      "      vf_loss: 1370.038818359375\n",
      "    sample_time_ms: 20918.954\n",
      "    update_time_ms: 5.926\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -41.33465952329646\n",
      "  time_since_restore: 4018.130364894867\n",
      "  time_this_iter_s: 23.606173992156982\n",
      "  time_total_s: 4018.130364894867\n",
      "  timestamp: 1554812314\n",
      "  timesteps_since_restore: 1550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1550000\n",
      "  training_iteration: 155\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 4018 s, 155 iter, 1550000 ts, -82.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-18-58\n",
      "  done: false\n",
      "  episode_len_mean: 89.36283185840708\n",
      "  episode_reward_max: 6.853893656808428\n",
      "  episode_reward_mean: -81.65455588821067\n",
      "  episode_reward_min: -200.11656001666307\n",
      "  episodes_this_iter: 113\n",
      "  episodes_total: 15332\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4328.403\n",
      "    load_time_ms: 1.646\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.36424392461776733\n",
      "      kl: 0.004872207064181566\n",
      "      policy_loss: -0.0003019925788976252\n",
      "      total_loss: 1402.7298583984375\n",
      "      vf_explained_var: 0.47142493724823\n",
      "      vf_loss: 1402.730224609375\n",
      "    sample_time_ms: 20886.443\n",
      "    update_time_ms: 5.885\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -40.82727794410532\n",
      "  time_since_restore: 4041.820620536804\n",
      "  time_this_iter_s: 23.690255641937256\n",
      "  time_total_s: 4041.820620536804\n",
      "  timestamp: 1554812338\n",
      "  timesteps_since_restore: 1560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 156\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 4041 s, 156 iter, 1560000 ts, -81.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-19-22\n",
      "  done: false\n",
      "  episode_len_mean: 86.79824561403508\n",
      "  episode_reward_max: 8.284739785671317\n",
      "  episode_reward_mean: -87.33637841354094\n",
      "  episode_reward_min: -200.1834146754062\n",
      "  episodes_this_iter: 114\n",
      "  episodes_total: 15446\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4160.082\n",
      "    load_time_ms: 1.656\n",
      "    num_steps_sampled: 1570000\n",
      "    num_steps_trained: 1570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3509944677352905\n",
      "      kl: 0.005499625112861395\n",
      "      policy_loss: -0.0010952443117275834\n",
      "      total_loss: 1417.018798828125\n",
      "      vf_explained_var: 0.49260133504867554\n",
      "      vf_loss: 1417.02001953125\n",
      "    sample_time_ms: 20297.129\n",
      "    update_time_ms: 5.882\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -43.668189206770464\n",
      "  time_since_restore: 4065.7745270729065\n",
      "  time_this_iter_s: 23.953906536102295\n",
      "  time_total_s: 4065.7745270729065\n",
      "  timestamp: 1554812362\n",
      "  timesteps_since_restore: 1570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1570000\n",
      "  training_iteration: 157\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 4065 s, 157 iter, 1570000 ts, -87.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-19-46\n",
      "  done: false\n",
      "  episode_len_mean: 89.74107142857143\n",
      "  episode_reward_max: 7.696632938334163\n",
      "  episode_reward_mean: -81.95895060869645\n",
      "  episode_reward_min: -200.1283416712502\n",
      "  episodes_this_iter: 112\n",
      "  episodes_total: 15558\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4167.73\n",
      "    load_time_ms: 1.684\n",
      "    num_steps_sampled: 1580000\n",
      "    num_steps_trained: 1580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3461938500404358\n",
      "      kl: 0.008182263933122158\n",
      "      policy_loss: -0.0023761813063174486\n",
      "      total_loss: 1445.3934326171875\n",
      "      vf_explained_var: 0.47208237648010254\n",
      "      vf_loss: 1445.3958740234375\n",
      "    sample_time_ms: 20052.091\n",
      "    update_time_ms: 5.716\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -40.97947530434823\n",
      "  time_since_restore: 4089.5315823554993\n",
      "  time_this_iter_s: 23.757055282592773\n",
      "  time_total_s: 4089.5315823554993\n",
      "  timestamp: 1554812386\n",
      "  timesteps_since_restore: 1580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1580000\n",
      "  training_iteration: 158\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 4089 s, 158 iter, 1580000 ts, -82 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-20-10\n",
      "  done: false\n",
      "  episode_len_mean: 80.09677419354838\n",
      "  episode_reward_max: 6.731748280547766\n",
      "  episode_reward_mean: -108.43567402904424\n",
      "  episode_reward_min: -200.24172298497427\n",
      "  episodes_this_iter: 124\n",
      "  episodes_total: 15682\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4162.759\n",
      "    load_time_ms: 1.702\n",
      "    num_steps_sampled: 1590000\n",
      "    num_steps_trained: 1590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3291178047657013\n",
      "      kl: 0.00705459201708436\n",
      "      policy_loss: -0.00014962293789722025\n",
      "      total_loss: 1287.629638671875\n",
      "      vf_explained_var: 0.5515305995941162\n",
      "      vf_loss: 1287.6297607421875\n",
      "    sample_time_ms: 20005.449\n",
      "    update_time_ms: 5.674\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -54.21783701452213\n",
      "  time_since_restore: 4113.466614246368\n",
      "  time_this_iter_s: 23.93503189086914\n",
      "  time_total_s: 4113.466614246368\n",
      "  timestamp: 1554812410\n",
      "  timesteps_since_restore: 1590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1590000\n",
      "  training_iteration: 159\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 4113 s, 159 iter, 1590000 ts, -108 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-20-35\n",
      "  done: false\n",
      "  episode_len_mean: 85.20338983050847\n",
      "  episode_reward_max: 6.479584224715126\n",
      "  episode_reward_mean: -91.49030062254067\n",
      "  episode_reward_min: -200.0972517419204\n",
      "  episodes_this_iter: 118\n",
      "  episodes_total: 15800\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4159.219\n",
      "    load_time_ms: 1.75\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3327788710594177\n",
      "      kl: 0.007540848106145859\n",
      "      policy_loss: -0.0019588652066886425\n",
      "      total_loss: 1378.5565185546875\n",
      "      vf_explained_var: 0.5005791187286377\n",
      "      vf_loss: 1378.558349609375\n",
      "    sample_time_ms: 20034.015\n",
      "    update_time_ms: 5.779\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -45.74515031127032\n",
      "  time_since_restore: 4138.614673376083\n",
      "  time_this_iter_s: 25.148059129714966\n",
      "  time_total_s: 4138.614673376083\n",
      "  timestamp: 1554812435\n",
      "  timesteps_since_restore: 1600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 160\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 4138 s, 160 iter, 1600000 ts, -91.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-09_14-21-03\n",
      "  done: false\n",
      "  episode_len_mean: 87.56521739130434\n",
      "  episode_reward_max: 7.174711101959087\n",
      "  episode_reward_mean: -86.8148458888353\n",
      "  episode_reward_min: -200.1712515667041\n",
      "  episodes_this_iter: 115\n",
      "  episodes_total: 15915\n",
      "  experiment_id: 22b1cc9446844e61a011e26f0fdde742\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 4187.136\n",
      "    load_time_ms: 1.7\n",
      "    num_steps_sampled: 1610000\n",
      "    num_steps_trained: 1610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32506662607192993\n",
      "      kl: 0.007866689004004002\n",
      "      policy_loss: -0.001986246556043625\n",
      "      total_loss: 1380.4046630859375\n",
      "      vf_explained_var: 0.5116827487945557\n",
      "      vf_loss: 1380.4066162109375\n",
      "    sample_time_ms: 20391.884\n",
      "    update_time_ms: 5.641\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 30302\n",
      "  policy_reward_mean:\n",
      "    rl_0: -43.40742294441765\n",
      "  time_since_restore: 4166.307604312897\n",
      "  time_this_iter_s: 27.692930936813354\n",
      "  time_total_s: 4166.307604312897\n",
      "  timestamp: 1554812463\n",
      "  timesteps_since_restore: 1610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 1610000\n",
      "  training_iteration: 161\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit-v0_0:\tRUNNING [pid=30302], 4166 s, 161 iter, 1610000 ts, -86.8 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,  # RL algorithm to run\n",
    "        \"env\": gym_name,  # environment name generated earlier\n",
    "        \"config\": {  # configuration params (must match \"run\" value)\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1000,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow_2)",
   "language": "python",
   "name": "flow_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
