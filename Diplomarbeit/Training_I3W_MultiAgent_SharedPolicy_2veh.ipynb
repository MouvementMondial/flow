{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING I3W\n",
    "\n",
    "\n",
    "# A) Create Envorinment, Vehicles etc\n",
    "\n",
    "### General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scenarios:\n",
      "['Scenario', 'BayBridgeScenario', 'BayBridgeTollScenario', 'BottleneckScenario', 'Figure8Scenario', 'SimpleGridScenario', 'HighwayScenario', 'LoopScenario', 'MergeScenario', 'TwoLoopsOneMergingScenario', 'MultiLoopScenario', 'IntersectionScenarioTW', 'TenaciousDScenario', 'IntersectionTWScenario_2']\n",
      "\n",
      "Available environments:\n",
      "['MultiEnv', 'MultiAgentAccelEnv', 'MultiWaveAttenuationPOEnv', 'MultiAgentIntersectionEnv', 'MultiAgentTeamSpiritIntersectionEnv', 'MultiAgentIntersectionEnv_baseline_1', 'MultiAgentIntersectionEnv_baseline_2', 'MultiAgentIntersectionEnv_baseline_3', 'MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit', 'MultiTenaciousDEnv', 'MultiAgentIntersectionEnv_sharedPolicy_2veh']\n"
     ]
    }
   ],
   "source": [
    "# Define horizon as a variable to ensure consistent use across notebook (length of one rollout)\n",
    "HORIZON=500                                 #103 max Horizon, wenn es vor verlassen abbrechen soll!, default war 500\n",
    "\n",
    "# name of the experiment\n",
    "experiment_name = \"IntersectionExample\"\n",
    "\n",
    "# scenario class\n",
    "import flow.scenarios as scenarios\n",
    "print(\"Available scenarios:\")\n",
    "print(scenarios.__all__)\n",
    "scenario_name = \"IntersectionTWScenario_2\"\n",
    "\n",
    "# environment class\n",
    "import flow.multiagent_envs as flowenvs\n",
    "print(\"\\nAvailable environments:\")\n",
    "print(flowenvs.__all__)\n",
    "env_name = \"MultiAgentIntersectionEnv_sharedPolicy_2veh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "from flow.scenarios.intersection import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "additionalNetParams = {\n",
    "            \"edge_length\": 80,\n",
    "            \"lanes\": 1,\n",
    "            \"speed_limit\": 30\n",
    "        }\n",
    "\n",
    "net_params = NetParams( no_internal_links=False,                  #default: True   !! damit Kreuzungen nicht Ã¼berspr. werden\n",
    "                        inflows=None,                             #default: None\n",
    "                        osm_path=None,                            #default: None\n",
    "                        netfile=None,                             #default: None\n",
    "                        additional_params=additionalNetParams     #default: None   !!\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InitialConfig Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "\n",
    "initial_config = InitialConfig( shuffle=True,                            #default: False         !!\n",
    "                                spacing=\"custom\",                        #default: \"uniform\"     !!\n",
    "                                min_gap=10,                              #default: 0\n",
    "                                perturbation=29.99,                      #default: 0.0            !!        \n",
    "                                x0=0,                                    #default: 0\n",
    "                                bunching=0,                              #default: 0\n",
    "                                lanes_distribution=float(\"inf\"),         #default: float(\"inf\")\n",
    "                                edges_distribution=\"all\",                #default: \"all\"\n",
    "                                additional_params=None )                 #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams( port = None,                  #default: None\n",
    "                          sim_step=0.1,                 #default: 0.1\n",
    "                          emission_path=None,           #default: None\n",
    "                          lateral_resolution=None,      #default: None\n",
    "                          no_step_log=True,             #default: True\n",
    "                          render=False,                 #default: False\n",
    "                          save_render=False,            #default: False\n",
    "                          sight_radius=25,              #default: 25\n",
    "                          show_radius=False,            #default: False\n",
    "                          pxpm=2,                       #default: 2\n",
    "                          overtake_right=False,         #default: False    \n",
    "                          seed=None,                    #default: None\n",
    "                          restart_instance=False,       #default: False\n",
    "                          print_warnings=True,          #default: True\n",
    "                          teleport_time=-1,             #default: -1\n",
    "                          num_clients=1,                #default: 1\n",
    "                          sumo_binary=None )            #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "additionalEnvParams = {\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 3,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 3,\n",
    "        \"target_velocity\": 30\n",
    "    }\n",
    "\n",
    "env_params = EnvParams( additional_params=additionalEnvParams, #default: None    !!\n",
    "                        horizon=HORIZON,                       #default: 500     !!\n",
    "                        warmup_steps=0,                        #default: 0       \n",
    "                        sims_per_step=1,                       #default: 1\n",
    "                        evaluate=False )                       #default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# import vehicles dynamics models\n",
    "#from flow.controllers import SumoCarFollowingController\n",
    "from flow.controllers import ContinuousRouter\n",
    "#from flow.controllers.lane_change_controllers import SumoLaneChangeController\n",
    "from flow.controllers.lane_change_controllers import StaticLaneChanger\n",
    "from flow.controllers import RLController\n",
    "from flow.core.params import SumoLaneChangeParams\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "from random import *\n",
    "\n",
    "vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RL-Agent controlled vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car following parameters, default: None\n",
    "cf_parameter = SumoCarFollowingParams(\n",
    "                speed_mode=\"aggressive\")\n",
    "# lane change parameters, default: None\n",
    "lc_parameter =  None\n",
    "\n",
    "vehicles.add( # name of the vehicle\n",
    "                veh_id = \"rl\",\n",
    "              # acceleration controller, default: (SumoCarFollowingController, {})\n",
    "                acceleration_controller=(RLController, {}),\n",
    "              # lane_change_controller, default: (SumoLaneChangeController, {})\n",
    "                lane_change_controller=(StaticLaneChanger,{}),\n",
    "              # routing controller, default: None\n",
    "                routing_controller=(ContinuousRouter, {}),\n",
    "              # initial speed, default: 0\n",
    "                initial_speed=0,\n",
    "              # number of vehicles, default: 1 \n",
    "                num_vehicles=2,\n",
    "                \n",
    "                car_following_params=cf_parameter\n",
    "              # speed mode, default: \"right_of_way\"\n",
    "                #speed_mode=\"aggressive\",\n",
    "              # lane change mode, default: \"no_lat_collide\"\n",
    "                #lane_change_mode=\"aggressive\", \n",
    "              # car following parameter, default: None\n",
    "                #sumo_car_following_params=cf_parameter,\n",
    "              # lane change parameter, default: None\n",
    "                #sumo_lc_params=lc_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict( # name of the experiment\n",
    "                      exp_tag=experiment_name,\n",
    "                    # name of the flow environment the experiment is running on\n",
    "                      env_name=env_name,\n",
    "                    # name of the scenario class the experiment uses\n",
    "                      scenario=scenario_name,\n",
    "                    # simulator that is used by the experiment\n",
    "                      simulator='traci',\n",
    "                    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "                      sim=sumo_params,\n",
    "                    # environment related parameters (see flow.core.params.EnvParams)\n",
    "                      env=env_params,\n",
    "                    # network-related parameters (see flow.core.params.NetParams and\n",
    "                    # the scenario's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "                      net=net_params,\n",
    "                    # vehicles to be placed in the network at the start of a rollout \n",
    "                    # (see flow.core.vehicles.Vehicles)\n",
    "                      veh=vehicles,\n",
    "                   # (optional) parameters affecting the positioning of vehicles upon \n",
    "                   # initialization/reset (see flow.core.params.InitialConfig)\n",
    "                      initial=initial_config\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-04-30_22-50-45_23679/logs.\n",
      "Waiting for redis server at 127.0.0.1:31122 to respond...\n",
      "Waiting for redis server at 127.0.0.1:16830 to respond...\n",
      "Starting the Plasma object store with 6.554658406 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8888/notebooks/ray_ui.ipynb?token=4e6dc2980ee4b41c8417906b374bf2f6993af1f858e799ce\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.16.123.117',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-04-30_22-50-45_23679/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-04-30_22-50-45_23679/sockets/raylet'],\n",
       " 'redis_address': '172.16.123.117:31122',\n",
       " 'webui_url': 'http://localhost:8888/notebooks/ray_ui.ipynb?token=4e6dc2980ee4b41c8417906b374bf2f6993af1f858e799ce'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "\n",
    "ray.init(redirect_output=True, num_cpus=N_CPUS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate default 0.999\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [100, 50, 25]})  # size of hidden layers in network defaule 64 32\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "#config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "#config[\"sample_batch_size\"] = config[\"train_batch_size\"]/config[\"num_workers\"] # 200 default, trotzdem zu hoch?\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 52923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Teamspirit:\n",
      "-0.3921303351670222\n",
      "0.7482375742940759\n",
      "[('bottom_intersection', 2.6936290634791717), ('top_intersection', 18.952837148955812)]\n"
     ]
    }
   ],
   "source": [
    "# multi agent policy mapping\n",
    "test_env = create_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "\n",
    "def gen_policy():\n",
    "    return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "# Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "policy_graphs = {'rl_0': gen_policy()}\n",
    "    \n",
    "def policy_mapping_fn(agent_id):\n",
    "    return 'rl_0'\n",
    "\n",
    "config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn),\n",
    "            'policies_to_train': ['rl_0']\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.9/16.4 GB\n",
      "\n",
      "Created LogSyncer for /home/thorsten/ray_results/IntersectionExample/PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0_2019-04-30_22-50-48tqduo587 -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 11.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-51-58\n",
      "  done: false\n",
      "  episode_len_mean: 493.1578947368421\n",
      "  episode_reward_max: 256.0580112210727\n",
      "  episode_reward_mean: 105.42489979261951\n",
      "  episode_reward_min: -129.17730171050277\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 19\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6218.388\n",
      "    load_time_ms: 55.894\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.20000004768371582\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4150316715240479\n",
      "      kl: 0.0013243365101516247\n",
      "      policy_loss: -0.0017564716981723905\n",
      "      total_loss: 24.96449851989746\n",
      "      vf_explained_var: 0.11804157495498657\n",
      "      vf_loss: 24.96599006652832\n",
      "    sample_time_ms: 22414.211\n",
      "    update_time_ms: 649.64\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.71244989630977\n",
      "  time_since_restore: 29.406869649887085\n",
      "  time_this_iter_s: 29.406869649887085\n",
      "  time_total_s: 29.406869649887085\n",
      "  timestamp: 1556657518\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 29 s, 1 iter, 10000 ts, 105 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-52-31\n",
      "  done: false\n",
      "  episode_len_mean: 491.8974358974359\n",
      "  episode_reward_max: 310.1937355231377\n",
      "  episode_reward_mean: 131.68494322693513\n",
      "  episode_reward_min: -129.17730171050277\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 39\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5916.19\n",
      "    load_time_ms: 28.784\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.10000002384185791\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4093780517578125\n",
      "      kl: 0.00834824051707983\n",
      "      policy_loss: -0.005325400270521641\n",
      "      total_loss: 47.55464553833008\n",
      "      vf_explained_var: 0.11877553910017014\n",
      "      vf_loss: 47.55913162231445\n",
      "    sample_time_ms: 24650.63\n",
      "    update_time_ms: 329.216\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.84247161346758\n",
      "  time_since_restore: 61.939417123794556\n",
      "  time_this_iter_s: 32.53254747390747\n",
      "  time_total_s: 61.939417123794556\n",
      "  timestamp: 1556657551\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 61 s, 2 iter, 20000 ts, 132 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-53-00\n",
      "  done: false\n",
      "  episode_len_mean: 484.2950819672131\n",
      "  episode_reward_max: 685.4141181848186\n",
      "  episode_reward_mean: 215.3644126940872\n",
      "  episode_reward_min: -129.17730171050277\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 61\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5649.336\n",
      "    load_time_ms: 19.711\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.050000011920928955\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3966829776763916\n",
      "      kl: 0.007579983677715063\n",
      "      policy_loss: -0.0030492024961858988\n",
      "      total_loss: 200.49632263183594\n",
      "      vf_explained_var: 0.04747728258371353\n",
      "      vf_loss: 200.49899291992188\n",
      "    sample_time_ms: 24479.544\n",
      "    update_time_ms: 225.101\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 107.6822063470436\n",
      "  time_since_restore: 91.23028254508972\n",
      "  time_this_iter_s: 29.290865421295166\n",
      "  time_total_s: 91.23028254508972\n",
      "  timestamp: 1556657580\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 91 s, 3 iter, 30000 ts, 215 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-53-28\n",
      "  done: false\n",
      "  episode_len_mean: 468.70588235294116\n",
      "  episode_reward_max: 1039.71109389113\n",
      "  episode_reward_mean: 271.5552898906977\n",
      "  episode_reward_min: -180.87628062843862\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 85\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5494.608\n",
      "    load_time_ms: 15.182\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.025000005960464478\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.395425796508789\n",
      "      kl: 0.011703049764037132\n",
      "      policy_loss: -0.00326979230158031\n",
      "      total_loss: 394.3140563964844\n",
      "      vf_explained_var: 0.13037468492984772\n",
      "      vf_loss: 394.3170471191406\n",
      "    sample_time_ms: 23910.05\n",
      "    update_time_ms: 170.501\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 135.77764494534884\n",
      "  time_since_restore: 118.48935055732727\n",
      "  time_this_iter_s: 27.25906801223755\n",
      "  time_total_s: 118.48935055732727\n",
      "  timestamp: 1556657608\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 118 s, 4 iter, 40000 ts, 272 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-53-55\n",
      "  done: false\n",
      "  episode_len_mean: 463.69\n",
      "  episode_reward_max: 1267.4407887131674\n",
      "  episode_reward_mean: 397.54488014102736\n",
      "  episode_reward_min: -181.55288258471978\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 106\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5451.978\n",
      "    load_time_ms: 12.442\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.025000005960464478\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3818182945251465\n",
      "      kl: 0.002328539965674281\n",
      "      policy_loss: -0.0008781269425526261\n",
      "      total_loss: 875.6636352539062\n",
      "      vf_explained_var: 0.06100287660956383\n",
      "      vf_loss: 875.6644897460938\n",
      "    sample_time_ms: 23565.291\n",
      "    update_time_ms: 138.075\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 198.77244007051374\n",
      "  time_since_restore: 145.98789596557617\n",
      "  time_this_iter_s: 27.4985454082489\n",
      "  time_total_s: 145.98789596557617\n",
      "  timestamp: 1556657635\n",
      "  timesteps_since_restore: 50000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 5\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 145 s, 5 iter, 50000 ts, 398 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-54-23\n",
      "  done: false\n",
      "  episode_len_mean: 448.9\n",
      "  episode_reward_max: 1267.4407887131674\n",
      "  episode_reward_mean: 488.03434709904917\n",
      "  episode_reward_min: -186.23258887717174\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 129\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5364.887\n",
      "    load_time_ms: 10.624\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.012500002980232239\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3977876901626587\n",
      "      kl: 0.006946011912077665\n",
      "      policy_loss: -0.00197492609731853\n",
      "      total_loss: 539.784423828125\n",
      "      vf_explained_var: 0.20630112290382385\n",
      "      vf_loss: 539.7862548828125\n",
      "    sample_time_ms: 23373.595\n",
      "    update_time_ms: 118.15\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 244.01717354952459\n",
      "  time_since_restore: 173.37151622772217\n",
      "  time_this_iter_s: 27.383620262145996\n",
      "  time_total_s: 173.37151622772217\n",
      "  timestamp: 1556657663\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 173 s, 6 iter, 60000 ts, 488 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-54-49\n",
      "  done: false\n",
      "  episode_len_mean: 441.09\n",
      "  episode_reward_max: 1267.4407887131674\n",
      "  episode_reward_mean: 578.0947679163971\n",
      "  episode_reward_min: -186.23258887717174\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 152\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5307.986\n",
      "    load_time_ms: 9.373\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.006250001490116119\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3875442743301392\n",
      "      kl: 0.004994636867195368\n",
      "      policy_loss: -0.002356350887566805\n",
      "      total_loss: 775.5771484375\n",
      "      vf_explained_var: 0.3173937499523163\n",
      "      vf_loss: 775.5794067382812\n",
      "    sample_time_ms: 23060.72\n",
      "    update_time_ms: 102.321\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 289.0473839581986\n",
      "  time_since_restore: 199.5491590499878\n",
      "  time_this_iter_s: 26.177642822265625\n",
      "  time_total_s: 199.5491590499878\n",
      "  timestamp: 1556657689\n",
      "  timesteps_since_restore: 70000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 199 s, 7 iter, 70000 ts, 578 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-55-17\n",
      "  done: false\n",
      "  episode_len_mean: 427.81\n",
      "  episode_reward_max: 1267.4407887131674\n",
      "  episode_reward_mean: 672.6017535158135\n",
      "  episode_reward_min: -186.23258887717174\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 178\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5255.323\n",
      "    load_time_ms: 8.369\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031250007450580597\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3764886856079102\n",
      "      kl: 0.010412411764264107\n",
      "      policy_loss: -0.002453421475365758\n",
      "      total_loss: 1150.1090087890625\n",
      "      vf_explained_var: 0.12344591319561005\n",
      "      vf_loss: 1150.111328125\n",
      "    sample_time_ms: 23068.3\n",
      "    update_time_ms: 90.391\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 336.3008767579067\n",
      "  time_since_restore: 227.5838885307312\n",
      "  time_this_iter_s: 28.034729480743408\n",
      "  time_total_s: 227.5838885307312\n",
      "  timestamp: 1556657717\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 227 s, 8 iter, 80000 ts, 673 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-55-44\n",
      "  done: false\n",
      "  episode_len_mean: 416.04\n",
      "  episode_reward_max: 1391.5038900908964\n",
      "  episode_reward_mean: 726.9556054804908\n",
      "  episode_reward_min: -186.23258887717174\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 203\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5225.314\n",
      "    load_time_ms: 7.582\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031250007450580597\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3788751363754272\n",
      "      kl: 0.0010188160231336951\n",
      "      policy_loss: -0.0006424330058507621\n",
      "      total_loss: 1503.5426025390625\n",
      "      vf_explained_var: 0.017284730449318886\n",
      "      vf_loss: 1503.543212890625\n",
      "    sample_time_ms: 22984.378\n",
      "    update_time_ms: 80.991\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 363.4778027402454\n",
      "  time_since_restore: 254.90724778175354\n",
      "  time_this_iter_s: 27.32335925102234\n",
      "  time_total_s: 254.90724778175354\n",
      "  timestamp: 1556657744\n",
      "  timesteps_since_restore: 90000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 9\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 254 s, 9 iter, 90000 ts, 727 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-56-13\n",
      "  done: false\n",
      "  episode_len_mean: 398.23\n",
      "  episode_reward_max: 1415.8143422949574\n",
      "  episode_reward_mean: 812.7026900748974\n",
      "  episode_reward_min: -181.37813296845314\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 230\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5284.004\n",
      "    load_time_ms: 7.034\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0015625003725290298\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.364436388015747\n",
      "      kl: 0.004673569928854704\n",
      "      policy_loss: -0.0007593902992084622\n",
      "      total_loss: 1523.4422607421875\n",
      "      vf_explained_var: 0.032950036227703094\n",
      "      vf_loss: 1523.443115234375\n",
      "    sample_time_ms: 22982.665\n",
      "    update_time_ms: 73.982\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 406.35134503744865\n",
      "  time_since_restore: 283.73040199279785\n",
      "  time_this_iter_s: 28.82315421104431\n",
      "  time_total_s: 283.73040199279785\n",
      "  timestamp: 1556657773\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 283 s, 10 iter, 100000 ts, 813 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-56-58\n",
      "  done: false\n",
      "  episode_len_mean: 384.66\n",
      "  episode_reward_max: 1415.8143422949574\n",
      "  episode_reward_mean: 907.3666804869396\n",
      "  episode_reward_min: -181.37813296845314\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 255\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5611.125\n",
      "    load_time_ms: 1.719\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0007812501862645149\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.402116060256958\n",
      "      kl: 0.019181285053491592\n",
      "      policy_loss: -0.005713180173188448\n",
      "      total_loss: 1813.5765380859375\n",
      "      vf_explained_var: 0.020336559042334557\n",
      "      vf_loss: 1813.58251953125\n",
      "    sample_time_ms: 24279.698\n",
      "    update_time_ms: 10.152\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 453.6833402434698\n",
      "  time_since_restore: 328.66249418258667\n",
      "  time_this_iter_s: 44.93209218978882\n",
      "  time_total_s: 328.66249418258667\n",
      "  timestamp: 1556657818\n",
      "  timesteps_since_restore: 110000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 11\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 328 s, 11 iter, 110000 ts, 907 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-58-08\n",
      "  done: false\n",
      "  episode_len_mean: 385.01\n",
      "  episode_reward_max: 1415.8143422949574\n",
      "  episode_reward_mean: 938.430258396914\n",
      "  episode_reward_min: -184.21590496378053\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 281\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6447.25\n",
      "    load_time_ms: 1.904\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0007812501862645149\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4424444437026978\n",
      "      kl: 0.0051236990839242935\n",
      "      policy_loss: -0.0014653874095529318\n",
      "      total_loss: 1421.5947265625\n",
      "      vf_explained_var: 0.1262216866016388\n",
      "      vf_loss: 1421.59619140625\n",
      "    sample_time_ms: 27136.064\n",
      "    update_time_ms: 10.991\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 469.215129198457\n",
      "  time_since_restore: 398.1482241153717\n",
      "  time_this_iter_s: 69.48572993278503\n",
      "  time_total_s: 398.1482241153717\n",
      "  timestamp: 1556657888\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 398 s, 12 iter, 120000 ts, 938 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_22-59-23\n",
      "  done: false\n",
      "  episode_len_mean: 394.93\n",
      "  episode_reward_max: 1415.8143422949574\n",
      "  episode_reward_mean: 984.0361998116147\n",
      "  episode_reward_min: -184.21590496378053\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 304\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7621.116\n",
      "    load_time_ms: 2.177\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00039062509313225746\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4127837419509888\n",
      "      kl: 0.0055690030567348\n",
      "      policy_loss: -0.0007025430677458644\n",
      "      total_loss: 1574.8143310546875\n",
      "      vf_explained_var: 0.03576235473155975\n",
      "      vf_loss: 1574.81494140625\n",
      "    sample_time_ms: 30583.837\n",
      "    update_time_ms: 11.696\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 492.01809990580745\n",
      "  time_since_restore: 473.6871340274811\n",
      "  time_this_iter_s: 75.53890991210938\n",
      "  time_total_s: 473.6871340274811\n",
      "  timestamp: 1556657963\n",
      "  timesteps_since_restore: 130000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 13\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 473 s, 13 iter, 130000 ts, 984 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-00-09\n",
      "  done: false\n",
      "  episode_len_mean: 398.26\n",
      "  episode_reward_max: 1412.611353680648\n",
      "  episode_reward_mean: 975.1349947580122\n",
      "  episode_reward_min: -184.21590496378053\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 330\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7609.569\n",
      "    load_time_ms: 2.219\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00019531254656612873\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4485613107681274\n",
      "      kl: 0.005352969281375408\n",
      "      policy_loss: -0.002110568806529045\n",
      "      total_loss: 1428.9093017578125\n",
      "      vf_explained_var: 0.11767151206731796\n",
      "      vf_loss: 1428.9112548828125\n",
      "    sample_time_ms: 32461.648\n",
      "    update_time_ms: 12.817\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 487.56749737900594\n",
      "  time_since_restore: 519.6216394901276\n",
      "  time_this_iter_s: 45.934505462646484\n",
      "  time_total_s: 519.6216394901276\n",
      "  timestamp: 1556658009\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 14\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 519 s, 14 iter, 140000 ts, 975 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-00-38\n",
      "  done: false\n",
      "  episode_len_mean: 387.01\n",
      "  episode_reward_max: 1411.422218771911\n",
      "  episode_reward_mean: 919.8227448793309\n",
      "  episode_reward_min: -185.59256079457674\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 358\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7610.023\n",
      "    load_time_ms: 2.212\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.765627328306437e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3942694664001465\n",
      "      kl: 0.004186901263892651\n",
      "      policy_loss: -0.0013194420607760549\n",
      "      total_loss: 1675.256591796875\n",
      "      vf_explained_var: 0.06474816799163818\n",
      "      vf_loss: 1675.2578125\n",
      "    sample_time_ms: 32548.468\n",
      "    update_time_ms: 12.597\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 459.91137243966546\n",
      "  time_since_restore: 547.9882111549377\n",
      "  time_this_iter_s: 28.36657166481018\n",
      "  time_total_s: 547.9882111549377\n",
      "  timestamp: 1556658038\n",
      "  timesteps_since_restore: 150000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 15\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 547 s, 15 iter, 150000 ts, 920 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-01-06\n",
      "  done: false\n",
      "  episode_len_mean: 375.75\n",
      "  episode_reward_max: 1419.8472652069586\n",
      "  episode_reward_mean: 929.5784038710538\n",
      "  episode_reward_min: -185.59256079457674\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 387\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7616.757\n",
      "    load_time_ms: 2.207\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.882813664153218e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.407396912574768\n",
      "      kl: 0.008030306547880173\n",
      "      policy_loss: -0.0010734081733971834\n",
      "      total_loss: 1740.960693359375\n",
      "      vf_explained_var: 0.16037826240062714\n",
      "      vf_loss: 1740.9619140625\n",
      "    sample_time_ms: 32566.689\n",
      "    update_time_ms: 11.469\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 464.7892019355268\n",
      "  time_since_restore: 575.6093091964722\n",
      "  time_this_iter_s: 27.621098041534424\n",
      "  time_total_s: 575.6093091964722\n",
      "  timestamp: 1556658066\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 16\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 575 s, 16 iter, 160000 ts, 930 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-01-38\n",
      "  done: false\n",
      "  episode_len_mean: 379.09\n",
      "  episode_reward_max: 1419.8472652069586\n",
      "  episode_reward_mean: 933.2460354557579\n",
      "  episode_reward_min: -185.59256079457674\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 410\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7667.34\n",
      "    load_time_ms: 2.186\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.441406832076609e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4476912021636963\n",
      "      kl: 0.004889502190053463\n",
      "      policy_loss: -0.0017920356476679444\n",
      "      total_loss: 1488.3880615234375\n",
      "      vf_explained_var: 0.04651958867907524\n",
      "      vf_loss: 1488.3897705078125\n",
      "    sample_time_ms: 33096.171\n",
      "    update_time_ms: 11.826\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 466.62301772787885\n",
      "  time_since_restore: 607.5949621200562\n",
      "  time_this_iter_s: 31.985652923583984\n",
      "  time_total_s: 607.5949621200562\n",
      "  timestamp: 1556658098\n",
      "  timesteps_since_restore: 170000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 17\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 607 s, 17 iter, 170000 ts, 933 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-02-19\n",
      "  done: false\n",
      "  episode_len_mean: 364.62\n",
      "  episode_reward_max: 1419.8472652069586\n",
      "  episode_reward_mean: 931.088058256405\n",
      "  episode_reward_min: -187.2189464004877\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 439\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8216.783\n",
      "    load_time_ms: 2.258\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2207034160383046e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4277774095535278\n",
      "      kl: 0.003531410126015544\n",
      "      policy_loss: -0.0011890383902937174\n",
      "      total_loss: 1852.808349609375\n",
      "      vf_explained_var: 0.1275540143251419\n",
      "      vf_loss: 1852.809326171875\n",
      "    sample_time_ms: 33890.732\n",
      "    update_time_ms: 11.901\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 465.54402912820245\n",
      "  time_since_restore: 649.0852315425873\n",
      "  time_this_iter_s: 41.49026942253113\n",
      "  time_total_s: 649.0852315425873\n",
      "  timestamp: 1556658139\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 18\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 649 s, 18 iter, 180000 ts, 931 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-02-55\n",
      "  done: false\n",
      "  episode_len_mean: 367.86\n",
      "  episode_reward_max: 1419.8472652069586\n",
      "  episode_reward_mean: 940.2255280534835\n",
      "  episode_reward_min: -187.2189464004877\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 467\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8259.769\n",
      "    load_time_ms: 2.29\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.103517080191523e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4408317804336548\n",
      "      kl: 0.003342911135405302\n",
      "      policy_loss: -0.0011728735407814384\n",
      "      total_loss: 1735.8681640625\n",
      "      vf_explained_var: 0.1616227626800537\n",
      "      vf_loss: 1735.869384765625\n",
      "    sample_time_ms: 34714.077\n",
      "    update_time_ms: 12.873\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 470.1127640267416\n",
      "  time_since_restore: 685.0834450721741\n",
      "  time_this_iter_s: 35.99821352958679\n",
      "  time_total_s: 685.0834450721741\n",
      "  timestamp: 1556658175\n",
      "  timesteps_since_restore: 190000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 19\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 685 s, 19 iter, 190000 ts, 940 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-03-34\n",
      "  done: false\n",
      "  episode_len_mean: 369.2\n",
      "  episode_reward_max: 1405.729194781115\n",
      "  episode_reward_mean: 940.3617793154523\n",
      "  episode_reward_min: -187.2189464004877\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 494\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8385.961\n",
      "    load_time_ms: 2.25\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.0517585400957614e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4382519721984863\n",
      "      kl: 0.004644350148737431\n",
      "      policy_loss: -0.0015127849765121937\n",
      "      total_loss: 1753.2449951171875\n",
      "      vf_explained_var: 0.09483568370342255\n",
      "      vf_loss: 1753.246337890625\n",
      "    sample_time_ms: 35632.418\n",
      "    update_time_ms: 12.5\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 470.18088965772614\n",
      "  time_since_restore: 724.3376696109772\n",
      "  time_this_iter_s: 39.2542245388031\n",
      "  time_total_s: 724.3376696109772\n",
      "  timestamp: 1556658214\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 20\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 724 s, 20 iter, 200000 ts, 940 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-04-11\n",
      "  done: false\n",
      "  episode_len_mean: 350.77\n",
      "  episode_reward_max: 1404.181516767624\n",
      "  episode_reward_mean: 902.0073416071891\n",
      "  episode_reward_min: -180.6351325359449\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 524\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8180.339\n",
      "    load_time_ms: 2.228\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5258792700478807e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4388272762298584\n",
      "      kl: 0.006952925585210323\n",
      "      policy_loss: -0.001292935572564602\n",
      "      total_loss: 1778.7197265625\n",
      "      vf_explained_var: 0.1850634217262268\n",
      "      vf_loss: 1778.72119140625\n",
      "    sample_time_ms: 35043.303\n",
      "    update_time_ms: 12.32\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 451.00367080359473\n",
      "  time_since_restore: 761.3065257072449\n",
      "  time_this_iter_s: 36.9688560962677\n",
      "  time_total_s: 761.3065257072449\n",
      "  timestamp: 1556658251\n",
      "  timesteps_since_restore: 210000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 21\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 761 s, 21 iter, 210000 ts, 902 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-04-48\n",
      "  done: false\n",
      "  episode_len_mean: 352.46\n",
      "  episode_reward_max: 1404.181516767624\n",
      "  episode_reward_mean: 927.7338719412378\n",
      "  episode_reward_min: -185.08902667741282\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 554\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7461.164\n",
      "    load_time_ms: 2.104\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.629396350239404e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4571689367294312\n",
      "      kl: 0.008601880632340908\n",
      "      policy_loss: -0.0029612986836582422\n",
      "      total_loss: 1803.2359619140625\n",
      "      vf_explained_var: 0.2399851530790329\n",
      "      vf_loss: 1803.23876953125\n",
      "    sample_time_ms: 32484.311\n",
      "    update_time_ms: 11.282\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 463.8669359706188\n",
      "  time_since_restore: 797.9893178939819\n",
      "  time_this_iter_s: 36.68279218673706\n",
      "  time_total_s: 797.9893178939819\n",
      "  timestamp: 1556658288\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 22\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 797 s, 22 iter, 220000 ts, 928 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-05-48\n",
      "  done: false\n",
      "  episode_len_mean: 322.81\n",
      "  episode_reward_max: 1400.5466829719642\n",
      "  episode_reward_mean: 854.6684063495196\n",
      "  episode_reward_min: -185.08902667741282\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 586\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6836.842\n",
      "    load_time_ms: 1.887\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814698175119702e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4575774669647217\n",
      "      kl: 0.0049046482890844345\n",
      "      policy_loss: -0.001457239966839552\n",
      "      total_loss: 1977.646240234375\n",
      "      vf_explained_var: 0.21209052205085754\n",
      "      vf_loss: 1977.6478271484375\n",
      "    sample_time_ms: 31518.123\n",
      "    update_time_ms: 9.506\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 427.33420317475975\n",
      "  time_since_restore: 857.5968627929688\n",
      "  time_this_iter_s: 59.607544898986816\n",
      "  time_total_s: 857.5968627929688\n",
      "  timestamp: 1556658348\n",
      "  timesteps_since_restore: 230000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 23\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 857 s, 23 iter, 230000 ts, 855 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-06-49\n",
      "  done: false\n",
      "  episode_len_mean: 330.62\n",
      "  episode_reward_max: 1416.5969156278336\n",
      "  episode_reward_mean: 923.3761289461452\n",
      "  episode_reward_min: -185.08902667741282\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 615\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7451.579\n",
      "    load_time_ms: 1.914\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.907349087559851e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4793695211410522\n",
      "      kl: 0.013144159689545631\n",
      "      policy_loss: -0.0028686977457255125\n",
      "      total_loss: 2116.353759765625\n",
      "      vf_explained_var: 0.05726214125752449\n",
      "      vf_loss: 2116.356689453125\n",
      "    sample_time_ms: 32422.883\n",
      "    update_time_ms: 9.875\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 461.68806447307276\n",
      "  time_since_restore: 918.7463085651398\n",
      "  time_this_iter_s: 61.14944577217102\n",
      "  time_total_s: 918.7463085651398\n",
      "  timestamp: 1556658409\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 24\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 918 s, 24 iter, 240000 ts, 923 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-07-54\n",
      "  done: false\n",
      "  episode_len_mean: 335.85\n",
      "  episode_reward_max: 1418.079309616454\n",
      "  episode_reward_mean: 952.1592958447114\n",
      "  episode_reward_min: -185.08902667741282\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 644\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8608.381\n",
      "    load_time_ms: 2.12\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.907349087559851e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.5153332948684692\n",
      "      kl: 0.0021830338519066572\n",
      "      policy_loss: -0.0009822241263464093\n",
      "      total_loss: 1966.1468505859375\n",
      "      vf_explained_var: 0.0710383951663971\n",
      "      vf_loss: 1966.1478271484375\n",
      "    sample_time_ms: 34869.707\n",
      "    update_time_ms: 11.123\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 476.0796479223556\n",
      "  time_since_restore: 983.1905324459076\n",
      "  time_this_iter_s: 64.44422388076782\n",
      "  time_total_s: 983.1905324459076\n",
      "  timestamp: 1556658474\n",
      "  timesteps_since_restore: 250000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 25\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 983 s, 25 iter, 250000 ts, 952 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-10-03\n",
      "  done: false\n",
      "  episode_len_mean: 337.9\n",
      "  episode_reward_max: 1418.079309616454\n",
      "  episode_reward_mean: 958.0664314384434\n",
      "  episode_reward_min: -180.80339955512994\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 673\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 12594.509\n",
      "    load_time_ms: 2.637\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.536745437799254e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.522360920906067\n",
      "      kl: 0.010245797224342823\n",
      "      policy_loss: -0.0020113347563892603\n",
      "      total_loss: 1989.60693359375\n",
      "      vf_explained_var: 0.14057029783725739\n",
      "      vf_loss: 1989.6090087890625\n",
      "    sample_time_ms: 41067.988\n",
      "    update_time_ms: 14.768\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 479.03321571922163\n",
      "  time_since_restore: 1112.797021150589\n",
      "  time_this_iter_s: 129.6064887046814\n",
      "  time_total_s: 1112.797021150589\n",
      "  timestamp: 1556658603\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 26\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 1112 s, 26 iter, 260000 ts, 958 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-11-14\n",
      "  done: false\n",
      "  episode_len_mean: 343.53\n",
      "  episode_reward_max: 1418.079309616454\n",
      "  episode_reward_mean: 942.678677731258\n",
      "  episode_reward_min: -183.7874063330836\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 703\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 12883.822\n",
      "    load_time_ms: 2.682\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.536745437799254e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.5337902307510376\n",
      "      kl: 0.006755727808922529\n",
      "      policy_loss: -0.0019165256526321173\n",
      "      total_loss: 1666.72314453125\n",
      "      vf_explained_var: 0.23662276566028595\n",
      "      vf_loss: 1666.7252197265625\n",
      "    sample_time_ms: 44614.247\n",
      "    update_time_ms: 16.172\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 471.33933886562886\n",
      "  time_since_restore: 1183.1514158248901\n",
      "  time_this_iter_s: 70.35439467430115\n",
      "  time_total_s: 1183.1514158248901\n",
      "  timestamp: 1556658674\n",
      "  timesteps_since_restore: 270000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 27\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 1183 s, 27 iter, 270000 ts, 943 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-12-04\n",
      "  done: false\n",
      "  episode_len_mean: 330.09\n",
      "  episode_reward_max: 1403.572553558213\n",
      "  episode_reward_mean: 858.0223877016609\n",
      "  episode_reward_min: -183.7874063330836\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 735\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 13218.12\n",
      "    load_time_ms: 2.677\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.768372718899627e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4970407485961914\n",
      "      kl: 0.0073963371105492115\n",
      "      policy_loss: -0.002207511104643345\n",
      "      total_loss: 1848.1153564453125\n",
      "      vf_explained_var: 0.16577450931072235\n",
      "      vf_loss: 1848.1177978515625\n",
      "    sample_time_ms: 45120.729\n",
      "    update_time_ms: 16.671\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 429.01119385083047\n",
      "  time_since_restore: 1233.0645265579224\n",
      "  time_this_iter_s: 49.91311073303223\n",
      "  time_total_s: 1233.0645265579224\n",
      "  timestamp: 1556658724\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 28\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 1233 s, 28 iter, 280000 ts, 858 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-13-09\n",
      "  done: false\n",
      "  episode_len_mean: 341.42\n",
      "  episode_reward_max: 1420.505308579849\n",
      "  episode_reward_mean: 906.6732820571784\n",
      "  episode_reward_min: -186.3295837277999\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 761\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 13896.326\n",
      "    load_time_ms: 2.798\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841863594498136e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4263521432876587\n",
      "      kl: 0.003341753501445055\n",
      "      policy_loss: -0.0006154272123239934\n",
      "      total_loss: 1829.0430908203125\n",
      "      vf_explained_var: 0.15690132975578308\n",
      "      vf_loss: 1829.0438232421875\n",
      "    sample_time_ms: 47301.277\n",
      "    update_time_ms: 17.664\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 453.3366410285892\n",
      "  time_since_restore: 1297.6772305965424\n",
      "  time_this_iter_s: 64.61270403862\n",
      "  time_total_s: 1297.6772305965424\n",
      "  timestamp: 1556658789\n",
      "  timesteps_since_restore: 290000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 29\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 1297 s, 29 iter, 290000 ts, 907 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-14-04\n",
      "  done: false\n",
      "  episode_len_mean: 334.71\n",
      "  episode_reward_max: 1420.505308579849\n",
      "  episode_reward_mean: 898.0806982552263\n",
      "  episode_reward_min: -186.3295837277999\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 793\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 14774.066\n",
      "    load_time_ms: 2.794\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1920931797249068e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.443880558013916\n",
      "      kl: 0.002498459769412875\n",
      "      policy_loss: -0.0002975047391373664\n",
      "      total_loss: 1764.0777587890625\n",
      "      vf_explained_var: 0.27040496468544006\n",
      "      vf_loss: 1764.0777587890625\n",
      "    sample_time_ms: 47970.417\n",
      "    update_time_ms: 18.526\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 449.0403491276133\n",
      "  time_since_restore: 1352.4327819347382\n",
      "  time_this_iter_s: 54.7555513381958\n",
      "  time_total_s: 1352.4327819347382\n",
      "  timestamp: 1556658844\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 30\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 1352 s, 30 iter, 300000 ts, 898 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-15-24\n",
      "  done: false\n",
      "  episode_len_mean: 339.94\n",
      "  episode_reward_max: 1432.8538469976413\n",
      "  episode_reward_mean: 952.0914577772294\n",
      "  episode_reward_min: -186.3295837277999\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 824\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 15266.503\n",
      "    load_time_ms: 2.809\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.960465898624534e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4284425973892212\n",
      "      kl: 0.0038877069018781185\n",
      "      policy_loss: -0.0014723195927217603\n",
      "      total_loss: 1898.3734130859375\n",
      "      vf_explained_var: 0.20559115707874298\n",
      "      vf_loss: 1898.374755859375\n",
      "    sample_time_ms: 51836.499\n",
      "    update_time_ms: 19.795\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 476.0457288886147\n",
      "  time_since_restore: 1433.0112080574036\n",
      "  time_this_iter_s: 80.5784261226654\n",
      "  time_total_s: 1433.0112080574036\n",
      "  timestamp: 1556658924\n",
      "  timesteps_since_restore: 310000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 31\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 1433 s, 31 iter, 310000 ts, 952 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-16-14\n",
      "  done: false\n",
      "  episode_len_mean: 314.92\n",
      "  episode_reward_max: 1432.8538469976413\n",
      "  episode_reward_mean: 877.4180305290018\n",
      "  episode_reward_min: -185.0878862523558\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 857\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 15676.605\n",
      "    load_time_ms: 2.776\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.980232949312267e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3994739055633545\n",
      "      kl: 0.004259646870195866\n",
      "      policy_loss: -0.0013641446130350232\n",
      "      total_loss: 2038.5506591796875\n",
      "      vf_explained_var: 0.24618259072303772\n",
      "      vf_loss: 2038.5516357421875\n",
      "    sample_time_ms: 52705.867\n",
      "    update_time_ms: 20.553\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 438.7090152645011\n",
      "  time_since_restore: 1482.5005054473877\n",
      "  time_this_iter_s: 49.48929738998413\n",
      "  time_total_s: 1482.5005054473877\n",
      "  timestamp: 1556658974\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 32\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 1482 s, 32 iter, 320000 ts, 877 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-17-22\n",
      "  done: false\n",
      "  episode_len_mean: 325.02\n",
      "  episode_reward_max: 1432.8538469976413\n",
      "  episode_reward_mean: 969.5582871279697\n",
      "  episode_reward_min: -185.0878862523558\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 887\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 15931.966\n",
      "    load_time_ms: 2.784\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4901164746561335e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3800606727600098\n",
      "      kl: 0.014826163649559021\n",
      "      policy_loss: -0.0029540355317294598\n",
      "      total_loss: 2202.331787109375\n",
      "      vf_explained_var: 0.281764417886734\n",
      "      vf_loss: 2202.33447265625\n",
      "    sample_time_ms: 53252.461\n",
      "    update_time_ms: 22.234\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 484.77914356398486\n",
      "  time_since_restore: 1550.1374907493591\n",
      "  time_this_iter_s: 67.63698530197144\n",
      "  time_total_s: 1550.1374907493591\n",
      "  timestamp: 1556659042\n",
      "  timesteps_since_restore: 330000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 33\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 1550 s, 33 iter, 330000 ts, 970 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-18-00\n",
      "  done: false\n",
      "  episode_len_mean: 304.98\n",
      "  episode_reward_max: 1417.011037711785\n",
      "  episode_reward_mean: 911.0615956933274\n",
      "  episode_reward_min: -185.71094681964183\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 921\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 15614.05\n",
      "    load_time_ms: 2.767\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4901164746561335e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4621858596801758\n",
      "      kl: 0.00820971094071865\n",
      "      policy_loss: -0.00131328369025141\n",
      "      total_loss: 2091.936767578125\n",
      "      vf_explained_var: 0.38731077313423157\n",
      "      vf_loss: 2091.93798828125\n",
      "    sample_time_ms: 51278.922\n",
      "    update_time_ms: 21.369\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 455.53079784666375\n",
      "  time_since_restore: 1588.3516147136688\n",
      "  time_this_iter_s: 38.21412396430969\n",
      "  time_total_s: 1588.3516147136688\n",
      "  timestamp: 1556659080\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 34\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 1588 s, 34 iter, 340000 ts, 911 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-19-48\n",
      "  done: false\n",
      "  episode_len_mean: 315.99\n",
      "  episode_reward_max: 1421.2501881044432\n",
      "  episode_reward_mean: 956.4725017518606\n",
      "  episode_reward_min: -185.71094681964183\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 951\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 16451.642\n",
      "    load_time_ms: 5.541\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.450582373280668e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4694674015045166\n",
      "      kl: 0.0038134302012622356\n",
      "      policy_loss: -0.0007163169793784618\n",
      "      total_loss: 2209.748046875\n",
      "      vf_explained_var: 0.10710683465003967\n",
      "      vf_loss: 2209.748779296875\n",
      "    sample_time_ms: 54813.451\n",
      "    update_time_ms: 21.502\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 478.2362508759303\n",
      "  time_since_restore: 1696.589295387268\n",
      "  time_this_iter_s: 108.23768067359924\n",
      "  time_total_s: 1696.589295387268\n",
      "  timestamp: 1556659188\n",
      "  timesteps_since_restore: 350000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 35\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 1696 s, 35 iter, 350000 ts, 956 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-21-16\n",
      "  done: false\n",
      "  episode_len_mean: 321.86\n",
      "  episode_reward_max: 1425.3648859882653\n",
      "  episode_reward_mean: 970.8793011722427\n",
      "  episode_reward_min: -185.71094681964183\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 980\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 13035.311\n",
      "    load_time_ms: 5.117\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.725291186640334e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4912707805633545\n",
      "      kl: 0.009293051436543465\n",
      "      policy_loss: -0.0034442113246768713\n",
      "      total_loss: 1986.195068359375\n",
      "      vf_explained_var: 0.3005276322364807\n",
      "      vf_loss: 1986.1983642578125\n",
      "    sample_time_ms: 54012.956\n",
      "    update_time_ms: 19.193\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 485.4396505861213\n",
      "  time_since_restore: 1783.9137682914734\n",
      "  time_this_iter_s: 87.32447290420532\n",
      "  time_total_s: 1783.9137682914734\n",
      "  timestamp: 1556659276\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 36\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 1783 s, 36 iter, 360000 ts, 971 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-23-00\n",
      "  done: false\n",
      "  episode_len_mean: 335.2\n",
      "  episode_reward_max: 1425.3648859882653\n",
      "  episode_reward_mean: 1013.8026721476741\n",
      "  episode_reward_min: -185.71094681964183\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1009\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 13769.603\n",
      "    load_time_ms: 5.19\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.862645593320167e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.449527621269226\n",
      "      kl: 0.006279806140810251\n",
      "      policy_loss: -0.0011344305239617825\n",
      "      total_loss: 1885.1497802734375\n",
      "      vf_explained_var: 0.3706667125225067\n",
      "      vf_loss: 1885.1510009765625\n",
      "    sample_time_ms: 56687.845\n",
      "    update_time_ms: 18.4\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 506.90133607383694\n",
      "  time_since_restore: 1888.408701658249\n",
      "  time_this_iter_s: 104.49493336677551\n",
      "  time_total_s: 1888.408701658249\n",
      "  timestamp: 1556659380\n",
      "  timesteps_since_restore: 370000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 37\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 1888 s, 37 iter, 370000 ts, 1.01e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-25-06\n",
      "  done: false\n",
      "  episode_len_mean: 340.24\n",
      "  episode_reward_max: 1425.3648859882653\n",
      "  episode_reward_mean: 1048.9945912288936\n",
      "  episode_reward_min: -184.80495490807894\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 1040\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 15232.03\n",
      "    load_time_ms: 5.452\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.313227966600834e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4441478252410889\n",
      "      kl: 0.007167404983192682\n",
      "      policy_loss: -0.0016096321633085608\n",
      "      total_loss: 2113.85986328125\n",
      "      vf_explained_var: 0.2994917929172516\n",
      "      vf_loss: 2113.86181640625\n",
      "    sample_time_ms: 62759.663\n",
      "    update_time_ms: 19.752\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 524.4972956144468\n",
      "  time_since_restore: 2013.7207999229431\n",
      "  time_this_iter_s: 125.31209826469421\n",
      "  time_total_s: 2013.7207999229431\n",
      "  timestamp: 1556659506\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 38\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 2013 s, 38 iter, 380000 ts, 1.05e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-27-39\n",
      "  done: false\n",
      "  episode_len_mean: 327.73\n",
      "  episode_reward_max: 1415.4428854134142\n",
      "  episode_reward_mean: 1017.7501406003424\n",
      "  episode_reward_min: -186.88681449710285\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1073\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 17027.158\n",
      "    load_time_ms: 5.512\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.656613983300417e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4174734354019165\n",
      "      kl: 0.004987653810530901\n",
      "      policy_loss: -0.0009113699197769165\n",
      "      total_loss: 2107.00048828125\n",
      "      vf_explained_var: 0.5117461681365967\n",
      "      vf_loss: 2107.001220703125\n",
      "    sample_time_ms: 69768.713\n",
      "    update_time_ms: 22.038\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 508.87507030017116\n",
      "  time_since_restore: 2166.435303926468\n",
      "  time_this_iter_s: 152.71450400352478\n",
      "  time_total_s: 2166.435303926468\n",
      "  timestamp: 1556659659\n",
      "  timesteps_since_restore: 390000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 39\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 2166 s, 39 iter, 390000 ts, 1.02e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-30-02\n",
      "  done: false\n",
      "  episode_len_mean: 315.96\n",
      "  episode_reward_max: 1415.4428854134142\n",
      "  episode_reward_mean: 984.715752815639\n",
      "  episode_reward_min: -188.52041327437985\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 1104\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18094.69\n",
      "    load_time_ms: 5.657\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283069916502086e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4149739742279053\n",
      "      kl: 0.004616323858499527\n",
      "      policy_loss: -0.0018244137754663825\n",
      "      total_loss: 2132.19140625\n",
      "      vf_explained_var: 0.3322380781173706\n",
      "      vf_loss: 2132.193115234375\n",
      "    sample_time_ms: 77503.314\n",
      "    update_time_ms: 25.971\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 492.35787640781945\n",
      "  time_since_restore: 2309.2917490005493\n",
      "  time_this_iter_s: 142.85644507408142\n",
      "  time_total_s: 2309.2917490005493\n",
      "  timestamp: 1556659802\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 40\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 2309 s, 40 iter, 400000 ts, 985 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-31-27\n",
      "  done: false\n",
      "  episode_len_mean: 317.62\n",
      "  episode_reward_max: 1415.4428854134142\n",
      "  episode_reward_mean: 997.1171558110119\n",
      "  episode_reward_min: -188.52041327437985\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 1135\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 17894.851\n",
      "    load_time_ms: 6.03\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1641534958251043e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4336013793945312\n",
      "      kl: 0.012249625287950039\n",
      "      policy_loss: -0.002873099409043789\n",
      "      total_loss: 2112.213134765625\n",
      "      vf_explained_var: 0.3056470453739166\n",
      "      vf_loss: 2112.216064453125\n",
      "    sample_time_ms: 78167.277\n",
      "    update_time_ms: 27.245\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 498.5585779055058\n",
      "  time_since_restore: 2394.5331058502197\n",
      "  time_this_iter_s: 85.24135684967041\n",
      "  time_total_s: 2394.5331058502197\n",
      "  timestamp: 1556659887\n",
      "  timesteps_since_restore: 410000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 41\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 2394 s, 41 iter, 410000 ts, 997 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-32-23\n",
      "  done: false\n",
      "  episode_len_mean: 325.52\n",
      "  episode_reward_max: 1415.4428854134142\n",
      "  episode_reward_mean: 1017.7811295819841\n",
      "  episode_reward_min: -189.75005959371057\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1165\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18213.918\n",
      "    load_time_ms: 7.819\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1641534958251043e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4535138607025146\n",
      "      kl: 0.007474297191947699\n",
      "      policy_loss: -0.0014299299800768495\n",
      "      total_loss: 2001.966796875\n",
      "      vf_explained_var: 0.38807135820388794\n",
      "      vf_loss: 2001.9683837890625\n",
      "    sample_time_ms: 78443.493\n",
      "    update_time_ms: 26.396\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 508.89056479099213\n",
      "  time_since_restore: 2450.0474853515625\n",
      "  time_this_iter_s: 55.51437950134277\n",
      "  time_total_s: 2450.0474853515625\n",
      "  timestamp: 1556659943\n",
      "  timesteps_since_restore: 420000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 42\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 2450 s, 42 iter, 420000 ts, 1.02e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-34-38\n",
      "  done: false\n",
      "  episode_len_mean: 315.19\n",
      "  episode_reward_max: 1412.6848037593577\n",
      "  episode_reward_mean: 941.6718656755035\n",
      "  episode_reward_min: -189.75005959371057\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1198\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18296.018\n",
      "    load_time_ms: 7.855\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.8207674791255215e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.504158854484558\n",
      "      kl: 0.005719893611967564\n",
      "      policy_loss: -0.00043850907240994275\n",
      "      total_loss: 1948.9512939453125\n",
      "      vf_explained_var: 0.3561842441558838\n",
      "      vf_loss: 1948.951904296875\n",
      "    sample_time_ms: 85066.013\n",
      "    update_time_ms: 25.835\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 470.83593283775167\n",
      "  time_since_restore: 2584.7689394950867\n",
      "  time_this_iter_s: 134.72145414352417\n",
      "  time_total_s: 2584.7689394950867\n",
      "  timestamp: 1556660078\n",
      "  timesteps_since_restore: 430000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 43\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 2584 s, 43 iter, 430000 ts, 942 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-35-25\n",
      "  done: false\n",
      "  episode_len_mean: 313.14\n",
      "  episode_reward_max: 1412.6848037593577\n",
      "  episode_reward_mean: 921.6453144207762\n",
      "  episode_reward_min: -189.75005959371057\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 1230\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18233.375\n",
      "    load_time_ms: 7.788\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9103837395627608e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4886537790298462\n",
      "      kl: 0.011486330069601536\n",
      "      policy_loss: -0.00255565601401031\n",
      "      total_loss: 1858.00439453125\n",
      "      vf_explained_var: 0.508164644241333\n",
      "      vf_loss: 1858.0072021484375\n",
      "    sample_time_ms: 86038.402\n",
      "    update_time_ms: 27.675\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 460.82265721038823\n",
      "  time_since_restore: 2632.0938634872437\n",
      "  time_this_iter_s: 47.32492399215698\n",
      "  time_total_s: 2632.0938634872437\n",
      "  timestamp: 1556660125\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 44\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 2632 s, 44 iter, 440000 ts, 922 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-36-04\n",
      "  done: false\n",
      "  episode_len_mean: 323.95\n",
      "  episode_reward_max: 1412.6848037593577\n",
      "  episode_reward_mean: 948.2531268590875\n",
      "  episode_reward_min: -182.76280085045482\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1258\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 16398.106\n",
      "    load_time_ms: 4.965\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9103837395627608e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4958767890930176\n",
      "      kl: 0.004864939488470554\n",
      "      policy_loss: 5.653271364280954e-05\n",
      "      total_loss: 1780.6883544921875\n",
      "      vf_explained_var: 0.4421144127845764\n",
      "      vf_loss: 1780.6883544921875\n",
      "    sample_time_ms: 80902.899\n",
      "    update_time_ms: 26.425\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 474.1265634295437\n",
      "  time_since_restore: 2670.5356526374817\n",
      "  time_this_iter_s: 38.44178915023804\n",
      "  time_total_s: 2670.5356526374817\n",
      "  timestamp: 1556660164\n",
      "  timesteps_since_restore: 450000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 45\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 2670 s, 45 iter, 450000 ts, 948 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-36-44\n",
      "  done: false\n",
      "  episode_len_mean: 335.51\n",
      "  episode_reward_max: 1393.0692854808865\n",
      "  episode_reward_mean: 982.1306820684227\n",
      "  episode_reward_min: -183.32031642506934\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1286\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 16048.042\n",
      "    load_time_ms: 4.917\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4551918697813804e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.501133680343628\n",
      "      kl: 0.004734060727059841\n",
      "      policy_loss: -0.0009301417740061879\n",
      "      total_loss: 1666.369140625\n",
      "      vf_explained_var: 0.5209365487098694\n",
      "      vf_loss: 1666.3699951171875\n",
      "    sample_time_ms: 76560.0\n",
      "    update_time_ms: 25.708\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 491.06534103421143\n",
      "  time_since_restore: 2710.921697616577\n",
      "  time_this_iter_s: 40.38604497909546\n",
      "  time_total_s: 2710.921697616577\n",
      "  timestamp: 1556660204\n",
      "  timesteps_since_restore: 460000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 46\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 2710 s, 46 iter, 460000 ts, 982 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-37-39\n",
      "  done: false\n",
      "  episode_len_mean: 351.56\n",
      "  episode_reward_max: 1399.964312070291\n",
      "  episode_reward_mean: 1052.6650002787264\n",
      "  episode_reward_min: -183.32031642506934\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1314\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 15680.651\n",
      "    load_time_ms: 4.907\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.275959348906902e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4812477827072144\n",
      "      kl: 0.008545653894543648\n",
      "      policy_loss: -0.0012909343931823969\n",
      "      total_loss: 1663.9383544921875\n",
      "      vf_explained_var: 0.4914611279964447\n",
      "      vf_loss: 1663.939697265625\n",
      "    sample_time_ms: 71926.188\n",
      "    update_time_ms: 26.618\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 526.3325001393634\n",
      "  time_since_restore: 2765.3789353370667\n",
      "  time_this_iter_s: 54.4572377204895\n",
      "  time_total_s: 2765.3789353370667\n",
      "  timestamp: 1556660259\n",
      "  timesteps_since_restore: 470000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 47\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 2765 s, 47 iter, 470000 ts, 1.05e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-39-23\n",
      "  done: false\n",
      "  episode_len_mean: 344.63\n",
      "  episode_reward_max: 1399.964312070291\n",
      "  episode_reward_mean: 1007.3470382527954\n",
      "  episode_reward_min: -183.58180981504947\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 1345\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 15150.341\n",
      "    load_time_ms: 4.853\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.637979674453451e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.505379319190979\n",
      "      kl: 0.006921945605427027\n",
      "      policy_loss: -0.0017759775510057807\n",
      "      total_loss: 1781.269287109375\n",
      "      vf_explained_var: 0.48719358444213867\n",
      "      vf_loss: 1781.27099609375\n",
      "    sample_time_ms: 70370.576\n",
      "    update_time_ms: 25.545\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 503.6735191263979\n",
      "  time_since_restore: 2869.775811433792\n",
      "  time_this_iter_s: 104.39687609672546\n",
      "  time_total_s: 2869.775811433792\n",
      "  timestamp: 1556660363\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 48\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 2869 s, 48 iter, 480000 ts, 1.01e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-41-00\n",
      "  done: false\n",
      "  episode_len_mean: 335.81\n",
      "  episode_reward_max: 1399.964312070291\n",
      "  episode_reward_mean: 964.7989136452363\n",
      "  episode_reward_min: -185.12895704001107\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 1377\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 14608.474\n",
      "    load_time_ms: 4.792\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8189898372267255e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.527361273765564\n",
      "      kl: 0.0038444872479885817\n",
      "      policy_loss: -0.0007497912738472223\n",
      "      total_loss: 1612.2774658203125\n",
      "      vf_explained_var: 0.5016213655471802\n",
      "      vf_loss: 1612.2781982421875\n",
      "    sample_time_ms: 65332.396\n",
      "    update_time_ms: 24.943\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 482.39945682261816\n",
      "  time_since_restore: 2966.696275472641\n",
      "  time_this_iter_s: 96.92046403884888\n",
      "  time_total_s: 2966.696275472641\n",
      "  timestamp: 1556660460\n",
      "  timesteps_since_restore: 490000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 49\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 2966 s, 49 iter, 490000 ts, 965 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-42-36\n",
      "  done: false\n",
      "  episode_len_mean: 328.02\n",
      "  episode_reward_max: 1404.1359614623707\n",
      "  episode_reward_mean: 922.230092766831\n",
      "  episode_reward_min: -187.41308860637884\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1407\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 13058.884\n",
      "    load_time_ms: 4.747\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.094949186133627e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.5205734968185425\n",
      "      kl: 0.004076680168509483\n",
      "      policy_loss: -0.0007656085072085261\n",
      "      total_loss: 1574.7569580078125\n",
      "      vf_explained_var: 0.5802327394485474\n",
      "      vf_loss: 1574.7579345703125\n",
      "    sample_time_ms: 62161.976\n",
      "    update_time_ms: 21.776\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 461.1150463834157\n",
      "  time_since_restore: 3062.2748844623566\n",
      "  time_this_iter_s: 95.57860898971558\n",
      "  time_total_s: 3062.2748844623566\n",
      "  timestamp: 1556660556\n",
      "  timesteps_since_restore: 500000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 50\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3062 s, 50 iter, 500000 ts, 922 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-43-38\n",
      "  done: false\n",
      "  episode_len_mean: 324.59\n",
      "  episode_reward_max: 1425.692913316443\n",
      "  episode_reward_mean: 921.4966359176128\n",
      "  episode_reward_min: -187.41308860637884\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1437\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 13483.586\n",
      "    load_time_ms: 4.539\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.5474745930668137e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4480640888214111\n",
      "      kl: 0.008285813964903355\n",
      "      policy_loss: -0.0013109102146700025\n",
      "      total_loss: 1732.869384765625\n",
      "      vf_explained_var: 0.6004480123519897\n",
      "      vf_loss: 1732.8709716796875\n",
      "    sample_time_ms: 59402.12\n",
      "    update_time_ms: 19.777\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 460.74831795880635\n",
      "  time_since_restore: 3124.136358976364\n",
      "  time_this_iter_s: 61.86147451400757\n",
      "  time_total_s: 3124.136358976364\n",
      "  timestamp: 1556660618\n",
      "  timesteps_since_restore: 510000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 51\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3124 s, 51 iter, 510000 ts, 921 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-44-59\n",
      "  done: false\n",
      "  episode_len_mean: 327.31\n",
      "  episode_reward_max: 1425.692913316443\n",
      "  episode_reward_mean: 918.6075697835473\n",
      "  episode_reward_min: -187.41308860637884\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1465\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 13336.742\n",
      "    load_time_ms: 2.975\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2737372965334068e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.5504945516586304\n",
      "      kl: 0.012219210155308247\n",
      "      policy_loss: -0.0017260011518374085\n",
      "      total_loss: 1523.1805419921875\n",
      "      vf_explained_var: 0.5955774188041687\n",
      "      vf_loss: 1523.1822509765625\n",
      "    sample_time_ms: 62075.284\n",
      "    update_time_ms: 20.92\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 459.3037848917735\n",
      "  time_since_restore: 3204.8509039878845\n",
      "  time_this_iter_s: 80.71454501152039\n",
      "  time_total_s: 3204.8509039878845\n",
      "  timestamp: 1556660699\n",
      "  timesteps_since_restore: 520000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 52\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 12.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3204 s, 52 iter, 520000 ts, 919 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-46-09\n",
      "  done: false\n",
      "  episode_len_mean: 344.83\n",
      "  episode_reward_max: 1425.692913316443\n",
      "  episode_reward_mean: 956.8848220007541\n",
      "  episode_reward_min: -184.6204307527409\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1494\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 13639.876\n",
      "    load_time_ms: 2.992\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.2737372965334068e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.6452645063400269\n",
      "      kl: 0.008060007356107235\n",
      "      policy_loss: -0.0021374921780079603\n",
      "      total_loss: 1316.270263671875\n",
      "      vf_explained_var: 0.6278533339500427\n",
      "      vf_loss: 1316.272216796875\n",
      "    sample_time_ms: 55313.078\n",
      "    update_time_ms: 21.264\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 478.44241100037704\n",
      "  time_since_restore: 3274.9778192043304\n",
      "  time_this_iter_s: 70.12691521644592\n",
      "  time_total_s: 3274.9778192043304\n",
      "  timestamp: 1556660769\n",
      "  timesteps_since_restore: 530000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 53\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3274 s, 53 iter, 530000 ts, 957 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-47-59\n",
      "  done: false\n",
      "  episode_len_mean: 348.0\n",
      "  episode_reward_max: 1427.6098325521411\n",
      "  episode_reward_mean: 956.416906774491\n",
      "  episode_reward_min: -183.10991701604522\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1523\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 14709.589\n",
      "    load_time_ms: 3.179\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1368686482667034e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.5341742038726807\n",
      "      kl: 0.005494275130331516\n",
      "      policy_loss: -0.0008587728370912373\n",
      "      total_loss: 1444.368408203125\n",
      "      vf_explained_var: 0.6269320249557495\n",
      "      vf_loss: 1444.3692626953125\n",
      "    sample_time_ms: 60526.414\n",
      "    update_time_ms: 21.485\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 478.2084533872455\n",
      "  time_since_restore: 3385.162417650223\n",
      "  time_this_iter_s: 110.18459844589233\n",
      "  time_total_s: 3385.162417650223\n",
      "  timestamp: 1556660879\n",
      "  timesteps_since_restore: 540000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 54\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3385 s, 54 iter, 540000 ts, 956 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-49-21\n",
      "  done: false\n",
      "  episode_len_mean: 346.1\n",
      "  episode_reward_max: 1427.6098325521411\n",
      "  episode_reward_mean: 920.193538162321\n",
      "  episode_reward_min: -180.41745559601415\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1553\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 15421.135\n",
      "    load_time_ms: 3.165\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.684343241333517e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.6023622751235962\n",
      "      kl: 0.007021479308605194\n",
      "      policy_loss: -0.001999141648411751\n",
      "      total_loss: 1401.7830810546875\n",
      "      vf_explained_var: 0.667628288269043\n",
      "      vf_loss: 1401.7850341796875\n",
      "    sample_time_ms: 64110.997\n",
      "    update_time_ms: 24.126\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 460.0967690811605\n",
      "  time_since_restore: 3466.5917496681213\n",
      "  time_this_iter_s: 81.42933201789856\n",
      "  time_total_s: 3466.5917496681213\n",
      "  timestamp: 1556660961\n",
      "  timesteps_since_restore: 550000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 55\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3466 s, 55 iter, 550000 ts, 920 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-50-42\n",
      "  done: false\n",
      "  episode_len_mean: 355.9\n",
      "  episode_reward_max: 1427.6098325521411\n",
      "  episode_reward_mean: 981.8659088915269\n",
      "  episode_reward_min: -180.41745559601415\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1579\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 16009.518\n",
      "    load_time_ms: 3.449\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.8421716206667585e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.518432855606079\n",
      "      kl: 0.003171443473547697\n",
      "      policy_loss: -0.00030561344465240836\n",
      "      total_loss: 1550.7740478515625\n",
      "      vf_explained_var: 0.6154422760009766\n",
      "      vf_loss: 1550.7744140625\n",
      "    sample_time_ms: 67618.152\n",
      "    update_time_ms: 24.364\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 490.9329544457636\n",
      "  time_since_restore: 3547.948877096176\n",
      "  time_this_iter_s: 81.35712742805481\n",
      "  time_total_s: 3547.948877096176\n",
      "  timestamp: 1556661042\n",
      "  timesteps_since_restore: 560000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 56\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3547 s, 56 iter, 560000 ts, 982 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-51-55\n",
      "  done: false\n",
      "  episode_len_mean: 362.89\n",
      "  episode_reward_max: 1402.441112400064\n",
      "  episode_reward_mean: 996.5211381778176\n",
      "  episode_reward_min: -179.56387860801928\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 1606\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 16107.111\n",
      "    load_time_ms: 3.437\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4210858103333793e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.598617434501648\n",
      "      kl: 0.005549508612602949\n",
      "      policy_loss: -0.0009478309657424688\n",
      "      total_loss: 1392.3011474609375\n",
      "      vf_explained_var: 0.6128568649291992\n",
      "      vf_loss: 1392.3021240234375\n",
      "    sample_time_ms: 69324.338\n",
      "    update_time_ms: 23.053\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 498.2605690889089\n",
      "  time_since_restore: 3620.4308190345764\n",
      "  time_this_iter_s: 72.48194193840027\n",
      "  time_total_s: 3620.4308190345764\n",
      "  timestamp: 1556661115\n",
      "  timesteps_since_restore: 570000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 57\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3620 s, 57 iter, 570000 ts, 997 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-52-50\n",
      "  done: false\n",
      "  episode_len_mean: 360.17\n",
      "  episode_reward_max: 1406.7336334070826\n",
      "  episode_reward_mean: 983.1178904287008\n",
      "  episode_reward_min: -182.86768180815815\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1636\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 14704.243\n",
      "    load_time_ms: 3.265\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.105429051666896e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.544952392578125\n",
      "      kl: 0.004361121915280819\n",
      "      policy_loss: -0.0005472954944707453\n",
      "      total_loss: 1430.9425048828125\n",
      "      vf_explained_var: 0.6923679709434509\n",
      "      vf_loss: 1430.9429931640625\n",
      "    sample_time_ms: 65786.731\n",
      "    update_time_ms: 23.176\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 491.5589452143504\n",
      "  time_since_restore: 3675.4162888526917\n",
      "  time_this_iter_s: 54.985469818115234\n",
      "  time_total_s: 3675.4162888526917\n",
      "  timestamp: 1556661170\n",
      "  timesteps_since_restore: 580000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 58\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3675 s, 58 iter, 580000 ts, 983 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-54-12\n",
      "  done: false\n",
      "  episode_len_mean: 361.43\n",
      "  episode_reward_max: 1406.7336334070826\n",
      "  episode_reward_mean: 984.9482194079526\n",
      "  episode_reward_min: -182.86768180815815\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1662\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 13430.99\n",
      "    load_time_ms: 3.354\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.552714525833448e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.5327123403549194\n",
      "      kl: 0.008331765420734882\n",
      "      policy_loss: -0.0022773428354412317\n",
      "      total_loss: 1294.5950927734375\n",
      "      vf_explained_var: 0.6388992071151733\n",
      "      vf_loss: 1294.5975341796875\n",
      "    sample_time_ms: 65583.994\n",
      "    update_time_ms: 20.005\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 492.4741097039763\n",
      "  time_since_restore: 3757.5004110336304\n",
      "  time_this_iter_s: 82.08412218093872\n",
      "  time_total_s: 3757.5004110336304\n",
      "  timestamp: 1556661252\n",
      "  timesteps_since_restore: 590000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 59\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3757 s, 59 iter, 590000 ts, 985 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-54-56\n",
      "  done: false\n",
      "  episode_len_mean: 370.08\n",
      "  episode_reward_max: 1410.6454575477906\n",
      "  episode_reward_mean: 1030.798917619179\n",
      "  episode_reward_min: -182.86768180815815\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1687\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 12777.01\n",
      "    load_time_ms: 3.227\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.776357262916724e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4356454610824585\n",
      "      kl: 0.009136793203651905\n",
      "      policy_loss: -0.0022808201611042023\n",
      "      total_loss: 1476.9310302734375\n",
      "      vf_explained_var: 0.5980162024497986\n",
      "      vf_loss: 1476.933349609375\n",
      "    sample_time_ms: 61044.384\n",
      "    update_time_ms: 19.079\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 515.3994588095895\n",
      "  time_since_restore: 3801.112770318985\n",
      "  time_this_iter_s: 43.612359285354614\n",
      "  time_total_s: 3801.112770318985\n",
      "  timestamp: 1556661296\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 60\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3801 s, 60 iter, 600000 ts, 1.03e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-55-23\n",
      "  done: false\n",
      "  episode_len_mean: 371.54\n",
      "  episode_reward_max: 1410.6454575477906\n",
      "  episode_reward_mean: 1063.1141648253533\n",
      "  episode_reward_min: -182.86768180815815\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 1714\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 11801.11\n",
      "    load_time_ms: 3.006\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.88178631458362e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4391510486602783\n",
      "      kl: 0.013606453314423561\n",
      "      policy_loss: -0.0016300154384225607\n",
      "      total_loss: 1476.13232421875\n",
      "      vf_explained_var: 0.604603111743927\n",
      "      vf_loss: 1476.134033203125\n",
      "    sample_time_ms: 58506.123\n",
      "    update_time_ms: 18.217\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 531.5570824126767\n",
      "  time_since_restore: 3827.80504155159\n",
      "  time_this_iter_s: 26.69227123260498\n",
      "  time_total_s: 3827.80504155159\n",
      "  timestamp: 1556661323\n",
      "  timesteps_since_restore: 610000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 61\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3827 s, 61 iter, 610000 ts, 1.06e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-55-49\n",
      "  done: false\n",
      "  episode_len_mean: 372.56\n",
      "  episode_reward_max: 1410.6454575477906\n",
      "  episode_reward_mean: 1068.7010112601774\n",
      "  episode_reward_min: -179.24555858073268\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1743\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 11002.418\n",
      "    load_time_ms: 2.723\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.88178631458362e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.469205617904663\n",
      "      kl: 0.00481680640950799\n",
      "      policy_loss: -0.0006907625356689095\n",
      "      total_loss: 1461.52490234375\n",
      "      vf_explained_var: 0.6039388179779053\n",
      "      vf_loss: 1461.525390625\n",
      "    sample_time_ms: 53899.772\n",
      "    update_time_ms: 17.163\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 534.3505056300886\n",
      "  time_since_restore: 3854.435443162918\n",
      "  time_this_iter_s: 26.630401611328125\n",
      "  time_total_s: 3854.435443162918\n",
      "  timestamp: 1556661349\n",
      "  timesteps_since_restore: 620000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 62\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3854 s, 62 iter, 620000 ts, 1.07e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-56-16\n",
      "  done: false\n",
      "  episode_len_mean: 374.65\n",
      "  episode_reward_max: 1410.6454575477906\n",
      "  episode_reward_mean: 1112.8203196746938\n",
      "  episode_reward_min: -179.24555858073268\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1769\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 9766.815\n",
      "    load_time_ms: 2.586\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.44089315729181e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.29700767993927\n",
      "      kl: 0.007156684063374996\n",
      "      policy_loss: -9.851205686572939e-05\n",
      "      total_loss: 1550.386962890625\n",
      "      vf_explained_var: 0.5938272476196289\n",
      "      vf_loss: 1550.38720703125\n",
      "    sample_time_ms: 50789.921\n",
      "    update_time_ms: 15.646\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 556.4101598373469\n",
      "  time_since_restore: 3881.045877456665\n",
      "  time_this_iter_s: 26.61043429374695\n",
      "  time_total_s: 3881.045877456665\n",
      "  timestamp: 1556661376\n",
      "  timesteps_since_restore: 630000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 63\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3881 s, 63 iter, 630000 ts, 1.11e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-56-43\n",
      "  done: false\n",
      "  episode_len_mean: 361.86\n",
      "  episode_reward_max: 1406.728079330447\n",
      "  episode_reward_mean: 1080.610295863849\n",
      "  episode_reward_min: -179.24555858073268\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1797\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 8432.991\n",
      "    load_time_ms: 2.392\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.220446578645905e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3471988439559937\n",
      "      kl: 0.01006647851318121\n",
      "      policy_loss: -0.002274179831147194\n",
      "      total_loss: 1477.951171875\n",
      "      vf_explained_var: 0.6675959825515747\n",
      "      vf_loss: 1477.953369140625\n",
      "    sample_time_ms: 43790.244\n",
      "    update_time_ms: 12.841\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 540.3051479319246\n",
      "  time_since_restore: 3907.839260339737\n",
      "  time_this_iter_s: 26.7933828830719\n",
      "  time_total_s: 3907.839260339737\n",
      "  timestamp: 1556661403\n",
      "  timesteps_since_restore: 640000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 64\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3907 s, 64 iter, 640000 ts, 1.08e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-57-09\n",
      "  done: false\n",
      "  episode_len_mean: 365.51\n",
      "  episode_reward_max: 1412.192791516768\n",
      "  episode_reward_mean: 1121.3137329365\n",
      "  episode_reward_min: -176.63339217641152\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1825\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 7493.463\n",
      "    load_time_ms: 2.243\n",
      "    num_steps_sampled: 650000\n",
      "    num_steps_trained: 650000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.220446578645905e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3071155548095703\n",
      "      kl: 0.008360438980162144\n",
      "      policy_loss: -0.0009285505511797965\n",
      "      total_loss: 1640.529296875\n",
      "      vf_explained_var: 0.5888267159461975\n",
      "      vf_loss: 1640.5301513671875\n",
      "    sample_time_ms: 39241.67\n",
      "    update_time_ms: 10.623\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 560.6568664682501\n",
      "  time_since_restore: 3934.3412342071533\n",
      "  time_this_iter_s: 26.501973867416382\n",
      "  time_total_s: 3934.3412342071533\n",
      "  timestamp: 1556661429\n",
      "  timesteps_since_restore: 650000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 650000\n",
      "  training_iteration: 65\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3934 s, 65 iter, 650000 ts, 1.12e+03 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-57-36\n",
      "  done: false\n",
      "  episode_len_mean: 342.15\n",
      "  episode_reward_max: 1412.192791516768\n",
      "  episode_reward_mean: 1032.0866740939186\n",
      "  episode_reward_min: -180.75067701806694\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1858\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6646.288\n",
      "    load_time_ms: 1.907\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1102232893229526e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3602583408355713\n",
      "      kl: 0.008110272698104382\n",
      "      policy_loss: -0.001963422866538167\n",
      "      total_loss: 1646.193359375\n",
      "      vf_explained_var: 0.7004605531692505\n",
      "      vf_loss: 1646.1954345703125\n",
      "    sample_time_ms: 34594.866\n",
      "    update_time_ms: 9.689\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 516.0433370469593\n",
      "  time_since_restore: 3960.724817752838\n",
      "  time_this_iter_s: 26.383583545684814\n",
      "  time_total_s: 3960.724817752838\n",
      "  timestamp: 1556661456\n",
      "  timesteps_since_restore: 660000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 66\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3960 s, 66 iter, 660000 ts, 1.03e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-58-08\n",
      "  done: false\n",
      "  episode_len_mean: 325.79\n",
      "  episode_reward_max: 1412.192791516768\n",
      "  episode_reward_mean: 953.9108387780685\n",
      "  episode_reward_min: -180.75067701806694\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 1889\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6263.459\n",
      "    load_time_ms: 1.87\n",
      "    num_steps_sampled: 670000\n",
      "    num_steps_trained: 670000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.551116446614763e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.395811676979065\n",
      "      kl: 0.00825220812112093\n",
      "      policy_loss: -0.0014590034261345863\n",
      "      total_loss: 1635.135986328125\n",
      "      vf_explained_var: 0.7226956486701965\n",
      "      vf_loss: 1635.137451171875\n",
      "    sample_time_ms: 30965.912\n",
      "    update_time_ms: 8.972\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 476.9554193890342\n",
      "  time_since_restore: 3993.0671479701996\n",
      "  time_this_iter_s: 32.34233021736145\n",
      "  time_total_s: 3993.0671479701996\n",
      "  timestamp: 1556661488\n",
      "  timesteps_since_restore: 670000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 670000\n",
      "  training_iteration: 67\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 3993 s, 67 iter, 670000 ts, 954 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-58-46\n",
      "  done: false\n",
      "  episode_len_mean: 327.65\n",
      "  episode_reward_max: 1410.9050311192636\n",
      "  episode_reward_mean: 944.8026946058137\n",
      "  episode_reward_min: -183.31346503170647\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1918\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 6009.335\n",
      "    load_time_ms: 1.791\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.7755582233073814e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3890827894210815\n",
      "      kl: 0.008227762766182423\n",
      "      policy_loss: -0.001946984906680882\n",
      "      total_loss: 1511.7474365234375\n",
      "      vf_explained_var: 0.6827231049537659\n",
      "      vf_loss: 1511.7493896484375\n",
      "    sample_time_ms: 29516.606\n",
      "    update_time_ms: 8.585\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 472.401347302907\n",
      "  time_since_restore: 4031.002851963043\n",
      "  time_this_iter_s: 37.93570399284363\n",
      "  time_total_s: 4031.002851963043\n",
      "  timestamp: 1556661526\n",
      "  timesteps_since_restore: 680000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 68\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 4031 s, 68 iter, 680000 ts, 945 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-04-30_23-59-31\n",
      "  done: false\n",
      "  episode_len_mean: 327.38\n",
      "  episode_reward_max: 1422.601035267312\n",
      "  episode_reward_mean: 935.9656076486249\n",
      "  episode_reward_min: -183.31346503170647\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1947\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5387.679\n",
      "    load_time_ms: 1.616\n",
      "    num_steps_sampled: 690000\n",
      "    num_steps_trained: 690000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3877791116536907e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4198483228683472\n",
      "      kl: 0.008834907785058022\n",
      "      policy_loss: -0.0010242288699373603\n",
      "      total_loss: 1574.1878662109375\n",
      "      vf_explained_var: 0.6493126749992371\n",
      "      vf_loss: 1574.1888427734375\n",
      "    sample_time_ms: 26435.519\n",
      "    update_time_ms: 8.705\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 467.98280382431227\n",
      "  time_since_restore: 4076.040870666504\n",
      "  time_this_iter_s: 45.03801870346069\n",
      "  time_total_s: 4076.040870666504\n",
      "  timestamp: 1556661571\n",
      "  timesteps_since_restore: 690000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 690000\n",
      "  training_iteration: 69\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 4076 s, 69 iter, 690000 ts, 936 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_00-00-15\n",
      "  done: false\n",
      "  episode_len_mean: 348.55\n",
      "  episode_reward_max: 1422.601035267312\n",
      "  episode_reward_mean: 977.0717223192968\n",
      "  episode_reward_min: -183.31346503170647\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 1974\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5457.395\n",
      "    load_time_ms: 1.624\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.9388955582684535e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4523855447769165\n",
      "      kl: 0.007319864351302385\n",
      "      policy_loss: -0.00039892434142529964\n",
      "      total_loss: 1345.567138671875\n",
      "      vf_explained_var: 0.7043105959892273\n",
      "      vf_loss: 1345.5673828125\n",
      "    sample_time_ms: 26350.422\n",
      "    update_time_ms: 8.012\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 488.5358611596483\n",
      "  time_since_restore: 4119.4960017204285\n",
      "  time_this_iter_s: 43.45513105392456\n",
      "  time_total_s: 4119.4960017204285\n",
      "  timestamp: 1556661615\n",
      "  timesteps_since_restore: 700000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 70\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 4119 s, 70 iter, 700000 ts, 977 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_00-00-45\n",
      "  done: false\n",
      "  episode_len_mean: 354.59\n",
      "  episode_reward_max: 1422.601035267312\n",
      "  episode_reward_mean: 972.0166743015643\n",
      "  episode_reward_min: -182.64855334205726\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 2003\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5638.437\n",
      "    load_time_ms: 1.623\n",
      "    num_steps_sampled: 710000\n",
      "    num_steps_trained: 710000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.4694477791342267e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.5245211124420166\n",
      "      kl: 0.010665715672075748\n",
      "      policy_loss: -0.0018647379474714398\n",
      "      total_loss: 1308.5731201171875\n",
      "      vf_explained_var: 0.6759455800056458\n",
      "      vf_loss: 1308.5750732421875\n",
      "    sample_time_ms: 26515.596\n",
      "    update_time_ms: 8.158\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 486.0083371507821\n",
      "  time_since_restore: 4149.652543067932\n",
      "  time_this_iter_s: 30.156541347503662\n",
      "  time_total_s: 4149.652543067932\n",
      "  timestamp: 1556661645\n",
      "  timesteps_since_restore: 710000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 710000\n",
      "  training_iteration: 71\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 4149 s, 71 iter, 710000 ts, 972 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_00-01-12\n",
      "  done: false\n",
      "  episode_len_mean: 365.67\n",
      "  episode_reward_max: 1422.601035267312\n",
      "  episode_reward_mean: 992.7474310816921\n",
      "  episode_reward_min: -182.64855334205726\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2029\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5638.951\n",
      "    load_time_ms: 1.615\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.4694477791342267e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4356529712677002\n",
      "      kl: 0.007350943051278591\n",
      "      policy_loss: -0.000935095245949924\n",
      "      total_loss: 1197.7353515625\n",
      "      vf_explained_var: 0.6988336443901062\n",
      "      vf_loss: 1197.736328125\n",
      "    sample_time_ms: 26523.339\n",
      "    update_time_ms: 8.386\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 496.37371554084586\n",
      "  time_since_restore: 4176.369470357895\n",
      "  time_this_iter_s: 26.71692728996277\n",
      "  time_total_s: 4176.369470357895\n",
      "  timestamp: 1556661672\n",
      "  timesteps_since_restore: 720000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 72\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 4176 s, 72 iter, 720000 ts, 993 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_00-01-39\n",
      "  done: false\n",
      "  episode_len_mean: 371.76\n",
      "  episode_reward_max: 1412.5022145087707\n",
      "  episode_reward_mean: 1012.3160674221665\n",
      "  episode_reward_min: -179.38606406486034\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2056\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5640.473\n",
      "    load_time_ms: 1.609\n",
      "    num_steps_sampled: 730000\n",
      "    num_steps_trained: 730000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7347238895671134e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3422391414642334\n",
      "      kl: 0.015108151361346245\n",
      "      policy_loss: -0.0024940387811511755\n",
      "      total_loss: 1273.939208984375\n",
      "      vf_explained_var: 0.7481684684753418\n",
      "      vf_loss: 1273.9417724609375\n",
      "    sample_time_ms: 26542.56\n",
      "    update_time_ms: 8.341\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 506.1580337110832\n",
      "  time_since_restore: 4203.186446428299\n",
      "  time_this_iter_s: 26.816976070404053\n",
      "  time_total_s: 4203.186446428299\n",
      "  timestamp: 1556661699\n",
      "  timesteps_since_restore: 730000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 730000\n",
      "  training_iteration: 73\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 4203 s, 73 iter, 730000 ts, 1.01e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_00-02-06\n",
      "  done: false\n",
      "  episode_len_mean: 372.45\n",
      "  episode_reward_max: 1418.3938413963856\n",
      "  episode_reward_mean: 1076.4266849065477\n",
      "  episode_reward_min: -179.38606406486034\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2083\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5661.67\n",
      "    load_time_ms: 1.631\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.7347238895671134e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2243220806121826\n",
      "      kl: 0.007010265719145536\n",
      "      policy_loss: -0.0007137669599615037\n",
      "      total_loss: 1379.441162109375\n",
      "      vf_explained_var: 0.6822547316551208\n",
      "      vf_loss: 1379.44189453125\n",
      "    sample_time_ms: 26570.152\n",
      "    update_time_ms: 8.713\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 538.213342453274\n",
      "  time_since_restore: 4230.472578287125\n",
      "  time_this_iter_s: 27.286131858825684\n",
      "  time_total_s: 4230.472578287125\n",
      "  timestamp: 1556661726\n",
      "  timesteps_since_restore: 740000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 74\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 4230 s, 74 iter, 740000 ts, 1.08e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_00-02-33\n",
      "  done: false\n",
      "  episode_len_mean: 367.72\n",
      "  episode_reward_max: 1418.3938413963856\n",
      "  episode_reward_mean: 1087.2572342628093\n",
      "  episode_reward_min: -179.38606406486034\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2110\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5675.329\n",
      "    load_time_ms: 1.634\n",
      "    num_steps_sampled: 750000\n",
      "    num_steps_trained: 750000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.673619447835567e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.235304832458496\n",
      "      kl: 0.005884024780243635\n",
      "      policy_loss: -0.0006086711655370891\n",
      "      total_loss: 1379.8990478515625\n",
      "      vf_explained_var: 0.7302163243293762\n",
      "      vf_loss: 1379.8995361328125\n",
      "    sample_time_ms: 26610.603\n",
      "    update_time_ms: 8.094\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 543.6286171314048\n",
      "  time_since_restore: 4257.508039712906\n",
      "  time_this_iter_s: 27.03546142578125\n",
      "  time_total_s: 4257.508039712906\n",
      "  timestamp: 1556661753\n",
      "  timesteps_since_restore: 750000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 750000\n",
      "  training_iteration: 75\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 4257 s, 75 iter, 750000 ts, 1.09e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_00-03-00\n",
      "  done: false\n",
      "  episode_len_mean: 353.08\n",
      "  episode_reward_max: 1418.3938413963856\n",
      "  episode_reward_mean: 1054.416796544462\n",
      "  episode_reward_min: -179.30739615579762\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 2142\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5686.964\n",
      "    load_time_ms: 1.627\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.3368097239177834e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2952773571014404\n",
      "      kl: 0.008466260507702827\n",
      "      policy_loss: -0.0012868401827290654\n",
      "      total_loss: 1391.2408447265625\n",
      "      vf_explained_var: 0.7050749659538269\n",
      "      vf_loss: 1391.241943359375\n",
      "    sample_time_ms: 26685.521\n",
      "    update_time_ms: 8.186\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 527.2083982722309\n",
      "  time_since_restore: 4284.75822353363\n",
      "  time_this_iter_s: 27.250183820724487\n",
      "  time_total_s: 4284.75822353363\n",
      "  timestamp: 1556661780\n",
      "  timesteps_since_restore: 760000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 76\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 4284 s, 76 iter, 760000 ts, 1.05e+03 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_00-03-32\n",
      "  done: false\n",
      "  episode_len_mean: 343.55\n",
      "  episode_reward_max: 1415.7198725960566\n",
      "  episode_reward_mean: 1038.4177213681658\n",
      "  episode_reward_min: -184.143513019882\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 2172\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5288.401\n",
      "    load_time_ms: 1.552\n",
      "    num_steps_sampled: 770000\n",
      "    num_steps_trained: 770000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.1684048619588917e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.164542555809021\n",
      "      kl: 0.007867121137678623\n",
      "      policy_loss: -0.000619006110355258\n",
      "      total_loss: 1556.715087890625\n",
      "      vf_explained_var: 0.7087563872337341\n",
      "      vf_loss: 1556.7156982421875\n",
      "    sample_time_ms: 26993.806\n",
      "    update_time_ms: 8.761\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 519.2088606840829\n",
      "  time_since_restore: 4316.195572853088\n",
      "  time_this_iter_s: 31.437349319458008\n",
      "  time_total_s: 4316.195572853088\n",
      "  timestamp: 1556661812\n",
      "  timesteps_since_restore: 770000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 770000\n",
      "  training_iteration: 77\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 4316 s, 77 iter, 770000 ts, 1.04e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_00-04-01\n",
      "  done: false\n",
      "  episode_len_mean: 340.13\n",
      "  episode_reward_max: 1415.7198725960566\n",
      "  episode_reward_mean: 1020.42312130387\n",
      "  episode_reward_min: -184.143513019882\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2199\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5186.217\n",
      "    load_time_ms: 1.546\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842024309794459e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2488796710968018\n",
      "      kl: 0.010927407070994377\n",
      "      policy_loss: -0.0021548864897340536\n",
      "      total_loss: 1350.598388671875\n",
      "      vf_explained_var: 0.7051394581794739\n",
      "      vf_loss: 1350.600341796875\n",
      "    sample_time_ms: 26230.484\n",
      "    update_time_ms: 8.425\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 510.21156065193514\n",
      "  time_since_restore: 4345.469700336456\n",
      "  time_this_iter_s: 29.27412748336792\n",
      "  time_total_s: 4345.469700336456\n",
      "  timestamp: 1556661841\n",
      "  timesteps_since_restore: 780000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 78\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 4345 s, 78 iter, 780000 ts, 1.02e+03 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-01_00-04-42\n",
      "  done: false\n",
      "  episode_len_mean: 351.1\n",
      "  episode_reward_max: 1415.7198725960566\n",
      "  episode_reward_mean: 1045.6474450517376\n",
      "  episode_reward_min: -187.67233580209972\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2226\n",
      "  experiment_id: 9b6f4aa7d04949df9d58e63adfb6a4fc\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 5331.097\n",
      "    load_time_ms: 1.543\n",
      "    num_steps_sampled: 790000\n",
      "    num_steps_trained: 790000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842024309794459e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2682387828826904\n",
      "      kl: 0.011407717131078243\n",
      "      policy_loss: -0.001342038274742663\n",
      "      total_loss: 1257.6953125\n",
      "      vf_explained_var: 0.7082810401916504\n",
      "      vf_loss: 1257.6966552734375\n",
      "    sample_time_ms: 25716.005\n",
      "    update_time_ms: 7.938\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 23730\n",
      "  policy_reward_mean:\n",
      "    rl_0: 522.8237225258688\n",
      "  time_since_restore: 4386.810272216797\n",
      "  time_this_iter_s: 41.340571880340576\n",
      "  time_total_s: 4386.810272216797\n",
      "  timestamp: 1556661882\n",
      "  timesteps_since_restore: 790000\n",
      "  timesteps_this_iter: 10000\n",
      "  timesteps_total: 790000\n",
      "  training_iteration: 79\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 13.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_2veh-v0_0:\tRUNNING [pid=23730], 4386 s, 79 iter, 790000 ts, 1.05e+03 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,  # RL algorithm to run\n",
    "        \"env\": gym_name,  # environment name generated earlier\n",
    "        \"config\": {  # configuration params (must match \"run\" value)\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1000,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow_2)",
   "language": "python",
   "name": "flow_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
