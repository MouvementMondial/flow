{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING I3W\n",
    "\n",
    "\n",
    "# A) Create Envorinment, Vehicles etc\n",
    "\n",
    "### General Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scenarios:\n",
      "['Scenario', 'BayBridgeScenario', 'BayBridgeTollScenario', 'BottleneckScenario', 'Figure8Scenario', 'SimpleGridScenario', 'HighwayScenario', 'LoopScenario', 'MergeScenario', 'TwoLoopsOneMergingScenario', 'MultiLoopScenario', 'IntersectionScenarioTW', 'TenaciousDScenario', 'IntersectionTWScenario_2']\n",
      "\n",
      "Available environments:\n",
      "['MultiEnv', 'MultiAgentAccelEnv', 'MultiWaveAttenuationPOEnv', 'MultiAgentIntersectionEnv', 'MultiAgentTeamSpiritIntersectionEnv', 'MultiAgentIntersectionEnv_baseline_1', 'MultiAgentIntersectionEnv_baseline_2', 'MultiAgentIntersectionEnv_baseline_3', 'MultiAgentIntersectionEnv_sharedPolicy_TeamSpirit', 'MultiTenaciousDEnv', 'MultiAgentIntersectionEnv_sharedPolicy_2veh', 'MultiAgentIntersectionEnv_sharedPolicy_4veh']\n"
     ]
    }
   ],
   "source": [
    "# Define horizon as a variable to ensure consistent use across notebook (length of one rollout)\n",
    "HORIZON=120                             #103 max Horizon, wenn es vor verlassen abbrechen soll!, default war 500\n",
    "\n",
    "# name of the experiment\n",
    "experiment_name = \"IntersectionExample\"\n",
    "\n",
    "# scenario class\n",
    "import flow.scenarios as scenarios\n",
    "print(\"Available scenarios:\")\n",
    "print(scenarios.__all__)\n",
    "scenario_name = \"IntersectionTWScenario_2\"\n",
    "\n",
    "# environment class\n",
    "import flow.multiagent_envs as flowenvs\n",
    "print(\"\\nAvailable environments:\")\n",
    "print(flowenvs.__all__)\n",
    "env_name = \"MultiAgentIntersectionEnv_sharedPolicy_4veh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "from flow.scenarios.intersection import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "additionalNetParams = {\n",
    "            \"edge_length\": 80,\n",
    "            \"lanes\": 1,\n",
    "            \"speed_limit\": 30\n",
    "        }\n",
    "\n",
    "net_params = NetParams( no_internal_links=False,                  #default: True   !! damit Kreuzungen nicht Ã¼berspr. werden\n",
    "                        inflows=None,                             #default: None\n",
    "                        osm_path=None,                            #default: None\n",
    "                        netfile=None,                             #default: None\n",
    "                        additional_params=additionalNetParams     #default: None   !!\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InitialConfig Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "\n",
    "initial_config = InitialConfig( shuffle=True,                            #default: False         !!\n",
    "                                spacing=\"custom\",                        #default: \"uniform\"     !!\n",
    "                                min_gap=0,                               #default: 0\n",
    "                                perturbation=29.99,                      #default: 0.0            !!        \n",
    "                                x0=0,                                    #default: 0\n",
    "                                bunching=0,                              #default: 0\n",
    "                                lanes_distribution=float(\"inf\"),         #default: float(\"inf\")\n",
    "                                edges_distribution=\"all\",                #default: \"all\"\n",
    "                                additional_params=None )                 #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMO Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams( port = None,                  #default: None\n",
    "                          sim_step=0.1,                 #default: 0.1\n",
    "                          emission_path=None,           #default: None\n",
    "                          lateral_resolution=None,      #default: None\n",
    "                          no_step_log=True,             #default: True\n",
    "                          render=False,                 #default: False\n",
    "                          save_render=False,            #default: False\n",
    "                          sight_radius=25,              #default: 25\n",
    "                          show_radius=False,            #default: False\n",
    "                          pxpm=2,                       #default: 2\n",
    "                          overtake_right=False,         #default: False    \n",
    "                          seed=None,                    #default: None\n",
    "                          restart_instance=False,       #default: False\n",
    "                          print_warnings=True,          #default: True\n",
    "                          teleport_time=-1,             #default: -1\n",
    "                          num_clients=1,                #default: 1\n",
    "                          sumo_binary=None )            #default: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "additionalEnvParams = {\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 3,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 3,\n",
    "        \"target_velocity\": 30\n",
    "    }\n",
    "\n",
    "env_params = EnvParams( additional_params=additionalEnvParams, #default: None    !!\n",
    "                        horizon=HORIZON,                       #default: 500     !!\n",
    "                        warmup_steps=0,                        #default: 0       \n",
    "                        sims_per_step=1,                       #default: 1\n",
    "                        evaluate=False )                       #default: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicles Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import VehicleParams\n",
    "\n",
    "# import vehicles dynamics models\n",
    "#from flow.controllers import SumoCarFollowingController\n",
    "from flow.controllers import ContinuousRouter\n",
    "#from flow.controllers.lane_change_controllers import SumoLaneChangeController\n",
    "from flow.controllers.lane_change_controllers import StaticLaneChanger\n",
    "from flow.controllers import RLController\n",
    "from flow.core.params import SumoLaneChangeParams\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "from random import *\n",
    "\n",
    "vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RL-Agent controlled vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car following parameters, default: None\n",
    "cf_parameter = SumoCarFollowingParams(\n",
    "                speed_mode=\"aggressive\")\n",
    "# lane change parameters, default: None\n",
    "lc_parameter =  None\n",
    "\n",
    "vehicles.add( # name of the vehicle\n",
    "                veh_id = \"rl\",\n",
    "              # acceleration controller, default: (SumoCarFollowingController, {})\n",
    "                acceleration_controller=(RLController, {}),\n",
    "              # lane_change_controller, default: (SumoLaneChangeController, {})\n",
    "                lane_change_controller=(StaticLaneChanger,{}),\n",
    "              # routing controller, default: None\n",
    "                routing_controller=(ContinuousRouter, {}),\n",
    "              # initial speed, default: 0\n",
    "                initial_speed=0,\n",
    "              # number of vehicles, default: 1 \n",
    "                num_vehicles=4,\n",
    "                \n",
    "                car_following_params=cf_parameter\n",
    "              # speed mode, default: \"right_of_way\"\n",
    "                #speed_mode=\"aggressive\",\n",
    "              # lane change mode, default: \"no_lat_collide\"\n",
    "                #lane_change_mode=\"aggressive\", \n",
    "              # car following parameter, default: None\n",
    "                #sumo_car_following_params=cf_parameter,\n",
    "              # lane change parameter, default: None\n",
    "                #sumo_lc_params=lc_parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict( # name of the experiment\n",
    "                      exp_tag=experiment_name,\n",
    "                    # name of the flow environment the experiment is running on\n",
    "                      env_name=env_name,\n",
    "                    # name of the scenario class the experiment uses\n",
    "                      scenario=scenario_name,\n",
    "                    # simulator that is used by the experiment\n",
    "                      simulator='traci',\n",
    "                    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "                      sim=sumo_params,\n",
    "                    # environment related parameters (see flow.core.params.EnvParams)\n",
    "                      env=env_params,\n",
    "                    # network-related parameters (see flow.core.params.NetParams and\n",
    "                    # the scenario's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "                      net=net_params,\n",
    "                    # vehicles to be placed in the network at the start of a rollout \n",
    "                    # (see flow.core.vehicles.Vehicles)\n",
    "                      veh=vehicles,\n",
    "                   # (optional) parameters affecting the positioning of vehicles upon \n",
    "                   # initialization/reset (see flow.core.params.InitialConfig)\n",
    "                      initial=initial_config\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import PPOPolicyGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-05-02_10-37-15_29615/logs.\n",
      "Waiting for redis server at 127.0.0.1:45900 to respond...\n",
      "Waiting for redis server at 127.0.0.1:56386 to respond...\n",
      "Starting the Plasma object store with 6.554658406 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8891/notebooks/ray_ui.ipynb?token=531b4b319102dd18351d8d87b240733bacbc0a59861293a8\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.16.123.117',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-05-02_10-37-15_29615/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-05-02_10-37-15_29615/sockets/raylet'],\n",
       " 'redis_address': '172.16.123.117:45900',\n",
       " 'webui_url': 'http://localhost:8891/notebooks/ray_ui.ipynb?token=531b4b319102dd18351d8d87b240733bacbc0a59861293a8'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 200\n",
    "\n",
    "ray.init(redirect_output=True, num_cpus=N_CPUS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate default 0.999\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [100, 50, 25]})  # size of hidden layers in network defaule 64 32\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "#config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "#config[\"sample_batch_size\"] = config[\"train_batch_size\"]/config[\"num_workers\"] # 200 default, trotzdem zu hoch?\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 10  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_paramshttps://www.tourdatenarchiv.de/setlist/71/07/Es-wird-eng/Frankfurt-Main-Festhalle/\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Starting SUMO on port 49175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Teamspirit:\n",
      "0.04880195980933988\n",
      "-0.5698350356183577\n",
      "[('bottom_intersection', 33.435266406731216), ('bottom_intersection', 49.43253509925844), ('top_intersection', 13.348699041252347), ('top_intersection', 52.85898909213359)]\n"
     ]
    }
   ],
   "source": [
    "# multi agent policy mapping\n",
    "test_env = create_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "\n",
    "def gen_policy():\n",
    "    return (PPOPolicyGraph, obs_space, act_space, {})\n",
    "\n",
    "# Setup PG with an ensemble of `num_policies` different policy graphs\n",
    "policy_graphs = {'rl_0': gen_policy()}\n",
    "    \n",
    "def policy_mapping_fn(agent_id):\n",
    "    return 'rl_0'\n",
    "\n",
    "config.update({\n",
    "        'multiagent': {\n",
    "            'policy_graphs': policy_graphs,\n",
    "            'policy_mapping_fn': tune.function(policy_mapping_fn),\n",
    "            'policies_to_train': ['rl_0']\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "\n",
      "Created LogSyncer for /home/thorsten/ray_results/IntersectionExample/PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0_2019-05-02_10-37-18hbo7ufcc -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 3.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_10-39-22\n",
      "  done: false\n",
      "  episode_len_mean: 119.25\n",
      "  episode_reward_max: 44.71218454332695\n",
      "  episode_reward_mean: 18.806414594426464\n",
      "  episode_reward_min: -389.4910954789854\n",
      "  episodes_this_iter: 200\n",
      "  episodes_total: 200\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20570.786\n",
      "    load_time_ms: 46.563\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.20000001788139343\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4313675165176392\n",
      "      kl: 0.006550817284733057\n",
      "      policy_loss: -0.004031199961900711\n",
      "      total_loss: 19.216630935668945\n",
      "      vf_explained_var: 0.09772299975156784\n",
      "      vf_loss: 19.219350814819336\n",
      "    sample_time_ms: 77243.546\n",
      "    update_time_ms: 576.732\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 4.701603648606616\n",
      "  time_since_restore: 98.5531177520752\n",
      "  time_this_iter_s: 98.5531177520752\n",
      "  time_total_s: 98.5531177520752\n",
      "  timestamp: 1556786362\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 1\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 98 s, 1 iter, 24000 ts, 18.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_10-41-01\n",
      "  done: false\n",
      "  episode_len_mean: 119.90049751243781\n",
      "  episode_reward_max: 86.75144015404584\n",
      "  episode_reward_mean: 37.38329625282299\n",
      "  episode_reward_min: -351.2644357057337\n",
      "  episodes_this_iter: 201\n",
      "  episodes_total: 401\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20892.332\n",
      "    load_time_ms: 25.089\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.10000000894069672\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4226890802383423\n",
      "      kl: 0.015282864682376385\n",
      "      policy_loss: -0.007249431684613228\n",
      "      total_loss: 20.68318748474121\n",
      "      vf_explained_var: 0.17646396160125732\n",
      "      vf_loss: 20.688907623291016\n",
      "    sample_time_ms: 77193.771\n",
      "    update_time_ms: 291.72\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 9.345824063205747\n",
      "  time_since_restore: 196.99275183677673\n",
      "  time_this_iter_s: 98.43963408470154\n",
      "  time_total_s: 196.99275183677673\n",
      "  timestamp: 1556786461\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 2\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 196 s, 2 iter, 48000 ts, 37.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_10-42-33\n",
      "  done: false\n",
      "  episode_len_mean: 117.85714285714286\n",
      "  episode_reward_max: 107.02966452531373\n",
      "  episode_reward_mean: 15.38666872429703\n",
      "  episode_reward_min: -355.45770824230215\n",
      "  episodes_this_iter: 203\n",
      "  episodes_total: 604\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20677.375\n",
      "    load_time_ms: 17.992\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.10000000894069672\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4198055267333984\n",
      "      kl: 0.0075709763914346695\n",
      "      policy_loss: -0.0029365336522459984\n",
      "      total_loss: 189.88587951660156\n",
      "      vf_explained_var: 0.11227177083492279\n",
      "      vf_loss: 189.88804626464844\n",
      "    sample_time_ms: 75390.594\n",
      "    update_time_ms: 196.258\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 3.8466671810742556\n",
      "  time_since_restore: 289.1066634654999\n",
      "  time_this_iter_s: 92.11391162872314\n",
      "  time_total_s: 289.1066634654999\n",
      "  timestamp: 1556786553\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 3\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 289 s, 3 iter, 72000 ts, 15.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_10-44-05\n",
      "  done: false\n",
      "  episode_len_mean: 114.92344497607655\n",
      "  episode_reward_max: 159.9892005770725\n",
      "  episode_reward_mean: 10.513952997363399\n",
      "  episode_reward_min: -358.64608952022536\n",
      "  episodes_this_iter: 209\n",
      "  episodes_total: 813\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20297.24\n",
      "    load_time_ms: 14.433\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.05000000447034836\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4119704961776733\n",
      "      kl: 0.00824650563299656\n",
      "      policy_loss: -0.0019970813300460577\n",
      "      total_loss: 281.1214599609375\n",
      "      vf_explained_var: 0.141172394156456\n",
      "      vf_loss: 281.123046875\n",
      "    sample_time_ms: 74883.747\n",
      "    update_time_ms: 148.426\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 2.6284882493408515\n",
      "  time_since_restore: 381.7015480995178\n",
      "  time_this_iter_s: 92.59488463401794\n",
      "  time_total_s: 381.7015480995178\n",
      "  timestamp: 1556786645\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 4\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 381 s, 4 iter, 96000 ts, 10.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_10-45-35\n",
      "  done: false\n",
      "  episode_len_mean: 109.13181818181818\n",
      "  episode_reward_max: 175.24345459641194\n",
      "  episode_reward_mean: -62.81750040042199\n",
      "  episode_reward_min: -359.7939574986881\n",
      "  episodes_this_iter: 220\n",
      "  episodes_total: 1033\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20502.145\n",
      "    load_time_ms: 12.502\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.02500000223517418\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4037545919418335\n",
      "      kl: 0.0058250389993190765\n",
      "      policy_loss: -0.0015476375119760633\n",
      "      total_loss: 495.6060791015625\n",
      "      vf_explained_var: 0.19759050011634827\n",
      "      vf_loss: 495.6074523925781\n",
      "    sample_time_ms: 73440.145\n",
      "    update_time_ms: 120.64\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -15.704375100105496\n",
      "  time_since_restore: 470.77304673194885\n",
      "  time_this_iter_s: 89.07149863243103\n",
      "  time_total_s: 470.77304673194885\n",
      "  timestamp: 1556786735\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 5\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 470 s, 5 iter, 120000 ts, -62.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_10-47-05\n",
      "  done: false\n",
      "  episode_len_mean: 106.5911111111111\n",
      "  episode_reward_max: 210.9356255193253\n",
      "  episode_reward_mean: -46.558701744159364\n",
      "  episode_reward_min: -350.2312174305498\n",
      "  episodes_this_iter: 225\n",
      "  episodes_total: 1258\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20545.255\n",
      "    load_time_ms: 11.05\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.01250000111758709\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3857861757278442\n",
      "      kl: 0.0058530461974442005\n",
      "      policy_loss: -0.0009943736949935555\n",
      "      total_loss: 507.4070129394531\n",
      "      vf_explained_var: 0.19912223517894745\n",
      "      vf_loss: 507.4079284667969\n",
      "    sample_time_ms: 72777.572\n",
      "    update_time_ms: 101.504\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -11.639675436039841\n",
      "  time_since_restore: 561.0721478462219\n",
      "  time_this_iter_s: 90.29910111427307\n",
      "  time_total_s: 561.0721478462219\n",
      "  timestamp: 1556786825\n",
      "  timesteps_since_restore: 144000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 6\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 561 s, 6 iter, 144000 ts, -46.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_10-48-45\n",
      "  done: false\n",
      "  episode_len_mean: 106.84\n",
      "  episode_reward_max: 249.20691426367344\n",
      "  episode_reward_mean: -7.68814775072689\n",
      "  episode_reward_min: -362.8283980007802\n",
      "  episodes_this_iter: 225\n",
      "  episodes_total: 1483\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20828.423\n",
      "    load_time_ms: 10.162\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.006250000558793545\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3797945976257324\n",
      "      kl: 0.007484192494302988\n",
      "      policy_loss: -0.0016921062488108873\n",
      "      total_loss: 427.5444030761719\n",
      "      vf_explained_var: 0.24121159315109253\n",
      "      vf_loss: 427.54608154296875\n",
      "    sample_time_ms: 73442.537\n",
      "    update_time_ms: 87.694\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -1.922036937681722\n",
      "  time_since_restore: 661.1187000274658\n",
      "  time_this_iter_s: 100.0465521812439\n",
      "  time_total_s: 661.1187000274658\n",
      "  timestamp: 1556786925\n",
      "  timesteps_since_restore: 168000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 7\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 661 s, 7 iter, 168000 ts, -7.69 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_10-50-20\n",
      "  done: false\n",
      "  episode_len_mean: 101.6936170212766\n",
      "  episode_reward_max: 245.0613852845741\n",
      "  episode_reward_mean: -37.01675881631484\n",
      "  episode_reward_min: -349.9928016284143\n",
      "  episodes_this_iter: 235\n",
      "  episodes_total: 1718\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20690.542\n",
      "    load_time_ms: 9.363\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0031250002793967724\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.375068187713623\n",
      "      kl: 0.007489996496587992\n",
      "      policy_loss: -0.0020886650308966637\n",
      "      total_loss: 476.6497497558594\n",
      "      vf_explained_var: 0.30654358863830566\n",
      "      vf_loss: 476.6518249511719\n",
      "    sample_time_ms: 73634.207\n",
      "    update_time_ms: 77.725\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -9.254189704078712\n",
      "  time_since_restore: 755.8986339569092\n",
      "  time_this_iter_s: 94.77993392944336\n",
      "  time_total_s: 755.8986339569092\n",
      "  timestamp: 1556787020\n",
      "  timesteps_since_restore: 192000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 8\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 755 s, 8 iter, 192000 ts, -37 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_10-52-06\n",
      "  done: false\n",
      "  episode_len_mean: 98.51836734693877\n",
      "  episode_reward_max: 274.98655211402735\n",
      "  episode_reward_mean: -27.282884440621665\n",
      "  episode_reward_min: -349.9778275585245\n",
      "  episodes_this_iter: 245\n",
      "  episodes_total: 1963\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 21495.209\n",
      "    load_time_ms: 8.875\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0015625001396983862\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3693767786026\n",
      "      kl: 0.005995715968310833\n",
      "      policy_loss: -0.0013262074207887053\n",
      "      total_loss: 495.4955749511719\n",
      "      vf_explained_var: 0.4156979024410248\n",
      "      vf_loss: 495.4969177246094\n",
      "    sample_time_ms: 74173.605\n",
      "    update_time_ms: 69.745\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -6.820721110155418\n",
      "  time_since_restore: 862.4114742279053\n",
      "  time_this_iter_s: 106.5128402709961\n",
      "  time_total_s: 862.4114742279053\n",
      "  timestamp: 1556787126\n",
      "  timesteps_since_restore: 216000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 9\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 862 s, 9 iter, 216000 ts, -27.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_10-54-19\n",
      "  done: false\n",
      "  episode_len_mean: 95.548\n",
      "  episode_reward_max: 289.4156449027876\n",
      "  episode_reward_mean: -58.63292202902708\n",
      "  episode_reward_min: -352.9445987259061\n",
      "  episodes_this_iter: 250\n",
      "  episodes_total: 2213\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 22503.412\n",
      "    load_time_ms: 8.373\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.0007812500698491931\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3618906736373901\n",
      "      kl: 0.006178838200867176\n",
      "      policy_loss: -0.0013800818705931306\n",
      "      total_loss: 594.3493041992188\n",
      "      vf_explained_var: 0.4203738868236542\n",
      "      vf_loss: 594.3506469726562\n",
      "    sample_time_ms: 76807.819\n",
      "    update_time_ms: 63.457\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -14.65823050725677\n",
      "  time_since_restore: 994.6167252063751\n",
      "  time_this_iter_s: 132.20525097846985\n",
      "  time_total_s: 994.6167252063751\n",
      "  timestamp: 1556787259\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 10\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 994 s, 10 iter, 240000 ts, -58.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_10-56-40\n",
      "  done: false\n",
      "  episode_len_mean: 95.95617529880478\n",
      "  episode_reward_max: 306.7063225335668\n",
      "  episode_reward_mean: -59.62094419283321\n",
      "  episode_reward_min: -357.6593962816094\n",
      "  episodes_this_iter: 251\n",
      "  episodes_total: 2464\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 23900.515\n",
      "    load_time_ms: 4.205\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00039062503492459655\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.352575659751892\n",
      "      kl: 0.00715090986341238\n",
      "      policy_loss: -0.0019051320850849152\n",
      "      total_loss: 589.0093994140625\n",
      "      vf_explained_var: 0.5111197829246521\n",
      "      vf_loss: 589.01123046875\n",
      "    sample_time_ms: 79718.244\n",
      "    update_time_ms: 6.567\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -14.905236048208302\n",
      "  time_since_restore: 1135.6250331401825\n",
      "  time_this_iter_s: 141.00830793380737\n",
      "  time_total_s: 1135.6250331401825\n",
      "  timestamp: 1556787400\n",
      "  timesteps_since_restore: 264000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 11\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 1135 s, 11 iter, 264000 ts, -59.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_10-58-44\n",
      "  done: false\n",
      "  episode_len_mean: 96.024\n",
      "  episode_reward_max: 316.01947705654436\n",
      "  episode_reward_mean: -50.52431117614351\n",
      "  episode_reward_min: -358.738271997502\n",
      "  episodes_this_iter: 250\n",
      "  episodes_total: 2714\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 24539.311\n",
      "    load_time_ms: 4.25\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 0.00019531251746229827\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.346972942352295\n",
      "      kl: 0.006177489645779133\n",
      "      policy_loss: -0.0020204682368785143\n",
      "      total_loss: 659.5860595703125\n",
      "      vf_explained_var: 0.4981005787849426\n",
      "      vf_loss: 659.5881958007812\n",
      "    sample_time_ms: 81675.281\n",
      "    update_time_ms: 6.962\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -12.631077794035878\n",
      "  time_since_restore: 1260.0415074825287\n",
      "  time_this_iter_s: 124.41647434234619\n",
      "  time_total_s: 1260.0415074825287\n",
      "  timestamp: 1556787524\n",
      "  timesteps_since_restore: 288000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 1260 s, 12 iter, 288000 ts, -50.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-00-42\n",
      "  done: false\n",
      "  episode_len_mean: 92.5019305019305\n",
      "  episode_reward_max: 321.1140598002181\n",
      "  episode_reward_mean: -85.6797170191527\n",
      "  episode_reward_min: -360.1834563320089\n",
      "  episodes_this_iter: 259\n",
      "  episodes_total: 2973\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 24404.693\n",
      "    load_time_ms: 4.244\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.765625873114914e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3367719650268555\n",
      "      kl: 0.005209301132708788\n",
      "      policy_loss: -0.0012024937896057963\n",
      "      total_loss: 611.082763671875\n",
      "      vf_explained_var: 0.5969948172569275\n",
      "      vf_loss: 611.083984375\n",
      "    sample_time_ms: 84331.733\n",
      "    update_time_ms: 7.545\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -21.41992925478818\n",
      "  time_since_restore: 1377.3746221065521\n",
      "  time_this_iter_s: 117.33311462402344\n",
      "  time_total_s: 1377.3746221065521\n",
      "  timestamp: 1556787642\n",
      "  timesteps_since_restore: 312000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 13\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 1377 s, 13 iter, 312000 ts, -85.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-02-10\n",
      "  done: false\n",
      "  episode_len_mean: 91.1596958174905\n",
      "  episode_reward_max: 334.87697683649617\n",
      "  episode_reward_mean: -72.15005926905188\n",
      "  episode_reward_min: -356.5012448792148\n",
      "  episodes_this_iter: 263\n",
      "  episodes_total: 3236\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 24386.459\n",
      "    load_time_ms: 4.231\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.882812936557457e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3353488445281982\n",
      "      kl: 0.005860025994479656\n",
      "      policy_loss: -0.0018920917063951492\n",
      "      total_loss: 611.8313598632812\n",
      "      vf_explained_var: 0.6315248608589172\n",
      "      vf_loss: 611.833251953125\n",
      "    sample_time_ms: 83943.543\n",
      "    update_time_ms: 7.698\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -18.03751481726297\n",
      "  time_since_restore: 1465.910626411438\n",
      "  time_this_iter_s: 88.53600430488586\n",
      "  time_total_s: 1465.910626411438\n",
      "  timestamp: 1556787730\n",
      "  timesteps_since_restore: 336000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 14\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 1465 s, 14 iter, 336000 ts, -72.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-03-41\n",
      "  done: false\n",
      "  episode_len_mean: 95.4780876494024\n",
      "  episode_reward_max: 348.2793846011592\n",
      "  episode_reward_mean: -32.40057096057209\n",
      "  episode_reward_min: -347.3223534888846\n",
      "  episodes_this_iter: 251\n",
      "  episodes_total: 3487\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 24144.367\n",
      "    load_time_ms: 4.121\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4414064682787284e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3256397247314453\n",
      "      kl: 0.0069051687605679035\n",
      "      policy_loss: -0.002618140308186412\n",
      "      total_loss: 531.6004638671875\n",
      "      vf_explained_var: 0.705134928226471\n",
      "      vf_loss: 531.6031494140625\n",
      "    sample_time_ms: 84396.17\n",
      "    update_time_ms: 7.342\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -8.100142740143022\n",
      "  time_since_restore: 1557.0793526172638\n",
      "  time_this_iter_s: 91.1687262058258\n",
      "  time_total_s: 1557.0793526172638\n",
      "  timestamp: 1556787821\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 15\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 1557 s, 15 iter, 360000 ts, -32.4 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-05-25\n",
      "  done: false\n",
      "  episode_len_mean: 92.86538461538461\n",
      "  episode_reward_max: 369.15039513794244\n",
      "  episode_reward_mean: -11.844585374319225\n",
      "  episode_reward_min: -354.5229569574018\n",
      "  episodes_this_iter: 260\n",
      "  episodes_total: 3747\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 24143.995\n",
      "    load_time_ms: 4.182\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2207032341393642e-05\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3137319087982178\n",
      "      kl: 0.004662933759391308\n",
      "      policy_loss: -0.0010725677711889148\n",
      "      total_loss: 536.1625366210938\n",
      "      vf_explained_var: 0.712064802646637\n",
      "      vf_loss: 536.1636352539062\n",
      "    sample_time_ms: 85714.917\n",
      "    update_time_ms: 7.488\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -2.9611463435798058\n",
      "  time_since_restore: 1660.5734164714813\n",
      "  time_this_iter_s: 103.49406385421753\n",
      "  time_total_s: 1660.5734164714813\n",
      "  timestamp: 1556787925\n",
      "  timesteps_since_restore: 384000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 16\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 1660 s, 16 iter, 384000 ts, -11.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-06-54\n",
      "  done: false\n",
      "  episode_len_mean: 90.2593984962406\n",
      "  episode_reward_max: 375.74816309415246\n",
      "  episode_reward_mean: -45.605702323018804\n",
      "  episode_reward_min: -362.0269074270523\n",
      "  episodes_this_iter: 266\n",
      "  episodes_total: 4013\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 23838.648\n",
      "    load_time_ms: 4.084\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.103516170696821e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.314544439315796\n",
      "      kl: 0.004707448184490204\n",
      "      policy_loss: -0.0012900965521112084\n",
      "      total_loss: 600.361328125\n",
      "      vf_explained_var: 0.723670482635498\n",
      "      vf_loss: 600.3626098632812\n",
      "    sample_time_ms: 84900.061\n",
      "    update_time_ms: 7.6\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -11.401425580754697\n",
      "  time_since_restore: 1749.4072070121765\n",
      "  time_this_iter_s: 88.83379054069519\n",
      "  time_total_s: 1749.4072070121765\n",
      "  timestamp: 1556788014\n",
      "  timesteps_since_restore: 408000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 17\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 4.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 1749 s, 17 iter, 408000 ts, -45.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-08-21\n",
      "  done: false\n",
      "  episode_len_mean: 91.88846153846154\n",
      "  episode_reward_max: 402.4102822637428\n",
      "  episode_reward_mean: -4.540710167924348\n",
      "  episode_reward_min: -352.9907493813066\n",
      "  episodes_this_iter: 260\n",
      "  episodes_total: 4273\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 23758.055\n",
      "    load_time_ms: 4.096\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.0517580853484105e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.309191107749939\n",
      "      kl: 0.0057250699028372765\n",
      "      policy_loss: -0.0017283700872212648\n",
      "      total_loss: 525.2396850585938\n",
      "      vf_explained_var: 0.7482016682624817\n",
      "      vf_loss: 525.2413940429688\n",
      "    sample_time_ms: 84197.238\n",
      "    update_time_ms: 7.364\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -1.135177541981087\n",
      "  time_since_restore: 1836.349900007248\n",
      "  time_this_iter_s: 86.94269299507141\n",
      "  time_total_s: 1836.349900007248\n",
      "  timestamp: 1556788101\n",
      "  timesteps_since_restore: 432000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 18\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 1836 s, 18 iter, 432000 ts, -4.54 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-09-48\n",
      "  done: false\n",
      "  episode_len_mean: 91.27376425855513\n",
      "  episode_reward_max: 403.1345387974359\n",
      "  episode_reward_mean: -19.068134826358797\n",
      "  episode_reward_min: -357.0987843107575\n",
      "  episodes_this_iter: 263\n",
      "  episodes_total: 4536\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 22953.789\n",
      "    load_time_ms: 3.96\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5258790426742053e-06\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3021321296691895\n",
      "      kl: 0.0059026856906712055\n",
      "      policy_loss: -0.001906029530800879\n",
      "      total_loss: 532.3212280273438\n",
      "      vf_explained_var: 0.7418947219848633\n",
      "      vf_loss: 532.3231811523438\n",
      "    sample_time_ms: 83121.971\n",
      "    update_time_ms: 7.374\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -4.767033706589697\n",
      "  time_since_restore: 1924.0545272827148\n",
      "  time_this_iter_s: 87.70462727546692\n",
      "  time_total_s: 1924.0545272827148\n",
      "  timestamp: 1556788188\n",
      "  timesteps_since_restore: 456000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 19\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 1924 s, 19 iter, 456000 ts, -19.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-11-27\n",
      "  done: false\n",
      "  episode_len_mean: 89.24907063197026\n",
      "  episode_reward_max: 427.56765606568223\n",
      "  episode_reward_mean: -14.066097875315915\n",
      "  episode_reward_min: -358.723304404753\n",
      "  episodes_this_iter: 269\n",
      "  episodes_total: 4805\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 21619.622\n",
      "    load_time_ms: 3.936\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.629395213371026e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2956526279449463\n",
      "      kl: 0.005301557946950197\n",
      "      policy_loss: -0.0017701912438496947\n",
      "      total_loss: 498.1821594238281\n",
      "      vf_explained_var: 0.7793930768966675\n",
      "      vf_loss: 498.1839294433594\n",
      "    sample_time_ms: 81043.298\n",
      "    update_time_ms: 7.205\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: -3.516524468828973\n",
      "  time_since_restore: 2022.088215827942\n",
      "  time_this_iter_s: 98.03368854522705\n",
      "  time_total_s: 2022.088215827942\n",
      "  timestamp: 1556788287\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 20\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 2022 s, 20 iter, 480000 ts, -14.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-12-56\n",
      "  done: false\n",
      "  episode_len_mean: 88.88475836431226\n",
      "  episode_reward_max: 416.7591870025437\n",
      "  episode_reward_mean: 1.426513391103856\n",
      "  episode_reward_min: -358.04740413352533\n",
      "  episodes_this_iter: 269\n",
      "  episodes_total: 5074\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20172.711\n",
      "    load_time_ms: 3.96\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.814697606685513e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2857118844985962\n",
      "      kl: 0.005757334176450968\n",
      "      policy_loss: -0.001601344789378345\n",
      "      total_loss: 502.2638244628906\n",
      "      vf_explained_var: 0.767779529094696\n",
      "      vf_loss: 502.2654724121094\n",
      "    sample_time_ms: 77321.145\n",
      "    update_time_ms: 6.901\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 0.3566283477759623\n",
      "  time_since_restore: 2111.3758931159973\n",
      "  time_this_iter_s: 89.28767728805542\n",
      "  time_total_s: 2111.3758931159973\n",
      "  timestamp: 1556788376\n",
      "  timesteps_since_restore: 504000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 21\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 2111 s, 21 iter, 504000 ts, 1.43 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-14-24\n",
      "  done: false\n",
      "  episode_len_mean: 90.3796992481203\n",
      "  episode_reward_max: 443.45326225471337\n",
      "  episode_reward_mean: 17.117335466812612\n",
      "  episode_reward_min: -358.6292672963119\n",
      "  episodes_this_iter: 266\n",
      "  episodes_total: 5340\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19337.344\n",
      "    load_time_ms: 3.914\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9073488033427566e-07\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2755107879638672\n",
      "      kl: 0.005140467546880245\n",
      "      policy_loss: -0.0012375077931210399\n",
      "      total_loss: 539.9653930664062\n",
      "      vf_explained_var: 0.7504706382751465\n",
      "      vf_loss: 539.9666137695312\n",
      "    sample_time_ms: 74500.491\n",
      "    update_time_ms: 6.277\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 4.279333866703157\n",
      "  time_since_restore: 2199.1993391513824\n",
      "  time_this_iter_s: 87.82344603538513\n",
      "  time_total_s: 2199.1993391513824\n",
      "  timestamp: 1556788464\n",
      "  timesteps_since_restore: 528000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 22\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 2199 s, 22 iter, 528000 ts, 17.1 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-16-00\n",
      "  done: false\n",
      "  episode_len_mean: 89.01476014760148\n",
      "  episode_reward_max: 447.4439710431637\n",
      "  episode_reward_mean: 12.960847643592668\n",
      "  episode_reward_min: -355.2487808130049\n",
      "  episodes_this_iter: 271\n",
      "  episodes_total: 5611\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19399.628\n",
      "    load_time_ms: 3.912\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.536744016713783e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2734369039535522\n",
      "      kl: 0.004537563771009445\n",
      "      policy_loss: -0.0012467782944440842\n",
      "      total_loss: 505.0350036621094\n",
      "      vf_explained_var: 0.7816994786262512\n",
      "      vf_loss: 505.0362548828125\n",
      "    sample_time_ms: 72375.964\n",
      "    update_time_ms: 5.606\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 3.2402119108981697\n",
      "  time_since_restore: 2295.89390707016\n",
      "  time_this_iter_s: 96.69456791877747\n",
      "  time_total_s: 2295.89390707016\n",
      "  timestamp: 1556788560\n",
      "  timesteps_since_restore: 552000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 23\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 2295 s, 23 iter, 552000 ts, 13 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-17-34\n",
      "  done: false\n",
      "  episode_len_mean: 92.44015444015444\n",
      "  episode_reward_max: 459.7833037944498\n",
      "  episode_reward_mean: 61.29643292724614\n",
      "  episode_reward_min: -358.78328723885335\n",
      "  episodes_this_iter: 259\n",
      "  episodes_total: 5870\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19346.385\n",
      "    load_time_ms: 3.92\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.7683720083568915e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2641280889511108\n",
      "      kl: 0.005111814010888338\n",
      "      policy_loss: -0.0014676126884296536\n",
      "      total_loss: 506.2099304199219\n",
      "      vf_explained_var: 0.7775941491127014\n",
      "      vf_loss: 506.21142578125\n",
      "    sample_time_ms: 72879.204\n",
      "    update_time_ms: 5.608\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 15.324108231811534\n",
      "  time_since_restore: 2388.919891357422\n",
      "  time_this_iter_s: 93.02598428726196\n",
      "  time_total_s: 2388.919891357422\n",
      "  timestamp: 1556788654\n",
      "  timesteps_since_restore: 576000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 24\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 2388 s, 24 iter, 576000 ts, 61.3 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-19-03\n",
      "  done: false\n",
      "  episode_len_mean: 88.49632352941177\n",
      "  episode_reward_max: 470.0023224920158\n",
      "  episode_reward_mean: 35.24277448728852\n",
      "  episode_reward_min: -353.64269570202464\n",
      "  episodes_this_iter: 272\n",
      "  episodes_total: 6142\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19420.256\n",
      "    load_time_ms: 3.91\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3841860041784457e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2483429908752441\n",
      "      kl: 0.0051692877896130085\n",
      "      policy_loss: -0.001149585354141891\n",
      "      total_loss: 581.4462280273438\n",
      "      vf_explained_var: 0.7687075138092041\n",
      "      vf_loss: 581.4473266601562\n",
      "    sample_time_ms: 72600.113\n",
      "    update_time_ms: 5.538\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 8.810693621822129\n",
      "  time_since_restore: 2478.036437034607\n",
      "  time_this_iter_s: 89.11654567718506\n",
      "  time_total_s: 2478.036437034607\n",
      "  timestamp: 1556788743\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 25\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 2478 s, 25 iter, 600000 ts, 35.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-21-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.99236641221374\n",
      "  episode_reward_max: 472.13150967168934\n",
      "  episode_reward_mean: 64.70632411043849\n",
      "  episode_reward_min: -357.70071422906983\n",
      "  episodes_this_iter: 262\n",
      "  episodes_total: 6404\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20635.788\n",
      "    load_time_ms: 3.832\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1920930020892229e-08\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2407188415527344\n",
      "      kl: 0.004781536757946014\n",
      "      policy_loss: -0.0010881938505917788\n",
      "      total_loss: 507.0929870605469\n",
      "      vf_explained_var: 0.7944499254226685\n",
      "      vf_loss: 507.0940856933594\n",
      "    sample_time_ms: 72890.995\n",
      "    update_time_ms: 5.51\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.176581027609625\n",
      "  time_since_restore: 2596.581941127777\n",
      "  time_this_iter_s: 118.54550409317017\n",
      "  time_total_s: 2596.581941127777\n",
      "  timestamp: 1556788861\n",
      "  timesteps_since_restore: 624000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 26\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 2596 s, 26 iter, 624000 ts, 64.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-22-47\n",
      "  done: false\n",
      "  episode_len_mean: 89.90671641791045\n",
      "  episode_reward_max: 482.2077691259504\n",
      "  episode_reward_mean: 54.95794646221249\n",
      "  episode_reward_min: -358.49783044925186\n",
      "  episodes_this_iter: 268\n",
      "  episodes_total: 6672\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20533.739\n",
      "    load_time_ms: 3.812\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.960465010446114e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2323460578918457\n",
      "      kl: 0.005916930735111237\n",
      "      policy_loss: -0.002030271105468273\n",
      "      total_loss: 521.961669921875\n",
      "      vf_explained_var: 0.8110859990119934\n",
      "      vf_loss: 521.9636840820312\n",
      "    sample_time_ms: 74634.755\n",
      "    update_time_ms: 5.404\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 13.73948661555312\n",
      "  time_since_restore: 2701.827623128891\n",
      "  time_this_iter_s: 105.24568200111389\n",
      "  time_total_s: 2701.827623128891\n",
      "  timestamp: 1556788967\n",
      "  timesteps_since_restore: 648000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 27\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 2701 s, 27 iter, 648000 ts, 55 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-24-18\n",
      "  done: false\n",
      "  episode_len_mean: 89.97752808988764\n",
      "  episode_reward_max: 482.1485572816804\n",
      "  episode_reward_mean: 74.45601876789944\n",
      "  episode_reward_min: -357.5074132481072\n",
      "  episodes_this_iter: 267\n",
      "  episodes_total: 6939\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20464.328\n",
      "    load_time_ms: 3.775\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.980232505223057e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2340079545974731\n",
      "      kl: 0.0046685910783708096\n",
      "      policy_loss: -0.0009704516851343215\n",
      "      total_loss: 512.2470703125\n",
      "      vf_explained_var: 0.8266429901123047\n",
      "      vf_loss: 512.2481079101562\n",
      "    sample_time_ms: 75175.54\n",
      "    update_time_ms: 5.333\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 18.61400469197486\n",
      "  time_since_restore: 2793.4744715690613\n",
      "  time_this_iter_s: 91.64684844017029\n",
      "  time_total_s: 2793.4744715690613\n",
      "  timestamp: 1556789058\n",
      "  timesteps_since_restore: 672000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 28\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 2793 s, 28 iter, 672000 ts, 74.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-25-45\n",
      "  done: false\n",
      "  episode_len_mean: 94.3740157480315\n",
      "  episode_reward_max: 487.3063999116373\n",
      "  episode_reward_mean: 118.20351537818415\n",
      "  episode_reward_min: -355.6765208638333\n",
      "  episodes_this_iter: 254\n",
      "  episodes_total: 7193\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20290.822\n",
      "    load_time_ms: 3.773\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4901162526115286e-09\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2268891334533691\n",
      "      kl: 0.006194418761879206\n",
      "      policy_loss: -0.001438031205907464\n",
      "      total_loss: 531.2562255859375\n",
      "      vf_explained_var: 0.7957870960235596\n",
      "      vf_loss: 531.2576293945312\n",
      "    sample_time_ms: 75235.82\n",
      "    update_time_ms: 5.232\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.550878844546038\n",
      "  time_since_restore: 2880.039729118347\n",
      "  time_this_iter_s: 86.56525754928589\n",
      "  time_total_s: 2880.039729118347\n",
      "  timestamp: 1556789145\n",
      "  timesteps_since_restore: 696000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 29\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 2880 s, 29 iter, 696000 ts, 118 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-27-12\n",
      "  done: false\n",
      "  episode_len_mean: 90.89056603773585\n",
      "  episode_reward_max: 496.59118802498705\n",
      "  episode_reward_mean: 95.94398789937898\n",
      "  episode_reward_min: -359.50800333801084\n",
      "  episodes_this_iter: 265\n",
      "  episodes_total: 7458\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20303.704\n",
      "    load_time_ms: 3.775\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.450581263057643e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2139967679977417\n",
      "      kl: 0.005178065504878759\n",
      "      policy_loss: -0.0012956312857568264\n",
      "      total_loss: 537.9463500976562\n",
      "      vf_explained_var: 0.8175302147865295\n",
      "      vf_loss: 537.9476928710938\n",
      "    sample_time_ms: 74096.25\n",
      "    update_time_ms: 5.22\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 23.985996974844745\n",
      "  time_since_restore: 2966.804712533951\n",
      "  time_this_iter_s: 86.76498341560364\n",
      "  time_total_s: 2966.804712533951\n",
      "  timestamp: 1556789232\n",
      "  timesteps_since_restore: 720000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 30\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 2966 s, 30 iter, 720000 ts, 95.9 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-28-41\n",
      "  done: false\n",
      "  episode_len_mean: 91.8923076923077\n",
      "  episode_reward_max: 507.13697791628\n",
      "  episode_reward_mean: 112.48926795211437\n",
      "  episode_reward_min: -355.37622304326567\n",
      "  episodes_this_iter: 260\n",
      "  episodes_total: 7718\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20127.269\n",
      "    load_time_ms: 3.619\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.7252906315288215e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2026816606521606\n",
      "      kl: 0.005578973330557346\n",
      "      policy_loss: -0.0011715893633663654\n",
      "      total_loss: 471.0245056152344\n",
      "      vf_explained_var: 0.8426649570465088\n",
      "      vf_loss: 471.0256652832031\n",
      "    sample_time_ms: 74239.777\n",
      "    update_time_ms: 5.25\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.1223169880286\n",
      "  time_since_restore: 3055.744045495987\n",
      "  time_this_iter_s: 88.93933296203613\n",
      "  time_total_s: 3055.744045495987\n",
      "  timestamp: 1556789321\n",
      "  timesteps_since_restore: 744000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 31\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 3055 s, 31 iter, 744000 ts, 112 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-30-08\n",
      "  done: false\n",
      "  episode_len_mean: 87.88278388278388\n",
      "  episode_reward_max: 520.5208244982789\n",
      "  episode_reward_mean: 83.18267357557373\n",
      "  episode_reward_min: -362.06586601831555\n",
      "  episodes_this_iter: 273\n",
      "  episodes_total: 7991\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20034.342\n",
      "    load_time_ms: 3.651\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8626453157644107e-10\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1959474086761475\n",
      "      kl: 0.00487778102979064\n",
      "      policy_loss: -0.001241835649125278\n",
      "      total_loss: 575.4776000976562\n",
      "      vf_explained_var: 0.8161233067512512\n",
      "      vf_loss: 575.4788208007812\n",
      "    sample_time_ms: 74308.923\n",
      "    update_time_ms: 5.4\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 20.795668393893433\n",
      "  time_since_restore: 3143.3282124996185\n",
      "  time_this_iter_s: 87.58416700363159\n",
      "  time_total_s: 3143.3282124996185\n",
      "  timestamp: 1556789408\n",
      "  timesteps_since_restore: 768000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 32\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 3143 s, 32 iter, 768000 ts, 83.2 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-31-35\n",
      "  done: false\n",
      "  episode_len_mean: 88.45018450184502\n",
      "  episode_reward_max: 523.5683042821342\n",
      "  episode_reward_mean: 83.5853741624893\n",
      "  episode_reward_min: -358.3969620523684\n",
      "  episodes_this_iter: 271\n",
      "  episodes_total: 8262\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19918.444\n",
      "    load_time_ms: 3.637\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.313226578822054e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1857696771621704\n",
      "      kl: 0.004802929703146219\n",
      "      policy_loss: -0.001211196300573647\n",
      "      total_loss: 535.0654907226562\n",
      "      vf_explained_var: 0.829883873462677\n",
      "      vf_loss: 535.0667724609375\n",
      "    sample_time_ms: 73429.448\n",
      "    update_time_ms: 5.467\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 20.896343540622325\n",
      "  time_since_restore: 3230.069471359253\n",
      "  time_this_iter_s: 86.7412588596344\n",
      "  time_total_s: 3230.069471359253\n",
      "  timestamp: 1556789495\n",
      "  timesteps_since_restore: 792000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 33\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 3230 s, 33 iter, 792000 ts, 83.6 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-33-04\n",
      "  done: false\n",
      "  episode_len_mean: 87.1376811594203\n",
      "  episode_reward_max: 533.8725560060029\n",
      "  episode_reward_mean: 71.72517664885825\n",
      "  episode_reward_min: -359.03680938168986\n",
      "  episodes_this_iter: 276\n",
      "  episodes_total: 8538\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19899.788\n",
      "    load_time_ms: 3.637\n",
      "    num_steps_sampled: 816000\n",
      "    num_steps_trained: 816000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.656613289411027e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1868340969085693\n",
      "      kl: 0.0039566149935126305\n",
      "      policy_loss: -0.0010544261895120144\n",
      "      total_loss: 622.5965576171875\n",
      "      vf_explained_var: 0.8065851330757141\n",
      "      vf_loss: 622.5975952148438\n",
      "    sample_time_ms: 72998.994\n",
      "    update_time_ms: 5.37\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 17.931294162214563\n",
      "  time_since_restore: 3318.6078927516937\n",
      "  time_this_iter_s: 88.5384213924408\n",
      "  time_total_s: 3318.6078927516937\n",
      "  timestamp: 1556789584\n",
      "  timesteps_since_restore: 816000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 816000\n",
      "  training_iteration: 34\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 3318 s, 34 iter, 816000 ts, 71.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-34-31\n",
      "  done: false\n",
      "  episode_len_mean: 89.64794007490637\n",
      "  episode_reward_max: 528.9557845539825\n",
      "  episode_reward_mean: 98.20935392087463\n",
      "  episode_reward_min: -356.5376845361351\n",
      "  episodes_this_iter: 267\n",
      "  episodes_total: 8805\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19767.747\n",
      "    load_time_ms: 3.647\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.3283066447055134e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1813873052597046\n",
      "      kl: 0.005184615030884743\n",
      "      policy_loss: -0.0012518868315964937\n",
      "      total_loss: 500.7302551269531\n",
      "      vf_explained_var: 0.8566540479660034\n",
      "      vf_loss: 500.73150634765625\n",
      "    sample_time_ms: 73000.539\n",
      "    update_time_ms: 5.473\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.552338480218655\n",
      "  time_since_restore: 3406.415773630142\n",
      "  time_this_iter_s: 87.80788087844849\n",
      "  time_total_s: 3406.415773630142\n",
      "  timestamp: 1556789671\n",
      "  timesteps_since_restore: 840000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 35\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 3406 s, 35 iter, 840000 ts, 98.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-35-58\n",
      "  done: false\n",
      "  episode_len_mean: 86.93840579710145\n",
      "  episode_reward_max: 533.9148745258561\n",
      "  episode_reward_mean: 74.82927618293911\n",
      "  episode_reward_min: -356.4775349122577\n",
      "  episodes_this_iter: 276\n",
      "  episodes_total: 9081\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18308.718\n",
      "    load_time_ms: 3.638\n",
      "    num_steps_sampled: 864000\n",
      "    num_steps_trained: 864000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.1641533223527567e-11\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1672838926315308\n",
      "      kl: 0.004858425352722406\n",
      "      policy_loss: -0.0010145243722945452\n",
      "      total_loss: 605.0879516601562\n",
      "      vf_explained_var: 0.8363022208213806\n",
      "      vf_loss: 605.0889282226562\n",
      "    sample_time_ms: 71292.23\n",
      "    update_time_ms: 5.374\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 18.707319045734778\n",
      "  time_since_restore: 3493.281097173691\n",
      "  time_this_iter_s: 86.86532354354858\n",
      "  time_total_s: 3493.281097173691\n",
      "  timestamp: 1556789758\n",
      "  timesteps_since_restore: 864000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 864000\n",
      "  training_iteration: 36\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 3493 s, 36 iter, 864000 ts, 74.8 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-37-26\n",
      "  done: false\n",
      "  episode_len_mean: 85.91814946619218\n",
      "  episode_reward_max: 531.0691728009522\n",
      "  episode_reward_mean: 67.49715948860532\n",
      "  episode_reward_min: -357.3464065120154\n",
      "  episodes_this_iter: 281\n",
      "  episodes_total: 9362\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18296.645\n",
      "    load_time_ms: 3.644\n",
      "    num_steps_sampled: 888000\n",
      "    num_steps_trained: 888000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.8207666117637835e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1585537195205688\n",
      "      kl: 0.006000030320137739\n",
      "      policy_loss: -0.001543615129776299\n",
      "      total_loss: 567.260498046875\n",
      "      vf_explained_var: 0.8560851216316223\n",
      "      vf_loss: 567.2620849609375\n",
      "    sample_time_ms: 69544.776\n",
      "    update_time_ms: 5.454\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 16.87428987215133\n",
      "  time_since_restore: 3580.9289088249207\n",
      "  time_this_iter_s: 87.64781165122986\n",
      "  time_total_s: 3580.9289088249207\n",
      "  timestamp: 1556789846\n",
      "  timesteps_since_restore: 888000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 888000\n",
      "  training_iteration: 37\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 3580 s, 37 iter, 888000 ts, 67.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-38-54\n",
      "  done: false\n",
      "  episode_len_mean: 93.5078125\n",
      "  episode_reward_max: 534.5395911983653\n",
      "  episode_reward_mean: 150.17480946439753\n",
      "  episode_reward_min: -357.7757585425998\n",
      "  episodes_this_iter: 256\n",
      "  episodes_total: 9618\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18286.697\n",
      "    load_time_ms: 3.644\n",
      "    num_steps_sampled: 912000\n",
      "    num_steps_trained: 912000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.9103833058818918e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1461315155029297\n",
      "      kl: 0.006513313390314579\n",
      "      policy_loss: -0.002022880595177412\n",
      "      total_loss: 520.38720703125\n",
      "      vf_explained_var: 0.8465635776519775\n",
      "      vf_loss: 520.3892211914062\n",
      "    sample_time_ms: 69214.078\n",
      "    update_time_ms: 5.579\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.543702366099396\n",
      "  time_since_restore: 3669.1692836284637\n",
      "  time_this_iter_s: 88.24037480354309\n",
      "  time_total_s: 3669.1692836284637\n",
      "  timestamp: 1556789934\n",
      "  timesteps_since_restore: 912000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 912000\n",
      "  training_iteration: 38\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 3669 s, 38 iter, 912000 ts, 150 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-40-22\n",
      "  done: false\n",
      "  episode_len_mean: 86.35971223021583\n",
      "  episode_reward_max: 540.0922585309808\n",
      "  episode_reward_mean: 71.16264994079128\n",
      "  episode_reward_min: -356.86085659097273\n",
      "  episodes_this_iter: 278\n",
      "  episodes_total: 9896\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18294.987\n",
      "    load_time_ms: 3.637\n",
      "    num_steps_sampled: 936000\n",
      "    num_steps_trained: 936000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4551916529409459e-12\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1290558576583862\n",
      "      kl: 0.0062414114363491535\n",
      "      policy_loss: -0.001332794432528317\n",
      "      total_loss: 588.1326904296875\n",
      "      vf_explained_var: 0.8461580276489258\n",
      "      vf_loss: 588.134033203125\n",
      "    sample_time_ms: 69303.203\n",
      "    update_time_ms: 5.501\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 17.790662485197828\n",
      "  time_since_restore: 3756.7049639225006\n",
      "  time_this_iter_s: 87.53568029403687\n",
      "  time_total_s: 3756.7049639225006\n",
      "  timestamp: 1556790022\n",
      "  timesteps_since_restore: 936000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 936000\n",
      "  training_iteration: 39\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 3756 s, 39 iter, 936000 ts, 71.2 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-41-49\n",
      "  done: false\n",
      "  episode_len_mean: 87.55677655677655\n",
      "  episode_reward_max: 552.8281075158908\n",
      "  episode_reward_mean: 101.87279781908342\n",
      "  episode_reward_min: -356.94763707483526\n",
      "  episodes_this_iter: 273\n",
      "  episodes_total: 10169\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18291.141\n",
      "    load_time_ms: 3.649\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.275958264704729e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1249256134033203\n",
      "      kl: 0.006072827614843845\n",
      "      policy_loss: -0.0012066082563251257\n",
      "      total_loss: 598.0421752929688\n",
      "      vf_explained_var: 0.8323308825492859\n",
      "      vf_loss: 598.0433349609375\n",
      "    sample_time_ms: 69364.386\n",
      "    update_time_ms: 5.415\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.468199454770847\n",
      "  time_since_restore: 3844.0442337989807\n",
      "  time_this_iter_s: 87.3392698764801\n",
      "  time_total_s: 3844.0442337989807\n",
      "  timestamp: 1556790109\n",
      "  timesteps_since_restore: 960000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 40\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 3844 s, 40 iter, 960000 ts, 102 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-43-18\n",
      "  done: false\n",
      "  episode_len_mean: 89.28518518518518\n",
      "  episode_reward_max: 559.3034795560509\n",
      "  episode_reward_mean: 116.1147577742691\n",
      "  episode_reward_min: -360.404850557507\n",
      "  episodes_this_iter: 270\n",
      "  episodes_total: 10439\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18281.764\n",
      "    load_time_ms: 3.651\n",
      "    num_steps_sampled: 984000\n",
      "    num_steps_trained: 984000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.6379791323523647e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1188240051269531\n",
      "      kl: 0.005849403329193592\n",
      "      policy_loss: -0.0017473272746428847\n",
      "      total_loss: 577.1644287109375\n",
      "      vf_explained_var: 0.8386451601982117\n",
      "      vf_loss: 577.1661376953125\n",
      "    sample_time_ms: 69318.264\n",
      "    update_time_ms: 5.472\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 29.028689443567266\n",
      "  time_since_restore: 3932.42413520813\n",
      "  time_this_iter_s: 88.37990140914917\n",
      "  time_total_s: 3932.42413520813\n",
      "  timestamp: 1556790198\n",
      "  timesteps_since_restore: 984000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 984000\n",
      "  training_iteration: 41\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 3932 s, 41 iter, 984000 ts, 116 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-44-46\n",
      "  done: false\n",
      "  episode_len_mean: 89.31970260223048\n",
      "  episode_reward_max: 558.3871927599833\n",
      "  episode_reward_mean: 126.03314996961659\n",
      "  episode_reward_min: -355.32982083987497\n",
      "  episodes_this_iter: 269\n",
      "  episodes_total: 10708\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18275.971\n",
      "    load_time_ms: 3.616\n",
      "    num_steps_sampled: 1008000\n",
      "    num_steps_trained: 1008000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.8189895661761823e-13\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1132843494415283\n",
      "      kl: 0.005105358548462391\n",
      "      policy_loss: -0.0009648966952227056\n",
      "      total_loss: 643.1062622070312\n",
      "      vf_explained_var: 0.8143542408943176\n",
      "      vf_loss: 643.1072387695312\n",
      "    sample_time_ms: 69382.328\n",
      "    update_time_ms: 5.468\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.508287492404147\n",
      "  time_since_restore: 4020.5956139564514\n",
      "  time_this_iter_s: 88.17147874832153\n",
      "  time_total_s: 4020.5956139564514\n",
      "  timestamp: 1556790286\n",
      "  timesteps_since_restore: 1008000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1008000\n",
      "  training_iteration: 42\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 4020 s, 42 iter, 1008000 ts, 126 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-46-13\n",
      "  done: false\n",
      "  episode_len_mean: 88.85185185185185\n",
      "  episode_reward_max: 554.6597007346387\n",
      "  episode_reward_mean: 123.39884506791337\n",
      "  episode_reward_min: -353.7678404049376\n",
      "  episodes_this_iter: 270\n",
      "  episodes_total: 10978\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18290.874\n",
      "    load_time_ms: 3.621\n",
      "    num_steps_sampled: 1032000\n",
      "    num_steps_trained: 1032000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.094947830880912e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.1062686443328857\n",
      "      kl: 0.005941380281001329\n",
      "      policy_loss: -0.000579862913582474\n",
      "      total_loss: 610.8284301757812\n",
      "      vf_explained_var: 0.826503574848175\n",
      "      vf_loss: 610.8289794921875\n",
      "    sample_time_ms: 69366.566\n",
      "    update_time_ms: 5.411\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.849711266978343\n",
      "  time_since_restore: 4107.327162742615\n",
      "  time_this_iter_s: 86.73154878616333\n",
      "  time_total_s: 4107.327162742615\n",
      "  timestamp: 1556790373\n",
      "  timesteps_since_restore: 1032000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1032000\n",
      "  training_iteration: 43\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 4107 s, 43 iter, 1032000 ts, 123 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-47-40\n",
      "  done: false\n",
      "  episode_len_mean: 84.89752650176679\n",
      "  episode_reward_max: 562.4580069108613\n",
      "  episode_reward_mean: 96.71122057745224\n",
      "  episode_reward_min: -356.52151953633074\n",
      "  episodes_this_iter: 283\n",
      "  episodes_total: 11261\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18305.934\n",
      "    load_time_ms: 3.626\n",
      "    num_steps_sampled: 1056000\n",
      "    num_steps_trained: 1056000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.547473915440456e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0904924869537354\n",
      "      kl: 0.005477508530020714\n",
      "      policy_loss: -0.0010644840076565742\n",
      "      total_loss: 550.9598388671875\n",
      "      vf_explained_var: 0.8588311076164246\n",
      "      vf_loss: 550.9609375\n",
      "    sample_time_ms: 69204.958\n",
      "    update_time_ms: 5.283\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 24.17780514436306\n",
      "  time_since_restore: 4194.397845983505\n",
      "  time_this_iter_s: 87.0706832408905\n",
      "  time_total_s: 4194.397845983505\n",
      "  timestamp: 1556790460\n",
      "  timesteps_since_restore: 1056000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1056000\n",
      "  training_iteration: 44\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 4194 s, 44 iter, 1056000 ts, 96.7 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-49-08\n",
      "  done: false\n",
      "  episode_len_mean: 88.23247232472325\n",
      "  episode_reward_max: 565.2773299762524\n",
      "  episode_reward_mean: 122.6945599469834\n",
      "  episode_reward_min: -355.6470497167078\n",
      "  episodes_this_iter: 271\n",
      "  episodes_total: 11532\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18301.38\n",
      "    load_time_ms: 3.596\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.273736957720228e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0855463743209839\n",
      "      kl: 0.006328856106847525\n",
      "      policy_loss: -0.001666105119511485\n",
      "      total_loss: 562.4221801757812\n",
      "      vf_explained_var: 0.8568660616874695\n",
      "      vf_loss: 562.423828125\n",
      "    sample_time_ms: 69270.991\n",
      "    update_time_ms: 5.217\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 30.673639986745858\n",
      "  time_since_restore: 4282.816820383072\n",
      "  time_this_iter_s: 88.41897439956665\n",
      "  time_total_s: 4282.816820383072\n",
      "  timestamp: 1556790548\n",
      "  timesteps_since_restore: 1080000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 45\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 4282 s, 45 iter, 1080000 ts, 123 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-50-36\n",
      "  done: false\n",
      "  episode_len_mean: 84.16140350877193\n",
      "  episode_reward_max: 561.1380993304501\n",
      "  episode_reward_mean: 89.50299561212458\n",
      "  episode_reward_min: -356.18626216679695\n",
      "  episodes_this_iter: 285\n",
      "  episodes_total: 11817\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18302.343\n",
      "    load_time_ms: 3.586\n",
      "    num_steps_sampled: 1104000\n",
      "    num_steps_trained: 1104000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.136868478860114e-14\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.07261323928833\n",
      "      kl: 0.006472812034189701\n",
      "      policy_loss: -0.0014360490022227168\n",
      "      total_loss: 688.5436401367188\n",
      "      vf_explained_var: 0.8326021432876587\n",
      "      vf_loss: 688.5451049804688\n",
      "    sample_time_ms: 69355.16\n",
      "    update_time_ms: 5.231\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 22.37574890303115\n",
      "  time_since_restore: 4370.5408635139465\n",
      "  time_this_iter_s: 87.72404313087463\n",
      "  time_total_s: 4370.5408635139465\n",
      "  timestamp: 1556790636\n",
      "  timesteps_since_restore: 1104000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1104000\n",
      "  training_iteration: 46\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 4370 s, 46 iter, 1104000 ts, 89.5 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-52-03\n",
      "  done: false\n",
      "  episode_len_mean: 91.18560606060606\n",
      "  episode_reward_max: 563.6077110222232\n",
      "  episode_reward_mean: 154.06455234361718\n",
      "  episode_reward_min: -359.75212613198664\n",
      "  episodes_this_iter: 264\n",
      "  episodes_total: 12081\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18302.81\n",
      "    load_time_ms: 3.564\n",
      "    num_steps_sampled: 1128000\n",
      "    num_steps_trained: 1128000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.68434239430057e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0711544752120972\n",
      "      kl: 0.006036378443241119\n",
      "      policy_loss: -0.0011858164798468351\n",
      "      total_loss: 575.7965087890625\n",
      "      vf_explained_var: 0.8512563705444336\n",
      "      vf_loss: 575.7977294921875\n",
      "    sample_time_ms: 69288.913\n",
      "    update_time_ms: 5.226\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.516138085904295\n",
      "  time_since_restore: 4457.532385826111\n",
      "  time_this_iter_s: 86.9915223121643\n",
      "  time_total_s: 4457.532385826111\n",
      "  timestamp: 1556790723\n",
      "  timesteps_since_restore: 1128000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1128000\n",
      "  training_iteration: 47\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 4457 s, 47 iter, 1128000 ts, 154 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 86.01075268817205\n",
      "  episode_reward_max: 554.6689426762288\n",
      "  episode_reward_mean: 112.50397041709068\n",
      "  episode_reward_min: -353.61162802875236\n",
      "  episodes_this_iter: 279\n",
      "  episodes_total: 12360\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18289.568\n",
      "    load_time_ms: 3.573\n",
      "    num_steps_sampled: 1152000\n",
      "    num_steps_trained: 1152000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.842171197150285e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0640445947647095\n",
      "      kl: 0.00568125257268548\n",
      "      policy_loss: -0.0013093982124701142\n",
      "      total_loss: 659.3883056640625\n",
      "      vf_explained_var: 0.8393752574920654\n",
      "      vf_loss: 659.3895874023438\n",
      "    sample_time_ms: 69155.134\n",
      "    update_time_ms: 5.224\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.125992604272668\n",
      "  time_since_restore: 4544.30329990387\n",
      "  time_this_iter_s: 86.77091407775879\n",
      "  time_total_s: 4544.30329990387\n",
      "  timestamp: 1556790810\n",
      "  timesteps_since_restore: 1152000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1152000\n",
      "  training_iteration: 48\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 4544 s, 48 iter, 1152000 ts, 113 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-54-57\n",
      "  done: false\n",
      "  episode_len_mean: 89.01486988847584\n",
      "  episode_reward_max: 554.1308002192332\n",
      "  episode_reward_mean: 132.5002326105419\n",
      "  episode_reward_min: -355.703122577322\n",
      "  episodes_this_iter: 269\n",
      "  episodes_total: 12629\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18271.297\n",
      "    load_time_ms: 3.564\n",
      "    num_steps_sampled: 1176000\n",
      "    num_steps_trained: 1176000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.4210855985751425e-15\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0690242052078247\n",
      "      kl: 0.005746598355472088\n",
      "      policy_loss: -0.0012314823688939214\n",
      "      total_loss: 554.6760864257812\n",
      "      vf_explained_var: 0.8642337322235107\n",
      "      vf_loss: 554.6773071289062\n",
      "    sample_time_ms: 69127.884\n",
      "    update_time_ms: 5.24\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.12505815263548\n",
      "  time_since_restore: 4631.381686210632\n",
      "  time_this_iter_s: 87.0783863067627\n",
      "  time_total_s: 4631.381686210632\n",
      "  timestamp: 1556790897\n",
      "  timesteps_since_restore: 1176000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1176000\n",
      "  training_iteration: 49\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 4631 s, 49 iter, 1176000 ts, 133 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 92.11538461538461\n",
      "  episode_reward_max: 559.2702026021935\n",
      "  episode_reward_mean: 169.3328276658146\n",
      "  episode_reward_min: -360.12429927852486\n",
      "  episodes_this_iter: 260\n",
      "  episodes_total: 12889\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18246.268\n",
      "    load_time_ms: 3.523\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.105427992875712e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0614876747131348\n",
      "      kl: 0.005849982611835003\n",
      "      policy_loss: -0.0013009849935770035\n",
      "      total_loss: 577.8623046875\n",
      "      vf_explained_var: 0.8550651669502258\n",
      "      vf_loss: 577.8636474609375\n",
      "    sample_time_ms: 69112.896\n",
      "    update_time_ms: 5.442\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.33320691645365\n",
      "  time_since_restore: 4718.3198783397675\n",
      "  time_this_iter_s: 86.93819212913513\n",
      "  time_total_s: 4718.3198783397675\n",
      "  timestamp: 1556790984\n",
      "  timesteps_since_restore: 1200000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 50\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 4718 s, 50 iter, 1200000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-57-51\n",
      "  done: false\n",
      "  episode_len_mean: 86.89891696750902\n",
      "  episode_reward_max: 563.9592356524265\n",
      "  episode_reward_mean: 126.20069736237997\n",
      "  episode_reward_min: -357.87185552259245\n",
      "  episodes_this_iter: 277\n",
      "  episodes_total: 13166\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18211.209\n",
      "    load_time_ms: 3.517\n",
      "    num_steps_sampled: 1224000\n",
      "    num_steps_trained: 1224000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.552713996437856e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.051671028137207\n",
      "      kl: 0.006668894551694393\n",
      "      policy_loss: -0.001632276806049049\n",
      "      total_loss: 572.8134155273438\n",
      "      vf_explained_var: 0.8605673909187317\n",
      "      vf_loss: 572.8150634765625\n",
      "    sample_time_ms: 68960.261\n",
      "    update_time_ms: 5.453\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 31.550174340594992\n",
      "  time_since_restore: 4804.824583768845\n",
      "  time_this_iter_s: 86.50470542907715\n",
      "  time_total_s: 4804.824583768845\n",
      "  timestamp: 1556791071\n",
      "  timesteps_since_restore: 1224000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1224000\n",
      "  training_iteration: 51\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 4804 s, 51 iter, 1224000 ts, 126 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_11-59-17\n",
      "  done: false\n",
      "  episode_len_mean: 87.96691176470588\n",
      "  episode_reward_max: 552.3175220584186\n",
      "  episode_reward_mean: 139.04286139238116\n",
      "  episode_reward_min: -355.42148788812756\n",
      "  episodes_this_iter: 272\n",
      "  episodes_total: 13438\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18172.302\n",
      "    load_time_ms: 3.499\n",
      "    num_steps_sampled: 1248000\n",
      "    num_steps_trained: 1248000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.776356998218928e-16\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0480886697769165\n",
      "      kl: 0.005958661902695894\n",
      "      policy_loss: -0.00127976608928293\n",
      "      total_loss: 582.241943359375\n",
      "      vf_explained_var: 0.8576259613037109\n",
      "      vf_loss: 582.2432250976562\n",
      "    sample_time_ms: 68816.621\n",
      "    update_time_ms: 5.386\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.76071534809529\n",
      "  time_since_restore: 4891.16800570488\n",
      "  time_this_iter_s: 86.34342193603516\n",
      "  time_total_s: 4891.16800570488\n",
      "  timestamp: 1556791157\n",
      "  timesteps_since_restore: 1248000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1248000\n",
      "  training_iteration: 52\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 4891 s, 52 iter, 1248000 ts, 139 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-00-42\n",
      "  done: false\n",
      "  episode_len_mean: 90.2247191011236\n",
      "  episode_reward_max: 563.7831669893709\n",
      "  episode_reward_mean: 155.31864068122343\n",
      "  episode_reward_min: -351.42384471190263\n",
      "  episodes_this_iter: 267\n",
      "  episodes_total: 13705\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18118.594\n",
      "    load_time_ms: 3.465\n",
      "    num_steps_sampled: 1272000\n",
      "    num_steps_trained: 1272000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.88178499109464e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0308105945587158\n",
      "      kl: 0.006549081765115261\n",
      "      policy_loss: -0.0011683740885928273\n",
      "      total_loss: 573.4965209960938\n",
      "      vf_explained_var: 0.8522584438323975\n",
      "      vf_loss: 573.4976196289062\n",
      "    sample_time_ms: 68695.562\n",
      "    update_time_ms: 5.428\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.82966017030586\n",
      "  time_since_restore: 4976.150218963623\n",
      "  time_this_iter_s: 84.98221325874329\n",
      "  time_total_s: 4976.150218963623\n",
      "  timestamp: 1556791242\n",
      "  timesteps_since_restore: 1272000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1272000\n",
      "  training_iteration: 53\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 4976 s, 53 iter, 1272000 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-02-11\n",
      "  done: false\n",
      "  episode_len_mean: 89.43122676579925\n",
      "  episode_reward_max: 558.7585740132472\n",
      "  episode_reward_mean: 159.13355737377597\n",
      "  episode_reward_min: -356.5766268137197\n",
      "  episodes_this_iter: 269\n",
      "  episodes_total: 13974\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18282.813\n",
      "    load_time_ms: 3.455\n",
      "    num_steps_sampled: 1296000\n",
      "    num_steps_trained: 1296000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.44089249554732e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0281450748443604\n",
      "      kl: 0.006688891910016537\n",
      "      policy_loss: -0.0010909447446465492\n",
      "      total_loss: 614.957275390625\n",
      "      vf_explained_var: 0.8399664759635925\n",
      "      vf_loss: 614.9583129882812\n",
      "    sample_time_ms: 68776.286\n",
      "    update_time_ms: 5.419\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.78338934344399\n",
      "  time_since_restore: 5065.664162397385\n",
      "  time_this_iter_s: 89.5139434337616\n",
      "  time_total_s: 5065.664162397385\n",
      "  timestamp: 1556791331\n",
      "  timesteps_since_restore: 1296000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1296000\n",
      "  training_iteration: 54\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 5065 s, 54 iter, 1296000 ts, 159 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-04-04\n",
      "  done: false\n",
      "  episode_len_mean: 89.54681647940075\n",
      "  episode_reward_max: 563.987105526123\n",
      "  episode_reward_mean: 156.96635014525665\n",
      "  episode_reward_min: -353.0255586805662\n",
      "  episodes_this_iter: 267\n",
      "  episodes_total: 14241\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18330.787\n",
      "    load_time_ms: 3.512\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.22044624777366e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.006966233253479\n",
      "      kl: 0.007145662792026997\n",
      "      policy_loss: -0.001686106319539249\n",
      "      total_loss: 604.7735595703125\n",
      "      vf_explained_var: 0.8491467833518982\n",
      "      vf_loss: 604.7752685546875\n",
      "    sample_time_ms: 71116.406\n",
      "    update_time_ms: 5.529\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.241587536314164\n",
      "  time_since_restore: 5177.9645528793335\n",
      "  time_this_iter_s: 112.30039048194885\n",
      "  time_total_s: 5177.9645528793335\n",
      "  timestamp: 1556791444\n",
      "  timesteps_since_restore: 1320000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 55\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 5177 s, 55 iter, 1320000 ts, 157 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-05-33\n",
      "  done: false\n",
      "  episode_len_mean: 89.86194029850746\n",
      "  episode_reward_max: 564.0005197006037\n",
      "  episode_reward_mean: 164.19716839014302\n",
      "  episode_reward_min: -356.0518028373577\n",
      "  episodes_this_iter: 268\n",
      "  episodes_total: 14509\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18322.224\n",
      "    load_time_ms: 3.537\n",
      "    num_steps_sampled: 1344000\n",
      "    num_steps_trained: 1344000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.11022312388683e-17\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9957886338233948\n",
      "      kl: 0.006187684368342161\n",
      "      policy_loss: -0.00119754986371845\n",
      "      total_loss: 550.7614135742188\n",
      "      vf_explained_var: 0.8669551014900208\n",
      "      vf_loss: 550.7625732421875\n",
      "    sample_time_ms: 71292.91\n",
      "    update_time_ms: 5.362\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.049292097535755\n",
      "  time_since_restore: 5267.365594148636\n",
      "  time_this_iter_s: 89.40104126930237\n",
      "  time_total_s: 5267.365594148636\n",
      "  timestamp: 1556791533\n",
      "  timesteps_since_restore: 1344000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1344000\n",
      "  training_iteration: 56\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 5267 s, 56 iter, 1344000 ts, 164 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-07-01\n",
      "  done: false\n",
      "  episode_len_mean: 88.4280442804428\n",
      "  episode_reward_max: 560.8253105219962\n",
      "  episode_reward_mean: 149.91440517578516\n",
      "  episode_reward_min: -353.41314689589046\n",
      "  episodes_this_iter: 271\n",
      "  episodes_total: 14780\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18305.928\n",
      "    load_time_ms: 3.552\n",
      "    num_steps_sampled: 1368000\n",
      "    num_steps_trained: 1368000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.55111561943415e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9835023880004883\n",
      "      kl: 0.007206999231129885\n",
      "      policy_loss: -0.0015389600303024054\n",
      "      total_loss: 573.0609741210938\n",
      "      vf_explained_var: 0.8646309971809387\n",
      "      vf_loss: 573.0624389648438\n",
      "    sample_time_ms: 71366.887\n",
      "    update_time_ms: 5.253\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.47860129394628\n",
      "  time_since_restore: 5354.9317507743835\n",
      "  time_this_iter_s: 87.56615662574768\n",
      "  time_total_s: 5354.9317507743835\n",
      "  timestamp: 1556791621\n",
      "  timesteps_since_restore: 1368000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1368000\n",
      "  training_iteration: 57\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 5354 s, 57 iter, 1368000 ts, 150 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-08-28\n",
      "  done: false\n",
      "  episode_len_mean: 89.59925093632958\n",
      "  episode_reward_max: 557.5988522503709\n",
      "  episode_reward_mean: 164.7275672878338\n",
      "  episode_reward_min: -358.5923619832276\n",
      "  episodes_this_iter: 267\n",
      "  episodes_total: 15047\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18328.501\n",
      "    load_time_ms: 3.552\n",
      "    num_steps_sampled: 1392000\n",
      "    num_steps_trained: 1392000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.775557809717075e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9762886762619019\n",
      "      kl: 0.00577919464558363\n",
      "      policy_loss: -0.0012558072339743376\n",
      "      total_loss: 649.36376953125\n",
      "      vf_explained_var: 0.8355695605278015\n",
      "      vf_loss: 649.364990234375\n",
      "    sample_time_ms: 71401.371\n",
      "    update_time_ms: 5.13\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.181891821958445\n",
      "  time_since_restore: 5442.277360916138\n",
      "  time_this_iter_s: 87.34561014175415\n",
      "  time_total_s: 5442.277360916138\n",
      "  timestamp: 1556791708\n",
      "  timesteps_since_restore: 1392000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1392000\n",
      "  training_iteration: 58\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 5442 s, 58 iter, 1392000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-09-56\n",
      "  done: false\n",
      "  episode_len_mean: 83.73776223776224\n",
      "  episode_reward_max: 566.5793515798325\n",
      "  episode_reward_mean: 101.54015815506179\n",
      "  episode_reward_min: -358.5343494328805\n",
      "  episodes_this_iter: 286\n",
      "  episodes_total: 15333\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18349.845\n",
      "    load_time_ms: 3.579\n",
      "    num_steps_sampled: 1416000\n",
      "    num_steps_trained: 1416000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3877789048585376e-18\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9655245542526245\n",
      "      kl: 0.007537169847637415\n",
      "      policy_loss: -0.0017940755933523178\n",
      "      total_loss: 651.650146484375\n",
      "      vf_explained_var: 0.8536314964294434\n",
      "      vf_loss: 651.6519165039062\n",
      "    sample_time_ms: 71486.196\n",
      "    update_time_ms: 5.172\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 25.385039538765447\n",
      "  time_since_restore: 5530.416387796402\n",
      "  time_this_iter_s: 88.13902688026428\n",
      "  time_total_s: 5530.416387796402\n",
      "  timestamp: 1556791796\n",
      "  timesteps_since_restore: 1416000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1416000\n",
      "  training_iteration: 59\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 5530 s, 59 iter, 1416000 ts, 102 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-11-25\n",
      "  done: false\n",
      "  episode_len_mean: 89.57835820895522\n",
      "  episode_reward_max: 572.4919822582117\n",
      "  episode_reward_mean: 179.82705223123395\n",
      "  episode_reward_min: -356.3536294770191\n",
      "  episodes_this_iter: 268\n",
      "  episodes_total: 15601\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18355.978\n",
      "    load_time_ms: 3.592\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.938894524292688e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.961135745048523\n",
      "      kl: 0.007017085328698158\n",
      "      policy_loss: -0.0013136412017047405\n",
      "      total_loss: 597.3241577148438\n",
      "      vf_explained_var: 0.8498907089233398\n",
      "      vf_loss: 597.3255004882812\n",
      "    sample_time_ms: 71611.548\n",
      "    update_time_ms: 5.224\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.9567630578085\n",
      "  time_since_restore: 5618.672118663788\n",
      "  time_this_iter_s: 88.25573086738586\n",
      "  time_total_s: 5618.672118663788\n",
      "  timestamp: 1556791885\n",
      "  timesteps_since_restore: 1440000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 60\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 5618 s, 60 iter, 1440000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-12-54\n",
      "  done: false\n",
      "  episode_len_mean: 91.48669201520913\n",
      "  episode_reward_max: 573.1614023058575\n",
      "  episode_reward_mean: 198.68009349864778\n",
      "  episode_reward_min: -353.34787937350745\n",
      "  episodes_this_iter: 263\n",
      "  episodes_total: 15864\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18396.78\n",
      "    load_time_ms: 3.59\n",
      "    num_steps_sampled: 1464000\n",
      "    num_steps_trained: 1464000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.469447262146344e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9605714678764343\n",
      "      kl: 0.006172244902700186\n",
      "      policy_loss: -0.001009641564451158\n",
      "      total_loss: 679.7939453125\n",
      "      vf_explained_var: 0.8091430068016052\n",
      "      vf_loss: 679.794921875\n",
      "    sample_time_ms: 71821.643\n",
      "    update_time_ms: 5.16\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.67002337466195\n",
      "  time_since_restore: 5707.684215545654\n",
      "  time_this_iter_s: 89.01209688186646\n",
      "  time_total_s: 5707.684215545654\n",
      "  timestamp: 1556791974\n",
      "  timesteps_since_restore: 1464000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1464000\n",
      "  training_iteration: 61\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 5707 s, 61 iter, 1464000 ts, 199 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-14-21\n",
      "  done: false\n",
      "  episode_len_mean: 83.58333333333333\n",
      "  episode_reward_max: 577.6547668333308\n",
      "  episode_reward_mean: 110.8779011130029\n",
      "  episode_reward_min: -359.53762843225564\n",
      "  episodes_this_iter: 288\n",
      "  episodes_total: 16152\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18429.987\n",
      "    load_time_ms: 3.613\n",
      "    num_steps_sampled: 1488000\n",
      "    num_steps_trained: 1488000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.734723631073172e-19\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9405187964439392\n",
      "      kl: 0.005929274950176477\n",
      "      policy_loss: -0.0007654113578610122\n",
      "      total_loss: 701.2249755859375\n",
      "      vf_explained_var: 0.8308115005493164\n",
      "      vf_loss: 701.225830078125\n",
      "    sample_time_ms: 71917.511\n",
      "    update_time_ms: 5.13\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 27.719475278250734\n",
      "  time_since_restore: 5795.32125043869\n",
      "  time_this_iter_s: 87.63703489303589\n",
      "  time_total_s: 5795.32125043869\n",
      "  timestamp: 1556792061\n",
      "  timesteps_since_restore: 1488000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1488000\n",
      "  training_iteration: 62\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 5795 s, 62 iter, 1488000 ts, 111 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-15-49\n",
      "  done: false\n",
      "  episode_len_mean: 86.00716845878136\n",
      "  episode_reward_max: 576.4787449403287\n",
      "  episode_reward_mean: 135.03628023947238\n",
      "  episode_reward_min: -360.52939987804854\n",
      "  episodes_this_iter: 279\n",
      "  episodes_total: 16431\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18457.167\n",
      "    load_time_ms: 3.653\n",
      "    num_steps_sampled: 1512000\n",
      "    num_steps_trained: 1512000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.67361815536586e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9301271438598633\n",
      "      kl: 0.006498674862086773\n",
      "      policy_loss: -0.001202868763357401\n",
      "      total_loss: 589.2416381835938\n",
      "      vf_explained_var: 0.8618839383125305\n",
      "      vf_loss: 589.242919921875\n",
      "    sample_time_ms: 72179.712\n",
      "    update_time_ms: 5.128\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 33.759070059868094\n",
      "  time_since_restore: 5883.202695608139\n",
      "  time_this_iter_s: 87.88144516944885\n",
      "  time_total_s: 5883.202695608139\n",
      "  timestamp: 1556792149\n",
      "  timesteps_since_restore: 1512000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1512000\n",
      "  training_iteration: 63\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 5883 s, 63 iter, 1512000 ts, 135 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-17-18\n",
      "  done: false\n",
      "  episode_len_mean: 86.64855072463769\n",
      "  episode_reward_max: 590.2675622927013\n",
      "  episode_reward_mean: 155.07145281377888\n",
      "  episode_reward_min: -353.20806692427817\n",
      "  episodes_this_iter: 276\n",
      "  episodes_total: 16707\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18294.701\n",
      "    load_time_ms: 3.63\n",
      "    num_steps_sampled: 1536000\n",
      "    num_steps_trained: 1536000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.33680907768293e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9228695631027222\n",
      "      kl: 0.007489761337637901\n",
      "      policy_loss: -0.0016088911797851324\n",
      "      total_loss: 592.8090209960938\n",
      "      vf_explained_var: 0.8573508262634277\n",
      "      vf_loss: 592.8106079101562\n",
      "    sample_time_ms: 72282.296\n",
      "    update_time_ms: 5.34\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.767863203444726\n",
      "  time_since_restore: 5972.132031440735\n",
      "  time_this_iter_s: 88.92933583259583\n",
      "  time_total_s: 5972.132031440735\n",
      "  timestamp: 1556792238\n",
      "  timesteps_since_restore: 1536000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1536000\n",
      "  training_iteration: 64\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 5972 s, 64 iter, 1536000 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-18-46\n",
      "  done: false\n",
      "  episode_len_mean: 88.24908424908425\n",
      "  episode_reward_max: 584.1030206706382\n",
      "  episode_reward_mean: 169.3328401738101\n",
      "  episode_reward_min: -353.9954306368619\n",
      "  episodes_this_iter: 273\n",
      "  episodes_total: 16980\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18240.019\n",
      "    load_time_ms: 3.578\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.168404538841465e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9127272963523865\n",
      "      kl: 0.007129637524485588\n",
      "      policy_loss: -0.0008747383835725486\n",
      "      total_loss: 594.3694458007812\n",
      "      vf_explained_var: 0.8591238856315613\n",
      "      vf_loss: 594.370361328125\n",
      "    sample_time_ms: 69910.689\n",
      "    update_time_ms: 5.339\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.333210043452524\n",
      "  time_since_restore: 6060.172502756119\n",
      "  time_this_iter_s: 88.04047131538391\n",
      "  time_total_s: 6060.172502756119\n",
      "  timestamp: 1556792326\n",
      "  timesteps_since_restore: 1560000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 65\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 5.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 6060 s, 65 iter, 1560000 ts, 169 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-20-17\n",
      "  done: false\n",
      "  episode_len_mean: 85.53928571428571\n",
      "  episode_reward_max: 597.2733717735116\n",
      "  episode_reward_mean: 143.57204275578667\n",
      "  episode_reward_min: -356.73142563469673\n",
      "  episodes_this_iter: 280\n",
      "  episodes_total: 17260\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18653.075\n",
      "    load_time_ms: 3.561\n",
      "    num_steps_sampled: 1584000\n",
      "    num_steps_trained: 1584000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0842022694207325e-20\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9013547897338867\n",
      "      kl: 0.006985373329371214\n",
      "      policy_loss: -0.001549607957713306\n",
      "      total_loss: 565.0505981445312\n",
      "      vf_explained_var: 0.8756411671638489\n",
      "      vf_loss: 565.0521850585938\n",
      "    sample_time_ms: 69658.293\n",
      "    update_time_ms: 5.351\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.89301068894667\n",
      "  time_since_restore: 6151.190114021301\n",
      "  time_this_iter_s: 91.0176112651825\n",
      "  time_total_s: 6151.190114021301\n",
      "  timestamp: 1556792417\n",
      "  timesteps_since_restore: 1584000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1584000\n",
      "  training_iteration: 66\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 6151 s, 66 iter, 1584000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-22-00\n",
      "  done: false\n",
      "  episode_len_mean: 87.03260869565217\n",
      "  episode_reward_max: 589.6764998765361\n",
      "  episode_reward_mean: 150.35138966129117\n",
      "  episode_reward_min: -353.9914819239174\n",
      "  episodes_this_iter: 276\n",
      "  episodes_total: 17536\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18854.333\n",
      "    load_time_ms: 3.594\n",
      "    num_steps_sampled: 1608000\n",
      "    num_steps_trained: 1608000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.421011347103662e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8906652927398682\n",
      "      kl: 0.006997997872531414\n",
      "      policy_loss: -0.0011374547611922026\n",
      "      total_loss: 658.7660522460938\n",
      "      vf_explained_var: 0.8470950722694397\n",
      "      vf_loss: 658.7672729492188\n",
      "    sample_time_ms: 70925.519\n",
      "    update_time_ms: 5.393\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.58784741532278\n",
      "  time_since_restore: 6253.448504447937\n",
      "  time_this_iter_s: 102.25839042663574\n",
      "  time_total_s: 6253.448504447937\n",
      "  timestamp: 1556792520\n",
      "  timesteps_since_restore: 1608000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1608000\n",
      "  training_iteration: 67\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 6253 s, 67 iter, 1608000 ts, 150 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-23-44\n",
      "  done: false\n",
      "  episode_len_mean: 84.13380281690141\n",
      "  episode_reward_max: 590.4614450827455\n",
      "  episode_reward_mean: 115.95855853423701\n",
      "  episode_reward_min: -356.603598275576\n",
      "  episodes_this_iter: 284\n",
      "  episodes_total: 17820\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18879.103\n",
      "    load_time_ms: 3.589\n",
      "    num_steps_sampled: 1632000\n",
      "    num_steps_trained: 1632000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.710505673551831e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8866713047027588\n",
      "      kl: 0.006745505146682262\n",
      "      policy_loss: -0.001119130407460034\n",
      "      total_loss: 671.0729370117188\n",
      "      vf_explained_var: 0.8510835766792297\n",
      "      vf_loss: 671.073974609375\n",
      "    sample_time_ms: 72564.867\n",
      "    update_time_ms: 5.592\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.98963963355925\n",
      "  time_since_restore: 6357.432904481888\n",
      "  time_this_iter_s: 103.9844000339508\n",
      "  time_total_s: 6357.432904481888\n",
      "  timestamp: 1556792624\n",
      "  timesteps_since_restore: 1632000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1632000\n",
      "  training_iteration: 68\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 6357 s, 68 iter, 1632000 ts, 116 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-25-14\n",
      "  done: false\n",
      "  episode_len_mean: 85.50709219858156\n",
      "  episode_reward_max: 586.20576328274\n",
      "  episode_reward_mean: 143.89731349099577\n",
      "  episode_reward_min: -352.2802483433057\n",
      "  episodes_this_iter: 282\n",
      "  episodes_total: 18102\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18966.46\n",
      "    load_time_ms: 3.57\n",
      "    num_steps_sampled: 1656000\n",
      "    num_steps_trained: 1656000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.3552528367759156e-21\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8747344017028809\n",
      "      kl: 0.007607161067426205\n",
      "      policy_loss: -0.0012759644305333495\n",
      "      total_loss: 593.45947265625\n",
      "      vf_explained_var: 0.8689935803413391\n",
      "      vf_loss: 593.460693359375\n",
      "    sample_time_ms: 72689.973\n",
      "    update_time_ms: 5.73\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.97432837274894\n",
      "  time_since_restore: 6447.705313205719\n",
      "  time_this_iter_s: 90.27240872383118\n",
      "  time_total_s: 6447.705313205719\n",
      "  timestamp: 1556792714\n",
      "  timesteps_since_restore: 1656000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1656000\n",
      "  training_iteration: 69\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 6447 s, 69 iter, 1656000 ts, 144 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-27-01\n",
      "  done: false\n",
      "  episode_len_mean: 88.25461254612546\n",
      "  episode_reward_max: 596.6018052326854\n",
      "  episode_reward_mean: 174.3418481482337\n",
      "  episode_reward_min: -354.87072753420875\n",
      "  episodes_this_iter: 271\n",
      "  episodes_total: 18373\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19556.25\n",
      "    load_time_ms: 3.631\n",
      "    num_steps_sampled: 1680000\n",
      "    num_steps_trained: 1680000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.776264183879578e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8601659536361694\n",
      "      kl: 0.00700583728030324\n",
      "      policy_loss: -0.0014785506064072251\n",
      "      total_loss: 615.4703369140625\n",
      "      vf_explained_var: 0.8637412190437317\n",
      "      vf_loss: 615.4718627929688\n",
      "    sample_time_ms: 73913.58\n",
      "    update_time_ms: 5.763\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.58546203705843\n",
      "  time_since_restore: 6554.1011526584625\n",
      "  time_this_iter_s: 106.39583945274353\n",
      "  time_total_s: 6554.1011526584625\n",
      "  timestamp: 1556792821\n",
      "  timesteps_since_restore: 1680000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1680000\n",
      "  training_iteration: 70\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 6554 s, 70 iter, 1680000 ts, 174 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-28-37\n",
      "  done: false\n",
      "  episode_len_mean: 85.60992907801419\n",
      "  episode_reward_max: 594.4771090232072\n",
      "  episode_reward_mean: 138.15384887655313\n",
      "  episode_reward_min: -358.6857032876801\n",
      "  episodes_this_iter: 282\n",
      "  episodes_total: 18655\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20133.831\n",
      "    load_time_ms: 3.647\n",
      "    num_steps_sampled: 1704000\n",
      "    num_steps_trained: 1704000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.388132091939789e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8573325276374817\n",
      "      kl: 0.0076169949024915695\n",
      "      policy_loss: -0.0012218402698636055\n",
      "      total_loss: 747.0349731445312\n",
      "      vf_explained_var: 0.835564136505127\n",
      "      vf_loss: 747.0361938476562\n",
      "    sample_time_ms: 74098.15\n",
      "    update_time_ms: 5.84\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.53846221913828\n",
      "  time_since_restore: 6650.753648757935\n",
      "  time_this_iter_s: 96.65249609947205\n",
      "  time_total_s: 6650.753648757935\n",
      "  timestamp: 1556792917\n",
      "  timesteps_since_restore: 1704000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1704000\n",
      "  training_iteration: 71\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 6650 s, 71 iter, 1704000 ts, 138 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-30-16\n",
      "  done: false\n",
      "  episode_len_mean: 86.13718411552347\n",
      "  episode_reward_max: 596.3323714235129\n",
      "  episode_reward_mean: 142.325333492792\n",
      "  episode_reward_min: -356.3762550431547\n",
      "  episodes_this_iter: 277\n",
      "  episodes_total: 18932\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20191.94\n",
      "    load_time_ms: 3.678\n",
      "    num_steps_sampled: 1728000\n",
      "    num_steps_trained: 1728000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.6940660459698945e-22\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8460237383842468\n",
      "      kl: 0.007563835009932518\n",
      "      policy_loss: -0.0012342915870249271\n",
      "      total_loss: 721.8319702148438\n",
      "      vf_explained_var: 0.8413246870040894\n",
      "      vf_loss: 721.833251953125\n",
      "    sample_time_ms: 75169.998\n",
      "    update_time_ms: 5.966\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 35.581333373198\n",
      "  time_since_restore: 6749.69021487236\n",
      "  time_this_iter_s: 98.93656611442566\n",
      "  time_total_s: 6749.69021487236\n",
      "  timestamp: 1556793016\n",
      "  timesteps_since_restore: 1728000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1728000\n",
      "  training_iteration: 72\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 6749 s, 72 iter, 1728000 ts, 142 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-31-45\n",
      "  done: false\n",
      "  episode_len_mean: 92.16475095785441\n",
      "  episode_reward_max: 595.2840393236183\n",
      "  episode_reward_mean: 232.04004666212057\n",
      "  episode_reward_min: -358.5934931405085\n",
      "  episodes_this_iter: 261\n",
      "  episodes_total: 19193\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20191.821\n",
      "    load_time_ms: 3.661\n",
      "    num_steps_sampled: 1752000\n",
      "    num_steps_trained: 1752000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.470330229849472e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8460783958435059\n",
      "      kl: 0.006994729395955801\n",
      "      policy_loss: -0.001325528253801167\n",
      "      total_loss: 526.6968383789062\n",
      "      vf_explained_var: 0.8777021765708923\n",
      "      vf_loss: 526.6981811523438\n",
      "    sample_time_ms: 75288.224\n",
      "    update_time_ms: 5.925\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.01001166553014\n",
      "  time_since_restore: 6838.7526795864105\n",
      "  time_this_iter_s: 89.0624647140503\n",
      "  time_total_s: 6838.7526795864105\n",
      "  timestamp: 1556793105\n",
      "  timesteps_since_restore: 1752000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1752000\n",
      "  training_iteration: 73\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 6838 s, 73 iter, 1752000 ts, 232 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-33-12\n",
      "  done: false\n",
      "  episode_len_mean: 86.02508960573476\n",
      "  episode_reward_max: 592.0361953914461\n",
      "  episode_reward_mean: 154.62698029177426\n",
      "  episode_reward_min: -355.7699870570072\n",
      "  episodes_this_iter: 279\n",
      "  episodes_total: 19472\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20175.09\n",
      "    load_time_ms: 3.688\n",
      "    num_steps_sampled: 1776000\n",
      "    num_steps_trained: 1776000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.235165114924736e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8306723833084106\n",
      "      kl: 0.008012434467673302\n",
      "      policy_loss: -0.0017708073137328029\n",
      "      total_loss: 602.228759765625\n",
      "      vf_explained_var: 0.875216007232666\n",
      "      vf_loss: 602.2305908203125\n",
      "    sample_time_ms: 75082.533\n",
      "    update_time_ms: 5.738\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.656745072943565\n",
      "  time_since_restore: 6925.447558164597\n",
      "  time_this_iter_s: 86.69487857818604\n",
      "  time_total_s: 6925.447558164597\n",
      "  timestamp: 1556793192\n",
      "  timesteps_since_restore: 1776000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1776000\n",
      "  training_iteration: 74\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 6925 s, 74 iter, 1776000 ts, 155 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-34-40\n",
      "  done: false\n",
      "  episode_len_mean: 85.65357142857142\n",
      "  episode_reward_max: 607.794700492739\n",
      "  episode_reward_mean: 157.79665366375758\n",
      "  episode_reward_min: -357.35065632285114\n",
      "  episodes_this_iter: 280\n",
      "  episodes_total: 19752\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20176.204\n",
      "    load_time_ms: 3.708\n",
      "    num_steps_sampled: 1800000\n",
      "    num_steps_trained: 1800000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.117582557462368e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8145071864128113\n",
      "      kl: 0.00764061463996768\n",
      "      policy_loss: -0.0016686060698702931\n",
      "      total_loss: 682.7681884765625\n",
      "      vf_explained_var: 0.8516319990158081\n",
      "      vf_loss: 682.7698364257812\n",
      "    sample_time_ms: 75027.399\n",
      "    update_time_ms: 5.603\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 39.44916341593939\n",
      "  time_since_restore: 7012.941052675247\n",
      "  time_this_iter_s: 87.49349451065063\n",
      "  time_total_s: 7012.941052675247\n",
      "  timestamp: 1556793280\n",
      "  timesteps_since_restore: 1800000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1800000\n",
      "  training_iteration: 75\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 7012 s, 75 iter, 1800000 ts, 158 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-36-08\n",
      "  done: false\n",
      "  episode_len_mean: 87.98168498168498\n",
      "  episode_reward_max: 601.2702278501584\n",
      "  episode_reward_mean: 179.23593226537741\n",
      "  episode_reward_min: -356.9354981328758\n",
      "  episodes_this_iter: 273\n",
      "  episodes_total: 20025\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19778.463\n",
      "    load_time_ms: 3.736\n",
      "    num_steps_sampled: 1824000\n",
      "    num_steps_trained: 1824000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.058791278731184e-23\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8111012578010559\n",
      "      kl: 0.007781517691910267\n",
      "      policy_loss: -0.0016814899863675237\n",
      "      total_loss: 625.9966430664062\n",
      "      vf_explained_var: 0.8627121448516846\n",
      "      vf_loss: 625.9983520507812\n",
      "    sample_time_ms: 75159.181\n",
      "    update_time_ms: 5.65\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.80898306634436\n",
      "  time_since_restore: 7101.288308620453\n",
      "  time_this_iter_s: 88.34725594520569\n",
      "  time_total_s: 7101.288308620453\n",
      "  timestamp: 1556793368\n",
      "  timesteps_since_restore: 1824000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1824000\n",
      "  training_iteration: 76\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 7101 s, 76 iter, 1824000 ts, 179 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-37-36\n",
      "  done: false\n",
      "  episode_len_mean: 84.06315789473685\n",
      "  episode_reward_max: 603.9137109201827\n",
      "  episode_reward_mean: 130.19107802066264\n",
      "  episode_reward_min: -350.2070850393542\n",
      "  episodes_this_iter: 285\n",
      "  episodes_total: 20310\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19576.046\n",
      "    load_time_ms: 3.707\n",
      "    num_steps_sampled: 1848000\n",
      "    num_steps_trained: 1848000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.29395639365592e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7983914613723755\n",
      "      kl: 0.0064638229086995125\n",
      "      policy_loss: -0.001161759253591299\n",
      "      total_loss: 829.5853881835938\n",
      "      vf_explained_var: 0.8245661854743958\n",
      "      vf_loss: 829.5866088867188\n",
      "    sample_time_ms: 73901.213\n",
      "    update_time_ms: 5.673\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 32.54776950516566\n",
      "  time_since_restore: 7188.934689998627\n",
      "  time_this_iter_s: 87.64638137817383\n",
      "  time_total_s: 7188.934689998627\n",
      "  timestamp: 1556793456\n",
      "  timesteps_since_restore: 1848000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1848000\n",
      "  training_iteration: 77\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 7188 s, 77 iter, 1848000 ts, 130 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-39-05\n",
      "  done: false\n",
      "  episode_len_mean: 86.83393501805054\n",
      "  episode_reward_max: 601.7783095522884\n",
      "  episode_reward_mean: 160.36737324594355\n",
      "  episode_reward_min: -354.6988876172169\n",
      "  episodes_this_iter: 277\n",
      "  episodes_total: 20587\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19550.837\n",
      "    load_time_ms: 3.71\n",
      "    num_steps_sampled: 1872000\n",
      "    num_steps_trained: 1872000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.64697819682796e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.8025376200675964\n",
      "      kl: 0.00795022863894701\n",
      "      policy_loss: -0.0014998666010797024\n",
      "      total_loss: 626.159912109375\n",
      "      vf_explained_var: 0.8695422410964966\n",
      "      vf_loss: 626.1614379882812\n",
      "    sample_time_ms: 72442.955\n",
      "    update_time_ms: 5.697\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.09184331148589\n",
      "  time_since_restore: 7278.0891289711\n",
      "  time_this_iter_s: 89.15443897247314\n",
      "  time_total_s: 7278.0891289711\n",
      "  timestamp: 1556793545\n",
      "  timesteps_since_restore: 1872000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1872000\n",
      "  training_iteration: 78\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 7278 s, 78 iter, 1872000 ts, 160 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-40-34\n",
      "  done: false\n",
      "  episode_len_mean: 88.3529411764706\n",
      "  episode_reward_max: 605.747523290326\n",
      "  episode_reward_mean: 178.10463488659147\n",
      "  episode_reward_min: -354.194256124348\n",
      "  episodes_this_iter: 272\n",
      "  episodes_total: 20859\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19468.859\n",
      "    load_time_ms: 3.735\n",
      "    num_steps_sampled: 1896000\n",
      "    num_steps_trained: 1896000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.32348909841398e-24\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7885347604751587\n",
      "      kl: 0.008221300318837166\n",
      "      policy_loss: -0.0014507630839943886\n",
      "      total_loss: 591.717529296875\n",
      "      vf_explained_var: 0.8741090297698975\n",
      "      vf_loss: 591.718994140625\n",
      "    sample_time_ms: 72370.438\n",
      "    update_time_ms: 5.586\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.52615872164785\n",
      "  time_since_restore: 7366.814057826996\n",
      "  time_this_iter_s: 88.724928855896\n",
      "  time_total_s: 7366.814057826996\n",
      "  timestamp: 1556793634\n",
      "  timesteps_since_restore: 1896000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1896000\n",
      "  training_iteration: 79\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 7366 s, 79 iter, 1896000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-42-02\n",
      "  done: false\n",
      "  episode_len_mean: 87.72893772893772\n",
      "  episode_reward_max: 609.4429999162332\n",
      "  episode_reward_mean: 180.39412089381284\n",
      "  episode_reward_min: -353.3745732902748\n",
      "  episodes_this_iter: 273\n",
      "  episodes_total: 21132\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18890.676\n",
      "    load_time_ms: 3.687\n",
      "    num_steps_sampled: 1920000\n",
      "    num_steps_trained: 1920000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.6174454920699e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7772425413131714\n",
      "      kl: 0.0068069566041231155\n",
      "      policy_loss: -0.0009773438796401024\n",
      "      total_loss: 652.9550170898438\n",
      "      vf_explained_var: 0.8597567677497864\n",
      "      vf_loss: 652.9559936523438\n",
      "    sample_time_ms: 71134.619\n",
      "    update_time_ms: 5.497\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.09853022345321\n",
      "  time_since_restore: 7455.062151670456\n",
      "  time_this_iter_s: 88.24809384346008\n",
      "  time_total_s: 7455.062151670456\n",
      "  timestamp: 1556793722\n",
      "  timesteps_since_restore: 1920000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1920000\n",
      "  training_iteration: 80\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 7455 s, 80 iter, 1920000 ts, 180 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-43-30\n",
      "  done: false\n",
      "  episode_len_mean: 83.96153846153847\n",
      "  episode_reward_max: 606.475317368329\n",
      "  episode_reward_mean: 137.76076925012276\n",
      "  episode_reward_min: -356.4657869233366\n",
      "  episodes_this_iter: 286\n",
      "  episodes_total: 21418\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18302.122\n",
      "    load_time_ms: 3.746\n",
      "    num_steps_sampled: 1944000\n",
      "    num_steps_trained: 1944000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.30872274603495e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.768968939781189\n",
      "      kl: 0.007759647443890572\n",
      "      policy_loss: -0.001554785412736237\n",
      "      total_loss: 733.7655639648438\n",
      "      vf_explained_var: 0.8472711443901062\n",
      "      vf_loss: 733.7671508789062\n",
      "    sample_time_ms: 70866.439\n",
      "    update_time_ms: 5.495\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 34.44019231253069\n",
      "  time_since_restore: 7543.132675409317\n",
      "  time_this_iter_s: 88.07052373886108\n",
      "  time_total_s: 7543.132675409317\n",
      "  timestamp: 1556793810\n",
      "  timesteps_since_restore: 1944000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1944000\n",
      "  training_iteration: 81\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 7543 s, 81 iter, 1944000 ts, 138 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-44-59\n",
      "  done: false\n",
      "  episode_len_mean: 87.7956204379562\n",
      "  episode_reward_max: 602.585080027107\n",
      "  episode_reward_mean: 182.14822703420853\n",
      "  episode_reward_min: -351.73330136006325\n",
      "  episodes_this_iter: 274\n",
      "  episodes_total: 21692\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18243.113\n",
      "    load_time_ms: 3.708\n",
      "    num_steps_sampled: 1968000\n",
      "    num_steps_trained: 1968000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.654361373017475e-25\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7739547491073608\n",
      "      kl: 0.007661010604351759\n",
      "      policy_loss: -0.001375157618895173\n",
      "      total_loss: 686.9252319335938\n",
      "      vf_explained_var: 0.843542218208313\n",
      "      vf_loss: 686.9265747070312\n",
      "    sample_time_ms: 69888.329\n",
      "    update_time_ms: 5.295\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.53705675855213\n",
      "  time_since_restore: 7631.696355819702\n",
      "  time_this_iter_s: 88.56368041038513\n",
      "  time_total_s: 7631.696355819702\n",
      "  timestamp: 1556793899\n",
      "  timesteps_since_restore: 1968000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1968000\n",
      "  training_iteration: 82\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 7631 s, 82 iter, 1968000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-46-26\n",
      "  done: false\n",
      "  episode_len_mean: 87.83088235294117\n",
      "  episode_reward_max: 609.9127199055969\n",
      "  episode_reward_mean: 182.69472809351458\n",
      "  episode_reward_min: -358.0130672277744\n",
      "  episodes_this_iter: 272\n",
      "  episodes_total: 21964\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18237.116\n",
      "    load_time_ms: 3.709\n",
      "    num_steps_sampled: 1992000\n",
      "    num_steps_trained: 1992000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.271806865087375e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7669594883918762\n",
      "      kl: 0.006991678848862648\n",
      "      policy_loss: -0.0012815275695174932\n",
      "      total_loss: 678.0313110351562\n",
      "      vf_explained_var: 0.8550846576690674\n",
      "      vf_loss: 678.0325927734375\n",
      "    sample_time_ms: 69763.038\n",
      "    update_time_ms: 5.302\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.673682023378646\n",
      "  time_since_restore: 7719.447007656097\n",
      "  time_this_iter_s: 87.75065183639526\n",
      "  time_total_s: 7719.447007656097\n",
      "  timestamp: 1556793986\n",
      "  timesteps_since_restore: 1992000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 1992000\n",
      "  training_iteration: 83\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 7719 s, 83 iter, 1992000 ts, 183 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-47-54\n",
      "  done: false\n",
      "  episode_len_mean: 88.87453874538745\n",
      "  episode_reward_max: 609.116912479661\n",
      "  episode_reward_mean: 183.87611825442787\n",
      "  episode_reward_min: -345.84344013609405\n",
      "  episodes_this_iter: 271\n",
      "  episodes_total: 22235\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18227.918\n",
      "    load_time_ms: 3.691\n",
      "    num_steps_sampled: 2016000\n",
      "    num_steps_trained: 2016000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.1359034325436877e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7529861927032471\n",
      "      kl: 0.00773873645812273\n",
      "      policy_loss: -0.000712837849278003\n",
      "      total_loss: 594.25244140625\n",
      "      vf_explained_var: 0.8737812638282776\n",
      "      vf_loss: 594.2531127929688\n",
      "    sample_time_ms: 69894.501\n",
      "    update_time_ms: 5.402\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.96902956360697\n",
      "  time_since_restore: 7807.362741947174\n",
      "  time_this_iter_s: 87.91573429107666\n",
      "  time_total_s: 7807.362741947174\n",
      "  timestamp: 1556794074\n",
      "  timesteps_since_restore: 2016000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2016000\n",
      "  training_iteration: 84\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 7807 s, 84 iter, 2016000 ts, 184 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-49-22\n",
      "  done: false\n",
      "  episode_len_mean: 92.12307692307692\n",
      "  episode_reward_max: 606.1667832002428\n",
      "  episode_reward_mean: 232.45217404131535\n",
      "  episode_reward_min: -355.0155676460302\n",
      "  episodes_this_iter: 260\n",
      "  episodes_total: 22495\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18232.738\n",
      "    load_time_ms: 3.68\n",
      "    num_steps_sampled: 2040000\n",
      "    num_steps_trained: 2040000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.0679517162718438e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.746557354927063\n",
      "      kl: 0.007124681957066059\n",
      "      policy_loss: -0.0009948303923010826\n",
      "      total_loss: 645.8131713867188\n",
      "      vf_explained_var: 0.8551573753356934\n",
      "      vf_loss: 645.8141479492188\n",
      "    sample_time_ms: 69893.293\n",
      "    update_time_ms: 5.36\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.113043510328836\n",
      "  time_since_restore: 7894.8929126262665\n",
      "  time_this_iter_s: 87.53017067909241\n",
      "  time_total_s: 7894.8929126262665\n",
      "  timestamp: 1556794162\n",
      "  timesteps_since_restore: 2040000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2040000\n",
      "  training_iteration: 85\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 7894 s, 85 iter, 2040000 ts, 232 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-50-51\n",
      "  done: false\n",
      "  episode_len_mean: 86.64492753623189\n",
      "  episode_reward_max: 606.0603881009799\n",
      "  episode_reward_mean: 165.44371353157877\n",
      "  episode_reward_min: -359.4515197985255\n",
      "  episodes_this_iter: 276\n",
      "  episodes_total: 22771\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18227.005\n",
      "    load_time_ms: 3.708\n",
      "    num_steps_sampled: 2064000\n",
      "    num_steps_trained: 2064000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0339758581359219e-26\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7354269027709961\n",
      "      kl: 0.008121052756905556\n",
      "      policy_loss: -0.0012468964559957385\n",
      "      total_loss: 658.1314697265625\n",
      "      vf_explained_var: 0.8644453287124634\n",
      "      vf_loss: 658.1327514648438\n",
      "    sample_time_ms: 69991.783\n",
      "    update_time_ms: 5.336\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.36092838289469\n",
      "  time_since_restore: 7984.165463447571\n",
      "  time_this_iter_s: 89.27255082130432\n",
      "  time_total_s: 7984.165463447571\n",
      "  timestamp: 1556794251\n",
      "  timesteps_since_restore: 2064000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2064000\n",
      "  training_iteration: 86\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 7984 s, 86 iter, 2064000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-52-19\n",
      "  done: false\n",
      "  episode_len_mean: 84.93639575971731\n",
      "  episode_reward_max: 603.3257845487536\n",
      "  episode_reward_mean: 148.56128387636662\n",
      "  episode_reward_min: -353.4809952412672\n",
      "  episodes_this_iter: 283\n",
      "  episodes_total: 23054\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18222.528\n",
      "    load_time_ms: 3.696\n",
      "    num_steps_sampled: 2088000\n",
      "    num_steps_trained: 2088000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.1698792906796096e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7309589982032776\n",
      "      kl: 0.007723192684352398\n",
      "      policy_loss: -0.0012799017131328583\n",
      "      total_loss: 676.117919921875\n",
      "      vf_explained_var: 0.8709075450897217\n",
      "      vf_loss: 676.1192626953125\n",
      "    sample_time_ms: 70047.834\n",
      "    update_time_ms: 5.315\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 37.140320969091654\n",
      "  time_since_restore: 8072.332322835922\n",
      "  time_this_iter_s: 88.16685938835144\n",
      "  time_total_s: 8072.332322835922\n",
      "  timestamp: 1556794339\n",
      "  timesteps_since_restore: 2088000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2088000\n",
      "  training_iteration: 87\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 8072 s, 87 iter, 2088000 ts, 149 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-53-47\n",
      "  done: false\n",
      "  episode_len_mean: 81.9795918367347\n",
      "  episode_reward_max: 607.8535976044473\n",
      "  episode_reward_mean: 115.06395252975283\n",
      "  episode_reward_min: -353.66131395245475\n",
      "  episodes_this_iter: 294\n",
      "  episodes_total: 23348\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18224.39\n",
      "    load_time_ms: 3.689\n",
      "    num_steps_sampled: 2112000\n",
      "    num_steps_trained: 2112000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.5849396453398048e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7184059023857117\n",
      "      kl: 0.008777911774814129\n",
      "      policy_loss: -0.001307475147768855\n",
      "      total_loss: 827.1179809570312\n",
      "      vf_explained_var: 0.8439996838569641\n",
      "      vf_loss: 827.1193237304688\n",
      "    sample_time_ms: 69875.332\n",
      "    update_time_ms: 5.034\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 28.765988132438206\n",
      "  time_since_restore: 8159.772099018097\n",
      "  time_this_iter_s: 87.43977618217468\n",
      "  time_total_s: 8159.772099018097\n",
      "  timestamp: 1556794427\n",
      "  timesteps_since_restore: 2112000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2112000\n",
      "  training_iteration: 88\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 8159 s, 88 iter, 2112000 ts, 115 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-55-15\n",
      "  done: false\n",
      "  episode_len_mean: 86.44404332129965\n",
      "  episode_reward_max: 601.0492924396605\n",
      "  episode_reward_mean: 168.51899698223167\n",
      "  episode_reward_min: -350.1232322413251\n",
      "  episodes_this_iter: 277\n",
      "  episodes_total: 23625\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18208.499\n",
      "    load_time_ms: 3.686\n",
      "    num_steps_sampled: 2136000\n",
      "    num_steps_trained: 2136000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2924698226699024e-27\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7213522791862488\n",
      "      kl: 0.007180170621722937\n",
      "      policy_loss: -0.0014636125415563583\n",
      "      total_loss: 781.5006713867188\n",
      "      vf_explained_var: 0.8327621817588806\n",
      "      vf_loss: 781.5020751953125\n",
      "    sample_time_ms: 69838.315\n",
      "    update_time_ms: 5.486\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.12974924555792\n",
      "  time_since_restore: 8247.970981121063\n",
      "  time_this_iter_s: 88.19888210296631\n",
      "  time_total_s: 8247.970981121063\n",
      "  timestamp: 1556794515\n",
      "  timesteps_since_restore: 2136000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2136000\n",
      "  training_iteration: 89\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 8247 s, 89 iter, 2136000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-56-45\n",
      "  done: false\n",
      "  episode_len_mean: 87.21376811594203\n",
      "  episode_reward_max: 604.2516275207314\n",
      "  episode_reward_mean: 176.14495886217372\n",
      "  episode_reward_min: -357.4879716268856\n",
      "  episodes_this_iter: 276\n",
      "  episodes_total: 23901\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18209.044\n",
      "    load_time_ms: 3.71\n",
      "    num_steps_sampled: 2160000\n",
      "    num_steps_trained: 2160000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.462349113349512e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7218334674835205\n",
      "      kl: 0.008390763774514198\n",
      "      policy_loss: -0.0014101715059950948\n",
      "      total_loss: 642.243896484375\n",
      "      vf_explained_var: 0.8712470531463623\n",
      "      vf_loss: 642.2453002929688\n",
      "    sample_time_ms: 69984.84\n",
      "    update_time_ms: 5.375\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.03623971554343\n",
      "  time_since_restore: 8337.68832039833\n",
      "  time_this_iter_s: 89.71733927726746\n",
      "  time_total_s: 8337.68832039833\n",
      "  timestamp: 1556794605\n",
      "  timesteps_since_restore: 2160000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2160000\n",
      "  training_iteration: 90\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 8337 s, 90 iter, 2160000 ts, 176 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-58-17\n",
      "  done: false\n",
      "  episode_len_mean: 86.58333333333333\n",
      "  episode_reward_max: 594.2975180776131\n",
      "  episode_reward_mean: 161.9718854244423\n",
      "  episode_reward_min: -358.7369465133188\n",
      "  episodes_this_iter: 276\n",
      "  episodes_total: 24177\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18281.716\n",
      "    load_time_ms: 3.729\n",
      "    num_steps_sampled: 2184000\n",
      "    num_steps_trained: 2184000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.231174556674756e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7157254815101624\n",
      "      kl: 0.00670913839712739\n",
      "      policy_loss: -0.0005213368567638099\n",
      "      total_loss: 686.2166137695312\n",
      "      vf_explained_var: 0.8609910607337952\n",
      "      vf_loss: 686.2171020507812\n",
      "    sample_time_ms: 70278.559\n",
      "    update_time_ms: 5.287\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 40.49297135611057\n",
      "  time_since_restore: 8429.436304807663\n",
      "  time_this_iter_s: 91.74798440933228\n",
      "  time_total_s: 8429.436304807663\n",
      "  timestamp: 1556794697\n",
      "  timesteps_since_restore: 2184000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2184000\n",
      "  training_iteration: 91\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 8429 s, 91 iter, 2184000 ts, 162 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_12-59-58\n",
      "  done: false\n",
      "  episode_len_mean: 86.96389891696751\n",
      "  episode_reward_max: 602.3800260805892\n",
      "  episode_reward_mean: 172.27498007912203\n",
      "  episode_reward_min: -349.2490721910867\n",
      "  episodes_this_iter: 277\n",
      "  episodes_total: 24454\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18491.162\n",
      "    load_time_ms: 3.788\n",
      "    num_steps_sampled: 2208000\n",
      "    num_steps_trained: 2208000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.615587278337378e-28\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7085586190223694\n",
      "      kl: 0.00820019468665123\n",
      "      policy_loss: -0.0013954400783404708\n",
      "      total_loss: 749.1812744140625\n",
      "      vf_explained_var: 0.8475244045257568\n",
      "      vf_loss: 749.1826782226562\n",
      "    sample_time_ms: 71377.159\n",
      "    update_time_ms: 5.321\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.06874501978051\n",
      "  time_since_restore: 8531.083335399628\n",
      "  time_this_iter_s: 101.64703059196472\n",
      "  time_total_s: 8531.083335399628\n",
      "  timestamp: 1556794798\n",
      "  timesteps_since_restore: 2208000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2208000\n",
      "  training_iteration: 92\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 8531 s, 92 iter, 2208000 ts, 172 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-01-28\n",
      "  done: false\n",
      "  episode_len_mean: 91.29277566539923\n",
      "  episode_reward_max: 606.420245488096\n",
      "  episode_reward_mean: 213.55639527515433\n",
      "  episode_reward_min: -354.4966394790729\n",
      "  episodes_this_iter: 263\n",
      "  episodes_total: 24717\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18501.62\n",
      "    load_time_ms: 3.813\n",
      "    num_steps_sampled: 2232000\n",
      "    num_steps_trained: 2232000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 8.07793639168689e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.7037172913551331\n",
      "      kl: 0.0077333273366093636\n",
      "      policy_loss: -0.0008396441116929054\n",
      "      total_loss: 661.148681640625\n",
      "      vf_explained_var: 0.8612175583839417\n",
      "      vf_loss: 661.1495361328125\n",
      "    sample_time_ms: 71499.913\n",
      "    update_time_ms: 5.42\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.38909881878858\n",
      "  time_since_restore: 8620.163403987885\n",
      "  time_this_iter_s: 89.08006858825684\n",
      "  time_total_s: 8620.163403987885\n",
      "  timestamp: 1556794888\n",
      "  timesteps_since_restore: 2232000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2232000\n",
      "  training_iteration: 93\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 8620 s, 93 iter, 2232000 ts, 214 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-02-57\n",
      "  done: false\n",
      "  episode_len_mean: 87.33090909090909\n",
      "  episode_reward_max: 602.067230282257\n",
      "  episode_reward_mean: 178.24238886820206\n",
      "  episode_reward_min: -357.3984137124486\n",
      "  episodes_this_iter: 275\n",
      "  episodes_total: 24992\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18501.322\n",
      "    load_time_ms: 3.846\n",
      "    num_steps_sampled: 2256000\n",
      "    num_steps_trained: 2256000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.038968195843445e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6888623833656311\n",
      "      kl: 0.00913595873862505\n",
      "      policy_loss: -0.001553497975692153\n",
      "      total_loss: 600.4778442382812\n",
      "      vf_explained_var: 0.8772358298301697\n",
      "      vf_loss: 600.4793090820312\n",
      "    sample_time_ms: 71600.261\n",
      "    update_time_ms: 5.455\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.56059721705052\n",
      "  time_since_restore: 8709.081394195557\n",
      "  time_this_iter_s: 88.91799020767212\n",
      "  time_total_s: 8709.081394195557\n",
      "  timestamp: 1556794977\n",
      "  timesteps_since_restore: 2256000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2256000\n",
      "  training_iteration: 94\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 8709 s, 94 iter, 2256000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-04-26\n",
      "  done: false\n",
      "  episode_len_mean: 89.51310861423221\n",
      "  episode_reward_max: 607.4282969940099\n",
      "  episode_reward_mean: 212.6240188689947\n",
      "  episode_reward_min: -354.7006883483781\n",
      "  episodes_this_iter: 267\n",
      "  episodes_total: 25259\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18494.621\n",
      "    load_time_ms: 3.878\n",
      "    num_steps_sampled: 2280000\n",
      "    num_steps_trained: 2280000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.0194840979217225e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6753914952278137\n",
      "      kl: 0.008387216366827488\n",
      "      policy_loss: -0.0010500740027055144\n",
      "      total_loss: 481.13128662109375\n",
      "      vf_explained_var: 0.898878812789917\n",
      "      vf_loss: 481.13232421875\n",
      "    sample_time_ms: 71753.81\n",
      "    update_time_ms: 5.548\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.15600471724868\n",
      "  time_since_restore: 8798.080915927887\n",
      "  time_this_iter_s: 88.99952173233032\n",
      "  time_total_s: 8798.080915927887\n",
      "  timestamp: 1556795066\n",
      "  timesteps_since_restore: 2280000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2280000\n",
      "  training_iteration: 95\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 8798 s, 95 iter, 2280000 ts, 213 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-05-55\n",
      "  done: false\n",
      "  episode_len_mean: 89.08518518518518\n",
      "  episode_reward_max: 611.6328412495265\n",
      "  episode_reward_mean: 195.75326261833186\n",
      "  episode_reward_min: -356.40458585104614\n",
      "  episodes_this_iter: 270\n",
      "  episodes_total: 25529\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18532.148\n",
      "    load_time_ms: 3.853\n",
      "    num_steps_sampled: 2304000\n",
      "    num_steps_trained: 2304000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.0097420489608613e-29\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6735079288482666\n",
      "      kl: 0.008521098643541336\n",
      "      policy_loss: -0.0009490254451520741\n",
      "      total_loss: 676.1900634765625\n",
      "      vf_explained_var: 0.8600991368293762\n",
      "      vf_loss: 676.1909790039062\n",
      "    sample_time_ms: 71758.935\n",
      "    update_time_ms: 5.573\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.93831565458297\n",
      "  time_since_restore: 8887.781048297882\n",
      "  time_this_iter_s: 89.70013236999512\n",
      "  time_total_s: 8887.781048297882\n",
      "  timestamp: 1556795155\n",
      "  timesteps_since_restore: 2304000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2304000\n",
      "  training_iteration: 96\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 8887 s, 96 iter, 2304000 ts, 196 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-07-23\n",
      "  done: false\n",
      "  episode_len_mean: 93.45703125\n",
      "  episode_reward_max: 607.0012685814515\n",
      "  episode_reward_mean: 262.0429533031529\n",
      "  episode_reward_min: -353.4855188690782\n",
      "  episodes_this_iter: 256\n",
      "  episodes_total: 25785\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18540.48\n",
      "    load_time_ms: 3.873\n",
      "    num_steps_sampled: 2328000\n",
      "    num_steps_trained: 2328000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 5.048710244804306e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6729711890220642\n",
      "      kl: 0.00768668157979846\n",
      "      policy_loss: -0.0009486039634793997\n",
      "      total_loss: 644.9591064453125\n",
      "      vf_explained_var: 0.8494936227798462\n",
      "      vf_loss: 644.9600830078125\n",
      "    sample_time_ms: 71664.451\n",
      "    update_time_ms: 5.619\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 65.51073832578822\n",
      "  time_since_restore: 8975.087189435959\n",
      "  time_this_iter_s: 87.30614113807678\n",
      "  time_total_s: 8975.087189435959\n",
      "  timestamp: 1556795243\n",
      "  timesteps_since_restore: 2328000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2328000\n",
      "  training_iteration: 97\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 8975 s, 97 iter, 2328000 ts, 262 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-08-50\n",
      "  done: false\n",
      "  episode_len_mean: 90.00373134328358\n",
      "  episode_reward_max: 611.1091635983183\n",
      "  episode_reward_mean: 220.04463965922642\n",
      "  episode_reward_min: -356.0904044531231\n",
      "  episodes_this_iter: 268\n",
      "  episodes_total: 26053\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18529.198\n",
      "    load_time_ms: 3.885\n",
      "    num_steps_sampled: 2352000\n",
      "    num_steps_trained: 2352000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.524355122402153e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6758105158805847\n",
      "      kl: 0.006733150687068701\n",
      "      policy_loss: -0.0008768645348027349\n",
      "      total_loss: 642.0601196289062\n",
      "      vf_explained_var: 0.8600071668624878\n",
      "      vf_loss: 642.0609741210938\n",
      "    sample_time_ms: 71675.543\n",
      "    update_time_ms: 5.658\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.011159914806605\n",
      "  time_since_restore: 9062.52914094925\n",
      "  time_this_iter_s: 87.4419515132904\n",
      "  time_total_s: 9062.52914094925\n",
      "  timestamp: 1556795330\n",
      "  timesteps_since_restore: 2352000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2352000\n",
      "  training_iteration: 98\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 9062 s, 98 iter, 2352000 ts, 220 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-10-19\n",
      "  done: false\n",
      "  episode_len_mean: 86.89492753623189\n",
      "  episode_reward_max: 612.1446614736902\n",
      "  episode_reward_mean: 183.84050650356252\n",
      "  episode_reward_min: -350.8423403858942\n",
      "  episodes_this_iter: 276\n",
      "  episodes_total: 26329\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18528.873\n",
      "    load_time_ms: 3.865\n",
      "    num_steps_sampled: 2376000\n",
      "    num_steps_trained: 2376000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2621775612010766e-30\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6643702983856201\n",
      "      kl: 0.007227424532175064\n",
      "      policy_loss: -0.000945896958000958\n",
      "      total_loss: 688.9576416015625\n",
      "      vf_explained_var: 0.8646296262741089\n",
      "      vf_loss: 688.9586791992188\n",
      "    sample_time_ms: 71724.448\n",
      "    update_time_ms: 5.181\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.96012662589063\n",
      "  time_since_restore: 9151.205849647522\n",
      "  time_this_iter_s: 88.6767086982727\n",
      "  time_total_s: 9151.205849647522\n",
      "  timestamp: 1556795419\n",
      "  timesteps_since_restore: 2376000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2376000\n",
      "  training_iteration: 99\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 9151 s, 99 iter, 2376000 ts, 184 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-11-47\n",
      "  done: false\n",
      "  episode_len_mean: 86.84057971014492\n",
      "  episode_reward_max: 610.622702225169\n",
      "  episode_reward_mean: 184.04071655690387\n",
      "  episode_reward_min: -352.659766463536\n",
      "  episodes_this_iter: 276\n",
      "  episodes_total: 26605\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18528.999\n",
      "    load_time_ms: 3.824\n",
      "    num_steps_sampled: 2400000\n",
      "    num_steps_trained: 2400000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.310887806005383e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6586069464683533\n",
      "      kl: 0.008262945339083672\n",
      "      policy_loss: -0.0008408618159592152\n",
      "      total_loss: 722.1420288085938\n",
      "      vf_explained_var: 0.8523635864257812\n",
      "      vf_loss: 722.1429443359375\n",
      "    sample_time_ms: 71552.727\n",
      "    update_time_ms: 5.091\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 46.010179139225954\n",
      "  time_since_restore: 9239.210116624832\n",
      "  time_this_iter_s: 88.00426697731018\n",
      "  time_total_s: 9239.210116624832\n",
      "  timestamp: 1556795507\n",
      "  timesteps_since_restore: 2400000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2400000\n",
      "  training_iteration: 100\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 9239 s, 100 iter, 2400000 ts, 184 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-13-14\n",
      "  done: false\n",
      "  episode_len_mean: 90.41509433962264\n",
      "  episode_reward_max: 615.4576336895974\n",
      "  episode_reward_mean: 220.43321732093048\n",
      "  episode_reward_min: -350.3930267781117\n",
      "  episodes_this_iter: 265\n",
      "  episodes_total: 26870\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18458.23\n",
      "    load_time_ms: 3.74\n",
      "    num_steps_sampled: 2424000\n",
      "    num_steps_trained: 2424000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.1554439030026914e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6539813876152039\n",
      "      kl: 0.009254412725567818\n",
      "      policy_loss: -0.000701646669767797\n",
      "      total_loss: 631.4425659179688\n",
      "      vf_explained_var: 0.8613022565841675\n",
      "      vf_loss: 631.4432983398438\n",
      "    sample_time_ms: 71199.154\n",
      "    update_time_ms: 5.038\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.10830433023262\n",
      "  time_since_restore: 9326.699824333191\n",
      "  time_this_iter_s: 87.48970770835876\n",
      "  time_total_s: 9326.699824333191\n",
      "  timestamp: 1556795594\n",
      "  timesteps_since_restore: 2424000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2424000\n",
      "  training_iteration: 101\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 9326 s, 101 iter, 2424000 ts, 220 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-14-43\n",
      "  done: false\n",
      "  episode_len_mean: 89.51119402985074\n",
      "  episode_reward_max: 618.3488922514636\n",
      "  episode_reward_mean: 210.7408882531381\n",
      "  episode_reward_min: -353.48925138321465\n",
      "  episodes_this_iter: 268\n",
      "  episodes_total: 27138\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18242.598\n",
      "    load_time_ms: 3.686\n",
      "    num_steps_sampled: 2448000\n",
      "    num_steps_trained: 2448000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.5777219515013457e-31\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6405859589576721\n",
      "      kl: 0.009160253219306469\n",
      "      policy_loss: -0.0007697111577726901\n",
      "      total_loss: 687.7344970703125\n",
      "      vf_explained_var: 0.8489251136779785\n",
      "      vf_loss: 687.7352294921875\n",
      "    sample_time_ms: 70073.513\n",
      "    update_time_ms: 5.112\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.68522206328452\n",
      "  time_since_restore: 9414.929818630219\n",
      "  time_this_iter_s: 88.22999429702759\n",
      "  time_total_s: 9414.929818630219\n",
      "  timestamp: 1556795683\n",
      "  timesteps_since_restore: 2448000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2448000\n",
      "  training_iteration: 102\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 9414 s, 102 iter, 2448000 ts, 211 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-16-10\n",
      "  done: false\n",
      "  episode_len_mean: 89.14074074074074\n",
      "  episode_reward_max: 616.7050691414303\n",
      "  episode_reward_mean: 208.08680344019058\n",
      "  episode_reward_min: -353.54767438552807\n",
      "  episodes_this_iter: 270\n",
      "  episodes_total: 27408\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18253.878\n",
      "    load_time_ms: 3.657\n",
      "    num_steps_sampled: 2472000\n",
      "    num_steps_trained: 2472000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.888609757506729e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6364205479621887\n",
      "      kl: 0.00833591166883707\n",
      "      policy_loss: -0.0010181424440816045\n",
      "      total_loss: 664.1583862304688\n",
      "      vf_explained_var: 0.8569675087928772\n",
      "      vf_loss: 664.1593627929688\n",
      "    sample_time_ms: 69849.394\n",
      "    update_time_ms: 5.048\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.021700860047645\n",
      "  time_since_restore: 9501.878597021103\n",
      "  time_this_iter_s: 86.9487783908844\n",
      "  time_total_s: 9501.878597021103\n",
      "  timestamp: 1556795770\n",
      "  timesteps_since_restore: 2472000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2472000\n",
      "  training_iteration: 103\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 9501 s, 103 iter, 2472000 ts, 208 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-17-37\n",
      "  done: false\n",
      "  episode_len_mean: 93.12015503875969\n",
      "  episode_reward_max: 618.1002455194781\n",
      "  episode_reward_mean: 256.3329831290981\n",
      "  episode_reward_min: -352.79522369057264\n",
      "  episodes_this_iter: 258\n",
      "  episodes_total: 27666\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18260.185\n",
      "    load_time_ms: 3.612\n",
      "    num_steps_sampled: 2496000\n",
      "    num_steps_trained: 2496000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.9443048787533643e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6270875930786133\n",
      "      kl: 0.009012112393975258\n",
      "      policy_loss: -0.001551831723190844\n",
      "      total_loss: 620.1979370117188\n",
      "      vf_explained_var: 0.8657776713371277\n",
      "      vf_loss: 620.1995239257812\n",
      "    sample_time_ms: 69710.841\n",
      "    update_time_ms: 5.494\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.08324578227452\n",
      "  time_since_restore: 9589.475651979446\n",
      "  time_this_iter_s: 87.5970549583435\n",
      "  time_total_s: 9589.475651979446\n",
      "  timestamp: 1556795857\n",
      "  timesteps_since_restore: 2496000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2496000\n",
      "  training_iteration: 104\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 9589 s, 104 iter, 2496000 ts, 256 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-19-05\n",
      "  done: false\n",
      "  episode_len_mean: 86.35740072202167\n",
      "  episode_reward_max: 619.6959049030495\n",
      "  episode_reward_mean: 165.41321704015022\n",
      "  episode_reward_min: -350.0202822118905\n",
      "  episodes_this_iter: 277\n",
      "  episodes_total: 27943\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18265.336\n",
      "    load_time_ms: 3.569\n",
      "    num_steps_sampled: 2520000\n",
      "    num_steps_trained: 2520000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9721524393766821e-32\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6088237166404724\n",
      "      kl: 0.00887087918817997\n",
      "      policy_loss: -0.0009252021554857492\n",
      "      total_loss: 740.3873901367188\n",
      "      vf_explained_var: 0.858190655708313\n",
      "      vf_loss: 740.3883056640625\n",
      "    sample_time_ms: 69534.063\n",
      "    update_time_ms: 5.434\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.35330426003755\n",
      "  time_since_restore: 9676.759590148926\n",
      "  time_this_iter_s: 87.28393816947937\n",
      "  time_total_s: 9676.759590148926\n",
      "  timestamp: 1556795945\n",
      "  timesteps_since_restore: 2520000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2520000\n",
      "  training_iteration: 105\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 9676 s, 105 iter, 2520000 ts, 165 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-20-32\n",
      "  done: false\n",
      "  episode_len_mean: 91.24714828897338\n",
      "  episode_reward_max: 614.024138324555\n",
      "  episode_reward_mean: 233.94661197006073\n",
      "  episode_reward_min: -361.39400902511466\n",
      "  episodes_this_iter: 263\n",
      "  episodes_total: 28206\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18213.455\n",
      "    load_time_ms: 3.537\n",
      "    num_steps_sampled: 2544000\n",
      "    num_steps_trained: 2544000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.860762196883411e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6078895330429077\n",
      "      kl: 0.009600908495485783\n",
      "      policy_loss: -0.0007528422866016626\n",
      "      total_loss: 700.2420043945312\n",
      "      vf_explained_var: 0.8486964702606201\n",
      "      vf_loss: 700.2427368164062\n",
      "    sample_time_ms: 69384.081\n",
      "    update_time_ms: 5.87\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.48665299251519\n",
      "  time_since_restore: 9764.446666955948\n",
      "  time_this_iter_s: 87.6870768070221\n",
      "  time_total_s: 9764.446666955948\n",
      "  timestamp: 1556796032\n",
      "  timesteps_since_restore: 2544000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2544000\n",
      "  training_iteration: 106\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 9764 s, 106 iter, 2544000 ts, 234 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-22-03\n",
      "  done: false\n",
      "  episode_len_mean: 85.66071428571429\n",
      "  episode_reward_max: 621.4855355734576\n",
      "  episode_reward_mean: 172.92566654443814\n",
      "  episode_reward_min: -355.628692535437\n",
      "  episodes_this_iter: 280\n",
      "  episodes_total: 28486\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18299.023\n",
      "    load_time_ms: 3.523\n",
      "    num_steps_sampled: 2568000\n",
      "    num_steps_trained: 2568000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.9303810984417053e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5849300026893616\n",
      "      kl: 0.01026257686316967\n",
      "      policy_loss: -0.0013209262397140265\n",
      "      total_loss: 685.766845703125\n",
      "      vf_explained_var: 0.8638644814491272\n",
      "      vf_loss: 685.7681884765625\n",
      "    sample_time_ms: 69596.952\n",
      "    update_time_ms: 5.811\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.231416636109536\n",
      "  time_since_restore: 9854.743621826172\n",
      "  time_this_iter_s: 90.296954870224\n",
      "  time_total_s: 9854.743621826172\n",
      "  timestamp: 1556796123\n",
      "  timesteps_since_restore: 2568000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2568000\n",
      "  training_iteration: 107\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 9854 s, 107 iter, 2568000 ts, 173 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-23-40\n",
      "  done: false\n",
      "  episode_len_mean: 90.82264150943396\n",
      "  episode_reward_max: 615.2794261797011\n",
      "  episode_reward_mean: 226.1511257681717\n",
      "  episode_reward_min: -348.91271103794566\n",
      "  episodes_this_iter: 265\n",
      "  episodes_total: 28751\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18534.753\n",
      "    load_time_ms: 3.514\n",
      "    num_steps_sampled: 2592000\n",
      "    num_steps_trained: 2592000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.9303810984417053e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5822020769119263\n",
      "      kl: 0.009336412884294987\n",
      "      policy_loss: -0.0010220148833468556\n",
      "      total_loss: 797.4853515625\n",
      "      vf_explained_var: 0.8235851526260376\n",
      "      vf_loss: 797.486328125\n",
      "    sample_time_ms: 70388.755\n",
      "    update_time_ms: 5.82\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 56.53778144204292\n",
      "  time_since_restore: 9952.46192073822\n",
      "  time_this_iter_s: 97.71829891204834\n",
      "  time_total_s: 9952.46192073822\n",
      "  timestamp: 1556796220\n",
      "  timesteps_since_restore: 2592000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2592000\n",
      "  training_iteration: 108\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 9952 s, 108 iter, 2592000 ts, 226 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-25-13\n",
      "  done: false\n",
      "  episode_len_mean: 89.51119402985074\n",
      "  episode_reward_max: 616.083060032151\n",
      "  episode_reward_mean: 216.56251695679927\n",
      "  episode_reward_min: -352.33702416538335\n",
      "  episodes_this_iter: 268\n",
      "  episodes_total: 29019\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18877.074\n",
      "    load_time_ms: 3.524\n",
      "    num_steps_sampled: 2616000\n",
      "    num_steps_trained: 2616000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.4651905492208527e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.575848400592804\n",
      "      kl: 0.008458164520561695\n",
      "      policy_loss: -0.0012434873497113585\n",
      "      total_loss: 654.82177734375\n",
      "      vf_explained_var: 0.8647550344467163\n",
      "      vf_loss: 654.8229370117188\n",
      "    sample_time_ms: 70378.957\n",
      "    update_time_ms: 6.022\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.14062923919982\n",
      "  time_since_restore: 10044.467848539352\n",
      "  time_this_iter_s: 92.0059278011322\n",
      "  time_total_s: 10044.467848539352\n",
      "  timestamp: 1556796313\n",
      "  timesteps_since_restore: 2616000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2616000\n",
      "  training_iteration: 109\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 10044 s, 109 iter, 2616000 ts, 217 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-27-39\n",
      "  done: false\n",
      "  episode_len_mean: 92.39382239382239\n",
      "  episode_reward_max: 619.1814056824721\n",
      "  episode_reward_mean: 254.1291519462642\n",
      "  episode_reward_min: -356.7521997174043\n",
      "  episodes_this_iter: 259\n",
      "  episodes_total: 29278\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20429.588\n",
      "    load_time_ms: 3.595\n",
      "    num_steps_sampled: 2640000\n",
      "    num_steps_trained: 2640000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2325952746104263e-33\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5580199360847473\n",
      "      kl: 0.009211837314069271\n",
      "      policy_loss: -0.0013614711351692677\n",
      "      total_loss: 685.732421875\n",
      "      vf_explained_var: 0.8440340161323547\n",
      "      vf_loss: 685.7337646484375\n",
      "    sample_time_ms: 74698.963\n",
      "    update_time_ms: 6.09\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 63.53228798656605\n",
      "  time_since_restore: 10191.220138072968\n",
      "  time_this_iter_s: 146.7522895336151\n",
      "  time_total_s: 10191.220138072968\n",
      "  timestamp: 1556796459\n",
      "  timesteps_since_restore: 2640000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2640000\n",
      "  training_iteration: 110\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 10191 s, 110 iter, 2640000 ts, 254 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 86.35125448028674\n",
      "  episode_reward_max: 616.6329276443583\n",
      "  episode_reward_mean: 181.81185627356166\n",
      "  episode_reward_min: -353.8221679870221\n",
      "  episodes_this_iter: 279\n",
      "  episodes_total: 29557\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 21902.897\n",
      "    load_time_ms: 3.661\n",
      "    num_steps_sampled: 2664000\n",
      "    num_steps_trained: 2664000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.162976373052132e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.53988116979599\n",
      "      kl: 0.00950243417173624\n",
      "      policy_loss: -0.0008900875109247863\n",
      "      total_loss: 804.7235107421875\n",
      "      vf_explained_var: 0.8338045477867126\n",
      "      vf_loss: 804.7243041992188\n",
      "    sample_time_ms: 81065.567\n",
      "    update_time_ms: 6.513\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.45296406839041\n",
      "  time_since_restore: 10357.13543510437\n",
      "  time_this_iter_s: 165.9152970314026\n",
      "  time_total_s: 10357.13543510437\n",
      "  timestamp: 1556796625\n",
      "  timesteps_since_restore: 2664000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2664000\n",
      "  training_iteration: 111\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 10357 s, 111 iter, 2664000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-33-16\n",
      "  done: false\n",
      "  episode_len_mean: 87.88278388278388\n",
      "  episode_reward_max: 621.1728871954331\n",
      "  episode_reward_mean: 196.66760868931482\n",
      "  episode_reward_min: -356.57158606292575\n",
      "  episodes_this_iter: 273\n",
      "  episodes_total: 29830\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 25206.509\n",
      "    load_time_ms: 3.841\n",
      "    num_steps_sampled: 2688000\n",
      "    num_steps_trained: 2688000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.081488186526066e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.533891499042511\n",
      "      kl: 0.00891183316707611\n",
      "      policy_loss: -0.0004961438244208694\n",
      "      total_loss: 568.2716064453125\n",
      "      vf_explained_var: 0.886786937713623\n",
      "      vf_loss: 568.2720947265625\n",
      "    sample_time_ms: 86031.479\n",
      "    update_time_ms: 7.011\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.166902172328705\n",
      "  time_since_restore: 10528.112775802612\n",
      "  time_this_iter_s: 170.9773406982422\n",
      "  time_total_s: 10528.112775802612\n",
      "  timestamp: 1556796796\n",
      "  timesteps_since_restore: 2688000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2688000\n",
      "  training_iteration: 112\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 10528 s, 112 iter, 2688000 ts, 197 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-36-00\n",
      "  done: false\n",
      "  episode_len_mean: 88.8996282527881\n",
      "  episode_reward_max: 622.9291956809257\n",
      "  episode_reward_mean: 202.63344277177524\n",
      "  episode_reward_min: -357.67412170293426\n",
      "  episodes_this_iter: 269\n",
      "  episodes_total: 30099\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 27969.992\n",
      "    load_time_ms: 4.123\n",
      "    num_steps_sampled: 2712000\n",
      "    num_steps_trained: 2712000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.540744093263033e-34\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5234076976776123\n",
      "      kl: 0.009154022671282291\n",
      "      policy_loss: -0.0013389178784564137\n",
      "      total_loss: 793.817138671875\n",
      "      vf_explained_var: 0.8350702524185181\n",
      "      vf_loss: 793.8184814453125\n",
      "    sample_time_ms: 90944.731\n",
      "    update_time_ms: 8.414\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.65836069294381\n",
      "  time_since_restore: 10691.889627933502\n",
      "  time_this_iter_s: 163.7768521308899\n",
      "  time_total_s: 10691.889627933502\n",
      "  timestamp: 1556796960\n",
      "  timesteps_since_restore: 2712000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2712000\n",
      "  training_iteration: 113\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 10691 s, 113 iter, 2712000 ts, 203 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-39-28\n",
      "  done: false\n",
      "  episode_len_mean: 92.45\n",
      "  episode_reward_max: 619.4065756836062\n",
      "  episode_reward_mean: 246.50171896781768\n",
      "  episode_reward_min: -356.4264163302576\n",
      "  episodes_this_iter: 260\n",
      "  episodes_total: 30359\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 28935.905\n",
      "    load_time_ms: 4.21\n",
      "    num_steps_sampled: 2736000\n",
      "    num_steps_trained: 2736000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 7.703720466315165e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5209450721740723\n",
      "      kl: 0.009046586230397224\n",
      "      policy_loss: -0.0007922705262899399\n",
      "      total_loss: 691.62060546875\n",
      "      vf_explained_var: 0.8487163186073303\n",
      "      vf_loss: 691.6213989257812\n",
      "    sample_time_ms: 101966.727\n",
      "    update_time_ms: 8.093\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.62542974195443\n",
      "  time_since_restore: 10899.382772684097\n",
      "  time_this_iter_s: 207.4931447505951\n",
      "  time_total_s: 10899.382772684097\n",
      "  timestamp: 1556797168\n",
      "  timesteps_since_restore: 2736000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2736000\n",
      "  training_iteration: 114\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 10899 s, 114 iter, 2736000 ts, 247 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-41-26\n",
      "  done: false\n",
      "  episode_len_mean: 88.20588235294117\n",
      "  episode_reward_max: 616.9438268302368\n",
      "  episode_reward_mean: 203.21181421289717\n",
      "  episode_reward_min: -353.8503158334218\n",
      "  episodes_this_iter: 272\n",
      "  episodes_total: 30631\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 29891.303\n",
      "    load_time_ms: 4.233\n",
      "    num_steps_sampled: 2760000\n",
      "    num_steps_trained: 2760000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 3.8518602331575823e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5099482536315918\n",
      "      kl: 0.009453988634049892\n",
      "      policy_loss: -0.0010475452290847898\n",
      "      total_loss: 643.1842651367188\n",
      "      vf_explained_var: 0.8654906153678894\n",
      "      vf_loss: 643.185302734375\n",
      "    sample_time_ms: 104113.356\n",
      "    update_time_ms: 8.606\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.802953553224285\n",
      "  time_since_restore: 11017.697343111038\n",
      "  time_this_iter_s: 118.31457042694092\n",
      "  time_total_s: 11017.697343111038\n",
      "  timestamp: 1556797286\n",
      "  timesteps_since_restore: 2760000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2760000\n",
      "  training_iteration: 115\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.8/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 11017 s, 115 iter, 2760000 ts, 203 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 89.28252788104089\n",
      "  episode_reward_max: 618.1243016385981\n",
      "  episode_reward_mean: 210.60651914218835\n",
      "  episode_reward_min: -354.4143967028126\n",
      "  episodes_this_iter: 269\n",
      "  episodes_total: 30900\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 30316.04\n",
      "    load_time_ms: 4.363\n",
      "    num_steps_sampled: 2784000\n",
      "    num_steps_trained: 2784000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9259301165787911e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5081906914710999\n",
      "      kl: 0.010086110793054104\n",
      "      policy_loss: -0.0013711430365219712\n",
      "      total_loss: 707.0502319335938\n",
      "      vf_explained_var: 0.8511067628860474\n",
      "      vf_loss: 707.0516967773438\n",
      "    sample_time_ms: 107674.836\n",
      "    update_time_ms: 8.145\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.65162978554709\n",
      "  time_since_restore: 11145.248425245285\n",
      "  time_this_iter_s: 127.55108213424683\n",
      "  time_total_s: 11145.248425245285\n",
      "  timestamp: 1556797414\n",
      "  timesteps_since_restore: 2784000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2784000\n",
      "  training_iteration: 116\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 11145 s, 116 iter, 2784000 ts, 211 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-45-04\n",
      "  done: false\n",
      "  episode_len_mean: 89.34200743494424\n",
      "  episode_reward_max: 623.1723023456601\n",
      "  episode_reward_mean: 220.58663338379606\n",
      "  episode_reward_min: -352.46793818235363\n",
      "  episodes_this_iter: 269\n",
      "  episodes_total: 31169\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 30244.574\n",
      "    load_time_ms: 4.416\n",
      "    num_steps_sampled: 2808000\n",
      "    num_steps_trained: 2808000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.9259301165787911e-35\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5056818127632141\n",
      "      kl: 0.0098932646214962\n",
      "      policy_loss: -0.0006589406402781606\n",
      "      total_loss: 623.2716064453125\n",
      "      vf_explained_var: 0.8682211637496948\n",
      "      vf_loss: 623.2722778320312\n",
      "    sample_time_ms: 107693.134\n",
      "    update_time_ms: 8.234\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.146658345949014\n",
      "  time_since_restore: 11235.003879785538\n",
      "  time_this_iter_s: 89.75545454025269\n",
      "  time_total_s: 11235.003879785538\n",
      "  timestamp: 1556797504\n",
      "  timesteps_since_restore: 2808000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2808000\n",
      "  training_iteration: 117\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 11235 s, 117 iter, 2808000 ts, 221 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-46-32\n",
      "  done: false\n",
      "  episode_len_mean: 93.42023346303502\n",
      "  episode_reward_max: 618.2032175323583\n",
      "  episode_reward_mean: 270.1709786466897\n",
      "  episode_reward_min: -353.66096543166725\n",
      "  episodes_this_iter: 257\n",
      "  episodes_total: 31426\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 30084.285\n",
      "    load_time_ms: 4.418\n",
      "    num_steps_sampled: 2832000\n",
      "    num_steps_trained: 2832000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.629650582893956e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4952448010444641\n",
      "      kl: 0.010783581994473934\n",
      "      policy_loss: -0.0012497439747676253\n",
      "      total_loss: 672.3716430664062\n",
      "      vf_explained_var: 0.845984697341919\n",
      "      vf_loss: 672.3729248046875\n",
      "    sample_time_ms: 106876.953\n",
      "    update_time_ms: 8.18\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 67.54274466167244\n",
      "  time_since_restore: 11322.96956729889\n",
      "  time_this_iter_s: 87.96568751335144\n",
      "  time_total_s: 11322.96956729889\n",
      "  timestamp: 1556797592\n",
      "  timesteps_since_restore: 2832000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2832000\n",
      "  training_iteration: 118\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 11322 s, 118 iter, 2832000 ts, 270 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-48-04\n",
      "  done: false\n",
      "  episode_len_mean: 91.37404580152672\n",
      "  episode_reward_max: 622.7040944034013\n",
      "  episode_reward_mean: 247.3884596239823\n",
      "  episode_reward_min: -356.07105921794107\n",
      "  episodes_this_iter: 262\n",
      "  episodes_total: 31688\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 29775.596\n",
      "    load_time_ms: 4.413\n",
      "    num_steps_sampled: 2856000\n",
      "    num_steps_trained: 2856000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 9.629650582893956e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4858487546443939\n",
      "      kl: 0.00995214655995369\n",
      "      policy_loss: -0.00023246232012752444\n",
      "      total_loss: 719.349609375\n",
      "      vf_explained_var: 0.8443233966827393\n",
      "      vf_loss: 719.349853515625\n",
      "    sample_time_ms: 107261.763\n",
      "    update_time_ms: 8.452\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.84711490599558\n",
      "  time_since_restore: 11415.738548994064\n",
      "  time_this_iter_s: 92.76898169517517\n",
      "  time_total_s: 11415.738548994064\n",
      "  timestamp: 1556797684\n",
      "  timesteps_since_restore: 2856000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2856000\n",
      "  training_iteration: 119\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 11415 s, 119 iter, 2856000 ts, 247 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-49-37\n",
      "  done: false\n",
      "  episode_len_mean: 88.84814814814816\n",
      "  episode_reward_max: 620.758261075296\n",
      "  episode_reward_mean: 214.38314902861165\n",
      "  episode_reward_min: -358.7425140318671\n",
      "  episodes_this_iter: 270\n",
      "  episodes_total: 31958\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 28235.281\n",
      "    load_time_ms: 4.356\n",
      "    num_steps_sampled: 2880000\n",
      "    num_steps_trained: 2880000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 4.814825291446978e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4718060791492462\n",
      "      kl: 0.009287858381867409\n",
      "      policy_loss: -0.0005185960326343775\n",
      "      total_loss: 674.7883911132812\n",
      "      vf_explained_var: 0.8575441241264343\n",
      "      vf_loss: 674.7889404296875\n",
      "    sample_time_ms: 103405.811\n",
      "    update_time_ms: 8.474\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.595787257152914\n",
      "  time_since_restore: 11508.503012180328\n",
      "  time_this_iter_s: 92.76446318626404\n",
      "  time_total_s: 11508.503012180328\n",
      "  timestamp: 1556797777\n",
      "  timesteps_since_restore: 2880000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2880000\n",
      "  training_iteration: 120\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 11508 s, 120 iter, 2880000 ts, 214 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-51-04\n",
      "  done: false\n",
      "  episode_len_mean: 87.46909090909091\n",
      "  episode_reward_max: 623.1209844791413\n",
      "  episode_reward_mean: 199.52108357090893\n",
      "  episode_reward_min: -354.0600038655473\n",
      "  episodes_this_iter: 275\n",
      "  episodes_total: 32233\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 26802.915\n",
      "    load_time_ms: 4.283\n",
      "    num_steps_sampled: 2904000\n",
      "    num_steps_trained: 2904000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.407412645723489e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.47941499948501587\n",
      "      kl: 0.0116651002317667\n",
      "      policy_loss: -0.0015540955355390906\n",
      "      total_loss: 706.50048828125\n",
      "      vf_explained_var: 0.8583970665931702\n",
      "      vf_loss: 706.5020751953125\n",
      "    sample_time_ms: 96948.297\n",
      "    update_time_ms: 8.131\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.88027089272723\n",
      "  time_since_restore: 11595.492114782333\n",
      "  time_this_iter_s: 86.989102602005\n",
      "  time_total_s: 11595.492114782333\n",
      "  timestamp: 1556797864\n",
      "  timesteps_since_restore: 2904000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2904000\n",
      "  training_iteration: 121\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 11595 s, 121 iter, 2904000 ts, 200 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-52-31\n",
      "  done: false\n",
      "  episode_len_mean: 85.575\n",
      "  episode_reward_max: 623.3293489317664\n",
      "  episode_reward_mean: 167.80463821854624\n",
      "  episode_reward_min: -355.63481089080415\n",
      "  episodes_this_iter: 280\n",
      "  episodes_total: 32513\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 23509.219\n",
      "    load_time_ms: 4.095\n",
      "    num_steps_sampled: 2928000\n",
      "    num_steps_trained: 2928000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.407412645723489e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4577559530735016\n",
      "      kl: 0.010362345725297928\n",
      "      policy_loss: -0.0013188683660700917\n",
      "      total_loss: 700.8477783203125\n",
      "      vf_explained_var: 0.8634307980537415\n",
      "      vf_loss: 700.8490600585938\n",
      "    sample_time_ms: 91850.891\n",
      "    update_time_ms: 7.579\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 41.95115955463656\n",
      "  time_since_restore: 11682.50437116623\n",
      "  time_this_iter_s: 87.01225638389587\n",
      "  time_total_s: 11682.50437116623\n",
      "  timestamp: 1556797951\n",
      "  timesteps_since_restore: 2928000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2928000\n",
      "  training_iteration: 122\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 11682 s, 122 iter, 2928000 ts, 168 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-53-57\n",
      "  done: false\n",
      "  episode_len_mean: 87.55636363636364\n",
      "  episode_reward_max: 623.6015880563366\n",
      "  episode_reward_mean: 206.8715758431974\n",
      "  episode_reward_min: -358.87040040701936\n",
      "  episodes_this_iter: 275\n",
      "  episodes_total: 32788\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20726.432\n",
      "    load_time_ms: 3.823\n",
      "    num_steps_sampled: 2952000\n",
      "    num_steps_trained: 2952000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 2.407412645723489e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4464588165283203\n",
      "      kl: 0.009729795157909393\n",
      "      policy_loss: -0.00037140940548852086\n",
      "      total_loss: 722.53564453125\n",
      "      vf_explained_var: 0.8539871573448181\n",
      "      vf_loss: 722.5360107421875\n",
      "    sample_time_ms: 86868.985\n",
      "    update_time_ms: 6.221\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.71789396079936\n",
      "  time_since_restore: 11768.5730509758\n",
      "  time_this_iter_s: 86.06867980957031\n",
      "  time_total_s: 11768.5730509758\n",
      "  timestamp: 1556798037\n",
      "  timesteps_since_restore: 2952000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2952000\n",
      "  training_iteration: 123\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 11768 s, 123 iter, 2952000 ts, 207 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-55-26\n",
      "  done: false\n",
      "  episode_len_mean: 90.13584905660377\n",
      "  episode_reward_max: 623.8114887684284\n",
      "  episode_reward_mean: 215.98070935792828\n",
      "  episode_reward_min: -352.9508005771044\n",
      "  episodes_this_iter: 265\n",
      "  episodes_total: 33053\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19755.681\n",
      "    load_time_ms: 3.756\n",
      "    num_steps_sampled: 2976000\n",
      "    num_steps_trained: 2976000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037063228617445e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.43775302171707153\n",
      "      kl: 0.010066356509923935\n",
      "      policy_loss: -0.00022798370628152043\n",
      "      total_loss: 691.071044921875\n",
      "      vf_explained_var: 0.8578611612319946\n",
      "      vf_loss: 691.0712280273438\n",
      "    sample_time_ms: 75942.439\n",
      "    update_time_ms: 5.975\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.99517733948206\n",
      "  time_since_restore: 11857.073008537292\n",
      "  time_this_iter_s: 88.49995756149292\n",
      "  time_total_s: 11857.073008537292\n",
      "  timestamp: 1556798126\n",
      "  timesteps_since_restore: 2976000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 2976000\n",
      "  training_iteration: 124\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 11857 s, 124 iter, 2976000 ts, 216 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-56-54\n",
      "  done: false\n",
      "  episode_len_mean: 86.02150537634408\n",
      "  episode_reward_max: 628.0866751472386\n",
      "  episode_reward_mean: 182.40237042093517\n",
      "  episode_reward_min: -358.6081029101926\n",
      "  episodes_this_iter: 279\n",
      "  episodes_total: 33332\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18786.289\n",
      "    load_time_ms: 3.758\n",
      "    num_steps_sampled: 3000000\n",
      "    num_steps_trained: 3000000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037063228617445e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4157956540584564\n",
      "      kl: 0.010548420250415802\n",
      "      policy_loss: -0.001066282158717513\n",
      "      total_loss: 778.8880004882812\n",
      "      vf_explained_var: 0.8484596610069275\n",
      "      vf_loss: 778.8889770507812\n",
      "    sample_time_ms: 73864.346\n",
      "    update_time_ms: 5.454\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 45.600592605233786\n",
      "  time_since_restore: 11944.904347419739\n",
      "  time_this_iter_s: 87.83133888244629\n",
      "  time_total_s: 11944.904347419739\n",
      "  timestamp: 1556798214\n",
      "  timesteps_since_restore: 3000000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3000000\n",
      "  training_iteration: 125\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 11944 s, 125 iter, 3000000 ts, 182 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_13-58-21\n",
      "  done: false\n",
      "  episode_len_mean: 88.28937728937728\n",
      "  episode_reward_max: 624.833991364963\n",
      "  episode_reward_mean: 203.33788775951152\n",
      "  episode_reward_min: -353.1779012744021\n",
      "  episodes_this_iter: 273\n",
      "  episodes_total: 33605\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18357.335\n",
      "    load_time_ms: 3.646\n",
      "    num_steps_sampled: 3024000\n",
      "    num_steps_trained: 3024000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037063228617445e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4205898344516754\n",
      "      kl: 0.010339760221540928\n",
      "      policy_loss: -0.0005883931880816817\n",
      "      total_loss: 762.4993896484375\n",
      "      vf_explained_var: 0.8398310542106628\n",
      "      vf_loss: 762.5\n",
      "    sample_time_ms: 70216.747\n",
      "    update_time_ms: 5.45\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.83447193987787\n",
      "  time_since_restore: 12031.682019472122\n",
      "  time_this_iter_s: 86.77767205238342\n",
      "  time_total_s: 12031.682019472122\n",
      "  timestamp: 1556798301\n",
      "  timesteps_since_restore: 3024000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3024000\n",
      "  training_iteration: 126\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 12031 s, 126 iter, 3024000 ts, 203 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-00-14\n",
      "  done: false\n",
      "  episode_len_mean: 89.08208955223881\n",
      "  episode_reward_max: 625.2198671369011\n",
      "  episode_reward_mean: 216.82436051270287\n",
      "  episode_reward_min: -357.43406202871927\n",
      "  episodes_this_iter: 268\n",
      "  episodes_total: 33873\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20502.549\n",
      "    load_time_ms: 3.617\n",
      "    num_steps_sampled: 3048000\n",
      "    num_steps_trained: 3048000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037063228617445e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.4116303324699402\n",
      "      kl: 0.01112984586507082\n",
      "      policy_loss: -6.233406020328403e-05\n",
      "      total_loss: 606.447509765625\n",
      "      vf_explained_var: 0.8790833950042725\n",
      "      vf_loss: 606.4475708007812\n",
      "    sample_time_ms: 70412.19\n",
      "    update_time_ms: 5.407\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.20609012817572\n",
      "  time_since_restore: 12144.876208543777\n",
      "  time_this_iter_s: 113.19418907165527\n",
      "  time_total_s: 12144.876208543777\n",
      "  timestamp: 1556798414\n",
      "  timesteps_since_restore: 3048000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3048000\n",
      "  training_iteration: 127\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 12144 s, 127 iter, 3048000 ts, 217 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-01-43\n",
      "  done: false\n",
      "  episode_len_mean: 87.2274368231047\n",
      "  episode_reward_max: 625.246687657058\n",
      "  episode_reward_mean: 203.36225392815052\n",
      "  episode_reward_min: -350.352907239876\n",
      "  episodes_this_iter: 277\n",
      "  episodes_total: 34150\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20429.849\n",
      "    load_time_ms: 3.627\n",
      "    num_steps_sampled: 3072000\n",
      "    num_steps_trained: 3072000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037063228617445e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39898982644081116\n",
      "      kl: 0.011677617207169533\n",
      "      policy_loss: 0.00012168470857432112\n",
      "      total_loss: 809.87451171875\n",
      "      vf_explained_var: 0.8356761336326599\n",
      "      vf_loss: 809.8743286132812\n",
      "    sample_time_ms: 70582.707\n",
      "    update_time_ms: 5.766\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.84056348203763\n",
      "  time_since_restore: 12233.810644865036\n",
      "  time_this_iter_s: 88.93443632125854\n",
      "  time_total_s: 12233.810644865036\n",
      "  timestamp: 1556798503\n",
      "  timesteps_since_restore: 3072000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3072000\n",
      "  training_iteration: 128\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 12233 s, 128 iter, 3072000 ts, 203 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-03-12\n",
      "  done: false\n",
      "  episode_len_mean: 87.58608058608058\n",
      "  episode_reward_max: 625.6783300755861\n",
      "  episode_reward_mean: 206.45928931475734\n",
      "  episode_reward_min: -355.4494923989597\n",
      "  episodes_this_iter: 273\n",
      "  episodes_total: 34423\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20390.631\n",
      "    load_time_ms: 3.627\n",
      "    num_steps_sampled: 3096000\n",
      "    num_steps_trained: 3096000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037063228617445e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.39387622475624084\n",
      "      kl: 0.010527413338422775\n",
      "      policy_loss: -0.00022365522454492748\n",
      "      total_loss: 660.367431640625\n",
      "      vf_explained_var: 0.8675232529640198\n",
      "      vf_loss: 660.36767578125\n",
      "    sample_time_ms: 70250.389\n",
      "    update_time_ms: 5.311\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.614822328689336\n",
      "  time_since_restore: 12322.865100622177\n",
      "  time_this_iter_s: 89.05445575714111\n",
      "  time_total_s: 12322.865100622177\n",
      "  timestamp: 1556798592\n",
      "  timesteps_since_restore: 3096000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3096000\n",
      "  training_iteration: 129\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 12322 s, 129 iter, 3096000 ts, 206 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-04-41\n",
      "  done: false\n",
      "  episode_len_mean: 90.04868913857678\n",
      "  episode_reward_max: 628.884041910006\n",
      "  episode_reward_mean: 232.9960155828952\n",
      "  episode_reward_min: -352.1877934231253\n",
      "  episodes_this_iter: 267\n",
      "  episodes_total: 34690\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20374.906\n",
      "    load_time_ms: 3.63\n",
      "    num_steps_sampled: 3120000\n",
      "    num_steps_trained: 3120000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037063228617445e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3824043273925781\n",
      "      kl: 0.01150032039731741\n",
      "      policy_loss: -0.000310616655042395\n",
      "      total_loss: 747.56884765625\n",
      "      vf_explained_var: 0.8473116755485535\n",
      "      vf_loss: 747.5691528320312\n",
      "    sample_time_ms: 69868.315\n",
      "    update_time_ms: 5.51\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.24900389572382\n",
      "  time_since_restore: 12411.655375480652\n",
      "  time_this_iter_s: 88.79027485847473\n",
      "  time_total_s: 12411.655375480652\n",
      "  timestamp: 1556798681\n",
      "  timesteps_since_restore: 3120000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3120000\n",
      "  training_iteration: 130\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 12411 s, 130 iter, 3120000 ts, 233 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-06-09\n",
      "  done: false\n",
      "  episode_len_mean: 85.02482269503547\n",
      "  episode_reward_max: 625.6394474616998\n",
      "  episode_reward_mean: 169.8151881789208\n",
      "  episode_reward_min: -356.62328476356026\n",
      "  episodes_this_iter: 282\n",
      "  episodes_total: 34972\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20339.38\n",
      "    load_time_ms: 3.664\n",
      "    num_steps_sampled: 3144000\n",
      "    num_steps_trained: 3144000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 1.2037063228617445e-36\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3729114234447479\n",
      "      kl: 0.009828574024140835\n",
      "      policy_loss: -0.00033003537100739777\n",
      "      total_loss: 760.1528930664062\n",
      "      vf_explained_var: 0.8535038828849792\n",
      "      vf_loss: 760.1531372070312\n",
      "    sample_time_ms: 69978.857\n",
      "    update_time_ms: 5.495\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.453797044730194\n",
      "  time_since_restore: 12499.395718812943\n",
      "  time_this_iter_s: 87.74034333229065\n",
      "  time_total_s: 12499.395718812943\n",
      "  timestamp: 1556798769\n",
      "  timesteps_since_restore: 3144000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3144000\n",
      "  training_iteration: 131\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 12499 s, 131 iter, 3144000 ts, 170 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-07-35\n",
      "  done: false\n",
      "  episode_len_mean: 89.3731343283582\n",
      "  episode_reward_max: 625.1173804937074\n",
      "  episode_reward_mean: 222.51785878022446\n",
      "  episode_reward_min: -349.25432913032563\n",
      "  episodes_this_iter: 268\n",
      "  episodes_total: 35240\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20337.526\n",
      "    load_time_ms: 3.695\n",
      "    num_steps_sampled: 3168000\n",
      "    num_steps_trained: 3168000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.38431286811828613\n",
      "      kl: 0.011162007227540016\n",
      "      policy_loss: -0.0005629229708574712\n",
      "      total_loss: 772.8761596679688\n",
      "      vf_explained_var: 0.8386759161949158\n",
      "      vf_loss: 772.8768310546875\n",
      "    sample_time_ms: 69939.165\n",
      "    update_time_ms: 5.546\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.62946469505611\n",
      "  time_since_restore: 12585.993549346924\n",
      "  time_this_iter_s: 86.59783053398132\n",
      "  time_total_s: 12585.993549346924\n",
      "  timestamp: 1556798855\n",
      "  timesteps_since_restore: 3168000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3168000\n",
      "  training_iteration: 132\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 12585 s, 132 iter, 3168000 ts, 223 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-09-02\n",
      "  done: false\n",
      "  episode_len_mean: 87.52189781021897\n",
      "  episode_reward_max: 626.1889457248666\n",
      "  episode_reward_mean: 201.66792148237298\n",
      "  episode_reward_min: -354.6051819839181\n",
      "  episodes_this_iter: 274\n",
      "  episodes_total: 35514\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20340.607\n",
      "    load_time_ms: 3.724\n",
      "    num_steps_sampled: 3192000\n",
      "    num_steps_trained: 3192000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3779875636100769\n",
      "      kl: 0.012069851160049438\n",
      "      policy_loss: -7.533486495958641e-05\n",
      "      total_loss: 905.3668212890625\n",
      "      vf_explained_var: 0.8095805048942566\n",
      "      vf_loss: 905.366943359375\n",
      "    sample_time_ms: 70043.069\n",
      "    update_time_ms: 5.438\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.41698037059324\n",
      "  time_since_restore: 12673.133025407791\n",
      "  time_this_iter_s: 87.13947606086731\n",
      "  time_total_s: 12673.133025407791\n",
      "  timestamp: 1556798942\n",
      "  timesteps_since_restore: 3192000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3192000\n",
      "  training_iteration: 133\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 6.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 12673 s, 133 iter, 3192000 ts, 202 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-10-31\n",
      "  done: false\n",
      "  episode_len_mean: 88.32352941176471\n",
      "  episode_reward_max: 627.3283722937766\n",
      "  episode_reward_mean: 217.95872796989175\n",
      "  episode_reward_min: -358.8428893443876\n",
      "  episodes_this_iter: 272\n",
      "  episodes_total: 35786\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20338.897\n",
      "    load_time_ms: 3.719\n",
      "    num_steps_sampled: 3216000\n",
      "    num_steps_trained: 3216000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3768389821052551\n",
      "      kl: 0.01338864117860794\n",
      "      policy_loss: -0.0007900543860159814\n",
      "      total_loss: 702.9447631835938\n",
      "      vf_explained_var: 0.855272114276886\n",
      "      vf_loss: 702.945556640625\n",
      "    sample_time_ms: 70047.985\n",
      "    update_time_ms: 5.431\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.48968199247294\n",
      "  time_since_restore: 12761.665155887604\n",
      "  time_this_iter_s: 88.53213047981262\n",
      "  time_total_s: 12761.665155887604\n",
      "  timestamp: 1556799031\n",
      "  timesteps_since_restore: 3216000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3216000\n",
      "  training_iteration: 134\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 12761 s, 134 iter, 3216000 ts, 218 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-12-00\n",
      "  done: false\n",
      "  episode_len_mean: 90.23220973782772\n",
      "  episode_reward_max: 629.8745485671035\n",
      "  episode_reward_mean: 233.60036623926953\n",
      "  episode_reward_min: -355.57861482065397\n",
      "  episodes_this_iter: 267\n",
      "  episodes_total: 36053\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20357.855\n",
      "    load_time_ms: 3.702\n",
      "    num_steps_sampled: 3240000\n",
      "    num_steps_trained: 3240000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3630973994731903\n",
      "      kl: 0.01121585350483656\n",
      "      policy_loss: 0.0005695389700122178\n",
      "      total_loss: 830.15380859375\n",
      "      vf_explained_var: 0.8230387568473816\n",
      "      vf_loss: 830.1532592773438\n",
      "    sample_time_ms: 70118.533\n",
      "    update_time_ms: 5.363\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.40009155981738\n",
      "  time_since_restore: 12850.392429351807\n",
      "  time_this_iter_s: 88.72727346420288\n",
      "  time_total_s: 12850.392429351807\n",
      "  timestamp: 1556799120\n",
      "  timesteps_since_restore: 3240000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3240000\n",
      "  training_iteration: 135\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 12850 s, 135 iter, 3240000 ts, 234 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-13-28\n",
      "  done: false\n",
      "  episode_len_mean: 90.68939393939394\n",
      "  episode_reward_max: 632.9563293282719\n",
      "  episode_reward_mean: 249.12264283650262\n",
      "  episode_reward_min: -361.1397922461225\n",
      "  episodes_this_iter: 264\n",
      "  episodes_total: 36317\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20359.325\n",
      "    load_time_ms: 3.679\n",
      "    num_steps_sampled: 3264000\n",
      "    num_steps_trained: 3264000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.36845946311950684\n",
      "      kl: 0.010758333839476109\n",
      "      policy_loss: -0.00033503310987725854\n",
      "      total_loss: 671.5187377929688\n",
      "      vf_explained_var: 0.8556601405143738\n",
      "      vf_loss: 671.5191040039062\n",
      "    sample_time_ms: 70274.844\n",
      "    update_time_ms: 5.52\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.280660709125655\n",
      "  time_since_restore: 12938.750148773193\n",
      "  time_this_iter_s: 88.35771942138672\n",
      "  time_total_s: 12938.750148773193\n",
      "  timestamp: 1556799208\n",
      "  timesteps_since_restore: 3264000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3264000\n",
      "  training_iteration: 136\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 12938 s, 136 iter, 3264000 ts, 249 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-15-01\n",
      "  done: false\n",
      "  episode_len_mean: 91.14828897338403\n",
      "  episode_reward_max: 631.2642115064109\n",
      "  episode_reward_mean: 244.69145635916368\n",
      "  episode_reward_min: -356.80382194445724\n",
      "  episodes_this_iter: 263\n",
      "  episodes_total: 36580\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18470.241\n",
      "    load_time_ms: 3.712\n",
      "    num_steps_sampled: 3288000\n",
      "    num_steps_trained: 3288000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.35654473304748535\n",
      "      kl: 0.010262010619044304\n",
      "      policy_loss: 7.482917862944305e-05\n",
      "      total_loss: 767.690185546875\n",
      "      vf_explained_var: 0.8351948261260986\n",
      "      vf_loss: 767.6900024414062\n",
      "    sample_time_ms: 70076.78\n",
      "    update_time_ms: 5.574\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.17286408979092\n",
      "  time_since_restore: 13031.049247026443\n",
      "  time_this_iter_s: 92.29909825325012\n",
      "  time_total_s: 13031.049247026443\n",
      "  timestamp: 1556799301\n",
      "  timesteps_since_restore: 3288000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3288000\n",
      "  training_iteration: 137\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 13031 s, 137 iter, 3288000 ts, 245 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-16-28\n",
      "  done: false\n",
      "  episode_len_mean: 84.16140350877193\n",
      "  episode_reward_max: 632.4086830725822\n",
      "  episode_reward_mean: 170.21214877304988\n",
      "  episode_reward_min: -354.94430612328614\n",
      "  episodes_this_iter: 285\n",
      "  episodes_total: 36865\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18464.089\n",
      "    load_time_ms: 3.683\n",
      "    num_steps_sampled: 3312000\n",
      "    num_steps_trained: 3312000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.34644612669944763\n",
      "      kl: 0.011477303691208363\n",
      "      policy_loss: -0.0005771747673861682\n",
      "      total_loss: 750.4559936523438\n",
      "      vf_explained_var: 0.8600260615348816\n",
      "      vf_loss: 750.4564819335938\n",
      "    sample_time_ms: 69906.067\n",
      "    update_time_ms: 5.713\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.55303719326246\n",
      "  time_since_restore: 13118.21940779686\n",
      "  time_this_iter_s: 87.17016077041626\n",
      "  time_total_s: 13118.21940779686\n",
      "  timestamp: 1556799388\n",
      "  timesteps_since_restore: 3312000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3312000\n",
      "  training_iteration: 138\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 13118 s, 138 iter, 3312000 ts, 170 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-18-04\n",
      "  done: false\n",
      "  episode_len_mean: 88.76014760147602\n",
      "  episode_reward_max: 636.111587674302\n",
      "  episode_reward_mean: 211.84345474448102\n",
      "  episode_reward_min: -355.5622502624303\n",
      "  episodes_this_iter: 271\n",
      "  episodes_total: 37136\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18491.478\n",
      "    load_time_ms: 3.725\n",
      "    num_steps_sampled: 3336000\n",
      "    num_steps_trained: 3336000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3466193377971649\n",
      "      kl: 0.010520347394049168\n",
      "      policy_loss: -0.00013411315740086138\n",
      "      total_loss: 790.065185546875\n",
      "      vf_explained_var: 0.8405608534812927\n",
      "      vf_loss: 790.0653076171875\n",
      "    sample_time_ms: 70602.131\n",
      "    update_time_ms: 5.822\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.960863686120256\n",
      "  time_since_restore: 13214.504215240479\n",
      "  time_this_iter_s: 96.28480744361877\n",
      "  time_total_s: 13214.504215240479\n",
      "  timestamp: 1556799484\n",
      "  timesteps_since_restore: 3336000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3336000\n",
      "  training_iteration: 139\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 13214 s, 139 iter, 3336000 ts, 212 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-19-47\n",
      "  done: false\n",
      "  episode_len_mean: 89.95864661654136\n",
      "  episode_reward_max: 634.0619381222996\n",
      "  episode_reward_mean: 234.54587200803587\n",
      "  episode_reward_min: -352.87344741339916\n",
      "  episodes_this_iter: 266\n",
      "  episodes_total: 37402\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18798.238\n",
      "    load_time_ms: 3.861\n",
      "    num_steps_sampled: 3360000\n",
      "    num_steps_trained: 3360000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.34423714876174927\n",
      "      kl: 0.01100795716047287\n",
      "      policy_loss: -0.0005255632568150759\n",
      "      total_loss: 819.73583984375\n",
      "      vf_explained_var: 0.8290141820907593\n",
      "      vf_loss: 819.736328125\n",
      "    sample_time_ms: 71677.669\n",
      "    update_time_ms: 5.547\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.63646800200896\n",
      "  time_since_restore: 13317.13888835907\n",
      "  time_this_iter_s: 102.63467311859131\n",
      "  time_total_s: 13317.13888835907\n",
      "  timestamp: 1556799587\n",
      "  timesteps_since_restore: 3360000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3360000\n",
      "  training_iteration: 140\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 13317 s, 140 iter, 3360000 ts, 235 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-21-45\n",
      "  done: false\n",
      "  episode_len_mean: 87.33695652173913\n",
      "  episode_reward_max: 632.2918655337705\n",
      "  episode_reward_mean: 200.24449759923172\n",
      "  episode_reward_min: -349.8683739503041\n",
      "  episodes_this_iter: 276\n",
      "  episodes_total: 37678\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19636.709\n",
      "    load_time_ms: 3.827\n",
      "    num_steps_sampled: 3384000\n",
      "    num_steps_trained: 3384000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3421890437602997\n",
      "      kl: 0.010822459124028683\n",
      "      policy_loss: -0.0003603305958677083\n",
      "      total_loss: 724.4340209960938\n",
      "      vf_explained_var: 0.8571944832801819\n",
      "      vf_loss: 724.434326171875\n",
      "    sample_time_ms: 73834.658\n",
      "    update_time_ms: 6.752\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.06112439980793\n",
      "  time_since_restore: 13434.852993011475\n",
      "  time_this_iter_s: 117.71410465240479\n",
      "  time_total_s: 13434.852993011475\n",
      "  timestamp: 1556799705\n",
      "  timesteps_since_restore: 3384000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3384000\n",
      "  training_iteration: 141\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 13434 s, 141 iter, 3384000 ts, 200 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-24-00\n",
      "  done: false\n",
      "  episode_len_mean: 86.37410071942446\n",
      "  episode_reward_max: 633.0540013213239\n",
      "  episode_reward_mean: 188.9959394917725\n",
      "  episode_reward_min: -355.7071970860663\n",
      "  episodes_this_iter: 278\n",
      "  episodes_total: 37956\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19640.062\n",
      "    load_time_ms: 3.808\n",
      "    num_steps_sampled: 3408000\n",
      "    num_steps_trained: 3408000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32441404461860657\n",
      "      kl: 0.011309592984616756\n",
      "      policy_loss: -0.0003981812915299088\n",
      "      total_loss: 748.5708618164062\n",
      "      vf_explained_var: 0.855465829372406\n",
      "      vf_loss: 748.5712280273438\n",
      "    sample_time_ms: 78709.434\n",
      "    update_time_ms: 7.24\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 47.248984872943126\n",
      "  time_since_restore: 13570.239353179932\n",
      "  time_this_iter_s: 135.38636016845703\n",
      "  time_total_s: 13570.239353179932\n",
      "  timestamp: 1556799840\n",
      "  timesteps_since_restore: 3408000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3408000\n",
      "  training_iteration: 142\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 13570 s, 142 iter, 3408000 ts, 189 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-25-34\n",
      "  done: false\n",
      "  episode_len_mean: 91.55172413793103\n",
      "  episode_reward_max: 635.4223816470782\n",
      "  episode_reward_mean: 251.39871900354626\n",
      "  episode_reward_min: -354.28111247535713\n",
      "  episodes_this_iter: 261\n",
      "  episodes_total: 38217\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19636.336\n",
      "    load_time_ms: 3.803\n",
      "    num_steps_sampled: 3432000\n",
      "    num_steps_trained: 3432000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.341434121131897\n",
      "      kl: 0.011419259011745453\n",
      "      policy_loss: -0.0008403187384828925\n",
      "      total_loss: 682.2095947265625\n",
      "      vf_explained_var: 0.8581069111824036\n",
      "      vf_loss: 682.2103881835938\n",
      "    sample_time_ms: 79370.049\n",
      "    update_time_ms: 7.403\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 62.849679750886565\n",
      "  time_since_restore: 13663.95218038559\n",
      "  time_this_iter_s: 93.71282720565796\n",
      "  time_total_s: 13663.95218038559\n",
      "  timestamp: 1556799934\n",
      "  timesteps_since_restore: 3432000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3432000\n",
      "  training_iteration: 143\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 13663 s, 143 iter, 3432000 ts, 251 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-27-08\n",
      "  done: false\n",
      "  episode_len_mean: 89.73880597014926\n",
      "  episode_reward_max: 636.9335393116307\n",
      "  episode_reward_mean: 233.5766442985926\n",
      "  episode_reward_min: -355.7014060262078\n",
      "  episodes_this_iter: 268\n",
      "  episodes_total: 38485\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19676.731\n",
      "    load_time_ms: 3.794\n",
      "    num_steps_sampled: 3456000\n",
      "    num_steps_trained: 3456000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3272860050201416\n",
      "      kl: 0.011682391166687012\n",
      "      policy_loss: -0.0006532394327223301\n",
      "      total_loss: 680.848876953125\n",
      "      vf_explained_var: 0.8623697757720947\n",
      "      vf_loss: 680.849609375\n",
      "    sample_time_ms: 79872.79\n",
      "    update_time_ms: 7.478\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.39416107464815\n",
      "  time_since_restore: 13757.918449401855\n",
      "  time_this_iter_s: 93.96626901626587\n",
      "  time_total_s: 13757.918449401855\n",
      "  timestamp: 1556800028\n",
      "  timesteps_since_restore: 3456000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3456000\n",
      "  training_iteration: 144\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 13757 s, 144 iter, 3456000 ts, 234 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-28-39\n",
      "  done: false\n",
      "  episode_len_mean: 90.11610486891385\n",
      "  episode_reward_max: 633.1648295596785\n",
      "  episode_reward_mean: 233.07847054077197\n",
      "  episode_reward_min: -358.4666108187213\n",
      "  episodes_this_iter: 267\n",
      "  episodes_total: 38752\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19908.913\n",
      "    load_time_ms: 3.802\n",
      "    num_steps_sampled: 3480000\n",
      "    num_steps_trained: 3480000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.33742019534111023\n",
      "      kl: 0.011887894943356514\n",
      "      policy_loss: -0.0006436118856072426\n",
      "      total_loss: 719.4025268554688\n",
      "      vf_explained_var: 0.8562464118003845\n",
      "      vf_loss: 719.4031372070312\n",
      "    sample_time_ms: 79877.337\n",
      "    update_time_ms: 7.491\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.26961763519301\n",
      "  time_since_restore: 13849.01100730896\n",
      "  time_this_iter_s: 91.09255790710449\n",
      "  time_total_s: 13849.01100730896\n",
      "  timestamp: 1556800119\n",
      "  timesteps_since_restore: 3480000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3480000\n",
      "  training_iteration: 145\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 13849 s, 145 iter, 3480000 ts, 233 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-31-12\n",
      "  done: false\n",
      "  episode_len_mean: 92.7984496124031\n",
      "  episode_reward_max: 634.2818948461849\n",
      "  episode_reward_mean: 265.8159210273384\n",
      "  episode_reward_min: -355.346142554929\n",
      "  episodes_this_iter: 258\n",
      "  episodes_total: 39010\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 21331.553\n",
      "    load_time_ms: 3.975\n",
      "    num_steps_sampled: 3504000\n",
      "    num_steps_trained: 3504000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3396873474121094\n",
      "      kl: 0.012171859852969646\n",
      "      policy_loss: -0.0005388144636526704\n",
      "      total_loss: 858.72900390625\n",
      "      vf_explained_var: 0.8104972243309021\n",
      "      vf_loss: 858.7294921875\n",
      "    sample_time_ms: 84899.549\n",
      "    update_time_ms: 7.435\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 66.45398025683461\n",
      "  time_since_restore: 14001.83134508133\n",
      "  time_this_iter_s: 152.82033777236938\n",
      "  time_total_s: 14001.83134508133\n",
      "  timestamp: 1556800272\n",
      "  timesteps_since_restore: 3504000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3504000\n",
      "  training_iteration: 146\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 14001 s, 146 iter, 3504000 ts, 266 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-33-07\n",
      "  done: false\n",
      "  episode_len_mean: 82.26116838487972\n",
      "  episode_reward_max: 636.9559916439704\n",
      "  episode_reward_mean: 155.66678967266623\n",
      "  episode_reward_min: -356.9118985920735\n",
      "  episodes_this_iter: 291\n",
      "  episodes_total: 39301\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 21326.741\n",
      "    load_time_ms: 3.963\n",
      "    num_steps_sampled: 3528000\n",
      "    num_steps_trained: 3528000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.313410222530365\n",
      "      kl: 0.011533083394169807\n",
      "      policy_loss: 0.0004031634598504752\n",
      "      total_loss: 834.5625610351562\n",
      "      vf_explained_var: 0.8517006635665894\n",
      "      vf_loss: 834.5621948242188\n",
      "    sample_time_ms: 87198.319\n",
      "    update_time_ms: 7.375\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 38.91669741816656\n",
      "  time_since_restore: 14117.06929731369\n",
      "  time_this_iter_s: 115.23795223236084\n",
      "  time_total_s: 14117.06929731369\n",
      "  timestamp: 1556800387\n",
      "  timesteps_since_restore: 3528000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3528000\n",
      "  training_iteration: 147\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 14117 s, 147 iter, 3528000 ts, 156 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-34-47\n",
      "  done: false\n",
      "  episode_len_mean: 90.26591760299625\n",
      "  episode_reward_max: 635.4297523792756\n",
      "  episode_reward_mean: 238.85068556017757\n",
      "  episode_reward_min: -352.1004659377192\n",
      "  episodes_this_iter: 267\n",
      "  episodes_total: 39568\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 21400.511\n",
      "    load_time_ms: 4.011\n",
      "    num_steps_sampled: 3552000\n",
      "    num_steps_trained: 3552000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.31902533769607544\n",
      "      kl: 0.011139672249555588\n",
      "      policy_loss: -0.0008893604390323162\n",
      "      total_loss: 820.2813110351562\n",
      "      vf_explained_var: 0.8334192037582397\n",
      "      vf_loss: 820.2822265625\n",
      "    sample_time_ms: 88427.676\n",
      "    update_time_ms: 7.015\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 59.71267139004439\n",
      "  time_since_restore: 14217.263828754425\n",
      "  time_this_iter_s: 100.19453144073486\n",
      "  time_total_s: 14217.263828754425\n",
      "  timestamp: 1556800487\n",
      "  timesteps_since_restore: 3552000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3552000\n",
      "  training_iteration: 148\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 14217 s, 148 iter, 3552000 ts, 239 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-36-26\n",
      "  done: false\n",
      "  episode_len_mean: 91.22053231939164\n",
      "  episode_reward_max: 633.2347522454295\n",
      "  episode_reward_mean: 256.0649512061719\n",
      "  episode_reward_min: -352.5121633312857\n",
      "  episodes_this_iter: 263\n",
      "  episodes_total: 39831\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 21610.272\n",
      "    load_time_ms: 4.007\n",
      "    num_steps_sampled: 3576000\n",
      "    num_steps_trained: 3576000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32601457834243774\n",
      "      kl: 0.012348777614533901\n",
      "      policy_loss: 0.0003143192734569311\n",
      "      total_loss: 678.4808349609375\n",
      "      vf_explained_var: 0.8648167252540588\n",
      "      vf_loss: 678.4805297851562\n",
      "    sample_time_ms: 88497.274\n",
      "    update_time_ms: 7.076\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 64.01623780154297\n",
      "  time_since_restore: 14316.351392507553\n",
      "  time_this_iter_s: 99.08756375312805\n",
      "  time_total_s: 14316.351392507553\n",
      "  timestamp: 1556800586\n",
      "  timesteps_since_restore: 3576000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3576000\n",
      "  training_iteration: 149\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 14316 s, 149 iter, 3576000 ts, 256 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-38-08\n",
      "  done: false\n",
      "  episode_len_mean: 88.45018450184502\n",
      "  episode_reward_max: 632.0175311862001\n",
      "  episode_reward_mean: 218.8778729157873\n",
      "  episode_reward_min: -355.80327030921603\n",
      "  episodes_this_iter: 271\n",
      "  episodes_total: 40102\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 21431.101\n",
      "    load_time_ms: 3.927\n",
      "    num_steps_sampled: 3600000\n",
      "    num_steps_trained: 3600000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3216663897037506\n",
      "      kl: 0.011380940675735474\n",
      "      policy_loss: -0.00018945630290545523\n",
      "      total_loss: 708.4021606445312\n",
      "      vf_explained_var: 0.8639274835586548\n",
      "      vf_loss: 708.40234375\n",
      "    sample_time_ms: 88555.459\n",
      "    update_time_ms: 7.919\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.71946822894682\n",
      "  time_since_restore: 14417.76286816597\n",
      "  time_this_iter_s: 101.41147565841675\n",
      "  time_total_s: 14417.76286816597\n",
      "  timestamp: 1556800688\n",
      "  timesteps_since_restore: 3600000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3600000\n",
      "  training_iteration: 150\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 14417 s, 150 iter, 3600000 ts, 219 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-39-43\n",
      "  done: false\n",
      "  episode_len_mean: 89.04832713754647\n",
      "  episode_reward_max: 632.4911165509125\n",
      "  episode_reward_mean: 232.92834913871337\n",
      "  episode_reward_min: -355.26110581107906\n",
      "  episodes_this_iter: 269\n",
      "  episodes_total: 40371\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20720.588\n",
      "    load_time_ms: 3.963\n",
      "    num_steps_sampled: 3624000\n",
      "    num_steps_trained: 3624000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.32532191276550293\n",
      "      kl: 0.011661392636597157\n",
      "      policy_loss: -0.00035819553886540234\n",
      "      total_loss: 776.0059814453125\n",
      "      vf_explained_var: 0.8436691761016846\n",
      "      vf_loss: 776.00634765625\n",
      "    sample_time_ms: 87044.141\n",
      "    update_time_ms: 6.797\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.232087284678336\n",
      "  time_since_restore: 14513.24731040001\n",
      "  time_this_iter_s: 95.4844422340393\n",
      "  time_total_s: 14513.24731040001\n",
      "  timestamp: 1556800783\n",
      "  timesteps_since_restore: 3624000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3624000\n",
      "  training_iteration: 151\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 14513 s, 151 iter, 3624000 ts, 233 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-41-24\n",
      "  done: false\n",
      "  episode_len_mean: 86.2831541218638\n",
      "  episode_reward_max: 632.0296311736454\n",
      "  episode_reward_mean: 199.35776362785032\n",
      "  episode_reward_min: -358.73421812367417\n",
      "  episodes_this_iter: 279\n",
      "  episodes_total: 40650\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20824.383\n",
      "    load_time_ms: 3.958\n",
      "    num_steps_sampled: 3648000\n",
      "    num_steps_trained: 3648000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3063051402568817\n",
      "      kl: 0.011471811681985855\n",
      "      policy_loss: 0.00023018138017505407\n",
      "      total_loss: 739.676513671875\n",
      "      vf_explained_var: 0.8611889481544495\n",
      "      vf_loss: 739.6763305664062\n",
      "    sample_time_ms: 83456.893\n",
      "    update_time_ms: 6.305\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.83944090696259\n",
      "  time_since_restore: 14613.800322532654\n",
      "  time_this_iter_s: 100.55301213264465\n",
      "  time_total_s: 14613.800322532654\n",
      "  timestamp: 1556800884\n",
      "  timesteps_since_restore: 3648000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3648000\n",
      "  training_iteration: 152\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 14613 s, 152 iter, 3648000 ts, 199 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-43-09\n",
      "  done: false\n",
      "  episode_len_mean: 84.80212014134275\n",
      "  episode_reward_max: 631.4105904231093\n",
      "  episode_reward_mean: 175.16984462671815\n",
      "  episode_reward_min: -359.0594728900228\n",
      "  episodes_this_iter: 283\n",
      "  episodes_total: 40933\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 21713.666\n",
      "    load_time_ms: 3.982\n",
      "    num_steps_sampled: 3672000\n",
      "    num_steps_trained: 3672000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.3029213547706604\n",
      "      kl: 0.012187756597995758\n",
      "      policy_loss: 3.879519135807641e-05\n",
      "      total_loss: 718.3834838867188\n",
      "      vf_explained_var: 0.8670821785926819\n",
      "      vf_loss: 718.3834228515625\n",
      "    sample_time_ms: 83674.963\n",
      "    update_time_ms: 6.339\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.792461156679536\n",
      "  time_since_restore: 14718.601633548737\n",
      "  time_this_iter_s: 104.80131101608276\n",
      "  time_total_s: 14718.601633548737\n",
      "  timestamp: 1556800989\n",
      "  timesteps_since_restore: 3672000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3672000\n",
      "  training_iteration: 153\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 14718 s, 153 iter, 3672000 ts, 175 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-44-55\n",
      "  done: false\n",
      "  episode_len_mean: 87.66788321167883\n",
      "  episode_reward_max: 631.2411965239359\n",
      "  episode_reward_mean: 207.45396656040236\n",
      "  episode_reward_min: -356.4847781707215\n",
      "  episodes_this_iter: 274\n",
      "  episodes_total: 41207\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 21746.648\n",
      "    load_time_ms: 3.998\n",
      "    num_steps_sampled: 3696000\n",
      "    num_steps_trained: 3696000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2958919405937195\n",
      "      kl: 0.011483888141810894\n",
      "      policy_loss: 4.702170826931251e-06\n",
      "      total_loss: 835.0214233398438\n",
      "      vf_explained_var: 0.8367815613746643\n",
      "      vf_loss: 835.021484375\n",
      "    sample_time_ms: 84845.127\n",
      "    update_time_ms: 6.477\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.8634916401006\n",
      "  time_since_restore: 14824.601212024689\n",
      "  time_this_iter_s: 105.99957847595215\n",
      "  time_total_s: 14824.601212024689\n",
      "  timestamp: 1556801095\n",
      "  timesteps_since_restore: 3696000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3696000\n",
      "  training_iteration: 154\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 14824 s, 154 iter, 3696000 ts, 207 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-46-26\n",
      "  done: false\n",
      "  episode_len_mean: 87.38095238095238\n",
      "  episode_reward_max: 633.623870751773\n",
      "  episode_reward_mean: 208.09070540989316\n",
      "  episode_reward_min: -355.28340295586906\n",
      "  episodes_this_iter: 273\n",
      "  episodes_total: 41480\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 21622.701\n",
      "    load_time_ms: 4.011\n",
      "    num_steps_sampled: 3720000\n",
      "    num_steps_trained: 3720000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.294936865568161\n",
      "      kl: 0.011500949040055275\n",
      "      policy_loss: -0.00026703087496571243\n",
      "      total_loss: 832.8549194335938\n",
      "      vf_explained_var: 0.8314360976219177\n",
      "      vf_loss: 832.8551635742188\n",
      "    sample_time_ms: 84950.858\n",
      "    update_time_ms: 6.658\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 52.02267635247329\n",
      "  time_since_restore: 14915.513282775879\n",
      "  time_this_iter_s: 90.91207075119019\n",
      "  time_total_s: 14915.513282775879\n",
      "  timestamp: 1556801186\n",
      "  timesteps_since_restore: 3720000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3720000\n",
      "  training_iteration: 155\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 14915 s, 155 iter, 3720000 ts, 208 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-47-56\n",
      "  done: false\n",
      "  episode_len_mean: 85.48226950354609\n",
      "  episode_reward_max: 631.7876239749598\n",
      "  episode_reward_mean: 175.418941799085\n",
      "  episode_reward_min: -352.3018188429944\n",
      "  episodes_this_iter: 282\n",
      "  episodes_total: 41762\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20280.831\n",
      "    load_time_ms: 3.866\n",
      "    num_steps_sampled: 3744000\n",
      "    num_steps_trained: 3744000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.287121444940567\n",
      "      kl: 0.011939827352762222\n",
      "      policy_loss: -0.00045363808749243617\n",
      "      total_loss: 860.9358520507812\n",
      "      vf_explained_var: 0.8357369899749756\n",
      "      vf_loss: 860.9361572265625\n",
      "    sample_time_ms: 79990.693\n",
      "    update_time_ms: 6.649\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 43.85473544977126\n",
      "  time_since_restore: 15005.302161693573\n",
      "  time_this_iter_s: 89.78887891769409\n",
      "  time_total_s: 15005.302161693573\n",
      "  timestamp: 1556801276\n",
      "  timesteps_since_restore: 3744000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3744000\n",
      "  training_iteration: 156\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 15005 s, 156 iter, 3744000 ts, 175 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-49-26\n",
      "  done: false\n",
      "  episode_len_mean: 88.41697416974169\n",
      "  episode_reward_max: 633.205308257427\n",
      "  episode_reward_mean: 220.1230085863571\n",
      "  episode_reward_min: -355.549262952375\n",
      "  episodes_this_iter: 271\n",
      "  episodes_total: 42033\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20080.918\n",
      "    load_time_ms: 3.833\n",
      "    num_steps_sampled: 3768000\n",
      "    num_steps_trained: 3768000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2925451993942261\n",
      "      kl: 0.013358711265027523\n",
      "      policy_loss: -0.00045508230687119067\n",
      "      total_loss: 777.39013671875\n",
      "      vf_explained_var: 0.8482298254966736\n",
      "      vf_loss: 777.3906860351562\n",
      "    sample_time_ms: 77659.517\n",
      "    update_time_ms: 6.76\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.030752146589265\n",
      "  time_since_restore: 15095.221653699875\n",
      "  time_this_iter_s: 89.91949200630188\n",
      "  time_total_s: 15095.221653699875\n",
      "  timestamp: 1556801366\n",
      "  timesteps_since_restore: 3768000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3768000\n",
      "  training_iteration: 157\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 15095 s, 157 iter, 3768000 ts, 220 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-50-56\n",
      "  done: false\n",
      "  episode_len_mean: 88.54612546125462\n",
      "  episode_reward_max: 633.2808381181915\n",
      "  episode_reward_mean: 222.22169440456787\n",
      "  episode_reward_min: -355.18542956494076\n",
      "  episodes_this_iter: 271\n",
      "  episodes_total: 42304\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20089.995\n",
      "    load_time_ms: 3.849\n",
      "    num_steps_sampled: 3792000\n",
      "    num_steps_trained: 3792000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2740902304649353\n",
      "      kl: 0.013090362772345543\n",
      "      policy_loss: -0.000201570350327529\n",
      "      total_loss: 711.5293579101562\n",
      "      vf_explained_var: 0.8605718016624451\n",
      "      vf_loss: 711.5294799804688\n",
      "    sample_time_ms: 76642.414\n",
      "    update_time_ms: 6.715\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 55.55542360114198\n",
      "  time_since_restore: 15185.33600115776\n",
      "  time_this_iter_s: 90.11434745788574\n",
      "  time_total_s: 15185.33600115776\n",
      "  timestamp: 1556801456\n",
      "  timesteps_since_restore: 3792000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3792000\n",
      "  training_iteration: 158\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 15185 s, 158 iter, 3792000 ts, 222 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-52-26\n",
      "  done: false\n",
      "  episode_len_mean: 85.02127659574468\n",
      "  episode_reward_max: 632.4470185537592\n",
      "  episode_reward_mean: 178.69753651842217\n",
      "  episode_reward_min: -357.2953846629688\n",
      "  episodes_this_iter: 282\n",
      "  episodes_total: 42586\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19930.395\n",
      "    load_time_ms: 3.831\n",
      "    num_steps_sampled: 3816000\n",
      "    num_steps_trained: 3816000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24714455008506775\n",
      "      kl: 0.011319192126393318\n",
      "      policy_loss: 0.0006259541260078549\n",
      "      total_loss: 935.9765625\n",
      "      vf_explained_var: 0.8181867003440857\n",
      "      vf_loss: 935.9760131835938\n",
      "    sample_time_ms: 75867.588\n",
      "    update_time_ms: 6.633\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.67438412960554\n",
      "  time_since_restore: 15275.076790332794\n",
      "  time_this_iter_s: 89.74078917503357\n",
      "  time_total_s: 15275.076790332794\n",
      "  timestamp: 1556801546\n",
      "  timesteps_since_restore: 3816000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3816000\n",
      "  training_iteration: 159\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 15275 s, 159 iter, 3816000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-53-54\n",
      "  done: false\n",
      "  episode_len_mean: 90.56439393939394\n",
      "  episode_reward_max: 634.9628674536231\n",
      "  episode_reward_mean: 245.9334480092195\n",
      "  episode_reward_min: -352.9140692205035\n",
      "  episodes_this_iter: 264\n",
      "  episodes_total: 42850\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19867.523\n",
      "    load_time_ms: 3.791\n",
      "    num_steps_sampled: 3840000\n",
      "    num_steps_trained: 3840000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2623051404953003\n",
      "      kl: 0.012673323042690754\n",
      "      policy_loss: -0.00040598027408123016\n",
      "      total_loss: 583.42333984375\n",
      "      vf_explained_var: 0.8831179738044739\n",
      "      vf_loss: 583.4237670898438\n",
      "    sample_time_ms: 74673.315\n",
      "    update_time_ms: 5.848\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 61.483362002304865\n",
      "  time_since_restore: 15363.904689311981\n",
      "  time_this_iter_s: 88.82789897918701\n",
      "  time_total_s: 15363.904689311981\n",
      "  timestamp: 1556801634\n",
      "  timesteps_since_restore: 3840000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3840000\n",
      "  training_iteration: 160\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 15363 s, 160 iter, 3840000 ts, 246 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-55-25\n",
      "  done: false\n",
      "  episode_len_mean: 84.01041666666667\n",
      "  episode_reward_max: 631.2936783202281\n",
      "  episode_reward_mean: 168.95488443506943\n",
      "  episode_reward_min: -355.79300864107233\n",
      "  episodes_this_iter: 288\n",
      "  episodes_total: 43138\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19807.506\n",
      "    load_time_ms: 3.757\n",
      "    num_steps_sampled: 3864000\n",
      "    num_steps_trained: 3864000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.24001562595367432\n",
      "      kl: 0.01213777158409357\n",
      "      policy_loss: 0.00042443355778232217\n",
      "      total_loss: 729.7885131835938\n",
      "      vf_explained_var: 0.8670879602432251\n",
      "      vf_loss: 729.7881469726562\n",
      "    sample_time_ms: 74190.022\n",
      "    update_time_ms: 5.723\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 42.23872110876736\n",
      "  time_since_restore: 15453.94843697548\n",
      "  time_this_iter_s: 90.04374766349792\n",
      "  time_total_s: 15453.94843697548\n",
      "  timestamp: 1556801725\n",
      "  timesteps_since_restore: 3864000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3864000\n",
      "  training_iteration: 161\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 15453 s, 161 iter, 3864000 ts, 169 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-56-54\n",
      "  done: false\n",
      "  episode_len_mean: 84.55633802816901\n",
      "  episode_reward_max: 632.7262196117803\n",
      "  episode_reward_mean: 177.52070951043405\n",
      "  episode_reward_min: -358.5027974073536\n",
      "  episodes_this_iter: 284\n",
      "  episodes_total: 43422\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19767.569\n",
      "    load_time_ms: 3.751\n",
      "    num_steps_sampled: 3888000\n",
      "    num_steps_trained: 3888000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2383195459842682\n",
      "      kl: 0.011511189863085747\n",
      "      policy_loss: 6.50550500722602e-05\n",
      "      total_loss: 699.908935546875\n",
      "      vf_explained_var: 0.8721480965614319\n",
      "      vf_loss: 699.908935546875\n",
      "    sample_time_ms: 73109.388\n",
      "    update_time_ms: 5.671\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.38017737760851\n",
      "  time_since_restore: 15543.294647455215\n",
      "  time_this_iter_s: 89.34621047973633\n",
      "  time_total_s: 15543.294647455215\n",
      "  timestamp: 1556801814\n",
      "  timesteps_since_restore: 3888000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3888000\n",
      "  training_iteration: 162\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 15543 s, 162 iter, 3888000 ts, 178 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-58-23\n",
      "  done: false\n",
      "  episode_len_mean: 87.53284671532846\n",
      "  episode_reward_max: 632.8264421437127\n",
      "  episode_reward_mean: 213.0213161765655\n",
      "  episode_reward_min: -351.16629879853133\n",
      "  episodes_this_iter: 274\n",
      "  episodes_total: 43696\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18950.38\n",
      "    load_time_ms: 3.716\n",
      "    num_steps_sampled: 3912000\n",
      "    num_steps_trained: 3912000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2389916181564331\n",
      "      kl: 0.012902786023914814\n",
      "      policy_loss: -0.0003931683022528887\n",
      "      total_loss: 638.112548828125\n",
      "      vf_explained_var: 0.8808366656303406\n",
      "      vf_loss: 638.1129760742188\n",
      "    sample_time_ms: 72326.074\n",
      "    update_time_ms: 5.542\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.25532904414138\n",
      "  time_since_restore: 15632.077752828598\n",
      "  time_this_iter_s: 88.78310537338257\n",
      "  time_total_s: 15632.077752828598\n",
      "  timestamp: 1556801903\n",
      "  timesteps_since_restore: 3912000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3912000\n",
      "  training_iteration: 163\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 15632 s, 163 iter, 3912000 ts, 213 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_14-59-52\n",
      "  done: false\n",
      "  episode_len_mean: 89.42164179104478\n",
      "  episode_reward_max: 632.7048200319441\n",
      "  episode_reward_mean: 229.01650907959646\n",
      "  episode_reward_min: -355.79532761973115\n",
      "  episodes_this_iter: 268\n",
      "  episodes_total: 43964\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18943.883\n",
      "    load_time_ms: 3.697\n",
      "    num_steps_sampled: 3936000\n",
      "    num_steps_trained: 3936000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.2278905212879181\n",
      "      kl: 0.012287070974707603\n",
      "      policy_loss: 0.0007722045411355793\n",
      "      total_loss: 633.2443237304688\n",
      "      vf_explained_var: 0.8764511942863464\n",
      "      vf_loss: 633.2435913085938\n",
      "    sample_time_ms: 70652.127\n",
      "    update_time_ms: 5.365\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 57.25412726989912\n",
      "  time_since_restore: 15721.265803098679\n",
      "  time_this_iter_s: 89.18805027008057\n",
      "  time_total_s: 15721.265803098679\n",
      "  timestamp: 1556801992\n",
      "  timesteps_since_restore: 3936000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3936000\n",
      "  training_iteration: 164\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 15721 s, 164 iter, 3936000 ts, 229 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-01-22\n",
      "  done: false\n",
      "  episode_len_mean: 86.00358422939068\n",
      "  episode_reward_max: 633.3031617660454\n",
      "  episode_reward_mean: 186.86793519539907\n",
      "  episode_reward_min: -356.14287738855904\n",
      "  episodes_this_iter: 279\n",
      "  episodes_total: 44243\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 18889.033\n",
      "    load_time_ms: 3.677\n",
      "    num_steps_sampled: 3960000\n",
      "    num_steps_trained: 3960000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.21115796267986298\n",
      "      kl: 0.012431297451257706\n",
      "      policy_loss: 0.00016732708900235593\n",
      "      total_loss: 610.74609375\n",
      "      vf_explained_var: 0.8934885859489441\n",
      "      vf_loss: 610.7459106445312\n",
      "    sample_time_ms: 70660.017\n",
      "    update_time_ms: 5.355\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 46.716983798849775\n",
      "  time_since_restore: 15811.708679199219\n",
      "  time_this_iter_s: 90.44287610054016\n",
      "  time_total_s: 15811.708679199219\n",
      "  timestamp: 1556802082\n",
      "  timesteps_since_restore: 3960000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3960000\n",
      "  training_iteration: 165\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 15811 s, 165 iter, 3960000 ts, 187 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-02-58\n",
      "  done: false\n",
      "  episode_len_mean: 86.58695652173913\n",
      "  episode_reward_max: 633.9548883233499\n",
      "  episode_reward_mean: 196.34726880281858\n",
      "  episode_reward_min: -355.9008889661384\n",
      "  episodes_this_iter: 276\n",
      "  episodes_total: 44519\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19155.261\n",
      "    load_time_ms: 3.67\n",
      "    num_steps_sampled: 3984000\n",
      "    num_steps_trained: 3984000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.20543301105499268\n",
      "      kl: 0.013411949388682842\n",
      "      policy_loss: 0.0007147556752897799\n",
      "      total_loss: 722.7205810546875\n",
      "      vf_explained_var: 0.8639755845069885\n",
      "      vf_loss: 722.7198486328125\n",
      "    sample_time_ms: 70976.607\n",
      "    update_time_ms: 5.318\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.086817200704644\n",
      "  time_since_restore: 15907.334139823914\n",
      "  time_this_iter_s: 95.62546062469482\n",
      "  time_total_s: 15907.334139823914\n",
      "  timestamp: 1556802178\n",
      "  timesteps_since_restore: 3984000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 3984000\n",
      "  training_iteration: 166\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 15907 s, 166 iter, 3984000 ts, 196 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-04-32\n",
      "  done: false\n",
      "  episode_len_mean: 87.74545454545455\n",
      "  episode_reward_max: 636.6575010914138\n",
      "  episode_reward_mean: 215.43464228329867\n",
      "  episode_reward_min: -351.27049349640635\n",
      "  episodes_this_iter: 275\n",
      "  episodes_total: 44794\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19230.397\n",
      "    load_time_ms: 3.674\n",
      "    num_steps_sampled: 4008000\n",
      "    num_steps_trained: 4008000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1955021321773529\n",
      "      kl: 0.0138765349984169\n",
      "      policy_loss: 0.00022909228573553264\n",
      "      total_loss: 766.6195068359375\n",
      "      vf_explained_var: 0.8528149127960205\n",
      "      vf_loss: 766.6193237304688\n",
      "    sample_time_ms: 71280.802\n",
      "    update_time_ms: 5.15\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.858660570824675\n",
      "  time_since_restore: 16001.045568466187\n",
      "  time_this_iter_s: 93.71142864227295\n",
      "  time_total_s: 16001.045568466187\n",
      "  timestamp: 1556802272\n",
      "  timesteps_since_restore: 4008000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4008000\n",
      "  training_iteration: 167\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 16001 s, 167 iter, 4008000 ts, 215 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-06-05\n",
      "  done: false\n",
      "  episode_len_mean: 89.15298507462687\n",
      "  episode_reward_max: 637.8743186613261\n",
      "  episode_reward_mean: 232.8670483397068\n",
      "  episode_reward_min: -357.2523419913063\n",
      "  episodes_this_iter: 268\n",
      "  episodes_total: 45062\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 19526.865\n",
      "    load_time_ms: 3.643\n",
      "    num_steps_sampled: 4032000\n",
      "    num_steps_trained: 4032000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.19357930123806\n",
      "      kl: 0.012893175706267357\n",
      "      policy_loss: 0.0005938397371210158\n",
      "      total_loss: 723.0051879882812\n",
      "      vf_explained_var: 0.8603795766830444\n",
      "      vf_loss: 723.004638671875\n",
      "    sample_time_ms: 71319.735\n",
      "    update_time_ms: 5.12\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.2167620849267\n",
      "  time_since_restore: 16094.51664018631\n",
      "  time_this_iter_s: 93.47107172012329\n",
      "  time_total_s: 16094.51664018631\n",
      "  timestamp: 1556802365\n",
      "  timesteps_since_restore: 4032000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4032000\n",
      "  training_iteration: 168\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.7/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 16094 s, 168 iter, 4032000 ts, 233 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-07-52\n",
      "  done: false\n",
      "  episode_len_mean: 86.70503597122303\n",
      "  episode_reward_max: 638.5920960526515\n",
      "  episode_reward_mean: 206.00835248727824\n",
      "  episode_reward_min: -354.16018268060805\n",
      "  episodes_this_iter: 278\n",
      "  episodes_total: 45340\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 20172.143\n",
      "    load_time_ms: 3.671\n",
      "    num_steps_sampled: 4056000\n",
      "    num_steps_trained: 4056000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.17746056616306305\n",
      "      kl: 0.013067500665783882\n",
      "      policy_loss: 0.00035381937050260603\n",
      "      total_loss: 735.4879150390625\n",
      "      vf_explained_var: 0.8597494959831238\n",
      "      vf_loss: 735.4876098632812\n",
      "    sample_time_ms: 72329.81\n",
      "    update_time_ms: 5.098\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.502088121819575\n",
      "  time_since_restore: 16200.82185459137\n",
      "  time_this_iter_s: 106.30521440505981\n",
      "  time_total_s: 16200.82185459137\n",
      "  timestamp: 1556802472\n",
      "  timesteps_since_restore: 4056000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4056000\n",
      "  training_iteration: 169\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 7.9/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 16200 s, 169 iter, 4056000 ts, 206 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-10-58\n",
      "  done: false\n",
      "  episode_len_mean: 87.56985294117646\n",
      "  episode_reward_max: 634.6274079817543\n",
      "  episode_reward_mean: 213.4787209513854\n",
      "  episode_reward_min: -355.46466313351107\n",
      "  episodes_this_iter: 272\n",
      "  episodes_total: 45612\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 22308.913\n",
      "    load_time_ms: 4.066\n",
      "    num_steps_sampled: 4080000\n",
      "    num_steps_trained: 4080000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.1667984426021576\n",
      "      kl: 0.015061984769999981\n",
      "      policy_loss: -0.00023159630654845387\n",
      "      total_loss: 685.3856811523438\n",
      "      vf_explained_var: 0.8703621625900269\n",
      "      vf_loss: 685.3859252929688\n",
      "    sample_time_ms: 79950.568\n",
      "    update_time_ms: 5.844\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.36968023784635\n",
      "  time_since_restore: 16387.31829714775\n",
      "  time_this_iter_s: 186.49644255638123\n",
      "  time_total_s: 16387.31829714775\n",
      "  timestamp: 1556802658\n",
      "  timesteps_since_restore: 4080000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4080000\n",
      "  training_iteration: 170\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 16387 s, 170 iter, 4080000 ts, 213 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-13-02\n",
      "  done: false\n",
      "  episode_len_mean: 86.52688172043011\n",
      "  episode_reward_max: 640.691631062095\n",
      "  episode_reward_mean: 199.4859566112759\n",
      "  episode_reward_min: -355.1839836238594\n",
      "  episodes_this_iter: 279\n",
      "  episodes_total: 45891\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 23198.541\n",
      "    load_time_ms: 4.099\n",
      "    num_steps_sampled: 4104000\n",
      "    num_steps_trained: 4104000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.16033411026000977\n",
      "      kl: 0.015169168822467327\n",
      "      policy_loss: 0.00044567426084540784\n",
      "      total_loss: 750.5410766601562\n",
      "      vf_explained_var: 0.862386167049408\n",
      "      vf_loss: 750.5406494140625\n",
      "    sample_time_ms: 82448.542\n",
      "    update_time_ms: 6.765\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.87148915281898\n",
      "  time_since_restore: 16511.27505683899\n",
      "  time_this_iter_s: 123.9567596912384\n",
      "  time_total_s: 16511.27505683899\n",
      "  timestamp: 1556802782\n",
      "  timesteps_since_restore: 4104000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4104000\n",
      "  training_iteration: 171\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 16511 s, 171 iter, 4104000 ts, 199 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-15-19\n",
      "  done: false\n",
      "  episode_len_mean: 87.48905109489051\n",
      "  episode_reward_max: 638.6267941625068\n",
      "  episode_reward_mean: 213.86318280359134\n",
      "  episode_reward_min: -348.9807092046366\n",
      "  episodes_this_iter: 274\n",
      "  episodes_total: 46165\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 23927.802\n",
      "    load_time_ms: 4.127\n",
      "    num_steps_sampled: 4128000\n",
      "    num_steps_trained: 4128000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.15146777033805847\n",
      "      kl: 0.015528475865721703\n",
      "      policy_loss: 0.00010260994895361364\n",
      "      total_loss: 711.4140014648438\n",
      "      vf_explained_var: 0.8630670309066772\n",
      "      vf_loss: 711.4139404296875\n",
      "    sample_time_ms: 86463.278\n",
      "    update_time_ms: 7.416\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.46579570089784\n",
      "  time_since_restore: 16648.077332019806\n",
      "  time_this_iter_s: 136.80227518081665\n",
      "  time_total_s: 16648.077332019806\n",
      "  timestamp: 1556802919\n",
      "  timesteps_since_restore: 4128000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4128000\n",
      "  training_iteration: 172\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.0/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 16648 s, 172 iter, 4128000 ts, 214 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-17-22\n",
      "  done: false\n",
      "  episode_len_mean: 87.5490909090909\n",
      "  episode_reward_max: 639.2135707574887\n",
      "  episode_reward_mean: 216.97873620616463\n",
      "  episode_reward_min: -355.478585366725\n",
      "  episodes_this_iter: 275\n",
      "  episodes_total: 46440\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 24507.93\n",
      "    load_time_ms: 4.158\n",
      "    num_steps_sampled: 4152000\n",
      "    num_steps_trained: 4152000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.13588863611221313\n",
      "      kl: 0.014161968603730202\n",
      "      policy_loss: 0.0013543664244934916\n",
      "      total_loss: 721.4623413085938\n",
      "      vf_explained_var: 0.8612445592880249\n",
      "      vf_loss: 721.4609985351562\n",
      "    sample_time_ms: 89227.182\n",
      "    update_time_ms: 7.979\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 54.24468405154117\n",
      "  time_since_restore: 16770.315845251083\n",
      "  time_this_iter_s: 122.23851323127747\n",
      "  time_total_s: 16770.315845251083\n",
      "  timestamp: 1556803042\n",
      "  timesteps_since_restore: 4152000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4152000\n",
      "  training_iteration: 173\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 16770 s, 173 iter, 4152000 ts, 217 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-19-48\n",
      "  done: false\n",
      "  episode_len_mean: 89.13382899628253\n",
      "  episode_reward_max: 637.6832845697288\n",
      "  episode_reward_mean: 233.64999414568695\n",
      "  episode_reward_min: -348.91810112807616\n",
      "  episodes_this_iter: 269\n",
      "  episodes_total: 46709\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 26732.958\n",
      "    load_time_ms: 4.524\n",
      "    num_steps_sampled: 4176000\n",
      "    num_steps_trained: 4176000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.13783687353134155\n",
      "      kl: 0.013863553293049335\n",
      "      policy_loss: 0.0005250879330560565\n",
      "      total_loss: 602.7485961914062\n",
      "      vf_explained_var: 0.8835887312889099\n",
      "      vf_loss: 602.7481079101562\n",
      "    sample_time_ms: 92709.37\n",
      "    update_time_ms: 8.334\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 58.41249853642174\n",
      "  time_since_restore: 16916.643691539764\n",
      "  time_this_iter_s: 146.32784628868103\n",
      "  time_total_s: 16916.643691539764\n",
      "  timestamp: 1556803188\n",
      "  timesteps_since_restore: 4176000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4176000\n",
      "  training_iteration: 174\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.1/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 16916 s, 174 iter, 4176000 ts, 234 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-21-54\n",
      "  done: false\n",
      "  episode_len_mean: 87.65201465201466\n",
      "  episode_reward_max: 641.5176214735628\n",
      "  episode_reward_mean: 207.02762758989587\n",
      "  episode_reward_min: -352.0679446052823\n",
      "  episodes_this_iter: 273\n",
      "  episodes_total: 46982\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 27512.665\n",
      "    load_time_ms: 4.616\n",
      "    num_steps_sampled: 4200000\n",
      "    num_steps_trained: 4200000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.12558239698410034\n",
      "      kl: 0.013296865858137608\n",
      "      policy_loss: 0.0003241954545956105\n",
      "      total_loss: 837.1259765625\n",
      "      vf_explained_var: 0.8393986225128174\n",
      "      vf_loss: 837.1256713867188\n",
      "    sample_time_ms: 95446.778\n",
      "    update_time_ms: 9.117\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.75690689747396\n",
      "  time_since_restore: 17042.294850349426\n",
      "  time_this_iter_s: 125.65115880966187\n",
      "  time_total_s: 17042.294850349426\n",
      "  timestamp: 1556803314\n",
      "  timesteps_since_restore: 4200000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4200000\n",
      "  training_iteration: 175\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 17042 s, 175 iter, 4200000 ts, 207 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-24-15\n",
      "  done: false\n",
      "  episode_len_mean: 87.78832116788321\n",
      "  episode_reward_max: 641.5453994260478\n",
      "  episode_reward_mean: 206.55034429643183\n",
      "  episode_reward_min: -353.6894277979851\n",
      "  episodes_this_iter: 274\n",
      "  episodes_total: 47256\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 28740.947\n",
      "    load_time_ms: 4.754\n",
      "    num_steps_sampled: 4224000\n",
      "    num_steps_trained: 4224000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.12763839960098267\n",
      "      kl: 0.013762842863798141\n",
      "      policy_loss: 0.0004640940169338137\n",
      "      total_loss: 612.9729614257812\n",
      "      vf_explained_var: 0.8857985138893127\n",
      "      vf_loss: 612.9724731445312\n",
      "    sample_time_ms: 98764.702\n",
      "    update_time_ms: 10.037\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 51.637586074107965\n",
      "  time_since_restore: 17183.416134119034\n",
      "  time_this_iter_s: 141.12128376960754\n",
      "  time_total_s: 17183.416134119034\n",
      "  timestamp: 1556803455\n",
      "  timesteps_since_restore: 4224000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4224000\n",
      "  training_iteration: 176\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.2/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 17183 s, 176 iter, 4224000 ts, 207 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-26-32\n",
      "  done: false\n",
      "  episode_len_mean: 87.09782608695652\n",
      "  episode_reward_max: 641.3742902069318\n",
      "  episode_reward_mean: 203.10635435923066\n",
      "  episode_reward_min: -357.2113850615657\n",
      "  episodes_this_iter: 276\n",
      "  episodes_total: 47532\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 29722.442\n",
      "    load_time_ms: 4.922\n",
      "    num_steps_sampled: 4248000\n",
      "    num_steps_trained: 4248000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.11607027053833008\n",
      "      kl: 0.01563446782529354\n",
      "      policy_loss: 0.0002931783674284816\n",
      "      total_loss: 706.9011840820312\n",
      "      vf_explained_var: 0.8677765130996704\n",
      "      vf_loss: 706.9008178710938\n",
      "    sample_time_ms: 102116.544\n",
      "    update_time_ms: 10.349\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 50.776588589807666\n",
      "  time_since_restore: 17320.514641284943\n",
      "  time_this_iter_s: 137.0985071659088\n",
      "  time_total_s: 17320.514641284943\n",
      "  timestamp: 1556803592\n",
      "  timesteps_since_restore: 4248000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4248000\n",
      "  training_iteration: 177\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 17320 s, 177 iter, 4248000 ts, 203 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-28-24\n",
      "  done: false\n",
      "  episode_len_mean: 84.50883392226149\n",
      "  episode_reward_max: 639.9681291483023\n",
      "  episode_reward_mean: 178.55507919608263\n",
      "  episode_reward_min: -355.30483808395195\n",
      "  episodes_this_iter: 283\n",
      "  episodes_total: 47815\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 29810.731\n",
      "    load_time_ms: 4.937\n",
      "    num_steps_sampled: 4272000\n",
      "    num_steps_trained: 4272000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.10417481511831284\n",
      "      kl: 0.014291999861598015\n",
      "      policy_loss: 0.00036657173768617213\n",
      "      total_loss: 766.34619140625\n",
      "      vf_explained_var: 0.8579016923904419\n",
      "      vf_loss: 766.3457641601562\n",
      "    sample_time_ms: 103908.102\n",
      "    update_time_ms: 11.596\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 44.63876979902067\n",
      "  time_since_restore: 17432.8022274971\n",
      "  time_this_iter_s: 112.2875862121582\n",
      "  time_total_s: 17432.8022274971\n",
      "  timestamp: 1556803704\n",
      "  timesteps_since_restore: 4272000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4272000\n",
      "  training_iteration: 178\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 17432 s, 178 iter, 4272000 ts, 179 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-30-05\n",
      "  done: false\n",
      "  episode_len_mean: 87.58909090909091\n",
      "  episode_reward_max: 640.8487843434294\n",
      "  episode_reward_mean: 215.40737302224025\n",
      "  episode_reward_min: -352.2467628821811\n",
      "  episodes_this_iter: 275\n",
      "  episodes_total: 48090\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 29615.831\n",
      "    load_time_ms: 4.935\n",
      "    num_steps_sampled: 4296000\n",
      "    num_steps_trained: 4296000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.10055186599493027\n",
      "      kl: 0.01585172303020954\n",
      "      policy_loss: 0.00040810887003317475\n",
      "      total_loss: 706.0114135742188\n",
      "      vf_explained_var: 0.8639559149742126\n",
      "      vf_loss: 706.010986328125\n",
      "    sample_time_ms: 103542.771\n",
      "    update_time_ms: 12.289\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 53.85184325556006\n",
      "  time_since_restore: 17533.508646011353\n",
      "  time_this_iter_s: 100.70641851425171\n",
      "  time_total_s: 17533.508646011353\n",
      "  timestamp: 1556803805\n",
      "  timesteps_since_restore: 4296000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4296000\n",
      "  training_iteration: 179\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.3/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 17533 s, 179 iter, 4296000 ts, 215 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-31-45\n",
      "  done: false\n",
      "  episode_len_mean: 85.30714285714286\n",
      "  episode_reward_max: 641.2482017966281\n",
      "  episode_reward_mean: 192.57677137498027\n",
      "  episode_reward_min: -352.0257919043613\n",
      "  episodes_this_iter: 280\n",
      "  episodes_total: 48370\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 27853.063\n",
      "    load_time_ms: 4.549\n",
      "    num_steps_sampled: 4320000\n",
      "    num_steps_trained: 4320000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.09364601969718933\n",
      "      kl: 0.014116248115897179\n",
      "      policy_loss: 0.00046611865400336683\n",
      "      total_loss: 671.5121459960938\n",
      "      vf_explained_var: 0.8794994950294495\n",
      "      vf_loss: 671.5115966796875\n",
      "    sample_time_ms: 96662.468\n",
      "    update_time_ms: 11.891\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 48.14419284374507\n",
      "  time_since_restore: 17633.493888616562\n",
      "  time_this_iter_s: 99.98524260520935\n",
      "  time_total_s: 17633.493888616562\n",
      "  timestamp: 1556803905\n",
      "  timesteps_since_restore: 4320000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4320000\n",
      "  training_iteration: 180\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.4/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 17633 s, 180 iter, 4320000 ts, 193 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-33-34\n",
      "  done: false\n",
      "  episode_len_mean: 89.46468401486989\n",
      "  episode_reward_max: 642.8047096518987\n",
      "  episode_reward_mean: 240.57656092214492\n",
      "  episode_reward_min: -349.508659526333\n",
      "  episodes_this_iter: 269\n",
      "  episodes_total: 48639\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 28013.175\n",
      "    load_time_ms: 4.546\n",
      "    num_steps_sampled: 4344000\n",
      "    num_steps_trained: 4344000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.10201722383499146\n",
      "      kl: 0.014210670255124569\n",
      "      policy_loss: 0.001091307494789362\n",
      "      total_loss: 740.3943481445312\n",
      "      vf_explained_var: 0.8542518019676208\n",
      "      vf_loss: 740.3931884765625\n",
      "    sample_time_ms: 95004.589\n",
      "    update_time_ms: 11.693\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 60.144140230536244\n",
      "  time_since_restore: 17742.45273923874\n",
      "  time_this_iter_s: 108.95885062217712\n",
      "  time_total_s: 17742.45273923874\n",
      "  timestamp: 1556804014\n",
      "  timesteps_since_restore: 4344000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4344000\n",
      "  training_iteration: 181\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 17742 s, 181 iter, 4344000 ts, 241 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-35-15\n",
      "  done: false\n",
      "  episode_len_mean: 86.38848920863309\n",
      "  episode_reward_max: 642.0571676714619\n",
      "  episode_reward_mean: 198.2403436065438\n",
      "  episode_reward_min: -349.9287014834258\n",
      "  episodes_this_iter: 278\n",
      "  episodes_total: 48917\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 27618.249\n",
      "    load_time_ms: 4.549\n",
      "    num_steps_sampled: 4368000\n",
      "    num_steps_trained: 4368000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.09265980869531631\n",
      "      kl: 0.015925023704767227\n",
      "      policy_loss: 0.0017566750757396221\n",
      "      total_loss: 880.2266845703125\n",
      "      vf_explained_var: 0.8347218632698059\n",
      "      vf_loss: 880.224853515625\n",
      "    sample_time_ms: 91830.417\n",
      "    update_time_ms: 11.315\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 49.56008590163596\n",
      "  time_since_restore: 17843.551708221436\n",
      "  time_this_iter_s: 101.09896898269653\n",
      "  time_total_s: 17843.551708221436\n",
      "  timestamp: 1556804115\n",
      "  timesteps_since_restore: 4368000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4368000\n",
      "  training_iteration: 182\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.5/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 17843 s, 182 iter, 4368000 ts, 198 rew\n",
      "\n",
      "Result for PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-05-02_15-36-57\n",
      "  done: false\n",
      "  episode_len_mean: 84.64664310954063\n",
      "  episode_reward_max: 647.5988498528386\n",
      "  episode_reward_mean: 185.18665171572746\n",
      "  episode_reward_min: -355.4335252467994\n",
      "  episodes_this_iter: 283\n",
      "  episodes_total: 49200\n",
      "  experiment_id: 617513c78ba74eb29654585d798eef12\n",
      "  hostname: Gandalf\n",
      "  info:\n",
      "    grad_time_ms: 27396.116\n",
      "    load_time_ms: 4.494\n",
      "    num_steps_sampled: 4392000\n",
      "    num_steps_trained: 4392000\n",
      "    rl_0:\n",
      "      cur_kl_coeff: 6.018531614308722e-37\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.08052577823400497\n",
      "      kl: 0.014976195991039276\n",
      "      policy_loss: 0.0011439875233918428\n",
      "      total_loss: 776.8136596679688\n",
      "      vf_explained_var: 0.860938310623169\n",
      "      vf_loss: 776.8125\n",
      "    sample_time_ms: 89945.075\n",
      "    update_time_ms: 11.599\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 172.16.123.117\n",
      "  num_metric_batches_dropped: 0\n",
      "  pid: 29660\n",
      "  policy_reward_mean:\n",
      "    rl_0: 46.29666292893187\n",
      "  time_since_restore: 17944.738639593124\n",
      "  time_this_iter_s: 101.18693137168884\n",
      "  time_total_s: 17944.738639593124\n",
      "  timestamp: 1556804217\n",
      "  timesteps_since_restore: 4392000\n",
      "  timesteps_this_iter: 24000\n",
      "  timesteps_total: 4392000\n",
      "  training_iteration: 183\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/3 CPUs, 0/1 GPUs\n",
      "Memory usage on this node: 8.6/16.4 GB\n",
      "Result logdir: /home/thorsten/ray_results/IntersectionExample\n",
      "RUNNING trials:\n",
      " - PPO_MultiAgentIntersectionEnv_sharedPolicy_4veh-v0_0:\tRUNNING [pid=29660], 17944 s, 183 iter, 4392000 ts, 185 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,  # RL algorithm to run\n",
    "        \"env\": gym_name,  # environment name generated earlier\n",
    "        \"config\": {  # configuration params (must match \"run\" value)\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 1000,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow_2)",
   "language": "python",
   "name": "flow_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
